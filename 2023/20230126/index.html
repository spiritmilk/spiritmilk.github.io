

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#AE945F">
  <meta name="author" content="LJX">
  <meta name="keywords" content="">
  
    <meta name="description" content="非官方支持的Google产品 由Google Research和Harvard University研究人员联合出品。 翻译By我。 先在github翻译完，在上传到博客。">
<meta property="og:type" content="article">
<meta property="og:title" content="【待更新】《深度学习调优手册》- 系统最大化深度学习模型性能">
<meta property="og:url" content="https://lijianxiong.space/2023/20230126/index.html">
<meta property="og:site_name" content="小熊的小站">
<meta property="og:description" content="非官方支持的Google产品 由Google Research和Harvard University研究人员联合出品。 翻译By我。 先在github翻译完，在上传到博客。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/bad_search_space.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/good_search_space.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/isolation_plot.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/have_we_sampled_enough.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/stride_instability.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/more_frequent_evals.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/instability_during_warmup.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/axis_model_with_instability.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/loss_model_with_instability.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/beneficial_effect_warmup.png">
<meta property="og:image" content="https://lijianxiong.space/2023/20230126/assets/gradient_clipping.png">
<meta property="article:published_time" content="2023-01-26T02:01:31.000Z">
<meta property="article:modified_time" content="2023-01-26T03:58:20.079Z">
<meta property="article:author" content="LJX">
<meta property="article:tag" content="人工智能">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://lijianxiong.space/2023/20230126/assets/bad_search_space.png">
  
  
  
  <title>【待更新】《深度学习调优手册》- 系统最大化深度学习模型性能 - 小熊的小站</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lijianxiong.space","root":"/","version":"1.9.8","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":false,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":false,"woyaola":21973729,"woyaola_pro_id":"3MvycZ6wPTE8DE3p","baidu":null,"google":{"measurement_id":"G-C811PDWV2Z"},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- 51.la Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('//js.users.51.la/21973729.js');
      }
    </script>
  
  
  
  <!-- 51.la Analytics v6 -->
  <script async>
    if (!Fluid.ctx.dnt) {
      // 1. 创建一个新的 script 元素用于加载 51.la 的 SDK
      var script = document.createElement('script');
      script.id = 'LA_COLLECT';
      script.src = '//sdk.51.la/js-sdk-pro.min.js';
      script.charset = 'UTF-8';

      // 2. 关键：当外部脚本加载并执行完毕后，再执行初始化函数
      script.onload = function() {
        LA.init({
          id: "3MvycZ6wPTE8DE3p",
          ck: "3MvycZ6wPTE8DE3p"
        });
      };

      // 3. 将创建的 script 元素插入到页面的 <head> 或 <body> 中，使其开始加载
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(script, s);
    }
  </script>


  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-C811PDWV2Z", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', 'G-C811PDWV2Z');
        });
      }
    </script>
  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Bear</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-books"></i>
                <span>目录</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/latexocr1/" target="_self">
                <i class="iconfont icon-exp-fill"></i>
                <span>latex识别</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/arxiv/" target="_self">
                <i class="iconfont icon-notebook"></i>
                <span>每日arxiv</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/huogui.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">【待更新】《深度学习调优手册》- 系统最大化深度学习模型性能</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-01-26 10:01" pubdate>
          2023年1月26日 上午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【待更新】《深度学习调优手册》- 系统最大化深度学习模型性能</h1>
            
            
              <div class="markdown-body">
                
                <link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><p><em>非官方支持的Google产品</em></p>
<p>由Google Research和Harvard University研究人员联合出品。</p>
<p>翻译By我。</p>
<p>先在github翻译完，在上传到博客。</p>
<span id="more"></span>

<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><a href="#%E8%BF%99%E4%B8%AA%E6%96%87%E6%A1%A3%E6%98%AF%E4%B8%BA%E8%B0%81%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F">这个文档是为谁设计的？</a></li>
<li><a href="#why-a-tuning-playbook">Why a tuning playbook?</a></li>
<li><a href="#%E5%BC%80%E5%A7%8B%E6%96%B0%E9%A1%B9%E7%9B%AE%E5%89%8D%E7%9A%84%E6%8C%87%E5%8D%97">开始新项目前的指南</a><ul>
<li><a href="#%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84">选择模型架构</a></li>
<li><a href="#%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8">选择优化器</a></li>
<li><a href="#%E9%80%89%E6%8B%A9batchsize">选择batchsize</a></li>
<li><a href="#%E9%80%89%E6%8B%A9%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE">选择初始配置</a></li>
</ul>
</li>
<li><a href="#a-scientific-approach-to-improving-model-performance">A scientific approach to improving model performance</a><ul>
<li><a href="#the-incremental-tuning-strategy">The incremental tuning strategy</a></li>
<li><a href="#exploration-vs-exploitation">Exploration vs exploitation</a></li>
<li><a href="#choosing-the-goal-for-the-next-round-of-experiments">Choosing the goal for the next round of experiments</a></li>
<li><a href="#Designing-the-next-round-of-experiments">Designing the next round of experiments</a></li>
<li><a href="#Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration">Determining whether to adopt a training pipeline change or<br>hyperparameter<br>configuration</a></li>
<li><a href="#After-exploration-concludes">After exploration concludes</a></li>
</ul>
</li>
<li><a href="#Determining-the-number-of-steps-for-each-training-run">Determining the number of steps for each training run</a><ul>
<li><a href="#Deciding-how-long-to-train-when-training-is-not-compute-bound">Deciding how long to train when training is not compute-bound</a></li>
<li><a href="#Deciding-how-long-to-train-when-training-is-compute-bound">Deciding how long to train when training is compute-bound</a></li>
</ul>
</li>
<li><a href="#Additional-guidance-for-the-training-pipeline">Additional guidance for the training pipeline</a><ul>
<li><a href="#Optimizing-the-input-pipeline">Optimizing the input pipeline</a></li>
<li><a href="Evaluating-model-performance">Evaluating model performance</a></li>
<li><a href="#Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint">Saving checkpoints and retrospectively selecting the best checkpoint</a></li>
<li><a href="#Setting-up-experiment-tracking">Setting up experiment tracking</a></li>
<li><a href="#Batch-normalization-implementation-details">Batch normalization implementation details</a></li>
<li><a href="#Considerations-for-multi-host-pipelines">Considerations for multi-host pipelines</a></li>
</ul>
</li>
<li><a href="#faqs">FAQs</a></li>
<li><a href="#acknowledgments">Acknowledgments</a></li>
<li><a href="#citing">Citing</a></li>
<li><a href="#contributing">Contributing</a></li>
</ul>
<h2 id="这个文档是为谁设计的？"><a href="#这个文档是为谁设计的？" class="headerlink" title="这个文档是为谁设计的？"></a>这个文档是为谁设计的？</h2><p>本文档面向对深度学习<strong>模型性能最优化</strong>感兴趣的工程师和研究人员（个人和团队）。我们假设你已经掌握了机器学习和深度学习概念的基本知识。</p>
<p>我们的重点是超参数调参过程。我们触及了深度学习培训的其他方面，例如pipeline应用和优化，但是我们对这些方面的处理并不完整。</p>
<p>我们假设机器学习问题是一个有监督的学习问题或类似的问题（例如自监督）。尽管如此，本文档中的一些规定也可能适用于其他类型的问题。</p>
<h2 id="Why-a-tuning-playbook"><a href="#Why-a-tuning-playbook" class="headerlink" title="Why a tuning playbook?"></a>Why a tuning playbook?</h2><p>Currently, there is an astonishing amount of toil and guesswork involved in<br>actually getting deep neural networks to work well in practice. Even worse, the<br>actual recipes people use to get good results with deep learning are rarely<br>documented. Papers gloss over the process that led to their final results in<br>order to present a cleaner story, and machine learning engineers working on<br>commercial problems rarely have time to take a step back and generalize their<br>process. Textbooks tend to eschew practical guidance and prioritize fundamental<br>principles, even if their authors have the necessary experience in applied work<br>to provide useful advice. When preparing to create this document, we couldn’t<br>find any comprehensive attempt to actually explain <em>how to get good results with<br>deep learning</em>. Instead, we found snippets of advice in blog posts and on social<br>media, tricks peeking out of the appendix of research papers, occasional case<br>studies about one particular project or pipeline, and a lot of confusion. There<br>is a vast gulf between the results achieved by deep learning experts and less<br>skilled practitioners using superficially similar methods. At the same time,<br>these very experts readily admit some of what they do might not be<br>well-justified. As deep learning matures and has a larger impact on the world,<br>the community needs more resources covering useful recipes, including all the<br>practical details that can be so critical for obtaining good results.</p>
<p>We are a team of five researchers and engineers who have worked in deep learning<br>for many years, some of us since as early as 2006. We have applied deep learning<br>to problems in everything from speech recognition to astronomy, and learned a<br>lot along the way. This document grew out of our own experience training neural<br>networks, teaching new machine learning engineers, and advising our colleagues<br>on the practice of deep learning. Although it has been gratifying to see deep<br>learning go from a machine learning approach practiced by a handful of academic<br>labs to a technology powering products used by billions of people, deep learning<br>is still in its infancy as an engineering discipline and we hope this document<br>encourages others to help systematize the field’s experimental protocols.</p>
<p>This document came about as we tried to crystalize our own approach to deep<br>learning and thus it represents the opinions of the authors at the time of<br>writing, not any sort of objective truth. Our own struggles with hyperparameter<br>tuning made it a particular focus of our guidance, but we also cover other<br>important issues we have encountered in our work (or seen go wrong). Our<br>intention is for this work to be a living document that grows and evolves as our<br>beliefs change. For example, the material on debugging and mitigating training<br>failures would not have been possible for us to write two years ago since it is<br>based on recent results and ongoing investigations. Inevitably, some of our<br>advice will need to be updated to account for new results and improved<br>workflows. We do not know the <em>optimal</em> deep learning recipe, but until the<br>community starts writing down and debating different procedures, we cannot hope<br>to find it. To that end, we would encourage readers who find issues with our<br>advice to produce alternative recommendations, along with convincing evidence,<br>so we can update the playbook. We would also love to see alternative guides and<br>playbooks that might have different recommendations so we can work towards best<br>practices as a community. Finally, any sections marked with a 🤖 emoji are places<br>we would like to do more research. Only after trying to write this playbook did<br>it become completely clear how many interesting and neglected research questions<br>can be found in the deep learning practitioner’s workflow.</p>
<h2 id="开始新项目前的指南"><a href="#开始新项目前的指南" class="headerlink" title="开始新项目前的指南"></a>开始新项目前的指南</h2><p>我们在调参过程中做出的许多决定可以在项目开始时做，只有在情况发生变化时偶尔会重新审视。</p>
<p>我们的指南提出了以下假设:</p>
<ul>
<li>已经完成了足够的问题制定（problem formulation），数据清洁等的基本工作，以使在模型架构和训练配置上花费时间是有意义的。</li>
<li>已经有了一个pipeline,可以进行训练和评估。并容易为各种感兴趣的模型进行训练和预测。</li>
<li>选定并应用了合适的metrics。这些应该尽可能地代表将在部署环境中测量的内容。</li>
</ul>
<h3 id="选择模型架构"><a href="#选择模型架构" class="headerlink" title="选择模型架构"></a>选择模型架构</h3><p><em><strong>摘要:</strong></em> <em>当开始一个新项目时，请尝试使用已经work的模型。</em></p>
<ul>
<li><p>选择一个成熟的、广泛使用的模型架构。 这样可以更好的在后续构建自定义模型。</p>
</li>
<li><p>模型架构常有很多超参数来确定模型的大小和其他细节。比如layers的数量和宽度，比如激活函数的类型。</p>
<ul>
<li>因此，选择模型架构实际上代表着选择一类模型，模型大家族内不同的模型代表着不同的一组超参数。</li>
<li>我们将在<a href="#%E9%80%89%E6%8B%A9%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE">选择初始配置</a>和<a href="#%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E7%9A%84%E7%A7%91%E5%AD%A6%E6%96%B9%E6%B3%95">提升模型性能的科学方法</a>两章考虑模型超参数选择的问题。</li>
</ul>
</li>
<li><p>如果可能，尝试找到一篇 解决了与手头问题尽可能相近的问题 的论文，并把复现论文中的模型作为起点。</p>
</li>
</ul>
<h3 id="选择优化器"><a href="#选择优化器" class="headerlink" title="选择优化器"></a>选择优化器</h3><p><em><strong>摘要:</strong></em> <em>从最受欢迎的优化器开始，用于解决手头问题。</em></p>
<ul>
<li>在所有类型的机器学习问题和模型架构中，没有优化器是“最佳”的。 甚至比较优化器性能是困难的。<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.05446">参考论文：《comparing the performance of optimizers is a difficult task》</a>.<br>🤖</li>
<li>我们推荐使用成熟的、广泛使用的优化器，尤其是在开始一个新项目的时候。<ul>
<li>理想情况下，选择用于相同类型问题下的最广泛使用的优化器。</li>
</ul>
</li>
<li>注意关注所选择优化器的<em><strong>所有</strong></em>超参数。<ul>
<li>具有更多超参数的优化器可能需要更多的调参经历来找到最佳配置。</li>
<li>这点很重要，尤其是在项目的开始阶段我们可能，而忽略了优化器的超参数，把它看成了无用且讨厌的参数（<a href="#identifying-scientific-nuisance-and-fixed-hyperparameters">nuisance parameters</a>）。</li>
<li>在项目的初始夹断，最好是从一个简单的优化器开始，比如使用固定动量的SGD或固定 $\epsilon$、 $\beta_{1}$、$\beta_{2}$的Adam。然后再切换到更通用的优化器。</li>
</ul>
</li>
<li>我们喜欢的成熟的优化器包括（但不限于）以下：<ul>
<li><a href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms">带动量的SGD</a><br>(我们喜欢 Nesterov变种的。)</li>
<li><a href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms">Adam 和NAdam</a>,<br>它们比带动量的SGD更通用。请注意，Adam有四个可调的超参数，而且<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.05446">它们都很重要</a>！<ul>
<li>可看<br><a href="#Adam%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E8%B0%83%E6%95%B4%EF%BC%9F">Adam的超参数应该如何调整？</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="选择batchsize"><a href="#选择batchsize" class="headerlink" title="选择batchsize"></a>选择batchsize</h3><p><em><strong>Summary:</strong></em> <em>The batch size governs the training speed and shouldn’t be used<br>to directly tune the validation set performance. Often, the ideal batch size<br>will be the largest batch size supported by the available hardware.</em></p>
<ul>
<li>The batch size is a key factor in determining the <em>training time</em> and<br><em>computing resource consumption</em>.</li>
<li>Increasing the batch size will often reduce the training time. This can be<br>highly beneficial because it, e.g.:<ul>
<li>Allows hyperparameters to be tuned more thoroughly within a fixed time<br>interval, potentially resulting in a better final model.</li>
<li>Reduces the latency of the development cycle, allowing new ideas to be<br>tested more frequently.</li>
</ul>
</li>
<li>Increasing the batch size may either decrease, increase, or not change the<br>resource consumption.</li>
<li>The batch size should <em>not be</em> treated as a tunable hyperparameter for<br>validation set performance.<ul>
<li>As long as all hyperparameters are well-tuned (especially the learning<br>rate and regularization hyperparameters) and the number of training<br>steps is sufficient, the same final performance should be attainable<br>using any batch size (see<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).</li>
<li>Please see <a href="#why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance">Why shouldn’t the batch size be tuned to directly improve<br>validation set<br>performance?</a></li>
</ul>
</li>
</ul>
<h4 id="Determining-the-feasible-batch-sizes-and-estimating-training-throughput"><a href="#Determining-the-feasible-batch-sizes-and-estimating-training-throughput" class="headerlink" title="Determining the feasible batch sizes and estimating training throughput"></a>Determining the feasible batch sizes and estimating training throughput</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>For a given model and optimizer, there will typically be a range of batch<br>sizes supported by the available hardware. The limiting factor is usually<br>accelerator memory.</li>
<li>Unfortunately, it can be difficult to calculate which batch sizes will fit<br>in memory without running, or at least compiling, the full training program.</li>
<li>The easiest solution is usually to run training jobs at different batch<br>sizes (e.g. increasing powers of 2) for a small number of steps until one of<br>the jobs exceeds the available memory.</li>
<li>For each batch size, we should train for long enough to get a reliable<br>estimate of the <em>training throughput</em></li>
</ul>
<p align="center">training throughput = (# examples processed per second)</p>

<p align="center">or, equivalently, the <em>time per step</em>.</p>

<p align="center">time per step = (batch size) / (training throughput)</p>

<ul>
<li>When the accelerators aren’t yet saturated, if the batch size doubles, the<br>training throughput should also double (or at least nearly double).<br>Equivalently, the time per step should be constant (or at least nearly<br>constant) as the batch size increases.</li>
<li>If this is not the case then the training pipeline has a bottleneck such as<br>I&#x2F;O or synchronization between compute nodes. This may be worth diagnosing<br>and correcting before proceeding.</li>
<li>If the training throughput increases only up to some maximum batch size,<br>then we should only consider batch sizes up to that maximum batch size, even<br>if a larger batch size is supported by the hardware.<ul>
<li>All benefits of using a larger batch size assume the training throughput<br>increases. If it doesn’t, fix the bottleneck or use the smaller batch<br>size.</li>
<li><strong>Gradient accumulation</strong> simulates a larger batch size than the<br>hardware can support and therefore does not provide any throughput<br>benefits. It should generally be avoided in applied work.</li>
</ul>
</li>
<li>These steps may need to be repeated every time the model or optimizer is<br>changed (e.g. a different model architecture may allow a larger batch size<br>to fit in memory).</li>
</ul>
</details>

<h4 id="Choosing-the-batch-size-to-minimize-training-time"><a href="#Choosing-the-batch-size-to-minimize-training-time" class="headerlink" title="Choosing the batch size to minimize training time"></a>Choosing the batch size to minimize training time</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<p align="center">Training time = (time per step) x (total number of steps)</p>

<ul>
<li>We can often consider the time per step to be approximately constant for all<br>feasible batch sizes. This is true when there is no overhead from parallel<br>computations and all training bottlenecks have been diagnosed and corrected<br>(see the<br><a href="#determining-the-feasible-batch-sizes-and-estimating-training-throughput">previous section</a><br>for how to identify training bottlenecks). In practice, there is usually at<br>least some overhead from increasing the batch size.</li>
<li>As the batch size increases, the total number of steps needed to reach a<br>fixed performance goal typically decreases (provided all relevant<br>hyperparameters are re-tuned when the batch size is changed;<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).<ul>
<li>E.g. Doubling the batch size might halve the total number of steps<br>required. This is called <strong>perfect scaling</strong>.</li>
<li>Perfect scaling holds for all batch sizes up to a critical batch size,<br>beyond which one achieves diminishing returns.</li>
<li>Eventually, increasing the batch size no longer reduces the number of<br>training steps (but never increases it).</li>
</ul>
</li>
<li>Therefore, the batch size that minimizes training time is usually the<br>largest batch size that still provides a reduction in the number of training<br>steps required.<ul>
<li>This batch size depends on the dataset, model, and optimizer, and it is<br>an open problem how to calculate it other than finding it experimentally<br>for every new problem. 🤖</li>
<li>When comparing batch sizes, beware the distinction between an example<br>budget&#x2F;<a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/glossary#epoch">epoch</a><br>budget (running all experiments while fixing the number of training<br>example presentations) and a step budget (running all experiments with<br>the number of training steps fixed).<ul>
<li>Comparing batch sizes with an epoch budget only probes the perfect<br>scaling regime, even when larger batch sizes might still provide a<br>meaningful speedup by reducing the number of training steps<br>required.</li>
</ul>
</li>
<li>Often, the largest batch size supported by the available hardware will<br>be smaller than the critical batch size. Therefore, a good rule of thumb<br>(without running any experiments) is to use the largest batch size<br>possible.</li>
</ul>
</li>
<li>There is no point in using a larger batch size if it ends up increasing the<br>training time.</li>
</ul>
</details>

<h4 id="Choosing-the-batch-size-to-minimize-resource-consumption"><a href="#Choosing-the-batch-size-to-minimize-resource-consumption" class="headerlink" title="Choosing the batch size to minimize resource consumption"></a>Choosing the batch size to minimize resource consumption</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>There are two types of resource costs associated with increasing the batch<br>size:<ol>
<li><em>Upfront costs</em>, e.g. purchasing new hardware or rewriting the training<br>pipeline to implement multi-GPU &#x2F; multi-TPU training.</li>
<li><em>Usage costs</em>, e.g. billing against the team’s resource budgets, billing<br>from a cloud provider, electricity &#x2F; maintenance costs.</li>
</ol>
</li>
<li>If there are significant upfront costs to increasing the batch size, it<br>might be better to defer increasing the batch size until the project has<br>matured and it is easier to assess the cost-benefit tradeoff. Implementing<br>multi-host parallel training programs can introduce<br><a href="#considerations-for-multi-host-pipelines">bugs</a> and<br><a href="#batch-normalization-implementation-details">subtle issues</a> so it is<br>probably better to start off with a simpler pipeline anyway. (On the other<br>hand, a large speedup in training time might be very beneficial early in the<br>process when a lot of tuning experiments are needed).</li>
<li>We refer to the total usage cost (which may include multiple different kinds<br>of costs) as the “resource consumption”. We can break down the resource<br>consumption into the following components:</li>
</ul>
<p align="center">Resource consumption = (resource consumption per step) x (total number of steps)</p>

<ul>
<li>Increasing the batch size usually allows us to<br><a href="#choosing-the-batch-size-to-minimize-training-time">reduce the total number of steps</a>.<br>Whether the resource consumption increases or decreases will depend on how<br>the consumption per step changes.<ul>
<li>Increasing the batch size might <em>decrease</em> the resource consumption. For<br>example, if each step with the larger batch size can be run on the same<br>hardware as the smaller batch size (with only a small increase in time<br>per step), then any increase in the resource consumption per step might<br>be outweighed by the decrease in the number of steps.</li>
<li>Increasing the batch size might <em>not change</em> the resource consumption.<br>For example, if doubling the batch size halves the number of steps<br>required and doubles the number of GPUs used, the total consumption (in<br>terms of GPU-hours) will not change.</li>
<li>Increasing the batch size might <em>increase</em> the resource consumption. For<br>example, if increasing the batch size requires upgraded hardware, the<br>increase in consumption per step might outweigh the reduction in the<br>number of steps.</li>
</ul>
</li>
</ul>
</details>

<h4 id="Changing-the-batch-size-requires-re-tuning-most-hyperparameters"><a href="#Changing-the-batch-size-requires-re-tuning-most-hyperparameters" class="headerlink" title="Changing the batch size requires re-tuning most hyperparameters"></a>Changing the batch size requires re-tuning most hyperparameters</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>The optimal values of most hyperparameters are sensitive to the batch size.<br>Therefore, changing the batch size typically requires starting the tuning<br>process all over again.</li>
<li>The hyperparameters that interact most strongly with the batch size, and therefore are most important to tune separately for each batch size, are the optimizer hyperparameters (e.g. learning rate, momentum) and the regularization hyperparameters.</li>
<li>Keep this in mind when choosing the batch size at the start of a project. If<br>you need to switch to a different batch size later on, it might be<br>difficult, time consuming, and expensive to re-tune everything for the new<br>batch size.</li>
</ul>
</details>

<h4 id="How-batch-norm-interacts-with-the-batch-size"><a href="#How-batch-norm-interacts-with-the-batch-size" class="headerlink" title="How batch norm interacts with the batch size"></a>How batch norm interacts with the batch size</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>Batch norm is complicated and, in general, should use a different batch size<br>than the gradient computation to compute statistics. See the<br><a href="#batch-normalization-implementation-details">batch norm section</a> for a<br>detailed discussion.</li>
</ul>
</details>

<h3 id="Choosing-the-initial-configuration"><a href="#Choosing-the-initial-configuration" class="headerlink" title="Choosing the initial configuration"></a>Choosing the initial configuration</h3><ul>
<li>Before beginning hyperparameter tuning we must determine the starting point.<br>This includes specifying (1) the model configuration (e.g. number of<br>layers), (2) the optimizer hyperparameters (e.g. learning rate), and (3) the<br>number of training steps.</li>
<li>Determining this initial configuration will require some manually configured<br>training runs and trial-and-error.</li>
<li>Our guiding principle is to find a simple, relatively fast, relatively<br>low-resource-consumption configuration that obtains a “reasonable” result.<ul>
<li>“Simple” means avoiding bells and whistles wherever possible; these can<br>always be added later. Even if bells and whistles prove helpful down the<br>road, adding them in the initial configuration risks wasting time tuning<br>unhelpful features and&#x2F;or baking in unnecessary complications.<ul>
<li>For example, start with a constant learning rate before adding fancy<br>decay schedules.</li>
</ul>
</li>
<li>Choosing an initial configuration that is fast and consumes minimal<br>resources will make hyperparameter tuning much more efficient.<ul>
<li>For example, start with a smaller model.</li>
</ul>
</li>
<li>“Reasonable” performance depends on the problem, but at minimum means<br>that the trained model performs much better than random chance on the<br>validation set (although it might be bad enough to not be worth<br>deploying).</li>
</ul>
</li>
<li>Choosing the number of training steps involves balancing the following<br>tension:<ul>
<li>On the one hand, training for more steps can improve performance and<br>makes hyperparameter tuning easier (see<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).</li>
<li>On the other hand, training for fewer steps means that each training run<br>is faster and uses fewer resources, boosting tuning efficiency by<br>reducing the time between cycles and allowing more experiments to be run<br>in parallel. Moreover, if an unnecessarily large step budget is chosen<br>initially, it might be hard to change it down the road, e.g. once the<br>learning rate schedule is tuned for that number of steps.</li>
</ul>
</li>
</ul>
<h2 id="A-scientific-approach-to-improving-model-performance"><a href="#A-scientific-approach-to-improving-model-performance" class="headerlink" title="A scientific approach to improving model performance"></a>A scientific approach to improving model performance</h2><p>For the purposes of this document, the ultimate goal of machine learning<br>development is to maximize the utility of the deployed model. Even though many<br>aspects of the development process differ between applications (e.g. length of<br>time, available computing resources, type of model), we can typically use the<br>same basic steps and principles on any problem.</p>
<p>Our guidance below makes the following assumptions:</p>
<ul>
<li>There is already a fully-running training pipeline along with a<br>configuration that obtains a reasonable result.</li>
<li>There are enough computational resources available to conduct meaningful<br>tuning experiments and run at least several training jobs in parallel.</li>
</ul>
<h3 id="The-incremental-tuning-strategy"><a href="#The-incremental-tuning-strategy" class="headerlink" title="The incremental tuning strategy"></a>The incremental tuning strategy</h3><p><em><strong>Summary:</strong></em> <em>Start with a simple configuration and incrementally make<br>improvements while building up insight into the problem. Make sure that any<br>improvement is based on strong evidence to avoid adding unnecessary complexity.</em></p>
<ul>
<li>Our ultimate goal is to find a configuration that maximizes the performance<br>of our model.<ul>
<li>In some cases, our goal will be to maximize how much we can improve the<br>model by a fixed deadline (e.g. submitting to a competition).</li>
<li>In other cases, we want to keep improving the model indefinitely (e.g.<br>continually improving a model used in production).</li>
</ul>
</li>
<li>In principle, we could maximize performance by using an algorithm to<br>automatically search the entire space of possible configurations, but this<br>is not a practical option.<ul>
<li>The space of possible configurations is extremely large and there are<br>not yet any algorithms sophisticated enough to efficiently search this<br>space without human guidance.</li>
</ul>
</li>
<li>Most automated search algorithms rely on a hand-designed <em>search space</em> that<br>defines the set of configurations to search in, and these search spaces can<br>matter quite a bit.</li>
<li>The most effective way to maximize performance is to start with a simple<br>configuration and incrementally add features and make improvements while<br>building up insight into the problem.<ul>
<li>We use automated search algorithms in each round of tuning and<br>continually update our search spaces as our understanding grows.</li>
</ul>
</li>
<li>As we explore, we will naturally find better and better configurations and<br>therefore our “best” model will continually improve.<ul>
<li>We call it a <em>launch</em> when we update our best configuration (which may<br>or may not correspond to an actual launch of a production model).</li>
<li>For each launch, we must make sure that the change is based on strong<br>evidence – not just random chance based on a lucky configuration – so<br>that we don’t add unnecessary complexity to the training pipeline.</li>
</ul>
</li>
</ul>
<p>At a high level, our incremental tuning strategy involves repeating the<br>following four steps:</p>
<ol>
<li>Identify an appropriately-scoped goal for the next round of experiments.</li>
<li>Design and run a set of experiments that makes progress towards this goal.</li>
<li>Learn what we can from the results.</li>
<li>Consider whether to launch the new best configuration.</li>
</ol>
<p>The remainder of this section will consider this strategy in much greater<br>detail.</p>
<h3 id="Exploration-vs-exploitation"><a href="#Exploration-vs-exploitation" class="headerlink" title="Exploration vs exploitation"></a>Exploration vs exploitation</h3><p><em><strong>Summary:</strong></em> <em>Most of the time, our primary goal is to gain insight into the<br>problem.</em></p>
<ul>
<li>Although one might think we would spend most of our time trying to maximize<br>performance on the validation set, in practice we spend the majority of our<br>time trying to gain insight into the problem, and comparatively little time<br>greedily focused on the validation error.<ul>
<li>In other words, we spend most of our time on “exploration” and only a<br>small amount on “exploitation”.</li>
</ul>
</li>
<li>In the long run, understanding the problem is critical if we want to<br>maximize our final performance. Prioritizing insight over short term gains<br>can help us:<ul>
<li>Avoid launching unnecessary changes that happened to be present in<br>well-performing runs merely through historical accident.</li>
<li>Identify which hyperparameters the validation error is most sensitive<br>to, which hyperparameters interact the most and therefore need to be<br>re-tuned together, and which hyperparameters are relatively insensitive<br>to other changes and can therefore be fixed in future experiments.</li>
<li>Suggest potential new features to try, such as new regularizers if<br>overfitting is an issue.</li>
<li>Identify features that don’t help and therefore can be removed, reducing<br>the complexity of future experiments.</li>
<li>Recognize when improvements from hyperparameter tuning have likely<br>saturated.</li>
<li>Narrow our search spaces around the optimal value to improve tuning<br>efficiency.</li>
</ul>
</li>
<li>When we are eventually ready to be greedy, we can focus purely on the<br>validation error even if the experiments aren’t maximally informative about<br>the structure of the tuning problem.</li>
</ul>
<h3 id="Choosing-the-goal-for-the-next-round-of-experiments"><a href="#Choosing-the-goal-for-the-next-round-of-experiments" class="headerlink" title="Choosing the goal for the next round of experiments"></a>Choosing the goal for the next round of experiments</h3><p><em><strong>Summary:</strong></em> <em>Each round of experiments should have a clear goal and be<br>sufficiently narrow in scope that the experiments can actually make progress<br>towards the goal.</em></p>
<ul>
<li>Each round of experiments should have a clear goal and be sufficiently<br>narrow in scope that the experiments can actually make progress towards the<br>goal: if we try to add multiple features or answer multiple questions at<br>once, we may not be able to disentangle the separate effects on the results.</li>
<li>Example goals include:<ul>
<li>Try a potential improvement to the pipeline (e.g. a new regularizer,<br>preprocessing choice, etc.).</li>
<li>Understand the impact of a particular model hyperparameter (e.g. the<br>activation function)</li>
<li>Greedily maximize validation error.</li>
</ul>
</li>
</ul>
<h3 id="Designing-the-next-round-of-experiments"><a href="#Designing-the-next-round-of-experiments" class="headerlink" title="Designing the next round of experiments"></a>Designing the next round of experiments</h3><p><em><strong>Summary:</strong></em> <em>Identify which hyperparameters are scientific, nuisance, and<br>fixed hyperparameters for the experimental goal. Create a sequence of studies to<br>compare different values of the scientific hyperparameters while optimizing over<br>the nuisance hyperparameters. Choose the search space of nuisance<br>hyperparameters to balance resource costs with scientific value.</em></p>
<h4 id="Identifying-scientific-nuisance-and-fixed-hyperparameters"><a href="#Identifying-scientific-nuisance-and-fixed-hyperparameters" class="headerlink" title="Identifying scientific, nuisance, and fixed hyperparameters"></a>Identifying scientific, nuisance, and fixed hyperparameters</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>For a given goal, all hyperparameters will be either <strong>scientific<br>hyperparameters</strong>, <strong>nuisance hyperparameters</strong>, or <strong>fixed<br>hyperparameters</strong>.<ul>
<li>Scientific hyperparameters are those whose effect on the model’s<br>performance we’re trying to measure.</li>
<li>Nuisance hyperparameters are those that need to be optimized over in<br>order to fairly compare different values of the scientific<br>hyperparameters. This is similar to the statistical concept of<br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nuisance_parameter">nuisance parameters</a>.</li>
<li>Fixed hyperparameters will have their values fixed in the current round<br>of experiments. These are hyperparameters whose values do not need to<br>(or we do not want them to) change when comparing different values of<br>the scientific hyperparameters.<ul>
<li>By fixing certain hyperparameters for a set of experiments, we must<br>accept that conclusions derived from the experiments might not be<br>valid for other settings of the fixed hyperparameters. In other<br>words, fixed hyperparameters create caveats for any conclusions we<br>draw from the experiments.</li>
</ul>
</li>
</ul>
</li>
<li>For example, if our goal is to “determine whether a model with more hidden<br>layers will reduce validation error”, then the number of hidden layers is a<br>scientific hyperparameter.<ul>
<li>The learning rate is a nuisance hyperparameter because we can only<br>fairly compare models with different numbers of hidden layers if the<br>learning rate is tuned separately for each number of layers (the optimal<br>learning rate generally depends on the model architecture).</li>
<li>The activation function could be a fixed hyperparameter if we have<br>determined in prior experiments that the best choice of activation<br>function is not sensitive to model depth, or if we are willing to limit<br>our conclusions about the number of hidden layers to only cover this<br>specific choice of activation function. Alternatively, it could be a<br>nuisance parameter if we are prepared to tune it separately for each<br>number of hidden layers.</li>
</ul>
</li>
<li>Whether a particular hyperparameter is a scientific hyperparameter, nuisance<br>hyperparameter, or fixed hyperparameter is not inherent to that<br>hyperparameter, but changes depending on the experimental goal.<ul>
<li>For example, the choice of activation function could be a scientific<br>hyperparameter (is ReLU or tanh a better choice for our problem?), a<br>nuisance hyperparameter (is the best 5-layer model better than the best<br>6-layer model when we allow several different possible activation<br>functions?), or a fixed hyperparameter (for ReLU nets, does adding batch<br>normalization in a particular position help?).</li>
</ul>
</li>
<li>When designing a new round of experiments, we first identify the scientific<br>hyperparameters for our experimental goal.<ul>
<li>At this stage, we consider all other hyperparameters to be nuisance<br>hyperparameters.</li>
</ul>
</li>
<li>Next, we convert some of the nuisance hyperparameters into fixed<br>hyperparameters.<ul>
<li>With limitless resources, we would leave all non-scientific<br>hyperparameters as nuisance hyperparameters so that the conclusions we<br>draw from our experiments are free from caveats about fixed<br>hyperparameter values.</li>
<li>However, the more nuisance hyperparameters we attempt to tune, the<br>greater the risk we fail to tune them sufficiently well for each setting<br>of the scientific hyperparameters and end up reaching the wrong<br>conclusions from our experiments.<ul>
<li>As described<br><a href="#striking-a-balance-between-informative-and-affordable-experiments">below</a>,<br>we could counter this risk by increasing the computational budget,<br>but often our maximum resource budget is less than would be needed<br>to tune over all non-scientific hyperparameters.</li>
</ul>
</li>
<li>We choose to convert a nuisance hyperparameter into a fixed<br>hyperparameter when, in our judgment, the caveats introduced by fixing<br>it are less burdensome than the cost of including it as a nuisance<br>hyperparameter.<ul>
<li>The more a given nuisance hyperparameter interacts with the<br>scientific hyperparameters, the more damaging it is to fix its<br>value. For example, the best value of the weight decay strength<br>typically depends on the model size, so comparing different model<br>sizes assuming a single specific value of the weight decay would not<br>be very insightful.</li>
</ul>
</li>
</ul>
</li>
<li>Although the type we assign to each hyperparameter depends on the<br>experimental goal, we have the following rules of thumb for certain<br>categories of hyperparameters:<ul>
<li>Of the various optimizer hyperparameters (e.g. the learning rate,<br>momentum, learning rate schedule parameters, Adam betas etc.), at least<br>some of them will be nuisance hyperparameters because they tend to<br>interact the most with other changes.<ul>
<li>They are rarely scientific hyperparameters because a goal like “what<br>is the best learning rate for the current pipeline?” doesn’t give<br>much insight – the best setting could easily change with the next<br>pipeline change anyway.</li>
<li>Although we might fix some of them occasionally due to resource<br>constraints or when we have particularly strong evidence that they<br>don’t interact with the scientific parameters, we should generally<br>assume that optimizer hyperparameters must be tuned separately to<br>make fair comparisons between different settings of the scientific<br>hyperparameters, and thus shouldn’t be fixed.<ul>
<li>Furthermore, we have no <em>a priori</em> reason to prefer one<br>optimizer hyperparameter value over another (e.g. they don’t<br>usually affect the computational cost of forward passes or<br>gradients in any way).</li>
</ul>
</li>
</ul>
</li>
<li>In contrast, the <em>choice</em> of optimizer is typically a scientific<br>hyperparameter or fixed hyperparameter.<ul>
<li>It is a scientific hyperparameter if our experimental goal involves<br>making fair comparisons between two or more different optimizers<br>(e.g. “determine which optimizer produces the lowest validation<br>error in a given number of steps”).</li>
<li>Alternatively, we might make it a fixed hyperparameter for a variety<br>of reasons, including (1) prior experiments make us believe that the<br>best optimizer for our problem is not sensitive to current<br>scientific hyperparameters; and&#x2F;or (2) we prefer to compare values<br>of the scientific hyperparameters using this optimizer because its<br>training curves are easier to reason about; and&#x2F;or (3) we prefer to<br>use this optimizer because it uses less memory than the<br>alternatives.</li>
</ul>
</li>
<li>Hyperparameters introduced by a regularization technique are typically<br>nuisance hyperparameters, but whether or not we include the<br>regularization technique at all is a scientific or fixed hyperparameter.<ul>
<li>For example, dropout adds code complexity, so when deciding whether<br>to include it we would make “no dropout” vs “dropout” a scientific<br>hyperparameter and the dropout rate a nuisance hyperparameter.<ul>
<li>If we decide to add dropout to our pipeline based on this<br>experiment, then the dropout rate would be a nuisance<br>hyperparameter in future experiments.</li>
</ul>
</li>
</ul>
</li>
<li>Architectural hyperparameters are often scientific or fixed<br>hyperparameters because architecture changes can affect serving and<br>training costs, latency, and memory requirements.<ul>
<li>For example, the number of layers is typically a scientific or fixed<br>hyperparameter since it tends to have dramatic consequences for<br>training speed and memory usage.</li>
</ul>
</li>
</ul>
</li>
<li>In some cases, the sets of nuisance and fixed hyperparameters will depend on<br>the values of the scientific hyperparameters.<ul>
<li>For example, suppose we are trying to determine which optimizer out of<br>Nesterov momentum and Adam results in the lowest validation error. The<br>scientific hyperparameter is the <code>optimizer</code>, which takes values<br><code>&#123;&quot;Nesterov_momentum&quot;, &quot;Adam&quot;&#125;</code>. The value<br><code>optimizer=&quot;Nesterov_momentum&quot;</code> introduces the nuisance&#x2F;fixed<br>hyperparameters <code>&#123;learning_rate, momentum&#125;</code>, but the value<br><code>optimizer=&quot;Adam&quot;</code> introduces the nuisance&#x2F;fixed hyperparameters<br><code>&#123;learning_rate, beta1, beta2, epsilon&#125;</code>.</li>
<li>Hyperparameters that are only present for certain values of the<br>scientific hyperparameters are called <strong>conditional hyperparameters</strong>.</li>
<li>We should not assume two conditional hyperparameters are the same just<br>because they have the same name! In the above example, the conditional<br>hyperparameter called <code>learning_rate</code> is a <em>different</em> hyperparameter<br>for <code>optimizer=&quot;Nesterov_momentum&quot;</code> versus <code>optimizer=&quot;Adam&quot;</code>. Its role<br>is similar (although not identical) in the two algorithms, but the range<br>of values that work well in each of the optimizers is typically<br>different by several orders of magnitude.</li>
</ul>
</li>
</ul>
</details>

<h4 id="Creating-a-set-of-studies"><a href="#Creating-a-set-of-studies" class="headerlink" title="Creating a set of studies"></a>Creating a set of studies</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>Once we have identified the scientific and nuisance hyperparameters, we<br>design a “study” or sequence of studies to make progress towards the<br>experimental goal.<ul>
<li>A study specifies a set of hyperparameter configurations to be run for<br>subsequent analysis. Each configuration is called a “trial”.</li>
<li>Creating a study typically involves choosing the hyperparameters that<br>will vary across trials, choosing what values those hyperparameters can<br>take on (the “search space”), choosing the number of trials, and<br>choosing an automated search algorithm to sample that many trials from<br>the search space. Alternatively, we could create a study by specifying<br>the set of hyperparameter configurations manually.</li>
</ul>
</li>
<li>The purpose of the studies is to run the pipeline with different values of<br>the scientific hyperparameters, while at the same time <strong>“optimizing away”</strong><br>(or “optimizing over”) the nuisance hyperparameters so that comparisons<br>between different values of the scientific hyperparameters are as fair as<br>possible.</li>
<li>In the simplest case, we would make a separate study for each configuration<br>of the scientific parameters, where each study tunes over the nuisance<br>hyperparameters.<ul>
<li>For example, if our goal is to select the best optimizer out of Nesterov<br>momentum and Adam, we could create one study in which<br><code>optimizer=&quot;Nesterov_momentum&quot;</code> and the nuisance hyperparameters are<br><code>&#123;learning_rate, momentum&#125;</code>, and another study in which<br><code>optimizer=&quot;Adam&quot;</code> and the nuisance hyperparameters are <code>&#123;learning_rate, beta1, beta2, epsilon&#125;</code>. We would compare the two optimizers by<br>selecting the best performing trial from each study.</li>
<li>We can use any gradient-free optimization algorithm, including methods<br>such as Bayesian optimization or evolutionary algorithms, to optimize<br>over the nuisance hyperparameters, although<br><a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">we prefer</a><br>to use quasi-random search in the<br><a href="#exploration-vs-exploitation">exploration phase</a> of tuning because of a<br>variety of advantages it has in this setting.<br><a href="#after-exploration-concludes">After exploration concludes</a>, if<br>state-of-the-art Bayesian optimization software is available, that is<br>our preferred choice.</li>
</ul>
</li>
<li>In the more complicated case where we want to compare a large number of<br>values of the scientific hyperparameters and it is impractical to make that<br>many independent studies, we can include the scientific parameters in the<br>same search space as the nuisance hyperparameters and use a search algorithm<br>to sample values of <em>both</em> the scientific and nuisance hyperparameters in a<br>single study.<ul>
<li>When taking this approach, conditional hyperparameters can cause<br>problems since it is hard to specify a search space unless the set of<br>nuisance hyperparameters is the same for all values of the scientific<br>hyperparameters.</li>
<li>In this case,<br><a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">our preference</a><br>for using quasi-random search over fancier black-box optimization tools<br>is even stronger, since it ensures that we obtain a relatively uniform<br>sampling of values of the scientific hyperparameters. Regardless of the<br>search algorithm, we need to make sure somehow that it searches the<br>scientific parameters uniformly.</li>
</ul>
</li>
</ul>
</details>

<h4 id="Striking-a-balance-between-informative-and-affordable-experiments"><a href="#Striking-a-balance-between-informative-and-affordable-experiments" class="headerlink" title="Striking a balance between informative and affordable experiments"></a>Striking a balance between informative and affordable experiments</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>When designing a study or sequence of studies, we need to allocate a limited<br>budget in order to adequately achieve the following three desiderata:<ol>
<li>Comparing enough different values of the scientific hyperparameters.</li>
<li>Tuning the nuisance hyperparameters over a large enough search space.</li>
<li>Sampling the search space of nuisance hyperparameters densely enough.</li>
</ol>
</li>
<li>The better we can achieve these three desiderata, the more insight we can<br>extract from our experiment.<ul>
<li>Comparing as many values of the scientific hyperparameters as possible<br>broadens the scope of the insights we gain from the experiment.</li>
<li>Including as many nuisance hyperparameters as possible and allowing each<br>nuisance hyperparameter to vary over as wide a range as possible<br>increases our confidence that a “good” value of the nuisance<br>hyperparameters <strong>exists</strong> in the search space for each configuration of<br>the scientific hyperparameters.<ul>
<li>Otherwise, we might make unfair comparisons between values of the<br>scientific hyperparameters by not searching possible regions of the<br>nuisance parameter space where better values might lie for some<br>values of the scientific parameters.</li>
</ul>
</li>
<li>Sampling the search space of nuisance hyperparameters as densely as<br>possible increases our confidence that any good settings for the<br>nuisance hyperparameters that happen to exist in our search space will<br>be found by the search procedure.<ul>
<li>Otherwise, we might make unfair comparisons between values of the<br>scientific parameters due to some values getting luckier with the<br>sampling of the nuisance hyperparameters.</li>
</ul>
</li>
</ul>
</li>
<li>Unfortunately, improvements in <em>any</em> of these three dimensions require<br>either increasing the number of trials, and therefore increasing the<br>resource cost, or finding a way to save resources in one of the other<br>dimensions.<ul>
<li>Every problem has its own idiosyncrasies and computational constraints,<br>so how to allocate resources across these three desiderata requires some<br>level of domain knowledge.</li>
<li>After running a study, we always try to get a sense of whether the study<br>tuned the nuisance hyperparameters well enough (i.e. searched a large<br>enough space extensively enough) to fairly compare the scientific<br>hyperparameters (as described in greater detail<br><a href="#extracting-insight-from-experimental-results">below</a>).</li>
</ul>
</li>
</ul>
</details>

<h3 id="Extracting-insight-from-experimental-results"><a href="#Extracting-insight-from-experimental-results" class="headerlink" title="Extracting insight from experimental results"></a>Extracting insight from experimental results</h3><p><em><strong>Summary:</strong></em> <em>In addition to trying to achieve the original scientific goal of<br>each group of experiments, go through a checklist of additional questions and,<br>if issues are discovered, revise the experiments and rerun them.</em></p>
<ul>
<li>Ultimately, each group of experiments has a specific goal and we want to<br>evaluate the evidence the experiments provide toward that goal.<ul>
<li>However, if we ask the right questions, we will often find issues that<br>need to be corrected before a given set of experiments can make much<br>progress towards their original goal.<ul>
<li>If we don’t ask these questions, we may draw incorrect conclusions.</li>
</ul>
</li>
<li>Since running experiments can be expensive, we also want to take the<br>opportunity to extract other useful insights from each group of<br>experiments, even if these insights are not immediately relevant to the<br>current goal.</li>
</ul>
</li>
<li>Before analyzing a given set of experiments to make progress toward their<br>original goal, we should ask ourselves the following additional questions:<ul>
<li><a href="#identifying-bad-search-space-boundaries">Is the search space large enough?</a><ul>
<li>If the optimal point from a study is near the boundary of the search<br>space in one or more dimensions, the search is probably not wide<br>enough. In this case, we should run another study with an expanded<br>search space.</li>
</ul>
</li>
<li><a href="#not-sampling-enough-points-in-the-search-space">Have we sampled enough points from the search space?</a><ul>
<li>If not, run more points or be less ambitious in the tuning goals.</li>
</ul>
</li>
<li>What fraction of the trials in each study are <strong>infeasible</strong> (i.e.<br>trials that diverge, get really bad loss values, or fail to run at all<br>because they violate some implicit constraint)?<ul>
<li>When a very large fraction of points in a study are <strong>infeasible</strong><br>we should try to adjust the search space to avoid sampling such<br>points, which sometimes requires reparameterizing the search space.</li>
<li>In some cases, a large number of infeasible points can indicate a<br>bug in the training code.</li>
</ul>
</li>
<li><a href="#how-can-optimization-failures-be-debugged-and-mitigated">Does the model exhibit optimization issues?</a></li>
<li><a href="#examining-the-training-curves">What can we learn from the training curves of the best trials?</a><ul>
<li>For example, do the best trials have training curves consistent with<br>problematic overfitting?</li>
</ul>
</li>
</ul>
</li>
<li>If necessary, based on the answers to the questions above, refine the most<br>recent study (or group of studies) to improve the search space and&#x2F;or sample<br>more trials, or take some other corrective action.</li>
<li>Once we have answered the above questions, we can move on to evaluating the<br>evidence the experiments provide towards our original goal (for example,<br><a href="#detecting-whether-a-change-is-useful-with-isolation-plots">evaluating whether a change is useful</a>).</li>
</ul>
<h4 id="Identifying-bad-search-space-boundaries"><a href="#Identifying-bad-search-space-boundaries" class="headerlink" title="Identifying bad search space boundaries"></a>Identifying bad search space boundaries</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>A search space is suspicious if the best point sampled from it is close to<br>its boundary. We might find an even better point if we expanded the search<br>range in that direction.</li>
<li>To check search space boundaries, we like to plot completed trials on what<br>we call <strong>basic hyperparameter axis plots</strong> where we plot the validation<br>objective value versus one of the hyperparameters (e.g. learning rate). Each<br>point on the plot corresponds to a single trial.<ul>
<li>The validation objective value for each trial should usually be the best<br>value it achieved over the course of training.</li>
</ul>
</li>
</ul>
<p align="center" id="figure-1">
<img src="assets/bad_search_space.png" width="49%" alt="Example of bad search space boundaries">
<img src="assets/good_search_space.png" width="49%" alt="Example of good search space boundaries">
</p>

<p align="center"><b>Figure 1:</b> Examples of bad search space boundaries and acceptable search space boundaries.</p>

<ul>
<li>The plots in <a href="#figure-1">Figure 1</a> show the error rate (lower is better)<br>against the initial learning rate.</li>
<li>If the best points cluster towards the edge of a search space (in some<br>dimension), then the search space boundaries might need to be expanded until<br>the best observed point is no longer close to the boundary.</li>
<li>Often, a study will include “infeasible” trials that diverge or get very bad<br>results (marked with red Xs in the above plots).<ul>
<li>If all trials are infeasible for learning rates greater than some<br>threshold value, and if the best performing trials have learning rates<br>at the edge of that region, the model <a href="#how-can-optimization-failures-be-debugged-and-mitigated">may suffer from stability issues<br>preventing it from accessing higher learning<br>rates</a>.</li>
</ul>
</li>
</ul>
</details>

<h4 id="Not-sampling-enough-points-in-the-search-space"><a href="#Not-sampling-enough-points-in-the-search-space" class="headerlink" title="Not sampling enough points in the search space"></a>Not sampling enough points in the search space</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<ul>
<li>In general,<br><a href="#how-many-trials-are-needed-to-get-good-results-with-quasi-random-search">it can be very difficult to know</a><br>if the search space has been sampled densely enough. 🤖</li>
<li>Running more trials is of course better, but comes at an obvious cost.</li>
<li>Since it is so hard to know when we have sampled enough, we usually sample<br>what we can afford and try to calibrate our intuitive confidence from<br>repeatedly looking at various hyperparameter axis plots and trying to get a<br>sense of how many points are in the “good” region of the search space.</li>
</ul>
</details>

<h4 id="Examining-the-training-curves"><a href="#Examining-the-training-curves" class="headerlink" title="Examining the training curves"></a>Examining the training curves</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<p><em><strong>Summary:</strong></em> <em>Examining the training curves is an easy way to identify common<br>failure modes and can help us prioritize what actions to take next.</em></p>
<ul>
<li>Although in many cases the primary objective of our experiments only<br>requires considering the validation error of each trial, we must be careful<br>when reducing each trial to a single number because it can hide important<br>details about what’s going on below the surface.</li>
<li>For every study, we always look at the <strong>training curves</strong> (training error<br>and validation error plotted versus training step over the duration of<br>training) of at least the best few trials.</li>
<li>Even if this is not necessary for addressing the primary experimental<br>objective, examining the training curves is an easy way to identify common<br>failure modes and can help us prioritize what actions to take next.</li>
<li>When examining the training curves, we are interested in the following<br>questions.</li>
<li>Are any of the trials exhibiting <strong>problematic overfitting?</strong><ul>
<li>Problematic overfitting occurs when the validation error starts<br><em>increasing</em> at some point during training.</li>
<li>In experimental settings where we optimize away nuisance hyperparameters<br>by selecting the “best” trial for each setting of the scientific<br>hyperparameters, we should check for problematic overfitting in <em>at<br>least</em> each of the best trials corresponding to the settings of the<br>scientific hyperparameters that we’re comparing.<ul>
<li>If any of the best trials exhibits problematic overfitting, we<br>usually want to re-run the experiment with additional regularization<br>techniques and&#x2F;or better tune the existing regularization parameters<br>before comparing the values of the scientific hyperparameters.<ul>
<li>This may not apply if the scientific hyperparameters include<br>regularization parameters, since then it would not be surprising<br>if low-strength settings of those regularization parameters<br>resulted in problematic overfitting.</li>
</ul>
</li>
<li>Reducing overfitting is often straightforward using common<br>regularization techniques that add minimal code complexity or extra<br>computation (e.g. dropout, label smoothing, weight decay), so it’s<br>usually no big deal to add one or more of these to the next round of<br>experiments.</li>
<li>For example, if the scientific hyperparameter is “number of hidden<br>layers” and the best trial that uses the largest number of hidden<br>layers exhibited problematic overfitting, then we would usually<br>prefer to try it again with additional regularization instead of<br>immediately selecting the smaller number of hidden layers.</li>
<li>Even if none of the “best” trials are exhibiting problematic<br>overfitting, there might still be a problem if it occurs in <em>any</em> of<br>the trials.<ul>
<li>Selecting the best trial suppresses configurations exhibiting<br>problematic overfitting and favors those that do not. In other<br>words, it will favor configurations with more regularization.</li>
<li>However, anything that makes training worse can act as a<br>regularizer, even if it wasn’t intended that way. For example,<br>choosing a smaller learning rate can regularize training by<br>hobbling the optimization process, but we typically don’t want<br>to choose the learning rate this way.</li>
<li>So we must be aware that the “best” trial for each setting of<br>the scientific hyperparameters might be selected in such a way<br>that favors “bad” values of some of the scientific or nuisance<br>hyperparameters.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Is there high step-to-step variance in the training or validation error late<br>in training?<ul>
<li>If so, this could interfere with our ability to compare different values<br>of the scientific hyperparameters (since each trial randomly ends on a<br>“lucky” or “unlucky” step) and our ability to reproduce the result of<br>the best trial in production (since the production model might not end<br>on the same “lucky” step as in the study).</li>
<li>The most likely causes of step-to-step variance are batch variance (from<br>randomly sampling examples from the training set for each batch), small<br>validation sets, and using a learning rate that’s too high late in<br>training.</li>
<li>Possible remedies include increasing the batch size, obtaining more<br>validation data, using learning rate decay, or using Polyak averaging.</li>
</ul>
</li>
<li>Are the trials still improving at the end of training?<ul>
<li>If so, this indicates that we are in the<br><a href="#determining-the-number-of-steps-for-each-training-run">“compute bound” regime</a><br>and we may benefit from<br><a href="#Deciding-how-long-to-train-when-training-is-compute-bound">increasing the number of training steps</a><br>or changing the learning rate schedule.</li>
</ul>
</li>
<li>Has performance on the training and validation sets saturated long before<br>the final training step?<ul>
<li>If so, this indicates that we are in the<br><a href="#determining-the-number-of-steps-for-each-training-run">“not compute-bound”</a><br>regime and that we may be able to<br><a href="#deciding-how-long-to-train-when-training-is-not-compute-bound">decrease the number of training steps</a>.</li>
</ul>
</li>
<li>Although we cannot enumerate them all, there are many other additional<br>behaviors that can become evident from examining the training curves (e.g.<br>training loss <em>increasing</em> during training usually indicates a bug in the<br>training pipeline).</li>
</ul>
</details>

<h4 id="Detecting-whether-a-change-is-useful-with-isolation-plots"><a href="#Detecting-whether-a-change-is-useful-with-isolation-plots" class="headerlink" title="Detecting whether a change is useful with isolation plots"></a>Detecting whether a change is useful with isolation plots</h4><details><summary><em>[Click to expand]</em></summary>

<br>


<p align="center" id="figure-2">
<img src="assets/isolation_plot.png" width="49%" alt="Isolation plot that investigates the best value of weight decay for ResNet-50
trained on ImageNet.">
</p>

<p align="center"><b>Figure 2:</b> Isolation plot that investigates the best value of weight decay for ResNet-50 trained on ImageNet.</p>

<ul>
<li>Often, the goal of a set of experiments is to compare different values of a<br>scientific hyperparameter.<ul>
<li>For example, we may want to determine the value of weight decay that<br>results in the best validation error.</li>
</ul>
</li>
<li>An <strong>isolation plot</strong> is a special case of the basic hyper-parameter axis<br>plot. Each point on an isolation plot corresponds to the performance of the<br><em>best</em> trial across some (or all) of the nuisance hyperparameters.<ul>
<li>In other words, we plot the model performance after “optimizing away”<br>the nuisance hyperparameters.</li>
</ul>
</li>
<li>An isolation plot makes it easier to perform an apples-to-apples comparison<br>between different values of the scientific hyperparameter.</li>
<li>For example, <a href="#figure-2">Figure 2</a> reveals the value of weight decay that<br>produces the best validation performance for a particular configuration of<br>ResNet-50 trained on ImageNet.<ul>
<li>If our goal is to determine whether to include weight decay at all, then<br>we would compare the best point from this plot against the baseline of<br>no weight decay. For a fair comparison, the baseline should also have<br>its learning rate equally well tuned.</li>
</ul>
</li>
<li>When we have data generated by (quasi)random search and are considering a<br>continuous hyperparameter for an isolation plot, we can approximate the<br>isolation plot by bucketing the x-axis values of the basic hyperparameter<br>axis plot and taking the best trial in each vertical slice defined by the<br>buckets.</li>
</ul>
</details>

<h4 id="Automate-generically-useful-plots"><a href="#Automate-generically-useful-plots" class="headerlink" title="Automate generically useful plots"></a>Automate generically useful plots</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>The more effort it is to generate plots, the less likely we are to look at<br>them as much as we should, so it behooves us to set up our infrastructure to<br>automatically produce as many of them as possible.</li>
<li>At a minimum, we automatically generate basic hyperparameter axis plots for<br>all hyperparameters that we vary in an experiment.</li>
<li>Additionally, we automatically produce training curves for all trials and<br>make it as easy as possible to find the best few trials of each study and<br>examine their training curves.</li>
<li>There are many other potential plots and visualizations we can add that can<br>be useful. Although the ones described above are a good starting point, to<br>paraphrase Geoffrey Hinton, “Every time you plot something new, you learn<br>something new.”</li>
</ul>
</details>

<h3 id="Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration"><a href="#Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration" class="headerlink" title="Determining whether to adopt a training pipeline change or hyperparameter configuration"></a>Determining whether to adopt a training pipeline change or hyperparameter configuration</h3><p><em><strong>Summary:</strong></em> <em>When deciding whether to make a change to our model or training<br>procedure or adopt a new hyperparameter configuration going forward, we need to<br>be aware of the different sources of variation in our results.</em></p>
<ul>
<li>When we are trying to improve our model, we might observe that a particular<br>candidate change initially achieves a better validation error compared to<br>our incumbent configuration, but find that after repeating the experiment<br>there is no consistent advantage. Informally, we can group the most<br>important sources of variation that might cause such an inconsistent result<br>into the following broad categories:<ul>
<li><strong>Training procedure variance</strong>, <strong>retrain variance</strong>, or <strong>trial<br>variance</strong>: the variation we see between training runs that use the same<br>hyperparameters, but different random seeds.<ul>
<li>For example, different random initializations, training data<br>shuffles, dropout masks, patterns of data augmentation operations,<br>and orderings of parallel arithmetic operations, are all potential<br>sources of trial variance.</li>
</ul>
</li>
<li><strong>Hyperparameter search variance</strong>, or <strong>study variance</strong>: the variation<br>in results caused by our procedure to select the hyperparameters.<ul>
<li>For example, we might run the same experiment with a particular<br>search space, but with two different seeds for quasi-random search<br>and end up selecting different hyperparameter values.</li>
</ul>
</li>
<li><strong>Data collection and sampling variance</strong>: the variance from any sort of<br>random split into training, validation, and test data or variance due to<br>the training data generation process more generally.</li>
</ul>
</li>
<li>It is all well and good to make comparisons of validation error rates<br>estimated on a finite validation set using fastidious statistical tests, but<br>often the trial variance alone can produce statistically significant<br>differences between two different trained models that use the same<br>hyperparameter settings.</li>
<li>We are most concerned about study variance when trying to make conclusions<br>that go beyond the level of an individual point in hyperparameters space.<ul>
<li>The study variance depends on the number of trials and the search space<br>and we have seen cases where it is larger than the trial variance as<br>well as cases where it is much smaller.</li>
</ul>
</li>
<li>Therefore, before adopting a candidate change, consider running the best<br>trial N times to characterize the run-to-run trial variance.<ul>
<li>Usually, we can get away with only recharacterizing the trial variance<br>after major changes to the pipeline, but in some applications we might<br>need fresher estimates.</li>
<li>In other applications, characterizing the trial variance is too costly<br>to be worth it.</li>
</ul>
</li>
<li>At the end of the day, although we only want to adopt changes (including new<br>hyperparameter configurations) that produce real improvements, demanding<br>complete certainty that something helps isn’t the right answer either.</li>
<li>Therefore, if a new hyperparameter point (or other change) gets a better<br>result than the baseline (taking into account the retrain variance of both<br>the new point and the baseline as best we can), then we probably should<br>adopt it as the new baseline for future comparisons.<ul>
<li>However, we should only adopt changes that produce improvements that<br>outweigh any complexity they add.</li>
</ul>
</li>
</ul>
<h3 id="After-exploration-concludes"><a href="#After-exploration-concludes" class="headerlink" title="After exploration concludes"></a>After exploration concludes</h3><p><em><strong>Summary:</strong></em> <em>Bayesian optimization tools are a compelling option once we’re<br>done exploring for good search spaces and have decided what hyperparameters even<br>should be tuned at all.</em></p>
<ul>
<li>At some point, our priorities will shift from learning more about the tuning<br>problem to producing a single best configuration to launch or otherwise use.</li>
<li>At this point, there should be a refined search space that comfortably<br>contains the local region around the best observed trial and has been<br>adequately sampled.</li>
<li>Our exploration work should have revealed the most essential hyperparameters<br>to tune (as well as sensible ranges for them) that we can use to construct a<br>search space for a final automated tuning study using as large a tuning<br>budget as possible.</li>
<li>Since we no longer care about maximizing our insight into the tuning<br>problem, many of<br><a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">the advantages of quasi-random search</a><br>no longer apply and Bayesian optimization tools should be used to<br>automatically find the best hyperparameter configuration.<ul>
<li>If the search space contains a non-trivial volume of divergent points<br>(points that get NaN training loss or even training loss many standard<br>deviations worse than the mean), it is important to use black box<br>optimization tools that properly handle trials that diverge (see<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1403.5607">Bayesian Optimization with Unknown Constraints</a><br>for an excellent way to deal with this issue).</li>
</ul>
</li>
<li>At this point, we should also consider checking the performance on the test<br>set.<ul>
<li>In principle, we could even fold the validation set into the training<br>set and retraining the best configuration found with Bayesian<br>optimization. However, this is only appropriate if there won’t be future<br>launches with this specific workload (e.g. a one-time Kaggle<br>competition).</li>
</ul>
</li>
</ul>
<h2 id="Determining-the-number-of-steps-for-each-training-run"><a href="#Determining-the-number-of-steps-for-each-training-run" class="headerlink" title="Determining the number of steps for each training run"></a>Determining the number of steps for each training run</h2><ul>
<li>There are two types of workloads: those that are compute-bound and those<br>that are not.</li>
<li>When training is <strong>compute-bound</strong>, training is limited by how long we are<br>willing to wait and not by how much training data we have or some other<br>factor.<ul>
<li>In this case, if we can somehow train longer or more efficiently, we<br>should see a lower training loss and, with proper tuning, an improved<br>validation loss.</li>
<li>In other words, <em>speeding up</em> training is equivalent to <em>improving</em><br>training and the “optimal” training time is always “as long as we can<br>afford.”</li>
<li>That said, just because a workload is compute-limited doesn’t mean<br>training longer&#x2F;faster is the only way to improve results.</li>
</ul>
</li>
<li>When training is <strong>not compute-bound</strong>, we can afford to train as long as we<br>would like to, and, at some point, training longer doesn’t help much (or<br>even causes problematic overfitting).<ul>
<li>In this case, we should expect to be able to train to very low training<br>loss, to the point where training longer might slightly reduce the<br>training loss, but will not meaningfully reduce the validation loss.</li>
<li>Particularly when training is not compute-bound, a more generous<br>training time budget can make tuning easier, especially when tuning<br>learning rate decay schedules, since they have a particularly strong<br>interaction with the training budget.<ul>
<li>In other words, very stingy training time budgets might require a<br>learning rate decay schedule tuned to perfection in order to achieve<br>a good error rate.</li>
</ul>
</li>
</ul>
</li>
<li>Regardless of whether a given workload is compute-bound or not, methods that<br>increase the variance of the gradients (across batches) will usually result<br>in slower training progress, and thus may increase the number of training<br>steps required to reach a particular validation loss. High gradient variance<br>can be caused by:<ul>
<li>Using a smaller batch size</li>
<li>Adding data augmentation</li>
<li>Adding some types of regularization (e.g. dropout)</li>
</ul>
</li>
</ul>
<h3 id="Deciding-how-long-to-train-when-training-is-not-compute-bound"><a href="#Deciding-how-long-to-train-when-training-is-not-compute-bound" class="headerlink" title="Deciding how long to train when training is not compute-bound"></a>Deciding how long to train when training is <em>not</em> compute-bound</h3><ul>
<li>Our main goal is to ensure we are training long enough for the model to<br>reach the best possible result, while avoiding being overly wasteful in the<br>number of training steps.</li>
<li>When in doubt, err on the side of training longer. Performance should never<br>degrade when training longer, assuming retrospective (optimal) checkpoint<br>selection is used properly and checkpoints are frequent enough.</li>
<li>Never tune the <code>max_train_steps</code> number in a study. Pick a value and use it<br>for all trials. From these trials, plot the training step that retrospective<br>checkpoint selection finds in order to refine the choice of<br><code>max_train_steps</code>.<ul>
<li>For example, if the best step is always during the first 10% of<br>training, then the maximum number of steps is way too high.</li>
<li>Alternatively, if the best step is consistently in the last 25% of<br>training we might benefit from training longer and re-tuning the decay<br>schedule.</li>
</ul>
</li>
<li>The ideal number of training steps can change when the architecture or data<br>changes (e.g. adding data augmentation).</li>
<li>Below we describe how to pick an initial candidate value for<br><code>max_train_steps</code> based on the number of steps necessary to “perfectly fit”<br>the training set using a constant learning rate.<ul>
<li>Note, we are not using the phrase “perfectly fit the training set” in a<br>precise or mathematically well-defined way. It is merely meant as an<br>informal descriptor to indicate a very low training loss.<ul>
<li>For example, when training with the log loss, absent regularization<br>terms, we might see the training loss keep slowly improving until we<br>reach floating point limits as the network weights grow without<br>bound and the predictions of the model on the training set become<br>increasingly confident. In this case, we might say the model<br>“perfectly fit” the training set around the time the<br>misclassification error reached zero on the training set.</li>
</ul>
</li>
<li>The starting value for <code>max_train_steps</code> we find may need to be<br>increased if the amount of gradient noise in the training procedure<br>increases.<ul>
<li>For example, if data augmentation or regularizers like dropout are<br>introduced to the model.</li>
</ul>
</li>
<li>It may be possible to decrease <code>max_train_steps</code> if the training process<br>improves somehow.<ul>
<li>For example, with a better tuned optimizer or a better tuned<br>learning rate schedule.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Algorithm-for-picking-an-initial-candidate-for-max-train-steps-using-a-learning-rate-sweep"><a href="#Algorithm-for-picking-an-initial-candidate-for-max-train-steps-using-a-learning-rate-sweep" class="headerlink" title="Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep"></a>Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>This procedure assumes it is possible to not only “perfectly” fit the<br>training set, but to do so using a constant learning rate schedule.</li>
<li>If it is possible to perfectly fit the entire training set, then there must<br>exist a configuration (with some value of <code>max_train_steps</code>) that perfectly<br>fits the training set; find any such configuration and use its value of<br><code>max_train_steps</code> as a starting point <code>N</code>.</li>
<li>Run a constant learning rate sweep (i.e. grid search the learning rate)<br>without data augmentation and without regularization where each trial trains<br>for <code>N</code> steps.</li>
<li>The number of steps required for the fastest trial in the sweep to reach<br>perfect training performance is our initial guess for <code>max_train_steps</code>.</li>
<li><strong>NOTE:</strong> Bad search spaces can make it possible to engage in<br>self-deception.<ul>
<li>For example, if all the learning rates in a study are too small, we<br>might incorrectly conclude that a very large value of <code>max_train_steps</code><br>is necessary.</li>
<li>At a minimum, we should check that the optimal learning rate in the<br>study is not at the boundary of the search space.</li>
</ul>
</li>
</ul>
</details>

<h3 id="Deciding-how-long-to-train-when-training-is-compute-bound"><a href="#Deciding-how-long-to-train-when-training-is-compute-bound" class="headerlink" title="Deciding how long to train when training is compute-bound"></a>Deciding how long to train when training is compute-bound</h3><ul>
<li>In some cases, training loss keeps improving indefinitely and our patience<br>and computational resources become the limiting factors.</li>
<li>If training loss (or even validation loss) keeps improving indefinitely,<br>should we always train as long as we can afford? Not necessarily.<ul>
<li>We might be able to tune more effectively by running a larger number of<br>shorter experiments and reserving the longest “production length” runs<br>for the models we hope to launch.</li>
<li>As the training time for trials approaches our patience limit, tuning<br>experiments become more relevant for our potential launch candidates,<br>but we can complete fewer of them.</li>
<li>There are probably many questions we can answer while only training for<br>~10% of the production length, but there is always a risk that our<br>conclusions at this time limit will not apply to experiments at 20% of<br>the production length, let alone 100%.</li>
</ul>
</li>
<li>Tuning in multiple rounds with increasing, per-trial training step limits is<br>a sensible approach.<ul>
<li>We can do as many rounds as we want, but usually 1-3 are the most<br>practical.</li>
<li>Essentially, try to obtain as much understanding of the problem as<br>possible using trials with a very quick turnaround time, trading off<br>tuning thoroughness with relevance to the final, longest runs.</li>
<li>Once a given per-trial time limit has generated useful insights, we can<br>increase the training time and continue tuning, double-checking our<br>conclusions from the shorter runs as needed.</li>
</ul>
</li>
<li>As a starting point, we recommend two rounds of tuning:<ul>
<li>Round 1: Shorter runs to find good model and optimizer hyperparameters.</li>
<li>Round 2: Very few long runs on good hyperparameter points to get the<br>final model.</li>
</ul>
</li>
<li>The biggest question going from <code>Round i</code> &rarr; <code>Round i+1</code> is how to<br>adjust learning rate decay schedules.<ul>
<li>One common pitfall when adjusting learning rate schedules between rounds<br>is using all the extra training steps with too small of a learning rate.</li>
</ul>
</li>
</ul>
<h4 id="Round-1"><a href="#Round-1" class="headerlink" title="Round 1"></a>Round 1</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>Unfortunately, there is no guarantee that good hyperparameters found in<br>short, incomplete training are still good choices when training length is<br>significantly increased. However, for some kinds of hyperparameters, they<br>are often correlated enough for Round 1 to be useful.</li>
<li>What hyperparameter values found in shorter runs do we expect to transfer to<br>longer training runs? For all of this, we need more research. But based on<br>what we know so far, here are the authors’ suspicions in order of decreasing<br>probability of transferring:<ul>
<li>Very likely to transfer<ul>
<li>Early training instability can be resolved in the first round of<br>tuning using a smaller number of training steps. Perhaps these<br>hyperparameters are the closest thing to a sure bet for transfer<br>that we have.<ul>
<li>Warmup length</li>
<li>Initialization</li>
</ul>
</li>
</ul>
</li>
<li>Likely to transfer<ul>
<li>Model architecture - A dramatic win in the model architecture will<br>usually transfer, but there are probably many counterexamples.</li>
</ul>
</li>
<li>Might transfer<ul>
<li>Optimization algorithm&#x2F;optimizer hyperparameters - We think this<br>would “loosely” transfer. It’s definitely weaker than the things<br>above it.</li>
<li>Data augmentation</li>
<li>Regularization<ul>
<li>If it isn’t possible to perfectly fit the training set, the<br>model might be in a regime where regularization is unlikely to<br>help very much.</li>
</ul>
</li>
</ul>
</li>
<li>Unlikely to transfer<ul>
<li>Learning rate schedule: unlikely to transfer perfectly.<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15556">This paper</a> suggests that<br>even decay schedule transfers, but we don’t believe this is true<br>in general. Example: Tuning sqrt decay on small # of training<br>steps then extending to large # will result in the majority of<br>training occurring at overly small steps.<ul>
<li>One can likely do “good enough” with most schedules in the<br>limit of extreme training budget, but noticeable performance<br>improvements can likely be seen if it is tuned.</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02021">Understanding Short-Horizon Bias in Stochastic<br>Meta-Optimization</a> describes<br>the dangers of trying to pick learning rates myopically.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<h4 id="Round-2"><a href="#Round-2" class="headerlink" title="Round 2"></a>Round 2</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>Run the best hyperparameter configuration from Round 1.</li>
<li><strong>(Speculation)</strong> 🤖 Use the extra steps to extend the period of training at<br>a high learning rate.<ul>
<li>E.g. if linear schedule then keep the length of the decay fixed from<br>Round 1 and extend the period of constant lr in the beginning.</li>
<li>For cosine decay, just keep the base lr from Round 1 and extend<br><code>max_train_steps</code> as in<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a>.</li>
</ul>
</li>
<li>More rounds might make sense for teams with very mature modeling and tuning<br>pipelines and very long and expensive production training runs, but they<br>will often be overkill.<ul>
<li>We’ve described how to transfer from Step 1 &rarr; Step 2. If we didn’t care<br>about analysis time and if making efficient use of compute was the<br>overriding concern, then the ideal would be to exponentially increase<br>the length of training runs (and thus the end-to-end time to complete a<br>study) over many different rounds of tuning.<ul>
<li>At each round we systematically ensure our choices continue to hold<br>up.</li>
<li>New ideas go through a pipeline that progressively derisks them<br>using increasingly long-running experiments from Step i to Step i+1.</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<h2 id="Additional-guidance-for-the-training-pipeline"><a href="#Additional-guidance-for-the-training-pipeline" class="headerlink" title="Additional guidance for the training pipeline"></a>Additional guidance for the training pipeline</h2><h3 id="Optimizing-the-input-pipeline"><a href="#Optimizing-the-input-pipeline" class="headerlink" title="Optimizing the input pipeline"></a>Optimizing the input pipeline</h3><p><em><strong>Summary:</strong></em> <em>The causes and interventions of input-bound pipelines are highly<br>task-dependent; use a profiler and look out for common issues.</em></p>
<ul>
<li>Use an appropriate profiler to diagnose input-bound pipelines. For example,<br><a target="_blank" rel="noopener" href="https://jax.readthedocs.io/en/latest/profiling.html">Perfetto</a> for JAX or<br><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/profiler">TensorFlow profiler</a> for<br>TensorFlow.</li>
<li>Ultimately, the specific causes and interventions will be highly<br>task-dependent. Broader engineering considerations (e.g. minimizing disk<br>footprint) may warrant worse input pipeline performance.</li>
<li>Common causes:<ul>
<li>Data are not colocated with the training process, causing I&#x2F;O latency<br>(this might happen when reading training data over a network).</li>
<li>Expensive online data preprocessing (consider doing this once offline<br>and saving).</li>
<li>Unintentional synchronization barriers that interfere with data pipeline<br>prefetching. For example, when synchronizing metrics between the device<br>and host in CommonLoopUtils<br>(<a target="_blank" rel="noopener" href="https://github.com/google/CommonLoopUtils/blob/fea2518ada8814a78e1492023fd9f00edb0b0568/clu/metrics.py#L291">link</a>).</li>
</ul>
</li>
<li>Common tips:<ul>
<li>Instrument input pipeline to prefetch examples (e.g.<br><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/data_performance#prefetching">tf.data.Dataset.prefetch</a>)</li>
<li>Remove unused features&#x2F;metadata from each as early in the pipeline as<br>possible.</li>
<li>Increase the replication of the number of jobs generating examples for<br>the input pipeline. For example, by using the<br><a target="_blank" rel="noopener" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service">tf.data service</a>.</li>
</ul>
</li>
</ul>
<h3 id="Evaluating-model-performance"><a href="#Evaluating-model-performance" class="headerlink" title="Evaluating model performance"></a>Evaluating model performance</h3><p><em><strong>Summary:</strong></em> <em>Run evaluation at larger batch sizes than training. Run<br>evaluations at regular step intervals, not regular time intervals.</em></p>
<h4 id="Evaluation-settings"><a href="#Evaluation-settings" class="headerlink" title="Evaluation settings"></a>Evaluation settings</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>There are several settings in which we can evaluate the performance of our<br>models.<ul>
<li><strong>Online evaluation</strong> - metrics are collected when the model is serving<br>predictions in a production environment.</li>
<li><strong>Offline evaluation</strong> - metrics are collected when the model is run on<br>offline train&#x2F;validation&#x2F;test sets that are representative of the<br>production environment.</li>
<li><strong>Periodic evaluations</strong> - metrics are collected during model training<br>that might either be a proxy for the offline evaluation, and&#x2F;or on a<br>subset of the data used in offline evaluation.</li>
</ul>
</li>
<li>Online evaluation is the gold standard, but is often impractical during the<br>model development phase.</li>
<li>Depending on the problem, offline evaluation can be fairly involved and<br>computationally expensive.</li>
<li>Periodic evaluations are the most practical and economical choice, but may<br>not fully represent the production environment.<ul>
<li>Our goal during periodic evaluation is to use an expedient proxy of the<br>offline evaluation, without sacrificing the reliability of the signal we<br>get during training.</li>
</ul>
</li>
</ul>
</details>

<h4 id="Setting-up-periodic-evaluations"><a href="#Setting-up-periodic-evaluations" class="headerlink" title="Setting up periodic evaluations"></a>Setting up periodic evaluations</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>We run periodic evaluations during training to monitor its progress in real<br>time, to<br><a href="#saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint">facilitate retrospective model checkpoint selection</a>,<br>and so that we can<br><a href="#examining-the-training-curves">examine the training curves at the end of training</a>.</li>
<li>The simplest configuration is to perform both training and periodic<br>evaluations within the same compute instance, periodically alternating<br>between training and evaluation.<ul>
<li>In this case, the batch size used to perform evaluations should be <em>at<br>least</em> as large as the batch size used for training because model<br>activations don’t need to be maintained during evaluation, lowering the<br>computational requirements per example.</li>
</ul>
</li>
<li>Periodic evaluations should be done at regular step intervals, not time<br>intervals.<ul>
<li>Evaluating based on time intervals can make it harder to interpret the<br>training curves, especially when training may suffer from preemptions of<br>the training jobs, network latency issues, etc.</li>
</ul>
</li>
<li>Periodicity in valid&#x2F;test metrics (when using a shuffled<br>train&#x2F;validation&#x2F;test split) can indicate implementation bugs such as test<br>data having overlap with training data, or training data not being properly<br>shuffled. Evaluating at regular step intervals can make these issues easier<br>to catch.</li>
<li>Partial batches can occur when the evaluation sets are not divisible by the<br>batch size. Ensure that the padded examples are correctly weighed to prevent<br>the loss function from being biased by them. Often, these padded examples<br>can be given a weight of zero.</li>
<li>Save sufficient information per evaluation to support offline analysis.<br>Ideally, we would save predictions on a selection of individual examples<br>since they can be invaluable for debugging.<ul>
<li>Generating artifacts like<br><a target="_blank" rel="noopener" href="https://www.tensorflow.org/guide/saved_model">SavedModels</a> make it easy<br>to do ad-hoc model inspection after evaluation jobs finish.</li>
</ul>
</li>
</ul>
</details>

<h4 id="Choosing-a-sample-for-periodic-evaluation"><a href="#Choosing-a-sample-for-periodic-evaluation" class="headerlink" title="Choosing a sample for periodic evaluation"></a>Choosing a sample for periodic evaluation</h4><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>The periodic evaluation job might not run fast enough to compute metrics on<br>the full offline evaluation set in a reasonable amount of time. This often<br>necessitates sampling data for periodic evaluation.</li>
<li>We consider the following factors when constructing a sampled dataset:<ul>
<li><ins>Sample size</ins><ul>
<li>Check that the performance computed on the sampled dataset used by<br>the periodic job matches the performance on the whole offline<br>evaluation set, i.e. there is no skew between the sampled set and<br>the full dataset.</li>
<li>The dataset used for periodic evaluation should be small enough that<br>it’s easy to generate model predictions over its entirety, but large<br>enough that improvements to the model can be accurately measured<br>(i.e. not overwhelmed by label noise).</li>
<li>It should be large enough to accommodate multiple such evaluations<br>across trials in sequence, and still produce accurate estimates.<br>That is, to avoid adaptively “fitting” to the validation set over<br>time, in a way that doesn’t generalize to a held-out test set.<br>However, this consideration is rarely a practical concern.</li>
</ul>
</li>
<li><ins>Imbalanced datasets</ins><ul>
<li>For imbalanced datasets, performance on rare classes of examples<br>will often be noisy.</li>
<li>For datasets with a small number of examples in a class label, log<br>the number of examples predicted correctly to get more insight into<br>accuracy improvements (.05 sensitivity improvement sounds exciting,<br>but was it just one more example correct?).</li>
</ul>
</li>
</ul>
</li>
</ul>
</details>

<h3 id="Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint"><a href="#Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint" class="headerlink" title="Saving checkpoints and retrospectively selecting the best checkpoint"></a>Saving checkpoints and retrospectively selecting the best checkpoint</h3><p><em><strong>Summary:</strong></em> <em>Run training for a fixed number of steps and retrospectively<br>choose the best checkpoint from the run.</em></p>
<ul>
<li>Most deep learning frameworks support<br><a target="_blank" rel="noopener" href="https://flax.readthedocs.io/en/latest/api_reference/flax.training.html">model checkpointing</a>.<br>That is, the current state of the model is periodically preserved on disk.<br>This allows the training job to be resilient to compute instance<br>interruptions.</li>
<li>The best checkpoint is often not the last checkpoint, particularly when the<br>validation set performance does not continue to increase over time but<br>rather fluctuates about a particular value.</li>
<li>Set up the pipeline to keep track of the N best checkpoints seen so far<br>during training. At the end of training, model selection is then a matter of<br>choosing the best checkpoint seen during training. We call this<br><strong>retrospective optimal checkpoint selection</strong>.</li>
<li>Supporting prospective early stopping is usually not necessary, since we’re<br>pre-specifying a trial budget and are preserving the N best checkpoints seen<br>so far.</li>
</ul>
<h3 id="Setting-up-experiment-tracking"><a href="#Setting-up-experiment-tracking" class="headerlink" title="Setting up experiment tracking"></a>Setting up experiment tracking</h3><p><em><strong>Summary:</strong></em> <em>When tracking different experiments, make sure to note a number<br>of essentials like the best performance of a checkpoint in the study, and a<br>short description of the study.</em></p>
<ul>
<li>We’ve found that keeping track of experiment results in a spreadsheet has<br>been helpful for the sorts of modeling problems we’ve worked on. It often<br>has the following columns:<ul>
<li>Study name</li>
<li>A link to wherever the config for the study is stored.</li>
<li>Notes or a short description of the study.</li>
<li>Number of trials run</li>
<li>Performance on the validation set of the best checkpoint in the study.</li>
<li>Specific reproduction commands or notes on what unsubmitted changes were<br>necessary to launch training.</li>
</ul>
</li>
<li>Find a tracking system that captures at least the information listed above<br>and is convenient for the people doing it. Untracked experiments might as<br>well not exist.</li>
</ul>
<h3 id="Batch-normalization-implementation-details"><a href="#Batch-normalization-implementation-details" class="headerlink" title="Batch normalization implementation details"></a>Batch normalization implementation details</h3><p><em><strong>Summary:</strong></em> <em>Nowadays batch norm can often be replaced with LayerNorm, but in<br>cases where it cannot, there are tricky details when changing the batch size or<br>number of hosts.</em></p>
<ul>
<li>Batch norm normalizes activations using their mean and variance over the<br>current batch, but in the multi-device setting these statistics are<br>different on each device unless explicitly synchronized.</li>
<li>Anecdotal reports (mostly on ImageNet) say calculating these normalizing<br>statistics using only ~64 examples actually works better in practice (see<br>Ghost Batch Norm from <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.08741">this paper</a>).</li>
<li>Decoupling the total batch size and the number of examples used to calculate<br>batch norm statistics is particularly useful for batch size comparisons.</li>
<li>Ghost batch norm implementations do not always correctly handle the case<br>where the per-device batch size &gt; virtual batch size. In this case we’d<br>actually need to subsample the batch on each device in order to get the<br>proper number of batch norm statistic examples.</li>
<li>Exponential moving averages used in test mode batch norm are just a linear<br>combination of training statistics, so these EMAs only need to be<br>synchronized before saving them in checkpoints. However, some common<br>implementations of batch norm do not synchronize these EMAs and only save<br>the EMA from the first device.</li>
</ul>
<h3 id="Considerations-for-multi-host-pipelines"><a href="#Considerations-for-multi-host-pipelines" class="headerlink" title="Considerations for multi-host pipelines"></a>Considerations for multi-host pipelines</h3><p><em><strong>Summary:</strong></em> <em>for logging, evals, RNGs, checkpointing, and data sharding,<br>multi-host training can make it very easy to introduce bugs!</em></p>
<ul>
<li>Ensure the pipeline is only logging and checkpointing on one host.</li>
<li>Make sure before evaluation or checkpointing is run, the batch norm<br>statistics are synchronized across hosts.</li>
<li>It is critical to have RNG seeds that are the same across hosts (for model<br>initialization), and seeds that are different across hosts (for data<br>shuffling&#x2F;preprocessing), so make sure to mark them appropriately.</li>
<li>Sharding data files across hosts is usually recommended for improved<br>performance.</li>
</ul>
<h2 id="FAQs"><a href="#FAQs" class="headerlink" title="FAQs"></a>FAQs</h2><h3 id="What-is-the-best-learning-rate-decay-schedule-family"><a href="#What-is-the-best-learning-rate-decay-schedule-family" class="headerlink" title="What is the best learning rate decay schedule family?"></a>What is the best learning rate decay schedule family?</h3><details><summary><em>[Click to expand]</em></summary>

<br>

<ul>
<li>It’s an open problem. It’s not clear how to construct a set of rigorous<br>experiments to confidently answer what the “best” LR decay schedule is.</li>
<li>Although we don’t know the best schedule family, we’re confident that it’s<br>important to have some (non-constant) schedule and that tuning it matters.</li>
<li>Different learning rates work best at different times during the<br>optimization process. Having some sort of schedule makes it more likely for<br>the model to hit a good learning rate.</li>
</ul>
</details>

<h3 id="Which-learning-rate-decay-should-I-use-as-a-default"><a href="#Which-learning-rate-decay-should-I-use-as-a-default" class="headerlink" title="Which learning rate decay should I use as a default?"></a>Which learning rate decay should I use as a default?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<ul>
<li>Our preference is either linear decay or cosine decay, and a bunch of other<br>schedule families are probably good too.</li>
</ul>
</details>

<h3 id="Why-do-some-papers-have-complicated-learning-rate-schedules"><a href="#Why-do-some-papers-have-complicated-learning-rate-schedules" class="headerlink" title="Why do some papers have complicated learning rate schedules?"></a>Why do some papers have complicated learning rate schedules?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<ul>
<li>It’s not uncommon to see papers with complicated piecewise learning rate<br>(LR) decay schedules.</li>
<li>Readers often wonder how the authors arrived at such a complicated study.</li>
<li>Many complicated LR decay schedules are the result of tuning the schedule as<br>a function of the validation set performance in an ad hoc way:<ol>
<li>Start a single training run with some simple LR decay (or a constant<br>learning rate).</li>
<li>Keep training running until the performance seems to stagnate. If this<br>happens, pause training. Resume it with a perhaps steeper LR decay<br>schedule (or smaller constant learning rate) from this point. Repeat<br>this process until the conference&#x2F;launch deadline.</li>
</ol>
</li>
<li>Blithely copying the resulting <em>schedule</em> is generally not a good idea since<br>the best particular schedule will be sensitive to a host of other<br>hyperparameter choices.<ul>
<li>Better to copy the <em>algorithm</em> that produced the schedule, although this<br>is rarely possible when arbitrary human judgment produced the schedule.</li>
</ul>
</li>
<li>This type of validation-error-sensitive schedule is fine to use if it can be<br>fully automated, but human-in-the-loop schedules that are a function of<br>validation error are brittle and not easily reproducible, so we recommend<br>avoiding them.<ul>
<li>Before publishing results that used such a schedule, please try to make<br>it fully reproducible.</li>
</ul>
</li>
</ul>
</details>

<h3 id="How-should-Adam’s-hyperparameters-be-tuned"><a href="#How-should-Adam’s-hyperparameters-be-tuned" class="headerlink" title="How should Adam’s hyperparameters be tuned?"></a>How should Adam’s hyperparameters be tuned?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<ul>
<li>As discussed above, making general statements about search spaces and how<br>many points one should sample from the search space is very difficult. Note<br>that not all the hyperparameters in Adam are equally important. The<br>following rules of thumb correspond to different “budgets” for the number of<br>trials in a study.<ul>
<li>If &lt; 10 trials in a study, only tune the (base) learning rate.</li>
<li>If 10-25 trials, tune learning rate and $\beta_1$.</li>
<li>If 25+ trials, tune the learning rate, $\beta_1$ and $\epsilon$.</li>
<li>If one can run substantially more than 25 trials, additionally tune<br>$\beta_2$.</li>
</ul>
</li>
</ul>
</details>

<h3 id="Why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning"><a href="#Why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning" class="headerlink" title="Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?"></a>Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?</h3><details><summary><em>[Click to expand]</em></summary>

<ul>
<li>Quasi-random search (based on<br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">low-discrepancy sequences</a>)<br>is our preference over fancier black box optimization tools when used as<br>part of an iterative tuning process intended to maximize insight into the<br>tuning problem (what we refer to as the “exploration phase”). Bayesian<br>optimization and similar tools are more appropriate for the exploitation<br>phase.</li>
<li>Quasi-random search based on randomly shifted low-discrepancy sequences can<br>be thought of as “jittered, shuffled grid search”, since it uniformly, but<br>randomly, explores a given search space and spreads out the search points<br>more than random search.</li>
<li>The advantages of quasi-random search over more sophisticated black box<br>optimization tools (e.g. Bayesian optimization, evolutionary algorithms)<br>include:<ol>
<li>Sampling the search space non-adaptively makes it possible to change the<br>tuning objective in post hoc analysis without rerunning experiments.<ul>
<li>For example, we usually want to find the best trial in terms of<br>validation error achieved at any point in training. But the<br>non-adaptive nature of quasi-random search makes it possible to find<br>the best trial based on final validation error, training error, or<br>some alternative evaluation metric without rerunning any<br>experiments.</li>
</ul>
</li>
<li>Quasi-random search behaves in a consistent and statistically<br>reproducible way.<ul>
<li>It should be possible to reproduce a study from six months ago even<br>if the implementation of the search algorithm changes, as long as it<br>maintains the same uniformity properties. If using sophisticated<br>Bayesian optimization software, the implementation might change in<br>an important way between versions, making it much harder to<br>reproduce an old search. It isn’t always possible to roll back to an<br>old implementation (e.g. if the optimization tool is run as a<br>service).</li>
</ul>
</li>
<li>Its uniform exploration of the search space makes it easier to reason<br>about the results and what they might suggest about the search space.<ul>
<li>For example, if the best point in the traversal of quasi-random<br>search is at the boundary of the search space, this is a good (but<br>not foolproof) signal that the search space bounds should be<br>changed. <a href="#identifying-bad-search-space-boundaries">This section</a><br>goes into more depth. However, an adaptive black box optimization<br>algorithm might have neglected the middle of the search space<br>because of some unlucky early trials even if it happens to contain<br>equally good points, since it is this exact sort of non-uniformity<br>that a good optimization algorithm needs to employ to speed up the<br>search.</li>
</ul>
</li>
<li>Running different numbers of trials in parallel versus sequentially will<br>not produce statistically different results when using quasi-random<br>search (or other non-adaptive search algorithms), unlike with adaptive<br>algorithms.</li>
<li>More sophisticated search algorithms may not always handle infeasible<br>points correctly, especially if they aren’t designed with neural network<br>hyperparameter tuning in mind.</li>
<li>Quasi-random search is simple and works especially well when many tuning<br>trials will be running in parallel.<ul>
<li>Anecdotally<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ben Recht and Kevin Jamieson
[pointed out](http://www.argmin.net/2016/06/20/hypertuning/) how strong
2X-budget random search is as a baseline (the
[Hyperband paper](https://jmlr.org/papers/volume18/16-558/16-558.pdf)
makes similar arguments), but it is certainly possible to find search
spaces and problems where state-of-the-art Bayesian optimization
techniques crush random search that has 2X the budget. However, in our
experience beating 2X-budget random search gets much harder in the
high-parallelism regime since Bayesian optimization has no opportunity to
observe the results of previous trials.">[3]</span></a></sup>, it is very hard for an adaptive algorithm to beat a<br>quasi-random search that has 2X its budget, especially when many<br>trials need to be run in parallel (and thus there are very few<br>chances to make use of previous trial results when launching new<br>trials).</li>
<li>Without expertise in Bayesian optimization and other advanced black<br>box optimization methods, we might not achieve the benefits they<br>are, in principle, capable of providing. It is hard to benchmark<br>advanced black box optimization algorithms in realistic deep<br>learning tuning conditions. They are a very active area of current<br>research, and the more sophisticated algorithms come with their own<br>pitfalls for inexperienced users. Experts in these methods are able<br>to get good results, but in high-parallelism conditions the search<br>space and budget tend to matter a lot more.</li>
</ul>
</li>
</ol>
</li>
<li>That said, if our computational resources only allow a small number of<br>trials to run in parallel and we can afford to run many trials in sequence,<br>Bayesian optimization becomes much more attractive despite making our tuning<br>results harder to interpret.</li>
</ul>
</details>

<h3 id="Where-can-I-find-an-implementation-of-quasi-random-search"><a href="#Where-can-I-find-an-implementation-of-quasi-random-search" class="headerlink" title="Where can I find an implementation of quasi-random search?"></a>Where can I find an implementation of quasi-random search?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<ul>
<li>We use<br><a target="_blank" rel="noopener" href="https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py">this implementation</a><br>that generates a Halton sequence for a given search space (intended to<br>implement a shifted, scrambled Halton sequence as recommended in<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03200">https://arxiv.org/abs/1706.03200</a>).</li>
<li>If a quasi-random search algorithm based on a low-discrepancy sequence is<br>not available, it is possible to substitute pseudo random uniform search<br>instead, although this is likely to be slightly less efficient.<ul>
<li>In 1-2 dimensions, grid search is also acceptable, although not in<br>higher dimensions (see<br><a target="_blank" rel="noopener" href="https://www.jmlr.org/papers/v13/bergstra12a.html">Bergstra &amp; Bengio, 2012</a>).</li>
</ul>
</li>
</ul>
</details>

<h3 id="How-many-trials-are-needed-to-get-good-results-with-quasi-random-search"><a href="#How-many-trials-are-needed-to-get-good-results-with-quasi-random-search" class="headerlink" title="How many trials are needed to get good results with quasi-random search?"></a>How many trials are needed to get good results with quasi-random search?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<p align="center">
<img src="assets/have_we_sampled_enough.png" width="49%" alt="A box plot showing the importance of sampling enough">
</p>

<p align="center"><b>Figure 3:</b> A ResNet-50 was tuned on ImageNet with 100
trials. Via bootstrapping, different amounts of tuning budget were simulated.
Box plots of the best performances for each trial budget are plotted above.

<ul>
<li>There is no way to answer this question in general, but we can look at<br>specific examples.</li>
<li>As the Figure 3 shows, the number of trials in a study can have a<br>substantial impact on the results.<ul>
<li>Notice how large the interquartile ranges are when 6 trials were<br>sampled, versus when 20 trials were sampled.</li>
<li>Even with 20 trials, it is likely that the difference between especially<br>lucky and unlucky studies will be larger than the typical variation<br>between re-trains of this model on different random seeds, with fixed<br>hyperparameters, which for this workload might be around +&#x2F;- 0.1% on a<br>validation error rate of ~23%.</li>
</ul>
</li>
</ul>
</details>

<h3 id="How-can-optimization-failures-be-debugged-and-mitigated"><a href="#How-can-optimization-failures-be-debugged-and-mitigated" class="headerlink" title="How can optimization failures be debugged and mitigated?"></a>How can optimization failures be debugged and mitigated?</h3><details><summary><em>[Click to expand]</em></summary>
<br>


<p><em><strong>Summary:</strong></em> <em>If the model is experiencing optimization difficulties, it’s<br>important to fix them before trying other things. Diagnosing and correcting<br>training failures is an active area of research.</em></p>
<p align="center">
<img src="assets/stride_instability.png" width="80%" alt="Changing the strides in a single residual block in a WideResnet results in training instability.">
</p>


<p align="center"><b>Figure 4:</b> Changing the strides in a single residual block (2x2 -> 1x1) in a WideResnet results in training instability. This does not degrade performance at low learning rates, but high learning rates no longer train well due to the instability. Applying 1000 steps of learning rate warmup resolves this particular instance of instability, allowing stable training at max learning rate of .1.</p>

<h4 id="Identifying-unstable-workloads"><a href="#Identifying-unstable-workloads" class="headerlink" title="Identifying unstable workloads"></a>Identifying unstable workloads</h4><ul>
<li>Any workload will become unstable if the learning rate is too large.<br>Instability is only an issue when it forces us to use a learning rate that’s<br>too small.</li>
<li>There are at least two types of training instability worth distinguishing:<ol>
<li>Instability at initialization&#x2F;early in training.</li>
<li>Sudden instability in the middle of training.</li>
</ol>
</li>
<li>We can take a systematic approach to identifying stability issues in our<br>workload.<ol>
<li>Do a learning rate sweep and find the best learning rate lr*.</li>
<li>Plot training loss curves for learning rates just above lr*.</li>
<li>If the learning rates &gt; lr* show loss instability (loss goes up not down<br>during periods of training), then it is likely that fixing the<br>instability will result in better training.</li>
</ol>
</li>
<li>Log the L2 norm of the full loss gradient during training, outlier values<br>can result in spurious instability in the middle of training. This can<br>inform how to pick gradient&#x2F;update clipping.</li>
</ul>
<p><strong>NOTE:</strong> Some models show very early instability followed by a recovery that<br>results in slow but stable training. <strong>Common evaluation schedules can miss<br>these issues by not evaluating frequently enough!</strong></p>
<p>To check for this, we can train for an abbreviated run of just ~500 steps using<br><code>lr = 2 * current best</code>, but evaluate every step.</p>
<p align="center">
<img src="assets/more_frequent_evals.png" width="80%" alt="Illustration of the value of more frequent evaluations at the start of
training.">
</p>

<p align="center"><b>Figure 5:</b> Illustration of the value of more frequent evaluations at the start of training. Useful if there’s a suspicion that the model suffers from early training instability.</p>

<h4 id="Potential-fixes-for-common-instability-patterns"><a href="#Potential-fixes-for-common-instability-patterns" class="headerlink" title="Potential fixes for common instability patterns"></a>Potential fixes for common instability patterns</h4><ul>
<li>Apply learning rate warmup<ul>
<li>Best for early training instability.</li>
</ul>
</li>
<li>Apply gradient clipping<ul>
<li>Good for both early and mid training instability, may fix some bad inits<br>that warmup cannot.</li>
</ul>
</li>
<li>Try a new optimizer<ul>
<li>Sometimes Adam can handle instabilities that Momentum can’t. This is an<br>active area of research.</li>
</ul>
</li>
<li>We can ensure that we’re using best practices&#x2F;initializations for our model<br>architecture (examples below).<ul>
<li>Add residual connections and normalization if the model doesn’t contain<br>it already.</li>
</ul>
</li>
<li>Normalization should be the last operation before the residual. E.g. x +<br>Norm(f(x)).</li>
<li>Norm(x + f(x)) known to cause issues.</li>
<li>Try initializing residual branches to 0 (e.g.<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.04887">ReZero init</a>).</li>
<li>Lower the learning rate<ul>
<li>This is a last resort.</li>
</ul>
</li>
</ul>
<h4 id="Learning-rate-warmup"><a href="#Learning-rate-warmup" class="headerlink" title="Learning rate warmup"></a>Learning rate warmup</h4><p align="center">
<img src="assets/instability_during_warmup.png" width="80%" alt="An example of instability during a warmup period (note the horizontal axis log
scale).">
</p>

<p align="center"><b>Figure 6:</b> An example of instability during a warmup period (note the horizontal axis log scale). 40k steps of warmup was needed for successful training in this case.</p>

<h5 id="When-to-apply-learning-rate-warmup"><a href="#When-to-apply-learning-rate-warmup" class="headerlink" title="When to apply learning rate warmup"></a>When to apply learning rate warmup</h5><p align="center">
<img src="assets/axis_model_with_instability.png" width="49%" alt="Axis plot for model with instability">
</p>

<p align="center"><b>Figure 7a:</b> An example of a hyperparameter axis plot for a model exhibiting training instability. The best learning rate is at the edge of what is feasible. An "infeasible" trial is defined as one that either produces NaNs or uncharacteristically high values of the loss.</p>

<p align="center">
<img src="assets/loss_model_with_instability.png" width="49%" alt="Loss curve for model with instability">
</p>

<p align="center"><b>Figure 7b:</b> The training loss of a model trained with a learning rate where we see instability.</p>

<ul>
<li>Figure 7a shows a hyperparameter axis plot that indicates a model<br>experiencing optimization instabilities, because the best learning rate is<br>right at the edge of instability.</li>
<li>Figure 7b shows how this can be double-checked by examining the training<br>loss of a model trained with a learning rate either 5x or 10x larger than<br>this peak. If that plot shows a sudden rise in the loss after a steady<br>decline (e.g. at step ~10k in the figure above), then the model likely<br>suffers from optimization instability.</li>
</ul>
<h5 id="How-to-apply-learning-rate-warmup"><a href="#How-to-apply-learning-rate-warmup" class="headerlink" title="How to apply learning rate warmup"></a>How to apply learning rate warmup</h5><p align="center">
<img src="assets/beneficial_effect_warmup.png" width="80%" alt="Beneficial effect of warmup on training instabilities">
</p>

<p align="center"><b>Figure 8:</b> Beneficial effect of learning rate warmup on addressing training instabilities.</p>

<ul>
<li>Using the section immediately above, we assume that the practitioner has<br>already identified the learning rate at which the model becomes unstable.<br>This is the <code>unstable_base_learning_rate</code>.</li>
<li>Warmup involves prepending a learning rate schedule that ramps up the<br>learning rate from 0 to some stable <code>base_learning_rate</code>, that is at least<br>one order of magnitude larger than <code>unstable_base_learning_rate</code>. The<br>default would be to try a <code>base_learning_rate</code> that’s 10x<br><code>unstable_base_learning_rate</code>. Although note that it’d be possible to run<br>this entire procedure again for something like 100x<br><code>unstable_base_learning_rate</code>. The specific schedule is:<ul>
<li>Ramp up from 0 to <code>base_learning_rate</code> over <code>warmup_steps</code>.</li>
<li>Train at a constant rate for <code>post_warmup_steps</code>.</li>
</ul>
</li>
<li>Our goal is to find the shortest number of <code>warmup_steps</code> that allows us to<br>access peak learning rates that are much higher than<br><code>unstable_base_learning_rate</code>.</li>
<li>So for each <code>base_learning_rate</code>, we need to tune <code>warmup_steps</code> and<br><code>post_warmup_steps</code>. It’s usually fine to set <code>post_warmup_steps</code> to be<br><code>2*warmup_steps</code>.</li>
<li>Warmup can be tuned independently of an existing decay schedule.<br><code>warmup_steps</code> should be swept at a few different orders of magnitude. For<br>example, an example study could try [10, 10<sup>3</sup>, 10<sup>4</sup>,<br>10<sup>5</sup>]. The largest feasible point shouldn’t be more than 10% of<br><code>max_train_steps</code>.</li>
<li>Once a <code>warmup_steps</code> that doesn’t blow up training at <code>base_learning_rate</code><br>has been established, it should be applied to the baseline model.<br>Essentially, we prepend this schedule onto the existing schedule, and use<br>the optimal checkpoint selection discussed above to compare this experiment<br>to the baseline. For example, if we originally had 10,000 <code>max_train_steps</code><br>and did <code>warmup_steps</code> for 1000 steps, the new training procedure should run<br>for 11,000 steps total.</li>
<li>If long <code>warmup_steps</code> are required for stable training (&gt;5% of<br><code>max_train_steps</code>), <code>max_train_steps</code> may need to be increased to account<br>for this.</li>
<li>There isn’t really a “typical” value across the full range of workloads.<br>Some models only need 100 steps, while others (particularly transformers)<br>may need 40k+.</li>
</ul>
<h4 id="Gradient-clipping"><a href="#Gradient-clipping" class="headerlink" title="Gradient clipping"></a>Gradient clipping</h4><p align="center">
<img src="assets/gradient_clipping.png" width="80%" alt="Gradient clipping on early training instabilities">
</p>

<p align="center"><b>Figure 9:</b> Illustration of gradient clipping correcting early training instability.</p>

<ul>
<li>Gradient clipping is most useful when large or outlier gradient issues<br>occur.</li>
<li>Clipping can fix either early training instability (large gradient norm<br>early), or mid training instabilities (sudden gradient spikes mid training).</li>
<li>Sometimes longer warmup periods can correct instabilities that clipping does<br>not: see <a href="#How-to-apply-learning-rate-warmup">this section above</a>.<ul>
<li>🤖 What about clipping during warmup?</li>
</ul>
</li>
<li>The ideal clip thresholds are just above the “typical” gradient norm.</li>
<li>Here’s an example of how gradient clipping could be done:<ul>
<li>If the norm of the gradient $\left | g \right |$ is greater than the<br>gradient clipping threshold $\lambda$, then do ${g}’&#x3D; \lambda \times \frac{g}{\left | g \right |}$ where ${g}’$ is the new gradient.</li>
</ul>
</li>
<li>Log the unclipped gradient norm during training. By default, generate:<ul>
<li>A plot of gradient norm vs step</li>
<li>A histogram of gradient norms aggregated over all steps</li>
</ul>
</li>
<li>Choose a gradient clipping threshold based on the 90th percentile of<br>gradient norms.<ul>
<li>The threshold will be workload dependent, but 90% is a good starting<br>point. If it doesn’t work, this threshold can be tuned.</li>
<li>🤖 What about some sort of adaptive strategy?</li>
</ul>
</li>
<li>If we try gradient clipping and the instability issues remain, we can try it<br>harder (i.e. make the threshold smaller).</li>
<li>Extremely aggressive gradient clipping is in essence a strange way of<br>reducing the learning rate. If we find ourselves using extremely aggressive<br>clipping, we probably should just cut the learning rate instead.</li>
<li>We would usually consider having &gt;50% of the updates getting clipped somehow<br>as “extremely aggressive”.</li>
<li>If we need to do extremely aggressive gradient clipping to deal with our<br>instability issues, then we might as well reduce the learning rate.</li>
</ul>
</details>

<h3 id="Why-do-you-call-the-learning-rate-and-other-optimization-parameters-hyperparameters-They-are-not-parameters-of-any-prior-distribution"><a href="#Why-do-you-call-the-learning-rate-and-other-optimization-parameters-hyperparameters-They-are-not-parameters-of-any-prior-distribution" class="headerlink" title="Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution."></a>Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution.</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<ul>
<li>It is true that the term “hyperparameter” has a precise<br><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Hyperparameter">meaning</a> in Bayesian machine<br>learning and referring to the learning rate and most of the other parameters<br>we tune in deep learning as “hyperparameters” is an abuse of terminology.</li>
<li>We would prefer to use the term “metaparameter” for learning rates,<br>architectural parameters, and all the other things we tune in deep learning,<br>since it avoids the potential for confusion that comes from misusing the<br>word “hyperparameter” (confusion that is especially likely when discussing<br>Bayesian optimization where the probabilistic response surface models have<br>their own true hyperparameters).</li>
<li>Unfortunately, although potentially confusing, the term hyperparameter has become<br>extremely common in the deep learning community.</li>
<li>Therefore, for a document, such as this one, intended for a wide audience<br>that includes many people who are unlikely to be aware of this technicality,<br>we made the choice to contribute to one source of confusion in the<br>field in hopes of avoiding another.</li>
<li>That said, we might make a different choice when publishing a research<br>paper, and we would encourage others to use “metaparameter” instead in most<br>contexts.</li>
</ul>
</details>

<h3 id="Why-shouldn’t-the-batch-size-be-tuned-to-directly-improve-validation-set-performance"><a href="#Why-shouldn’t-the-batch-size-be-tuned-to-directly-improve-validation-set-performance" class="headerlink" title="Why shouldn’t the batch size be tuned to directly improve validation set performance?"></a>Why shouldn’t the batch size be tuned to directly improve validation set performance?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<ul>
<li>Changing the batch size <em>without changing any other details of the training pipeline</em> will often affect the validation set performance.</li>
<li>However, the difference in validation set performance between two batch sizes typically goes away if the training pipeline is optimized independently for each batch size.</li>
<li>The hyperparameters that interact most strongly with the batch size, and therefore are most important to tune separately for each batch size, are the optimizer hyperparameters (e.g. learning rate, momentum) and the regularization hyperparameters.<ul>
<li>Smaller batch sizes introduce more noise into the training algorithm due to sample variance, and this noise can have a regularizing effect. Thus, larger batch sizes can be more prone to overfitting and may require stronger regularization and&#x2F;or additional regularization techniques.</li>
</ul>
</li>
<li>In addition, <a href="#choosing-the-batch-size-to-minimize-training-time">the number of training steps may need to be adjusted</a> when changing the batch size.</li>
<li>Once all these effects are taken into account, there is currently no convincing evidence that the batch size affects the maximum achievable validation performance (see <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).</li>
</ul>
</details>

<h3 id="What-are-the-update-rules-for-all-the-popular-optimization-algorithms"><a href="#What-are-the-update-rules-for-all-the-popular-optimization-algorithms" class="headerlink" title="What are the update rules for all the popular optimization algorithms?"></a>What are the update rules for all the popular optimization algorithms?</h3><details><summary><em>[Click to expand]</em></summary>
<br>

<h4 id="Stochastic-gradient-descent-SGD"><a href="#Stochastic-gradient-descent-SGD" class="headerlink" title="Stochastic gradient descent (SGD)"></a>Stochastic gradient descent (SGD)</h4><p>$$\theta_{t+1} &#x3D; \theta_{t} - \eta_t \nabla \mathcal{l}(\theta_t)$$</p>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>$$v_0 &#x3D; 0$$</p>
<p>$$v_{t+1} &#x3D; \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$</p>
<p>$$\theta_{t+1} &#x3D; \theta_{t} - \eta_t v_{t+1}$$</p>
<h4 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h4><p>$$v_0 &#x3D; 0$$</p>
<p>$$v_{t+1} &#x3D; \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$</p>
<p>$$\theta_{t+1} &#x3D; \theta_{t} - \eta_t( \gamma v_{t+1} + \nabla \mathcal{l}(\theta_{t})$$</p>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>$$v_0 &#x3D; 1 \text{,} m_0 &#x3D; 0$$</p>
<p>$$v_{t+1} &#x3D; \rho v_{t} + (1 - \rho) \nabla \mathcal{l}(\theta_t)^2$$</p>
<p>$$m_{t+1} &#x3D; \gamma m_{t} + \frac{\eta_t}{\sqrt{v_{t+1} + \epsilon}}\nabla \mathcal{l}(\theta_t)$$</p>
<p>$$\theta_{t+1} &#x3D; \theta_{t} - m_{t+1}$$</p>
<h4 id="ADAM"><a href="#ADAM" class="headerlink" title="ADAM"></a>ADAM</h4><p>$$m_0 &#x3D; 0 \text{,} v_0 &#x3D; 0$$</p>
<p>$$m_{t+1} &#x3D; \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$</p>
<p>$$v_{t+1} &#x3D; \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l}(\theta_t)^2$$</p>
<p>$$b_{t+1} &#x3D; \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$</p>
<p>$$\theta_{t+1} &#x3D; \theta_{t} - \alpha_t \frac{m_{t+1}}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$</p>
<h4 id="NADAM"><a href="#NADAM" class="headerlink" title="NADAM"></a>NADAM</h4><p>$$m_0 &#x3D; 0 \text{,} v_0 &#x3D; 0$$</p>
<p>$$m_{t+1} &#x3D; \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$</p>
<p>$$v_{t+1} &#x3D; \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l} (\theta_t)^2$$</p>
<p>$$b_{t+1} &#x3D; \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$</p>
<p>$$\theta_{t+1} &#x3D; \theta_{t} - \alpha_t \frac{\beta_1 m_{t+1} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$</p>
</details>

<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><ul>
<li>我们要感谢Max Bileschi，Roy Frostig，Zelda Mariet，Stan Bileschi，Mohammad Norouzi，Chris Dubois和Charles Sutton阅读了手稿并提供了宝贵的反馈。</li>
<li>我们使用了了一些最初由Naman Agarwal生产的其他联合研究机构的实验数据。</li>
<li>我们要感谢Will Chen在文档的介绍中的宝贵建议。</li>
<li>We would also like to thank Rohan Anil for useful discussions.</li>
</ul>
<h2 id="Citing"><a href="#Citing" class="headerlink" title="Citing"></a>Citing</h2><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-comment">@misc&#123;tuningplaybookgithub,</span><br>  author = &#123;Varun Godbole <span class="hljs-keyword">and</span> George E. Dahl <span class="hljs-keyword">and</span> Justin Gilmer <span class="hljs-keyword">and</span> Christopher J. Shallue <span class="hljs-keyword">and</span> Zachary Nado&#125;,<br>  title = &#123;Deep Learning Tuning Playbook&#125;,<br>  url = &#123;http:<span class="hljs-comment">//github.com/google/tuning_playbook&#125;,</span><br>  year = &#123;<span class="hljs-number">2023</span>&#125;,<br>  note = &#123;Version <span class="hljs-number">1</span>.<span class="hljs-number">0</span>&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="Contributing"><a href="#Contributing" class="headerlink" title="Contributing"></a>Contributing</h2><ul>
<li><p>This is not an officially supported Google product.</p>
</li>
<li><p>We’d love to hear your feedback!</p>
<ul>
<li>If you like the playbook, please <a target="_blank" rel="noopener" href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository">leave a star</a>! Or email<br>deep-learning-tuning-playbook [at] googlegroups.com. Testimonials help<br>us justify creating more resources like this.</li>
<li>If anything seems incorrect, please file an issue to start a discussion.<br>For questions or other messages where an issue isn’t appropriate, please<br>open a new discussion topic on GitHub.</li>
</ul>
</li>
<li><p>As discussed in the preamble, this is a living document. We anticipate<br>making periodic improvements, both small and large. If you’d like to be<br>notified, please watch our repository (see <a target="_blank" rel="noopener" href="https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository">instructions</a>).</p>
</li>
<li><p>Please don’t file a pull request without first coordinating with the authors<br>via the issue tracking system.</p>
</li>
</ul>
<h3 id="Contributor-License-Agreement"><a href="#Contributor-License-Agreement" class="headerlink" title="Contributor License Agreement"></a>Contributor License Agreement</h3><p>Contributions to this project must be accompanied by a Contributor License<br>Agreement (CLA). You (or your employer) retain the copyright to your<br>contribution; this simply gives us permission to use and redistribute your<br>contributions as part of the project. Head over to<br><a target="_blank" rel="noopener" href="https://cla.developers.google.com/">https://cla.developers.google.com/</a> to see your current agreements on file or<br>to sign a new one.</p>
<p>You generally only need to submit a CLA once, so if you’ve already submitted one<br>(even if it was for a different project), you probably don’t need to do it<br>again.</p>
<h3 id="Code-Reviews"><a href="#Code-Reviews" class="headerlink" title="Code Reviews"></a>Code Reviews</h3><p>All submissions, including submissions by project members, require review. We<br>use GitHub pull requests for this purpose. Consult<br><a target="_blank" rel="noopener" href="https://help.github.com/articles/about-pull-requests/">GitHub Help</a> for more<br>information on using pull requests.</p>
<h3 id="Community-Guidelines"><a href="#Community-Guidelines" class="headerlink" title="Community Guidelines"></a>Community Guidelines</h3><p>This project follows<br><a target="_blank" rel="noopener" href="https://opensource.google/conduct/">Google’s Open Source Community Guidelines</a>.</p>
<section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:3" class="footnote-text"><span>Ben Recht and Kevin Jamieson    <a target="_blank" rel="noopener" href="http://www.argmin.net/2016/06/20/hypertuning/">pointed out</a> how strong    2X-budget random search is as a baseline (the    <a target="_blank" rel="noopener" href="https://jmlr.org/papers/volume18/16-558/16-558.pdf">Hyperband paper</a>    makes similar arguments), but it is certainly possible to find search    spaces and problems where state-of-the-art Bayesian optimization    techniques crush random search that has 2X the budget. However, in our    experience beating 2X-budget random search gets much harder in the    high-parallelism regime since Bayesian optimization has no opportunity to    observe the results of previous trials.
<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" class="print-no-link">#人工智能</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【待更新】《深度学习调优手册》- 系统最大化深度学习模型性能</div>
      <div>https://lijianxiong.space/2023/20230126/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>LJX</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年1月26日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/20230128/" title="408复习攻略">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">408复习攻略</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/20230120/" title="重复的眼泪 就是你自欺的安慰奖">
                        <span class="hidden-mobile">重复的眼泪 就是你自欺的安慰奖</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <span>LJX</span> <i class="iconfont icon-love"></i> <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> 
    </div>
  

  
    <div style="margin-top: 8px;font-size: 16px;"> 
      博客已经运行 <span id="daysSinceLJXCustomDate"></span> 天
    </div>

  </div>
  



  

  
</div>


<script>
function displayDaysSinceLJX() {
  // 将起始日期设置为 '2019-03-14' 的零点（当地时区）
  const startDate = new Date('2019-03-14T00:00:00'); 
  const currentDate = new Date();
  
  // 获取当前日期的零点（当地时区），以确保我们计算的是完整的天数
  const currentDayStart = new Date(currentDate.getFullYear(), currentDate.getMonth(), currentDate.getDate());

  // 计算两个日期之间的毫秒差
  const timeDifference = currentDayStart.getTime() - startDate.getTime();
  
  // 将毫秒差转换为天数，并向下取整
  // 这样可以确保我们只计算已经过去的完整天数
  // 例如，如果今天是2019年3月14日（但还未到2019年3月15日的零点），则结果为0天
  let daysPassed = Math.floor(timeDifference / (1000 * 60 * 60 * 24));

  // 确保天数不为负（例如，如果客户端时间不正确导致当前日期早于起始日期）
  daysPassed = Math.max(0, daysPassed); 
  
  const daysElement = document.getElementById('daysSinceLJXCustomDate');
  if (daysElement) {
    daysElement.innerText = daysPassed;
  } else {
    console.error("Element with ID 'daysSinceLJXCustomDate' not found.");
  }
}

// 确保在DOM完全加载后执行脚本
if (document.readyState === 'loading') {
  document.addEventListener('DOMContentLoaded', displayDaysSinceLJX);
} else {
  // DOMContentLoaded 事件已经触发
  displayDaysSinceLJX();
}
</script>
  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>








  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="https://s4.zstatic.net/ajax/libs/echarts/5.6.0/echarts.min.js"></script>
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const heatmapChartDom = document.getElementById('heatmapChart');
                if(heatmapChartDom){
                    const heatmapChart = echarts.init(heatmapChartDom, 'light');
                    const cellSize = [18, 18];
                    
                    const groupByYear = (data) => {
                        const result = {};
                        data.forEach(([date, value]) => {
                            const [year] = date.split('-').map(Number);
                            if (!result[year]) {
                                result[year] = [];
                            }
                            result[year].push([date, value]);
                        });
                        return result;
                    };
                    
                    const groupedData = groupByYear([["2019-03-14",1],["2019-03-23",1],["2019-04-30",1],["2019-06-24",1],["2019-07-18",1],["2019-07-27",1],["2019-07-29",1],["2019-08-09",1],["2019-08-10",1],["2019-08-15",1],["2019-08-31",1],["2020-01-11",1],["2020-04-26",1],["2020-09-05",1],["2020-10-09",1],["2020-10-22",1],["2020-10-25",1],["2020-10-26",1],["2020-11-15",1],["2020-11-17",1],["2020-12-07",1],["2020-12-10",1],["2020-12-21",1],["2020-12-31",1],["2021-01-07",1],["2021-01-15",1],["2021-01-22",2],["2021-02-24",1],["2021-03-09",1],["2021-03-13",1],["2021-03-26",1],["2021-03-31",1],["2021-04-05",1],["2021-04-10",1],["2021-04-11",1],["2021-04-15",1],["2021-05-09",1],["2021-05-14",1],["2021-05-30",1],["2021-06-06",1],["2021-06-07",1],["2021-06-08",1],["2021-06-12",1],["2021-06-14",1],["2021-07-08",1],["2021-07-10",1],["2021-07-16",1],["2021-07-18",1],["2021-07-24",1],["2021-07-27",1],["2021-08-05",1],["2021-08-13",1],["2021-08-25",1],["2021-08-28",1],["2021-08-29",1],["2021-09-19",1],["2021-09-21",1],["2021-10-09",1],["2021-11-02",1],["2022-01-11",1],["2022-01-18",1],["2022-01-19",1],["2022-01-20",1],["2022-01-21",1],["2022-01-22",1],["2022-02-01",1],["2022-02-24",1],["2022-02-25",1],["2022-02-27",1],["2022-02-28",1],["2022-03-09",1],["2022-03-11",1],["2022-03-16",1],["2022-03-28",1],["2022-04-03",1],["2022-04-08",1],["2022-04-17",1],["2022-04-29",1],["2022-04-30",1],["2022-05-05",1],["2022-05-07",1],["2022-05-08",1],["2022-05-09",1],["2022-05-13",1],["2022-05-14",1],["2022-05-15",1],["2022-05-22",1],["2022-05-31",1],["2022-06-18",1],["2022-06-27",1],["2022-07-08",1],["2022-07-10",1],["2022-07-13",1],["2022-07-19",1],["2022-07-20",1],["2022-07-30",1],["2022-08-01",1],["2022-08-15",1],["2022-08-17",1],["2022-08-18",1],["2022-10-09",1],["2022-10-12",1],["2022-10-13",1],["2022-11-22",1],["2022-11-26",1],["2022-11-28",1],["2023-01-01",1],["2023-01-16",1],["2023-01-18",1],["2023-01-20",1],["2023-01-26",1],["2023-01-28",1],["2023-02-04",1],["2023-02-06",1],["2023-02-21",1],["2023-03-19",1],["2023-03-20",1],["2023-03-23",1],["2023-03-24",1],["2023-03-27",1],["2023-03-30",1],["2023-04-01",1],["2023-04-02",1],["2023-06-29",1],["2023-07-01",1],["2023-07-03",1],["2023-07-08",1],["2023-07-11",2],["2023-07-18",1],["2024-01-13",1],["2024-01-16",1],["2024-01-31",1],["2024-02-06",1],["2024-02-11",1],["2024-02-14",1],["2024-02-19",1],["2024-02-26",1],["2024-02-27",1],["2024-02-29",1],["2024-03-03",1],["2024-03-08",1],["2024-04-03",1],["2024-04-10",1],["2024-04-29",1],["2024-05-11",1],["2024-05-12",1],["2024-05-13",1],["2024-05-16",1],["2024-05-20",1],["2024-05-21",1],["2024-06-05",1],["2024-06-29",1],["2024-07-08",1],["2024-08-11",1],["2024-08-14",1],["2024-08-24",1],["2024-08-28",1],["2024-08-30",1],["2024-10-06",1],["2024-11-01",1],["2024-12-16",1],["2024-12-26",1],["2025-01-10",1],["2025-01-15",1],["2025-02-10",1],["2025-02-15",1],["2025-02-16",1],["2025-02-17",1],["2025-02-20",1],["2025-02-22",1],["2025-02-24",1],["2025-03-15",1],["2025-03-19",1],["2025-03-21",1],["2025-04-24",1],["2025-04-25",1],["2025-04-26",1],["2025-04-27",1],["2025-05-01",1],["2025-05-02",1],["2025-05-03",1],["2025-05-11",1],["2025-05-12",1],["2025-05-13",1],["2025-05-15",1],["2025-05-17",1],["2025-05-18",1],["2025-05-20",1],["2025-05-21",1],["2025-05-22",1],["2025-05-23",1],["2025-05-26",1],["2025-05-27",1],["2025-05-30",1],["2025-06-01",1],["2025-06-03",1],["2025-06-06",1],["2025-06-09",1],["2025-06-10",1],["2025-06-13",1],["2025-06-15",1],["2025-06-17",1],["2025-07-05",1],["2025-07-06",1],["2025-07-11",1],["2025-07-14",1],["2025-07-16",1],["2025-07-19",1],["2025-07-20",1],["2025-07-22",1],["2025-07-24",1],["2025-07-26",1],["2025-07-27",1],["2025-08-05",1],["2025-08-07",1],["2025-08-10",1],["2025-08-12",1],["2025-08-13",1],["2025-08-17",1],["2025-08-26",1],["2025-08-29",1],["2025-08-31",1],["2025-09-02",1],["2025-09-03",1],["2025-09-07",1],["2025-09-08",1],["2025-09-10",1],["2025-09-13",1],["2025-09-14",1],["2025-09-15",1],["2025-09-17",1],["2025-09-19",1],["2025-09-26",1],["2025-09-27",1],["2025-09-28",1],["2025-10-05",1],["2025-10-06",1],["2025-10-11",1],["2025-10-14",1],["2025-10-16",1],["2025-10-21",1],["2025-10-22",1],["2025-10-26",1],["2025-10-29",1],["2025-10-31",1]]);
                    const years = Object.keys(groupedData).reverse();
                    
                    var initYear = parseInt(heatmapChartDom.getAttribute('year')) || new Date().getFullYear();
                    const minYear = years[years.length - 1];
                    const maxYear = years[0];
                    if (initYear < minYear || initYear > maxYear) {
                        initYear = maxYear;
                    }
                    console.log('[hexo-graph]generateHeatmapChart|initYear:', initYear, 'minYear:', minYear, 'maxYear:', maxYear);
                    
                    heatmapChart.setOption({
                        grid: {},
                        tooltip: { 
                            position: 'top', 
                            formatter: params => `${params.value[0]}: ${params.value[1]} Articles` 
                        },
                        calendar: { 
                            top: '10%',
                            left: 'left', 
                            right: '8%',
                            range: initYear,
                            cellSize: cellSize, 
                            splitLine: { lineStyle: { color: '#E0E0E0', width: 1 } }, 
                            itemStyle: { borderWidth: 1, borderColor: '#E0E0E0' }, 
                            dayLabel: { show: false },
                            monthLabel: { show: true },
                            yearLabel: { show: false },
                        },
                        visualMap: { 
                            show: true,
                            right: '8%',
                            bottom: '5%',
                            type: 'piecewise',
                            orient: 'horizontal',
                            text: ['More', 'Less'],
                            min: 0,
                            max: Math.max(...groupedData[initYear].map(item => item[1])),
                            inRange: { color: ["#ACE7AE","#69C16D","#549F57"] }
                        },
                        legend: {
                            type: 'scroll',
                            icon: 'none',
                            data: years,
                            orient: 'vertical',
                            top: '5%',
                            right: 'right',
                            itemWidth: 20,
                            itemHeight: 20,
                            itemGap: 10,
                            pageIconSize: 10,
                            pageTextStyle: { fontSize: 14 },
                            selectedMode: 'single',
                        },
                        series: years.map(year => ({
                            type: 'heatmap',
                            coordinateSystem: 'calendar',
                            data: groupedData[year],
                            name: year,
                            emphasis: {
                                disabled: true,
                            },
                            silent: year !== initYear,
                        })),
                    });
                    
                    // init selected year
                    heatmapChart.dispatchAction({
                        type: 'legendSelect',
                        name: initYear,
                    });
                    
                    heatmapChart.on('legendselectchanged', function(params) {
                        console.log('[hexo-graph]generateHeatmapChart|legendselectchanged:', params);
                        const selectedYear = Object.keys(params.selected).find(key => params.selected[key]);
                        if (selectedYear && groupedData[selectedYear]) {
                            heatmapChart.setOption({
                                calendar: {
                                    range: selectedYear,
                                },
                                visualMap: {
                                    max: Math.max(...groupedData[selectedYear].map(item => item[1])),
                                },
                                series: years.map(year => ({
                                    type: 'heatmap',
                                    coordinateSystem: 'calendar',
                                    data: groupedData[year],
                                    name: year,
                                    emphasis: {
                                        disabled: true,
                                    },
                                    silent: year !== selectedYear,
                                })),
                            });
                        }
                    });
                    
                    heatmapChart.on('click', function (params) {
                        if (params.componentType === 'series') {
                            const [year, month] = params.value[0].split('-');
                            window.location.href = '/archives/' + year + '/' + month;
                        }
                    });
                }
            });
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const monthlyChartDom = document.getElementById('monthlyChart');
                if(monthlyChartDom){
                    const monthlyChart = echarts.init(monthlyChartDom, 'light');
                    monthlyChart.setOption({
                        xAxis: { 
                            type: 'category', 
                            data: ["2019-03","2019-04","2019-06","2019-07","2019-08","2020-01","2020-04","2020-09","2020-10","2020-11","2020-12","2021-01","2021-02","2021-03","2021-04","2021-05","2021-06","2021-07","2021-08","2021-09","2021-10","2021-11","2022-01","2022-02","2022-03","2022-04","2022-05","2022-06","2022-07","2022-08","2022-10","2022-11","2023-01","2023-02","2023-03","2023-04","2023-06","2023-07","2024-01","2024-02","2024-03","2024-04","2024-05","2024-06","2024-07","2024-08","2024-10","2024-11","2024-12","2025-01","2025-02","2025-03","2025-04","2025-05","2025-06","2025-07","2025-08","2025-09","2025-10"], 
                            axisLabel: { fontSize: 14, fontWeight: 'bold', fontFamily: 'Microsoft YaHei, SimSun, serif' }
                        },
                        yAxis: { type: 'value', splitLine: { lineStyle: { type: 'dashed', color: '#ccc' } } },
                        series: [{
                            name: 'Articles',
                            type: 'line',
                            data: [2,1,1,3,4,1,1,1,4,2,4,4,1,4,4,3,5,6,5,2,1,1,6,5,4,5,9,2,6,4,3,3,6,3,6,2,1,6,3,7,2,3,6,2,1,5,1,1,2,2,7,3,4,16,8,11,9,13,10],
                            smooth: true,
                            lineStyle: { color: '#5470C6', width: 2 },
                            itemStyle: { color: '#5470C6' },
                            areaStyle: { color: 'rgba(84, 112, 198, 0.4)' },
                            symbolSize: 10,
                            label: {
                                show: true,
                                position: 'top',
                                formatter: params => params.value,
                                fontSize: 14,
                                color: '#000',
                                fontWeight: 'bold',
                                fontFamily: 'Microsoft YaHei, SimSun, serif'
                            }
                        }]
                    });

                    monthlyChart.on('click', function (params) {
                        const [year, month] = params.name.split('-');
                        window.location.href = '/archives/' + year + '/' + month;
                    });
                }
            })
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const tagsChartDom = document.getElementById('tagsChart');
                if(tagsChartDom){
                    const tagsChart = echarts.init(tagsChartDom, 'light');
                    tagsChart.setOption({
                        tooltip: { trigger: 'item', formatter: '{b}: {c} ({d}%)' },
                        series: [{
                            type: 'pie',
                            radius: '60%',
                            data: [{"name":"深度学习","value":112},{"name":"人工智能","value":44},{"name":"笔记","value":34},{"name":"机器学习","value":33},{"name":"数学","value":25},{"name":"大模型","value":21},{"name":"图神经网络","value":16},{"name":"阅读","value":13}],
                            label: {
                                position: 'outside',
                                formatter: '{b} {c} ({d}%)',
                                fontSize: 14,
                                fontWeight: 'bold',
                                fontFamily: 'Microsoft YaHei, SimSun, serif'
                            },
                            color: ["#5470C6","#91CC75","#FAC858","#EE6666","#73C0DE","#3BA272","#FC8452","#9A60B4"],
                            labelLine: { show: true }
                        }],
                        legend: {
                            bottom: '0',
                            left: 'center',
                            data: [{"name":"深度学习","value":112},{"name":"人工智能","value":44},{"name":"笔记","value":34},{"name":"机器学习","value":33},{"name":"数学","value":25},{"name":"大模型","value":21},{"name":"图神经网络","value":16},{"name":"阅读","value":13}].map(tag => tag.name),
                            textStyle: { fontSize: 14, fontWeight: 'bold', fontFamily: 'Microsoft YaHei, SimSun, serif' }
                        }
                    });

                    tagsChart.on('click', function (params) {
                        window.location.href = '/tags/' + params.name;
                    });
                }
            })
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const categoriesChartDom = document.getElementById('categoriesChart');
                if(categoriesChartDom){
                    const categoriesChart = echarts.init(categoriesChartDom, 'light');
                    categoriesChart.setOption({
                        xAxis: { type: 'value', splitLine: { lineStyle: { type: 'dashed', color: '#ccc' } } },
                        yAxis: { 
                            type: 'category', 
                            data: [].map(category => category.name).reverse(), 
                            axisLabel: { fontSize: 14, fontWeight: 'bold', fontFamily: 'Microsoft YaHei, SimSun, serif' }
                        },
                        series: [{
                            name: 'Category Count',
                            type: 'bar',
                            data: [].map(category => category.value).reverse(),
                            label: {
                                show: true,
                                position: 'right',
                                formatter: params => params.value,
                                fontSize: 14,
                                color: '#000',
                                fontWeight: 'bold',
                                fontFamily: 'Microsoft YaHei, SimSun, serif'
                            },
                            itemStyle: {
                                color: new echarts.graphic.LinearGradient(0, 0, 1, 0, [
                                    { offset: 0, color: '#91CC75' },
                                    { offset: 1, color: '#73C0DE' }
                                ])
                            }
                        }]
                    });

                    categoriesChart.on('click', function (params) {
                        window.location.href = '/categories/' + params.name;
                    });
                }
            });
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const categoriesTreeChartDom = document.getElementById('categoriesTreeChart');
                if(categoriesTreeChartDom){
                    const treeChart = echarts.init(categoriesTreeChartDom, 'light');
                    treeChart.setOption({
                        title: {
                            text: '操作提示：单击展开分类，双击进入具体分类页面',
                            textStyle: {
                                fontSize: 12,
                                color: '#999',
                                fontWeight: 'normal'
                            },
                            bottom: 0,
                            left: 'center'
                        },
                        tooltip: {
                            trigger: 'item',
                            triggerOn: 'mousemove'
                        },
                        series: [{
                            type: 'tree',
                            data: [{"name":"Categories","children":[],"count":0,"path":""}],
                            initialTreeDepth: -1,
                            top: '5%',
                            bottom: '10%',
                            left: '0%',
                            right: '0%',
                            symbolSize: 15,
                            layout: 'orthogonal',
                            orient: 'TB',
                            itemStyle: {
                                color: '#91CC75',
                                borderColor: '#73C0DE'
                            },
                            label: {
                                position: 'bottom',
                                verticalAlign: 'middle',
                                align: 'center',
                                fontSize: 14,
                                distance: 28,
                                formatter: function(params) {
                                    return params.data.name + (params.data.count ? ' (' + params.data.count + ')' : '');
                                }
                            },
                            leaves: {
                                label: {
                                    position: 'top',
                                    verticalAlign: 'middle',
                                    align: 'center'
                                }
                            },
                            emphasis: {
                                focus: 'descendant'
                            },
                            expandAndCollapse: true
                        }]
                    });

                    treeChart.on('dblclick', function (params) {
                        if (params.data && params.data.path) {
                            window.location.href = '/categories/' + params.data.path;
                        }
                    });
                }
            });
        </script>
    
    </body>
</html>
