

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#AE945F">
  <meta name="author" content="LJX">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文将介绍一系列的图彩票论文。  《a unified lottery ticket hypothesis for graph neural networks(2021ICML).pdf》相关工作Lottery Ticket Hypothesis该论文首先提到了ICLR 2019最佳论文:The Lottery Ticket Hypothesis: Finding Sparse, Trainabl">
<meta property="og:type" content="article">
<meta property="og:title" content="图彩票论文速览">
<meta property="og:url" content="https://lijianxiong.space/2024/20240521/index.html">
<meta property="og:site_name" content="小熊的小站">
<meta property="og:description" content="本文将介绍一系列的图彩票论文。  《a unified lottery ticket hypothesis for graph neural networks(2021ICML).pdf》相关工作Lottery Ticket Hypothesis该论文首先提到了ICLR 2019最佳论文:The Lottery Ticket Hypothesis: Finding Sparse, Trainabl">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518120157877.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518184812842.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518185703168.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518185452420.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518191841521.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518123750349.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518125402298.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518195611405.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240520164219825.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518132924951.png">
<meta property="og:image" content="https://lijianxiong.space/2024/20240521/image-20240518133345382.png">
<meta property="article:published_time" content="2024-05-21T07:42:47.000Z">
<meta property="article:modified_time" content="2025-05-14T05:47:26.225Z">
<meta property="article:author" content="LJX">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://lijianxiong.space/2024/20240521/image-20240518120157877.png">
  
  
  
  <title>图彩票论文速览 - 小熊的小站</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lijianxiong.space","root":"/","version":"1.9.8","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":false,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"follow_dnt":false,"woyaola":21973729,"woyaola_pro_id":"3MvycZ6wPTE8DE3p","baidu":null,"google":{"measurement_id":"G-C811PDWV2Z"},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  

  
    <!-- 51.la Analytics -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript('//js.users.51.la/21973729.js');
      }
    </script>
  
  
  
  <!-- 51.la Analytics v6 -->
  <script async>
    if (!Fluid.ctx.dnt) {
      // 1. 创建一个新的 script 元素用于加载 51.la 的 SDK
      var script = document.createElement('script');
      script.id = 'LA_COLLECT';
      script.src = '//sdk.51.la/js-sdk-pro.min.js';
      script.charset = 'UTF-8';

      // 2. 关键：当外部脚本加载并执行完毕后，再执行初始化函数
      script.onload = function() {
        LA.init({
          id: "3MvycZ6wPTE8DE3p",
          ck: "3MvycZ6wPTE8DE3p"
        });
      };

      // 3. 将创建的 script 元素插入到页面的 <head> 或 <body> 中，使其开始加载
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(script, s);
    }
  </script>


  

  
    <!-- Google tag (gtag.js) -->
    <script async>
      if (!Fluid.ctx.dnt) {
        Fluid.utils.createScript("https://www.googletagmanager.com/gtag/js?id=G-C811PDWV2Z", function() {
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag('js', new Date());
          gtag('config', 'G-C811PDWV2Z');
        });
      }
    </script>
  

  

  

  



  
<meta name="generator" content="Hexo 7.3.0"><style>
    figure.codeblock {
       margin: 0;
    }
    figure figcaption .tabs {
      display: flex;
      margin: 0;
    }
    figure figcaption .tabs .tab {
      cursor: pointer;
      list-style: none;
      padding: 5px 15px;
    }
    figure figcaption .tabs .tab.active {
      background: #2d2d2d;
      color: white;
    }
  </style></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Bear</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-books"></i>
                <span>目录</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/latexocr1/" target="_self">
                <i class="iconfont icon-exp-fill"></i>
                <span>latex识别</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/arxiv/" target="_self">
                <i class="iconfont icon-notebook"></i>
                <span>每日arxiv</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/huogui.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">图彩票论文速览</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-21 15:42" pubdate>
          2024年5月21日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          6.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          53 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">图彩票论文速览</h1>
            
            
              <div class="markdown-body">
                
                <link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><p>本文将介绍一系列的图彩票论文。</p>
<hr>
<h2 id="《a-unified-lottery-ticket-hypothesis-for-graph-neural-networks-2021ICML-pdf》"><a href="#《a-unified-lottery-ticket-hypothesis-for-graph-neural-networks-2021ICML-pdf》" class="headerlink" title="《a unified lottery ticket hypothesis for graph neural networks(2021ICML).pdf》"></a>《a unified lottery ticket hypothesis for graph neural networks(2021ICML).pdf》</h2><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="Lottery-Ticket-Hypothesis"><a href="#Lottery-Ticket-Hypothesis" class="headerlink" title="Lottery Ticket Hypothesis"></a>Lottery Ticket Hypothesis</h4><p>该论文首先提到了ICLR 2019最佳论文:The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks。该论文提出了彩票假说：密集的、随机初始化的、前馈网络包含子网络（中奖票），这些子网络在孤立地训练时，在类似数量的迭代中达到与原始网络相当的测试精度。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.01067">《Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask》</a>也有类似的内容。展示了为什么将权重设置为零很重要，如何使用符号来进行重新初始化的网络训练，以及为什么掩蔽的行为类似于训练。最后，我们发现了超级掩码的存在，这些掩码可以应用于未经训练的随机初始化网络，以生成性能远远优于偶然的模型（MNIST 上为 86%，CIFAR-10 上为 41%）。</p>
<p>ICLR2020 的《<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/malach20a/malach20a.pdf">Proving the Lottery Ticket Hypothesis: Pruning is All You Need</a>宣称证明了The Lottery Ticket Hypothesis。一句话概括：只要对随机初始化的神经网络做个好剪枝，不怎么训练也能有个好效果。</p>
<p>该文证明了：</p>
<p>Fix some target fully-connected ReLU-network F of width k, depth d and input dimension n.Fix$\delta&gt;0$.Then,arandomly-initialized network $G$ of width $poly(d,n,k,1&#x2F;\epsilon,\log(1&#x2F;\delta))$ and depth 2d, has w.p. $\geq1-\delta$ a subnetwork $\tilde{G}$ that approximates F up to $\epsilon.$</p>
<p>简单的说，给定一个深度为d的Relu目标网络。那么一个深度为2d，且足够宽的随机网络里，必然可以找到一个可以逼近目标网络的子网络。</p>
<span id="more"></span>

<h3 id="本文"><a href="#本文" class="headerlink" title="本文"></a>本文</h3><p>本文这项工作不仅是第一个将 LTH 推广到 GNN 的工作，而且也是第一个将 LTH 从简化模型扩展到新的数据模型联合简化前景的工作。</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="/2024/20240521/image-20240518120157877.png"></p>
<h5 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h5><p>GLT 的推理时间复杂度为,其中 L 是层数。</p>
<p>$||\boldsymbol{m} _g\odot\boldsymbol{A}|| _0$是稀疏图中剩余边的数量。</p>
<p>F是节点特征的维度，$|\mathcal{V}|$是节点的数量。内存复杂度为$o(\mathcal{L} \times \left| \mathcal{V} \right| \times \mathcal{F}+ \mathcal{L} \times \left| m_ \theta \right| _ 0 \times \mathcal{F}^2)$。在我们的实现中，剪枝的边将从$\varepsilon$ （边集合）中删除，并且不会参与下一轮的计算。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h4><p>我们来直接看<a target="_blank" rel="noopener" href="https://github.com/VITA-Group/Unified-LTH-GNN/blob/main/NodeClassification/main_pruning_imp.py">代码</a>：</p>
<h5 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h5><p>主函数中：</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">	<span class="comment">####...........</span></span><br><span class="line">    rewind_weight = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        final_mask_dict, rewind_weight = run_get_mask(args, seed, p, rewind_weight)</span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        <span class="comment">###从final_mask_dict中保存mask到rewind_weight，剪枝但保持其他权重和初始化一样</span></span><br><span class="line">        <span class="comment">###</span></span><br><span class="line">        best_acc_val, final_acc_test, final_epoch_list, adj_spar, wei_spar = run_fix_mask(args, seed, rewind_weight)</span><br><span class="line">        <span class="comment">###省略所有的print</span></span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>每一个epochs中包括了run_get_mask和run_fix_mask，前者是获得mask，后者是保持mask，对模型继续训练。</p>
<h5 id="run-get-mask函数"><a href="#run-get-mask函数" class="headerlink" title="run_get_mask函数"></a><strong>run_get_mask函数</strong></h5><p>模型代码：</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">net_gcn</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim, adj</span>):</span><br><span class="line">        <span class="variable language_">self</span>.adj_mask1_train = nn.Parameter(<span class="variable language_">self</span>.generate_adj_mask(adj))</span><br><span class="line">		<span class="comment">###省略</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, adj, val_test=<span class="literal">False</span></span>):</span><br><span class="line">        adj = torch.mul(adj, <span class="variable language_">self</span>.adj_mask1_train)<span class="comment">#点乘mask</span></span><br><span class="line">        adj = torch.mul(adj, <span class="variable language_">self</span>.adj_mask2_fixed)<span class="comment">#点乘mask</span></span><br><span class="line">        adj = <span class="variable language_">self</span>.normalize(adj)</span><br><span class="line">        <span class="keyword">for</span> ln <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.layer_num):</span><br><span class="line">            x = torch.mm(adj, x)</span><br><span class="line">            x = <span class="variable language_">self</span>.net_layer[ln](x)</span><br><span class="line">            <span class="keyword">if</span> ln == <span class="variable language_">self</span>.layer_num - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">            <span class="keyword">if</span> val_test:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            x = <span class="variable language_">self</span>.dropout(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="comment">###省略</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate_adj_mask</span>(<span class="params">self, input_adj</span>):</span><br><span class="line">        </span><br><span class="line">        sparse_adj = input_adj</span><br><span class="line">        zeros = torch.zeros_like(sparse_adj)</span><br><span class="line">        ones = torch.ones_like(sparse_adj)</span><br><span class="line">        mask = torch.where(sparse_adj != <span class="number">0</span>, ones, zeros)</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line">    <span class="comment">###省略</span></span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>训练部分：</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">####....</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line">net_gcn = net.net_gcn(embedding_dim=args[<span class="string">'embedding_dim'</span>], adj=adj)</span><br><span class="line">pruning.add_mask(net_gcn)<span class="comment">#给边加mask</span></span><br><span class="line"><span class="comment">####....</span></span><br><span class="line"><span class="keyword">if</span> rewind_weight_mask:</span><br><span class="line">        net_gcn.load_state_dict(rewind_weight_mask) <span class="comment">#恢复权重</span></span><br><span class="line"><span class="comment">####....</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args[<span class="string">'total_epoch'</span>]):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = net_gcn(features, adj)</span><br><span class="line">        loss = loss_func(output[idx_train], labels[idx_train])</span><br><span class="line">        loss.backward()</span><br><span class="line">        pruning.subgradient_update_mask(net_gcn, args) <span class="comment"># l1 norm</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment">####以下为验证部分，忽略</span></span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>我们仔细分析这部分：</p>
<p>对于add_mask函数，对边加mask。</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_mask</span>(<span class="params">model, init_mask_dict=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> init_mask_dict <span class="keyword">is</span> <span class="literal">None</span>: </span><br><span class="line">        mask1_train = nn.Parameter(torch.ones_like(model.net_layer[<span class="number">0</span>].weight))</span><br><span class="line">        mask1_fixed = nn.Parameter(torch.ones_like(model.net_layer[<span class="number">0</span>].weight), requires_grad=<span class="literal">False</span>)</span><br><span class="line">        mask2_train = nn.Parameter(torch.ones_like(model.net_layer[<span class="number">1</span>].weight))</span><br><span class="line">        mask2_fixed = nn.Parameter(torch.ones_like(model.net_layer[<span class="number">1</span>].weight), requires_grad=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        mask1_train = nn.Parameter(init_mask_dict[<span class="string">'mask1_train'</span>])</span><br><span class="line">        mask1_fixed = nn.Parameter(init_mask_dict[<span class="string">'mask1_fixed'</span>], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        mask2_train = nn.Parameter(init_mask_dict[<span class="string">'mask2_train'</span>])</span><br><span class="line">        mask2_fixed = nn.Parameter(init_mask_dict[<span class="string">'mask2_fixed'</span>], requires_grad=<span class="literal">False</span>)</span><br><span class="line">    AddTrainableMask.apply(model.net_layer[<span class="number">0</span>], <span class="string">'weight'</span>, mask1_train, mask1_fixed)</span><br><span class="line">    AddTrainableMask.apply(model.net_layer[<span class="number">1</span>], <span class="string">'weight'</span>, mask2_train, mask2_fixed)</span><br><span class="line">....   </span><br><span class="line"><span class="comment">#AddTrainableMask.apply部分</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply</span>(<span class="params">cls, module, name, mask_train, mask_fixed, *args, **kwargs</span>):</span><br><span class="line">    method = cls(*args, **kwargs)  </span><br><span class="line">    method._tensor_name = name</span><br><span class="line">    orig = <span class="built_in">getattr</span>(module, name)</span><br><span class="line">    module.register_parameter(name + <span class="string">"_mask_train"</span>, mask_train.to(dtype=orig.dtype))</span><br><span class="line">    module.register_parameter(name + <span class="string">"_mask_fixed"</span>, mask_fixed.to(dtype=orig.dtype))</span><br><span class="line">    module.register_parameter(name + <span class="string">"_orig_weight"</span>, orig)</span><br><span class="line">    <span class="keyword">del</span> module._parameters[name]</span><br><span class="line">    <span class="built_in">setattr</span>(module, name, method.apply_mask(module))</span><br><span class="line">    module.register_forward_pre_hook(method)</span><br><span class="line">    <span class="keyword">return</span> method</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>对于subgradient_update_mask函数，他是一个 l1 norm</p>
<p>具体而言</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subgradient_update_mask</span>(<span class="params">model, args</span>):</span><br><span class="line">    model.adj_mask1_train.grad.data.add_(args[<span class="string">'s1'</span>] * torch.sign(model.adj_mask1_train.data))</span><br><span class="line">    model.net_layer[<span class="number">0</span>].weight_mask_train.grad.data.add_(args[<span class="string">'s2'</span>] * torch.sign(model.net_layer[<span class="number">0</span>].weight_mask_train.data))</span><br><span class="line">    model.net_layer[<span class="number">1</span>].weight_mask_train.grad.data.add_(args[<span class="string">'s2'</span>] * torch.sign(model.net_layer[<span class="number">1</span>].weight_mask_train.data))</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>简单来说，我们知道，$\frac{d}{dx}|x|&#x3D;sgn(x)$，这里相当于做了一个梯度下降。</p>
<p>其余的就是传统的三件套</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></tbody></table></figure></div></figure>

<h5 id="run-fix-mask函数"><a href="#run-fix-mask函数" class="headerlink" title="run_fix_mask函数"></a><strong>run_fix_mask函数</strong></h5><figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">loss_func = nn.CrossEntropyLoss()</span><br><span class="line">net_gcn = net.net_gcn(embedding_dim=args[<span class="string">'embedding_dim'</span>], adj=adj)</span><br><span class="line">pruning.add_mask(net_gcn)</span><br><span class="line">net_gcn = net_gcn.cuda()</span><br><span class="line">net_gcn.load_state_dict(rewind_weight_mask)</span><br><span class="line">adj_spar, wei_spar = pruning.print_sparsity(net_gcn)</span><br><span class="line"><span class="comment">#多了这部分，将所有的mask都移出训练</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> net_gcn.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'mask'</span> <span class="keyword">in</span> name:</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line">optimizer = torch.optim.Adam(net_gcn.parameters(), lr=args[<span class="string">'lr'</span>], weight_decay=args[<span class="string">'weight_decay'</span>])</span><br><span class="line">acc_test = <span class="number">0.0</span></span><br><span class="line">best_val_acc = {<span class="string">'val_acc'</span>: <span class="number">0</span>, <span class="string">'epoch'</span> : <span class="number">0</span>, <span class="string">'test_acc'</span>: <span class="number">0</span>}</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">200</span>):<span class="comment">#不能指定epochs</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    output = net_gcn(features, adj)</span><br><span class="line">    loss = loss_func(output[idx_train], labels[idx_train])</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#此处少了pruning.subgradient_update_mask(net_gcn, args) # l1 normsubgradient_update_mask</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment">####以下为验证部分，忽略</span></span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>基本和run_get_mask一样，不同在于，将mask移出训练，也少了l1，和限定epochs。</p>
<h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>pass</p>
<h2 id="《searching-lottery-tickets-in-graph-neural-networks-a-dual-perspective-2023ICLR-pdf"><a href="#《searching-lottery-tickets-in-graph-neural-networks-a-dual-perspective-2023ICLR-pdf" class="headerlink" title="《searching lottery tickets in graph neural networks a dual perspective(2023ICLR).pdf&gt;"></a>《searching lottery tickets in graph neural networks a dual perspective(2023ICLR).pdf&gt;</h2><p>代码：<a target="_blank" rel="noopener" href="https://github.com/Lyccl/RGLT">https://github.com/Lyccl/RGLT</a></p>
<h3 id="相关研究"><a href="#相关研究" class="headerlink" title="相关研究"></a>相关研究</h3><p>探索了其对偶问题并提出对偶彩票假说 DLTH：给定随机初始化的网络，其随机挑选的子网络可以被转换成彩票子网络，并得到与 LTH 找到的彩票子网络相当甚至更好的准确率。</p>
<p>算法</p>
<p>DiffPool+mask+GIR（Gradually Increased Regularization）</p>
<p>它的mask矩阵只作用在领接矩阵上。</p>
<p><img src="/2024/20240521/image-20240518184812842.png" alt="image-20240518184812842"></p>
<p>整体算法逻辑为：</p>
<p>1.DiffPool模型训练+GIR</p>
<p>2.one_shot_prune</p>
<p>3.run_fine_tune</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">model = DiffPool(input_dim,</span><br><span class="line">                     hidden_dim,</span><br><span class="line">                     embedding_dim,</span><br><span class="line">                     label_dim,</span><br><span class="line">                     activation,</span><br><span class="line">                     prog_args.gc_per_block,</span><br><span class="line">                     prog_args.dropout,</span><br><span class="line">                     prog_args.num_pool,</span><br><span class="line">                     prog_args.linkpred,</span><br><span class="line">                     prog_args.batch_size,</span><br><span class="line">                     <span class="string">'meanpool'</span>,</span><br><span class="line">                     assign_dim,</span><br><span class="line">                     prog_args.pool_ratio)</span><br><span class="line"><span class="comment">###省略</span></span><br><span class="line">weight_decays = get_weight_decays(count)</span><br><span class="line">masks, unmasks = getMasks(model, w_ratio)<span class="comment">#随机获取mask?</span></span><br><span class="line">logger = train(</span><br><span class="line">    mask,</span><br><span class="line">    train_dataloader,</span><br><span class="line">    model,</span><br><span class="line">    optimizer,</span><br><span class="line">    prog_args,</span><br><span class="line">    weight_decays,</span><br><span class="line">    count,</span><br><span class="line">    masks,</span><br><span class="line">    val_dataset=val_dataloader)</span><br><span class="line"><span class="comment">#省略评估</span></span><br><span class="line">one_shot_prune(model, unmasks)</span><br><span class="line">new_logger = run_fine_tune(mask, model, optimizer, count, prog_args, train_dataloader, weight_decays, masks, unmasks,</span><br><span class="line">                           logger)</span><br></pre></td></tr></tbody></table></figure></div></figure>

<h3 id="DiffPool"><a href="#DiffPool" class="headerlink" title="DiffPool"></a>DiffPool</h3><p>这部分该论文的代码和dgl库的Diffpool完全一样，该论文加了个mask。</p>
<p>例如下图所示，多了红框处的代码。</p>
<p><img src="/2024/20240521/image-20240518185703168.png" alt="image-20240518185703168"></p>
<p>该论文出自NeurIPS 2018，它是一种可微图池化模块，可以生成图的层次表示，并可以以端到端的方式与各种图神经网络架构相结合。</p>
<p>模型框架：</p>
<p><img src="/2024/20240521/image-20240518185452420.png" alt="image-20240518185452420"></p>
<p>DIFFPOOL 可以表达为 :</p>
<p>$\text{}\left(A^{(l+1)},X^{(l+1)}\right)&#x3D;\mathrm{DiFF~POOL}\left(A^{(l)},Z^{(l)}\right)$</p>
<p>即<br>$$<br>\begin{aligned}&amp;X^{(l+1)}&#x3D;S^{(l)^{T}}Z^{(l)}\in\mathbb{R}^{n_ {l+1}\times d}\quad(3)\&amp;A^{(l+1)}&#x3D;S^{(l)^{T}}A^{(l)}S^{(l)}\in\mathbb{R}^{n_ {l+1}\times n_ {l+1}}\quad(4)\end{aligned}<br>$$<br>Z称为嵌入矩阵，S称为分配矩阵。</p>
<p>并设计了两套GNN，来获得嵌入矩阵和分配矩阵。<br>$$Z^{(l)}&#x3D;\mathrm{GNN}_ {l,\mathrm{~embed}}\left(A^{(l)},X^{(l)}\right)$$</p>
<p>$$S^{(l)}&#x3D;\mathrm{softmax}\big(\mathrm{GNN}_ {l,\mathrm{pool}}\big(A^{(l)},X^{(l)}\big)\big)$$<br>Note: 最后一层设置聚类分配矩阵设置输出大小为 1。</p>
<p>作者说，4很难通过梯度进行训练，所以本文采用 最小化Frobenius norm<br>$$<br>L_ {\mathrm{LP}}&#x3D;\left|A^{(l)},S^{(l)}S^{(l)^{T}}\right|_ {F}<br>$$</p>
<blockquote>
<p>这里写的不明白，应该是$$L_ {\mathrm{LP}}&#x3D;\left|A^{(l)}-S^{(l)}S^{(l)^{T}}\right|_ {F}$$</p>
</blockquote>
<p>每个聚类分配矩阵 被希望接近于一个 one-hot 向量，以便明确每个簇的隶属关系，所以本文通过最小化簇分配的熵：<br>$$<br>\bar{L}_ {E}&#x3D;\frac{1}{n}\sum_ {i&#x3D;1}^{n}H(S_ {i})<br>$$<br>其中：H为熵函数$H(X)&#x3D;-\sum_x\in\tau p(x)\log(x);$<br>$S_i$为 $S$的第$i$行；</p>
<blockquote>
<p>然而，据作者<a target="_blank" rel="noopener" href="https://github.com/RexYing/diffpool/issues/24">所说</a>，原因是由于分配预测包含 [0,1] 之间的值，因此交叉熵比 Frobenius 范数中的 l2 更有效。<a target="_blank" rel="noopener" href="https://github.com/RexYing/diffpool">官方的代码</a>是使用了交叉熵来代替 Frobenius 范数。</p>
<p>具体而言：</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="variable language_">self</span>.link_loss = -adj * torch.log(pred_adj+eps) - (<span class="number">1</span>-adj) * torch.log(<span class="number">1</span>-pred_adj+eps)</span><br><span class="line">&gt;</span><br></pre></td></tr></tbody></table></figure></div></figure></blockquote>
<p><strong>模型训练部分：</strong></p>
<p>也是常规的backward三件套，直接来看loss。</p>
<blockquote>
<p>loss &#x3D; model.loss(ypred, graph_labels)<br>reg_loss &#x3D; Regularization(model, weight_decays[int(count &#x2F; 10)], masks, p&#x3D;2)<br>pool_loss &#x3D; cau_loss(mask, model, weight_decays[int(count &#x2F; 10)])<br>my_reg &#x3D; reg_loss(model)<br>loss &#x3D; loss + my_reg + pool_loss</p>
</blockquote>
<p>即对应论文的</p>
<p><img src="/2024/20240521/image-20240518191841521.png" alt="image-20240518191841521"></p>
<p>由于正则项的系数会发生递增变化，也就是Gradually Increased Regularization。</p>
<p>我们仔细看下去</p>
<p>对于cau_loss，他是对mask进行正则项的计算，对应上式的第2部分：</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cau_loss</span>(<span class="params">mask, model, weight_decay</span>):</span><br><span class="line">    reg_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> name, w <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'mask'</span> <span class="keyword">in</span> name:<span class="comment">##对mask进行正则项的计算</span></span><br><span class="line">            temp = np.array(Tensor.cpu(w.data) * Tensor.cpu(mask))</span><br><span class="line">            new_data = torch.from_numpy(temp).cuda()</span><br><span class="line">            l2_reg = torch.norm(new_data, p=<span class="number">2</span>)</span><br><span class="line">            reg_loss = reg_loss + l2_reg</span><br><span class="line">    reg_loss = weight_decay * reg_loss</span><br><span class="line">    <span class="keyword">return</span> reg_loss</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>对于reg_loss，他是对模型参数进行正则项的计算，对应上式的第3部分：</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">····</span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:<span class="comment">#对模型参数进行正则项的计算</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'norms'</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> <span class="string">'bn'</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        weight = (name, param)</span><br><span class="line">        weight_list.append(weight)</span><br><span class="line">····</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>注意：mask和masks</p>
<h3 id="one-shot-prune"><a href="#one-shot-prune" class="headerlink" title="one_shot_prune"></a>one_shot_prune</h3><figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">py</li></ul></figcaption><div class="tabs-content"><figure class="highlight py" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getMasks</span>(<span class="params">model, w_ratio</span>):</span><br><span class="line">    <span class="comment"># w_ratio代表剩余网络参数的比例</span></span><br><span class="line">    unmasks = []</span><br><span class="line">    masks = []</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'norms'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">'bn'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="built_in">print</span>(name)</span><br><span class="line">            mask = torch.zeros_like(param)</span><br><span class="line">            unmask = torch.ones_like(param)</span><br><span class="line">            shape0 = mask.shape[<span class="number">0</span>]</span><br><span class="line">            shape1 = mask.shape[<span class="number">1</span>]</span><br><span class="line">            mask = mask.reshape(-<span class="number">1</span>)</span><br><span class="line">            indices = np.random.choice(np.arange(torch.tensor(mask.shape).item()), replace=<span class="literal">False</span>,</span><br><span class="line">                                       size=<span class="built_in">int</span>(torch.tensor(mask.shape).item() * (<span class="number">1</span> - w_ratio)))<span class="comment">#随机抽取</span></span><br><span class="line">            mask[indices] = <span class="number">1</span></span><br><span class="line">            mask = mask.reshape(shape0, shape1)</span><br><span class="line">            unmask = unmask - mask</span><br><span class="line">            masks.append(mask)</span><br><span class="line">            unmasks.append(unmask)</span><br><span class="line">    <span class="keyword">return</span> masks, unmasks</span><br><span class="line"><span class="comment">###省略</span></span><br><span class="line">masks, unmasks = getMasks(model, w_ratio)</span><br><span class="line"><span class="comment">###省略</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_shot_prune</span>(<span class="params">model, unmasks</span>):</span><br><span class="line">    my_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'norms'</span> <span class="keyword">in</span> name:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> <span class="string">'bn'</span> <span class="keyword">in</span> name:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                param[:] = param * unmasks[my_count]</span><br><span class="line">                my_count += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>进行<strong>随机</strong>剪枝操作,和UGS等不同。</p>
<h3 id="run-fine-tune"><a href="#run-fine-tune" class="headerlink" title="run_fine_tune"></a>run_fine_tune</h3><p>和train函数是一样的，多了每一epoch后执行类似于one shot_prune的操作</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment">###省略省略</span></span><br><span class="line">    训练</span><br><span class="line">    <span class="comment">###省略省略</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">	<span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    	<span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> name:</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'norms'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="string">'bn'</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            param[:] = param * unmasks[count]</span><br><span class="line">            count += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure></div></figure>

<h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><h4 id="时间复杂度计算"><a href="#时间复杂度计算" class="headerlink" title="时间复杂度计算"></a>时间复杂度计算</h4><p>GLT是$o(\mathcal{L}\times\left|\boldsymbol{m}_ g\odot\boldsymbol{A}\right|_ 0\times\mathcal{F}+\mathcal{L}\times\left|\boldsymbol{m}_ \theta\right|_0\times\left|\mathcal{V}\right|\times\mathcal{F}^2)$</p>
<p>而DGLT是$\mathcal{O}\left(\left|\left|m_ {A}\odot A_ {all}\right|\right|_ {0}\times F+\left|\left|m^{*}\right|\right|_ {0}\times\left|\mathcal{V}\right|\times F^{2}\right)+\mathcal{O}\left(\mathcal{K}\right)$,其中 $m_ {A}&#x3D; { m_ {A}^{0},:\hat{m}_ {A}^{1}\ldots m_ {A}^{L} }$ 所有领接矩阵的mask。$\mathcal{O}(K)$ 为学习节点嵌入和分配矩阵的推理时间复杂度。它们由多个矩阵相乘得到，推理时间复杂度为$\mathcal{O}\left(\mathcal{K}\right)&#x3D;\mathcal{O}\left(L\times|\mathcal{V}|^{3}+L\times|\mathcal{V}|\times F\right).$</p>
<h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>pass</p>
<h3 id="另外"><a href="#另外" class="headerlink" title="另外"></a>另外</h3><p><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=Dvs-a3aymPe">https://openreview.net/forum?id=Dvs-a3aymPe</a></p>
<p>DGLT 声称可以将随机预定义的图转换为具有高信息量形式的适当条件。如果这个猜想是正确的，那么它具有相当有希望的实际意义——它表明训练 GNN 模型的消息传递功能（即信息聚合）实际上是不必要的，因为只需要选择邻接矩阵的目标大小或目标GNN的子结构，然后使用层次图稀疏（HGS）算法或逐渐增加正则化进行信息挤出。</p>
<h2 id="《Brave-the-Wind-and-the-Waves-Discovering-Robust-and-Generalizable-Graph-Lottery-Tickets-2023PAMI-pdf》"><a href="#《Brave-the-Wind-and-the-Waves-Discovering-Robust-and-Generalizable-Graph-Lottery-Tickets-2023PAMI-pdf》" class="headerlink" title="《Brave_the_Wind_and_the_Waves_Discovering_Robust_and_Generalizable_Graph_Lottery_Tickets(2023PAMI).pdf》"></a>《Brave_the_Wind_and_the_Waves_Discovering_Robust_and_Generalizable_Graph_Lottery_Tickets(2023PAMI).pdf》</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在现实场景中，未见过的测试数据的分布通常是多种多样的。我们将分布外（OOD）数据的失败归因于无法辨别因果模式，而因果模式在分布变化中仍然保持稳定。在传统的空间图学习中，当图&#x2F;网络稀疏度超过一定的高水平时，模型性能会急剧恶化。更糟糕的是，由于手头的训练集有限，修剪后的 GNN 很难推广到看不见的图数据。为了解决这些问题，我们提出了弹性图彩票（RGLT），以在 GNN 中找到更强大和更通用的 GLT。具体来说，我们通过每个剪枝点的瞬时梯度信息重新激活一部分权重&#x2F;边缘。经过充分的修剪后，我们进行环境干预以推断潜在的测试分布。最后，我们执行最后几轮模型平均值以进一步提高泛化能力。</p>
<p>处理大型图有两个主要研究方向，要么简化图，要么压缩 GNN 模型。第一种，各种图形采样策略或稀疏化方法。在第二个流上所做的努力要少得多，即修剪 GNN  ，因为 GNN 通常比其他学科中的 DNN 参数化程度较低。</p>
<p>GLT仍然有改进空间：</p>
<p>**鲁棒性降低：**在 GLT 中，当图（或网络）稀疏度达到一定程度 时，GNN 的性能将急剧下降，例如超过70%。从概念上讲，GLT 通过基于幅度的剪枝来识别“幸运”图彩票，这可以看作是极化剪枝，在后续训练中不为中等幅度的权重或边缘留下一些余地。在高稀疏度下，模型很难探索完整的权重空间，并且由于稀疏度约束 ，模型更新路线被切断。</p>
<p><strong>泛化能力降低：</strong></p>
<p>然而，图上的剪枝可能会降低模型的泛化性，因为 GNN 与深度学习网络（例如卷积神经网络）一样需要大量数据。</p>
<p>此外，《<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2206.08684">Sparse Double Descent: Where Network Pruning Aggravates Overfitting</a>》（ICML2022）揭示了一个相反的现象——网络剪枝有时甚至会在超稀疏和某些中度稀疏现象下恶化泛化性。该文是第一个报告稀疏双下降现象的工作。更具体地说，证明高模型稀疏度可以显着减轻过度拟合，而中等模型稀疏度可能导致更严重的过度拟合。极端的模型稀疏性 ( →100% ) 往往会丢失所有学到的信息。另外，还得到了和 <em>lottery ticket hypothesis</em> 的相反的结论，从原始初始化重新训练稀疏模型可能不会始终获胜。例如，在某些情况下，随机重新初始化的修剪模型可以在很大程度上超越在某些稀疏度下具有原始初始化的模型。</p>
<p>这个意外问题使 GLT 在具有不同样本和实例的实际应用程序中的使用变得复杂。</p>
<h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p><img src="/2024/20240521/image-20240518123750349.png" alt="image-20240518123750349"></p>
<p>首先，我们执行鲁棒彩票搜索（LoRS）来生成稀疏网络和图的组合。在每次迭代中，我们根据边和权重的大小来修剪边和权重，然后重新激活具有前 k 个梯度的边和权重。然后，我们在核心子图上利用 Lottery Graph Intervention (LoGI) 来推断测试分布，并将增强图传递到剪枝模型以进行下一轮训练。在最后几轮中，我们进行模型平均以进一步提高模型的泛化性。值得注意的是，LoRS 可以独立运行来发现鲁棒图彩票和我们的 LoGI，而 LoGI 算法依赖于 LoRS 识别的核心子图。我们提出的两种算法协同工作，有助于大规模 GNN 应用的落地。</p>
<h4 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h4><p>本文意图解决一个更有挑战性的问题，</p>
<p>提高模型的泛化能力。假设 $S\text{ 是环境 }^{1}$的支持（support of the environments，？），$f(·)$ 是预测函数，我们的目标是最小化不同数据分布下的经验风险：<br>$$\min\limits_ {f}\max\limits_ {e\in\mathcal{S}}\mathbb{E}_ {(\mathcal{G},Y)\sim p(\mathcal{G},Y|e)}:[\mathcal{L}\left(f\left(\mathcal{G}\right),Y\right)|e]$$</p>
<blockquote>
<p>我的理解是类似于最小化$L^\infty$距离</p>
</blockquote>
<h4 id="Robust-Lottery-Searching-LoRS"><a href="#Robust-Lottery-Searching-LoRS" class="headerlink" title="Robust Lottery Searching (LoRS)"></a>Robust Lottery Searching (LoRS)</h4><p><img src="/2024/20240521/image-20240518125402298.png" alt="image-20240518125402298"></p>
<p>前面的步骤和UGS类似，多了一步，将丢弃的边中梯度最大的若干个恢复，代码如右图红框所示。</p>
<p><img src="/2024/20240521/image-20240518195611405.png" alt="image-20240518195611405"></p>
<h4 id="lottery-Graph-Intervention-LoGI"><a href="#lottery-Graph-Intervention-LoGI" class="headerlink" title="lottery Graph Intervention (LoGI)"></a>lottery Graph Intervention (LoGI)</h4><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p>基于UGS的代码，有大量相同的地方。</p>
<p>和UGS一样，主函数也是包括</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> .....</span><br><span class="line">	run_get_mask</span><br><span class="line">    XXXX<span class="comment">###从final_mask_dict中保存mask到rewind_weight，剪枝但保持其他权重和初始化一样</span></span><br><span class="line">    run_fix_mask</span><br></pre></td></tr></tbody></table></figure></div></figure>

<h4 id="run-fix-mask"><a href="#run-fix-mask" class="headerlink" title="run_fix_mask"></a>run_fix_mask</h4><figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(net_gcn.parameters(), lr=args[<span class="string">'lr'</span>], weight_decay=args[<span class="string">'weight_decay'</span>])</span><br><span class="line">optimizer_aug = torch.optim.AdamW(gl.parameters(), lr=args[<span class="string">'lr_a'</span>])</span><br><span class="line">l2_loss = torch.nn.MSELoss()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args[<span class="string">'epochs'</span>] ):</span><br><span class="line">    beta = <span class="number">1</span> * args[<span class="string">"beta"</span>] * epoch / args[<span class="string">'epochs'</span>] + args[<span class="string">"beta"</span>] * (<span class="number">1</span> - epoch / args[<span class="string">'epochs'</span>] )</span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> <span class="built_in">range</span>(args[<span class="string">'T'</span>]):</span><br><span class="line">        ori_tensor = net_gcn(features, adj, gragh_editor=<span class="literal">True</span>)</span><br><span class="line">        graph_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(args[<span class="string">'K'</span>]):</span><br><span class="line">            edge_index_k = gl(adj, dataset_tr.graph[<span class="string">'num_nodes'</span>], args[<span class="string">'num_sample'</span>], k, args[<span class="string">'rate'</span>])</span><br><span class="line">            output = net_gcn(features, edge_index_k)</span><br><span class="line">            labels = labels.squeeze(dim=-<span class="number">1</span>)</span><br><span class="line">            loss = loss_func(output, labels)</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            graph_tensor = net_gcn(features, edge_index_k, gragh_editor=<span class="literal">True</span>)</span><br><span class="line">            graph_loss = graph_loss + l2_loss(ori_tensor, graph_tensor)</span><br><span class="line">        inner_loss = -<span class="number">1.5</span>*graph_loss <span class="comment">#????</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'inner_loss'</span>,inner_loss)</span><br><span class="line">        optimizer_aug.zero_grad()</span><br><span class="line">        inner_loss.backward()</span><br><span class="line">        optimizer_aug.step()</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>不断优化gcn，劣化gl</p>
<h2 id="《Analyzing-Adversarial-Vulnerabilities-of-Graph-Lottery-Tickets-ICASSP2024-pdf》"><a href="#《Analyzing-Adversarial-Vulnerabilities-of-Graph-Lottery-Tickets-ICASSP2024-pdf》" class="headerlink" title="《Analyzing_Adversarial_Vulnerabilities_of_Graph_Lottery_Tickets(ICASSP2024).pdf》"></a>《Analyzing_Adversarial_Vulnerabilities_of_Graph_Lottery_Tickets(ICASSP2024).pdf》</h2><p>和finding_adversarially_robust_graph lottery tickets原作者，内容基本一样。</p>
<p>除了少了平滑项。</p>
<h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><p>pass</p>
<h2 id="《finding-adversarially-robust-graph-lottery-tickets-under-review-pdf》"><a href="#《finding-adversarially-robust-graph-lottery-tickets-under-review-pdf》" class="headerlink" title="《finding_adversarially_robust_graph lottery tickets(under review).pdf》"></a>《finding_adversarially_robust_graph lottery tickets(under review).pdf》</h2><p><img src="/2024/20240521/image-20240520164219825.png" alt="image-20240520164219825">被拒了。</p>
<p>AC拒稿理由：</p>
<p>本文提出了一种减少图彩票对图结构的对抗性扰动的脆弱性的技术。结果似乎对这个问题相当有效。审稿人提出了一些担忧，包括设置本身（结构扰动真的是正确的威胁模型吗？关注这一点是否依赖于其他方面不受攻击？）、方法本身的复杂性（超参数太多）以及大小正在研究的图表的数量（它们足够大吗？）。我同意第一个担忧：这真的是一个重要问题吗？如果对图彩票的对抗性攻击是一个重要问题，那么这些类型的攻击在实践中是否重要？我对接受持矛盾态度，并且基于所研究问题的重要性，我倾向于拒绝。对于这个特定问题来说，这似乎是一个合理的贡献，但问题本身却非常小众。</p>
<h3 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h3><p>pass</p>
<h3 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h3><p>总所周知，两层的GCN可以表示为<br>$$<br>Z&#x3D;f({ A, X },\Theta)&#x3D; \mathcal{S}(\hat{A} \sigma ( \hat A X W_ {(0)}) W_ {(1)})<br>$$<br>设计了一个transductive semi-supervised node classification (SSNC) loss：<br>$$<br>\mathcal{L} _ 0 (f({A, X}, \Theta))&#x3D;-\sum _ {l \in \mathcal{Y} _ {TL}} \sum _ {j&#x3D;1} ^C  Y _ {l_j} log( Z _ {l_j})<br>$$<br>其中$\mathcal{Y}_ {TL}$是训练节点的索引，C是类总数，$Y_l$是$v_l$one hot 标签。</p>
<p>posion 攻击者的目标是找到一个最优的扰动A ‘，欺骗GNN做出错误的预测。这可以表述为一个双层优化问题(Zugner et al.， 2018;zugner &amp; gunnemann, 2019):<br>$$<br>\begin{align}<br>arg \max\mathcal{L}_ {atk}(f({A’,X},\Theta ^\ast))\\<br>A’\in\Phi(A)\\<br>\mathrm{s.t.}\quad\Theta^{\ast}&#x3D;\arg\min_ {\Theta}\mathcal{L}_ {0}(f({A’,X},\Theta))<br>\end{align}<br>$$<br>其中$\Phi(A)$是满足$\frac{|A’-A|{0}}{|A|{0}}\leq\Delta$的领接矩阵。$\mathcal{L}_ {atk}$ 是攻击loss函数，$\Delta$ 是 perturbation rate，$\Theta ^\ast$是摄动图上GNN的最优参数。</p>
<p>为了帮助消除对抗边和鼓励特征平滑，对于homophilic graphs：<br>$$<br>\mathcal{L}_ {fs} (A’,X)&#x3D;\frac{1}{2} \sum_ {i,j&#x3D;1}A_ {ij}’ (x_i-x_j)^2<br>$$<br>对于heterophilic graphs：<br>$$<br>\mathcal{L}_ {fs}(A’)&#x3D;\frac{1}{2}\sum_ {i,j&#x3D;1}A_ {ij}’(y_ {i}-y_ {j})^{2}<br>$$</p>
<blockquote>
<p>以上有点像<em>dirichlet</em> energy。</p>
<p><em>dirichlet</em> energy：<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{2}\sum _ {i,j}W[i,j] (x[j]-x[i])^2<br>$$<br>进一步归一化：<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{2}\sum _ {i,j}W[i,j] (\frac{x[j]}{\sqrt{1+d_j}}-\frac{x[i]}{\sqrt{1+d_i}})^2<br>$$<br>（上式来自《<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/file/b6417f112bd27848533e54885b66c288-Paper.pdf">Dirichlet Energy Constrained Learning for Deep Graph Neural Networks</a>》）</p>
<p>或<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{4}\sum _ {i,j}W[i,j]|\frac{x[j]}{\sqrt{d_j}}-\frac{x[i]}{\sqrt{d_i}}|_2^2<br>$$<br>（上式来自《<a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=kS7ED7eE74">A Fractional Graph Laplacian Approach to Oversmoothing</a>》）</p>
<p>其中d为节点的度。</p>
</blockquote>
<p>其中yi∈R P为输入图G上运行DeepWalk算法得到的节点i, j的位置特征，P为节点位置特征个数。</p>
<blockquote>
<p> 查看上面部分的代码，我们可以发现：</p>
 <figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;  <span class="keyword">def</span> <span class="title function_">feature_smoothing</span>(<span class="params">self, adj, X</span>):</span><br><span class="line">&gt;   adj = (adj.t() + adj)/<span class="number">2</span></span><br><span class="line">&gt;   rowsum = adj.<span class="built_in">sum</span>(<span class="number">1</span>)</span><br><span class="line">&gt;   r_inv = rowsum.flatten()</span><br><span class="line">&gt;   D = torch.diag(r_inv)</span><br><span class="line">&gt;   L = D - adj</span><br><span class="line">&gt;  </span><br><span class="line">&gt;   r_inv = r_inv  + <span class="number">1e-3</span></span><br><span class="line">&gt;   r_inv = r_inv.<span class="built_in">pow</span>(-<span class="number">1</span>/<span class="number">2</span>).flatten()</span><br><span class="line">&gt;   r_inv[torch.isinf(r_inv)] = <span class="number">0.</span></span><br><span class="line">&gt;   r_mat_inv = torch.diag(r_inv)</span><br><span class="line">&gt;   L = r_mat_inv @ L @ r_mat_inv</span><br><span class="line">&gt;  </span><br><span class="line">&gt;   XLXT = torch.matmul(torch.matmul(X.t(), L), X)</span><br><span class="line">&gt;   loss_smooth_feat = torch.trace(XLXT)</span><br><span class="line">&gt;   <span class="keyword">return</span> loss_smooth_feat</span><br><span class="line">&gt;</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p> 迹的计算又出现了。</p>
</blockquote>
<p>另外，作者还训练了一个简单的两层MLP。mlp使用训练集做训练，然后对使用训练好的MLP来预测测试节点的标签。称这些标签为伪标签。最后，利用MLP预测置信度较高的测试节点计算测试节点CE损失项。</p>
<p>设$Y_ {P L}$为MLP预测置信度较高的测试节点集，$Y_ {mlp}$为MLP的预测值。CE损失为：<br>$$<br>\mathcal{L} _ 1(f({A’,X},\Theta))&#x3D;-\sum_ {l\in\mathcal{Y}_ {TL}}\sum_ {j&#x3D;1}^C Y_ {mlp_ {l_j}}\log(Z_ {l_j})<br>$$<br>最终loss为：<br>$$<br>\begin{align}<br>\mathcal{L}_ {ARGS}&#x3D;\alpha\mathcal{L}_ {0}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\beta\mathcal{L}_ {fs}(m_ {g}\odot A’,X)\\+\gamma\mathcal{L}_ {1}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\lambda_ {1}||m_ {g}||_ {1}+\lambda_ {2}||m_ {\theta}||_ {1}<br>\end{align}<br>$$<br>其中，$\alpha$和$\gamma$设置为1。$m_g$用于领接矩阵，$m_ \theta$用于模型权重。</p>
<h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p><a target="_blank" rel="noopener" href="https://github.com/SubhajitDuttaChowdhury/ARGS">github</a></p>
<p>完全基于UGS的代码，有大量相同的地方。</p>
<p>和UGS一样，主函数也是包括</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> .....</span><br><span class="line">	run_get_mask</span><br><span class="line">    XXXX<span class="comment">###从final_mask_dict中保存mask到rewind_weight，剪枝但保持其他权重和初始化一样</span></span><br><span class="line">    run_fix_mask</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>run_get_mask函数不同点：run_get_mask中加入了平滑项和伪标签的分类误差。</p>
<p>即loss为<br>$$<br>\begin{align}<br>\mathcal{L}_ {run_get_mask}&#x3D;\alpha\mathcal{L}_ {0}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\beta\mathcal{L}_ {fs}(m_ {g}\odot A’,X)\\<br>+\gamma\mathcal{L}_ {1}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\lambda_ {1}||m_ {g}||_ {1}+\lambda_ {2}||m_ {\theta}||_ {1}<br>\end{align}<br>$$</p>
<p>run_fix_mask函数不同点：run_fix_mask中加入了伪标签的分类误差。</p>
<p>即loss为<br>$$<br>\mathcal{L}_ {run_fix_mask}&#x3D;\alpha\mathcal{L}_ {0}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\gamma\mathcal{L}_ {1}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))<br>$$</p>
<h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><p>pass</p>
<h2 id="《inductive-lottery-ticket-learning-for-graph-neural-networks-under-review-pdf》"><a href="#《inductive-lottery-ticket-learning-for-graph-neural-networks-under-review-pdf》" class="headerlink" title="《inductive lottery ticket learning for graph neural networks(under review).pdf》"></a>《inductive lottery ticket learning for graph neural networks(under review).pdf》</h2><p>Accepted by JCST 2023</p>
<p>Rejected by ICLR 2022</p>
<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>过往的有以下缺点</p>
<p>1）也就是说，边缘遮罩被限制在给定的图中，使得UGS在归纳设置中不可行，因为边缘遮罩很难推广到看不见的边或全新的图。</p>
<p>2)对每条边单独应用掩码只能提供对边缘的局部理解，而不是整个图的全局视图(例如，在节点分类中)或多个图(例如，在图分类中)</p>
<p>此外，创建可训练边缘掩模的方式会使gnn的参数加倍，这在某种程度上违背了修剪的目的。</p>
<p>因此，这些边缘掩模可能是次优的，以指导修剪。(3)不理想的图剪枝会对模型权值的剪枝产生负面影响。更糟糕的是，低质量的权值剪枝会反过来放大边缘掩模的误导信号。它们相互影响，形成恶性循环。我们将所有这些UGS的局限性归因于它的转导性质。因此，在归纳设置中进行组合修剪对于高质量中奖彩票至关重要。</p>
<h3 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h3><p><img src="/2024/20240521/image-20240518132924951.png" alt="image-20240518132924951"></p>
<p>本文提出了一个AutoMasker，具体而言，他设计了一套网络用来生成mask的选择。</p>
<p>它使用一个GNN $g(·)$来获取每个节点的 representations。</p>
<p>$H&#x3D;g(A,X)$</p>
<p>每一行代表着节点的representation。故可由计算节点的重要性，<br>$$<br>s_ {ij}&#x3D;\sigma{(\alpha_ {ij})},a_ {ij} &#x3D; MLP([h_i,h_j])<br>$$</p>
<p>对于图，我们采用AutoMasker来预测每个图的所有边的重要性。然后根据掩码值对某图的边进行排序，对最小值为5%的边进行剪接，得到二值图掩码mG。</p>
<p>对于GNN，我们根据权重量级对参数进行排序，并对最低量级的参数进行20%的修剪，得到二值模型掩码mΘ。在当前的稀疏度水平下，我们现在成功地得到了模型的稀疏化图g0 &#x3D; (mG A, X)和稀疏化掩码mΘ。</p>
<p>最后，我们需要检查稀疏性是否满足我们的条件。如果满足稀疏性，则算法完成;如果没有，我们需要重用找到的GLT来更新原始图和GNN模型，并迭代使用步骤1和步骤2(图1中虚线箭头)，直到满足条件。</p>
<p><img src="/2024/20240521/image-20240518133345382.png" alt="image-20240518133345382"></p>
<h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><h4 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h4><p>GAT:</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GATNet</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, train_dataset</span>):</span><br><span class="line">        <span class="built_in">super</span>(GATNet, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GATConv(train_dataset.num_features, <span class="number">256</span>, heads=<span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lin1 = torch.nn.Linear(train_dataset.num_features, <span class="number">4</span> * <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = GATConv(<span class="number">4</span> * <span class="number">256</span>, <span class="number">256</span>, heads=<span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lin2 = torch.nn.Linear(<span class="number">4</span> * <span class="number">256</span>, <span class="number">4</span> * <span class="number">256</span>)</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = GATConv(<span class="number">4</span> * <span class="number">256</span>, train_dataset.num_classes, heads=<span class="number">6</span>, concat=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lin3 = torch.nn.Linear(<span class="number">4</span> * <span class="number">256</span>, train_dataset.num_classes)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index, data_mask=<span class="literal">None</span></span>):</span><br><span class="line">        x = F.elu(<span class="variable language_">self</span>.conv1(x, edge_index, edge_weight=data_mask) + <span class="variable language_">self</span>.lin1(x))</span><br><span class="line">        x = F.elu(<span class="variable language_">self</span>.conv2(x, edge_index, edge_weight=data_mask) + <span class="variable language_">self</span>.lin2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.conv3(x, edge_index, edge_weight=data_mask) + <span class="variable language_">self</span>.lin3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>Masker</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Masker</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, train_dataset, hidden=<span class="number">128</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Masker, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv1 = GATConv(train_dataset.num_features, hidden, heads=<span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lin1 = torch.nn.Linear(train_dataset.num_features, <span class="number">4</span> * hidden)</span><br><span class="line">        <span class="variable language_">self</span>.conv2 = GATConv(<span class="number">4</span> * hidden, hidden, heads=<span class="number">4</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lin2 = torch.nn.Linear(<span class="number">4</span> * hidden, <span class="number">4</span> * hidden)</span><br><span class="line">        <span class="variable language_">self</span>.conv3 = GATConv(<span class="number">4</span> * hidden, hidden, heads=<span class="number">6</span>, concat=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.lin3 = torch.nn.Linear(<span class="number">4</span> * hidden, hidden)</span><br><span class="line">        <span class="variable language_">self</span>.mlp = torch.nn.Linear(hidden * <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.sigmoid = torch.nn.Sigmoid()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        x = F.elu(<span class="variable language_">self</span>.conv1(x, edge_index) + <span class="variable language_">self</span>.lin1(x))</span><br><span class="line">        x = F.elu(<span class="variable language_">self</span>.conv2(x, edge_index) + <span class="variable language_">self</span>.lin2(x))</span><br><span class="line">        x = <span class="variable language_">self</span>.conv3(x, edge_index) + <span class="variable language_">self</span>.lin3(x)</span><br><span class="line">        link_score = <span class="variable language_">self</span>.concat_mlp_score(x, edge_index)</span><br><span class="line">        <span class="keyword">return</span> link_score</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">concat_mlp_score</span>(<span class="params">self, x, edge_index</span>):</span><br><span class="line">        row, col = edge_index</span><br><span class="line">        link_score = torch.cat((x[row], x[col]), dim=<span class="number">1</span>)</span><br><span class="line">        link_score = <span class="variable language_">self</span>.mlp(link_score)</span><br><span class="line">        link_score = <span class="variable language_">self</span>.sigmoid(link_score).view(-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> link_score</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>GAT和Masker相比，masker的隐藏层更小，多了inner_product_score（上文省略了）和concat_mlp_score的函数。GAT最后一层是分类器，Masker最后一层输出边的分数。</p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam([{<span class="string">'params'</span>: model.parameters(), <span class="string">'lr'</span>: <span class="number">0.005</span>},</span><br><span class="line">                                  {<span class="string">'params'</span>: masker.parameters(), <span class="string">'lr'</span>: masker_lr}])<span class="comment">#不同学习率</span></span><br><span class="line"><span class="comment">####省略</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, total_epoch + <span class="number">1</span>):</span><br><span class="line">        loss, mask_distribution = train_model_and_masker(model, masker, optimizer, train_loader)</span><br><span class="line">        <span class="comment">##评估省略</span></span><br><span class="line">pruning.pruning_model(model, <span class="number">0.2</span>, random=<span class="literal">False</span>)</span><br><span class="line">_ = pruning.see_zero_rate(model)</span><br><span class="line">model_mask_dict = pruning.extract_mask(model)</span><br><span class="line"></span><br><span class="line">masker.load_state_dict(best_masker_state_dict)</span><br><span class="line">pruning.grad_model(masker, <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_dataset_pru = pruning.masker_pruning_dataset(train_dataset_pru, masker, <span class="number">1</span>, <span class="number">0.05</span>)</span><br><span class="line">val_dataset_pru = pruning.masker_pruning_dataset(val_dataset_pru, masker, <span class="number">2</span>, <span class="number">0.05</span>)</span><br><span class="line">test_dataset_pru = pruning.masker_pruning_dataset(test_dataset_pru, masker, <span class="number">2</span>, <span class="number">0.05</span>)</span><br><span class="line"><span class="comment">##省略print</span></span><br><span class="line">things_dict[<span class="string">'train_dataset_pru'</span>] = train_dataset_pru </span><br><span class="line">things_dict[<span class="string">'val_dataset_pru'</span>] = val_dataset_pru </span><br><span class="line">things_dict[<span class="string">'test_dataset_pru'</span>] = test_dataset_pru </span><br><span class="line">things_dict[<span class="string">'rewind_weight'</span>] = rewind_weight</span><br><span class="line">things_dict[<span class="string">'rewind_weight2'</span>] = rewind_weight2</span><br><span class="line">things_dict[<span class="string">'model_mask_dict'</span>] = model_mask_dict</span><br></pre></td></tr></tbody></table></figure></div></figure>

<h5 id="train-model-and-masker函数"><a href="#train-model-and-masker函数" class="headerlink" title="train_model_and_masker函数"></a>train_model_and_masker函数</h5><figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">loss_op = torch.nn.BCEWithLogitsLoss()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model_and_masker</span>(<span class="params">model, masker, optimizer, train_loader</span>):</span><br><span class="line">	<span class="comment">###省略</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    mask_distribution = []</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        data = data.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        data_mask = masker(data.x, data.edge_index)</span><br><span class="line">        mask_distribution.append(pruning.plot_mask(data_mask))</span><br><span class="line">        out = model(data.x, data.edge_index, data_mask)</span><br><span class="line">        loss = loss_op(out, data.y)</span><br><span class="line">        total_loss += loss.item() * data.num_graphs</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    mask_distribution = torch.tensor(mask_distribution).mean(dim=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(train_loader.dataset), mask_distribution</span><br></pre></td></tr></tbody></table></figure></div></figure>

<p>和UGS的过程其实差不多，权重&#x3D;mask*权重，使用CEloss进行训练。UGS的mask为网络中的参数，而该算法的mask则由另一套神经网络生成。</p>
<h5 id="pruning-model"><a href="#pruning-model" class="headerlink" title="pruning_model"></a>pruning_model</h5><p>本部分使用了pytorch 的torch.nn.utils.prune</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pruning_model</span>(<span class="params">model, px, random=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> px == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        parameters_to_prune =[]</span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">                parameters_to_prune.append((m,<span class="string">'weight'</span>))</span><br><span class="line">                <span class="comment"># print(m)</span></span><br><span class="line">        </span><br><span class="line">        parameters_to_prune = <span class="built_in">tuple</span>(parameters_to_prune)</span><br><span class="line">        <span class="keyword">if</span> random:</span><br><span class="line">            prune.global_unstructured(</span><br><span class="line">                parameters_to_prune,</span><br><span class="line">                pruning_method=prune.RandomUnstructured,</span><br><span class="line">                amount=px,</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prune.global_unstructured(</span><br><span class="line">                parameters_to_prune,</span><br><span class="line">                pruning_method=prune.L1Unstructured,</span><br><span class="line">                amount=px,</span><br><span class="line">            )</span><br></pre></td></tr></tbody></table></figure></div></figure>

<blockquote>
<p>L1：基于权重绝对值</p>
<p>random：完全随机</p>
</blockquote>
<h5 id="grad-model"><a href="#grad-model" class="headerlink" title="grad_model"></a>grad_model</h5><p>源代码为<code>pruning.grad_model(masker, False)</code>，冻结梯度</p>
<figure class="codeblock codeblock--tabbed"><figcaption><ul class="tabs"><li class="tab active">python</li></ul></figcaption><div class="tabs-content"><figure class="highlight python" style="display: block;"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">grad_model</span>(<span class="params">model, fix=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        param.requires_grad = fix</span><br></pre></td></tr></tbody></table></figure></div></figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>图彩票论文速览</div>
      <div>https://lijianxiong.space/2024/20240521/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>LJX</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年5月21日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/20240605/" title="Softmax Linear Units Softmax">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Softmax Linear Units Softmax</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/20240520/" title="信息瓶颈与在图中的应用">
                        <span class="hidden-mobile">信息瓶颈与在图中的应用</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <span>LJX</span> <i class="iconfont icon-love"></i> <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> 
    </div>
  

  
    <div style="margin-top: 8px;font-size: 16px;"> 
      博客已经运行 <span id="daysSinceLJXCustomDate"></span> 天
    </div>

  </div>
  



  

  
</div>


<script>
function displayDaysSinceLJX() {
  // 将起始日期设置为 '2019-03-14' 的零点（当地时区）
  const startDate = new Date('2019-03-14T00:00:00'); 
  const currentDate = new Date();
  
  // 获取当前日期的零点（当地时区），以确保我们计算的是完整的天数
  const currentDayStart = new Date(currentDate.getFullYear(), currentDate.getMonth(), currentDate.getDate());

  // 计算两个日期之间的毫秒差
  const timeDifference = currentDayStart.getTime() - startDate.getTime();
  
  // 将毫秒差转换为天数，并向下取整
  // 这样可以确保我们只计算已经过去的完整天数
  // 例如，如果今天是2019年3月14日（但还未到2019年3月15日的零点），则结果为0天
  let daysPassed = Math.floor(timeDifference / (1000 * 60 * 60 * 24));

  // 确保天数不为负（例如，如果客户端时间不正确导致当前日期早于起始日期）
  daysPassed = Math.max(0, daysPassed); 
  
  const daysElement = document.getElementById('daysSinceLJXCustomDate');
  if (daysElement) {
    daysElement.innerText = daysPassed;
  } else {
    console.error("Element with ID 'daysSinceLJXCustomDate' not found.");
  }
}

// 确保在DOM完全加载后执行脚本
if (document.readyState === 'loading') {
  document.addEventListener('DOMContentLoaded', displayDaysSinceLJX);
} else {
  // DOMContentLoaded 事件已经触发
  displayDaysSinceLJX();
}
</script>
  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>








  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<script src="https://s4.zstatic.net/ajax/libs/echarts/5.6.0/echarts.min.js"></script>
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const heatmapChartDom = document.getElementById('heatmapChart');
                if(heatmapChartDom){
                    const heatmapChart = echarts.init(heatmapChartDom, 'light');
                    const cellSize = [18, 18];
                    
                    const groupByYear = (data) => {
                        const result = {};
                        data.forEach(([date, value]) => {
                            const [year] = date.split('-').map(Number);
                            if (!result[year]) {
                                result[year] = [];
                            }
                            result[year].push([date, value]);
                        });
                        return result;
                    };
                    
                    const groupedData = groupByYear([["2019-03-14",1],["2019-03-23",1],["2019-04-30",1],["2019-06-24",1],["2019-07-18",1],["2019-07-27",1],["2019-07-29",1],["2019-08-09",1],["2019-08-10",1],["2019-08-15",1],["2019-08-31",1],["2020-01-11",1],["2020-04-26",1],["2020-09-05",1],["2020-10-09",1],["2020-10-22",1],["2020-10-25",1],["2020-10-26",1],["2020-11-15",1],["2020-11-17",1],["2020-12-07",1],["2020-12-10",1],["2020-12-21",1],["2020-12-31",1],["2021-01-07",1],["2021-01-15",1],["2021-01-22",2],["2021-02-24",1],["2021-03-09",1],["2021-03-13",1],["2021-03-26",1],["2021-03-31",1],["2021-04-05",1],["2021-04-10",1],["2021-04-11",1],["2021-04-15",1],["2021-05-09",1],["2021-05-14",1],["2021-05-30",1],["2021-06-06",1],["2021-06-07",1],["2021-06-08",1],["2021-06-12",1],["2021-06-14",1],["2021-07-08",1],["2021-07-10",1],["2021-07-16",1],["2021-07-18",1],["2021-07-24",1],["2021-07-27",1],["2021-08-05",1],["2021-08-13",1],["2021-08-25",1],["2021-08-28",1],["2021-08-29",1],["2021-09-19",1],["2021-09-21",1],["2021-10-09",1],["2021-11-02",1],["2022-01-11",1],["2022-01-18",1],["2022-01-19",1],["2022-01-20",1],["2022-01-21",1],["2022-01-22",1],["2022-02-01",1],["2022-02-24",1],["2022-02-25",1],["2022-02-27",1],["2022-02-28",1],["2022-03-09",1],["2022-03-11",1],["2022-03-16",1],["2022-03-28",1],["2022-04-03",1],["2022-04-08",1],["2022-04-17",1],["2022-04-29",1],["2022-04-30",1],["2022-05-05",1],["2022-05-07",1],["2022-05-08",1],["2022-05-09",1],["2022-05-13",1],["2022-05-14",1],["2022-05-15",1],["2022-05-22",1],["2022-05-31",1],["2022-06-18",1],["2022-06-27",1],["2022-07-08",1],["2022-07-10",1],["2022-07-13",1],["2022-07-19",1],["2022-07-20",1],["2022-07-30",1],["2022-08-01",1],["2022-08-15",1],["2022-08-17",1],["2022-08-18",1],["2022-08-19",1],["2022-10-09",1],["2022-10-12",1],["2022-10-13",1],["2022-11-22",1],["2022-11-26",1],["2022-11-28",1],["2023-01-01",1],["2023-01-16",1],["2023-01-18",1],["2023-01-20",1],["2023-01-26",1],["2023-01-28",1],["2023-02-04",1],["2023-02-06",1],["2023-02-21",1],["2023-03-19",1],["2023-03-20",1],["2023-03-23",1],["2023-03-24",1],["2023-03-27",1],["2023-03-30",1],["2023-04-01",1],["2023-04-02",1],["2023-06-29",1],["2023-07-01",1],["2023-07-03",1],["2023-07-05",1],["2023-07-08",1],["2023-07-11",2],["2023-07-18",1],["2023-08-02",1],["2024-01-13",1],["2024-01-16",1],["2024-01-31",1],["2024-02-06",1],["2024-02-11",1],["2024-02-14",1],["2024-02-19",1],["2024-02-26",1],["2024-02-27",1],["2024-02-29",1],["2024-03-03",1],["2024-03-08",1],["2024-04-03",1],["2024-04-10",1],["2024-04-29",1],["2024-05-11",1],["2024-05-12",1],["2024-05-13",1],["2024-05-16",1],["2024-05-20",1],["2024-05-21",1],["2024-06-05",1],["2024-06-29",1],["2024-07-08",1],["2024-08-11",1],["2024-08-14",1],["2024-08-24",1],["2024-08-28",1],["2024-08-30",1],["2024-10-06",1],["2024-11-01",1],["2024-12-16",1],["2024-12-26",1],["2025-01-10",1],["2025-01-15",1],["2025-02-10",1],["2025-02-15",1],["2025-02-16",1],["2025-02-17",1],["2025-02-20",1],["2025-02-22",1],["2025-02-24",1],["2025-03-15",1],["2025-03-19",1],["2025-03-21",1],["2025-04-24",1],["2025-04-25",1],["2025-04-26",1],["2025-04-27",1],["2025-05-01",1],["2025-05-02",1],["2025-05-03",1],["2025-05-11",1],["2025-05-12",1],["2025-05-13",1],["2025-05-15",1],["2025-05-17",1],["2025-05-18",1],["2025-05-20",1],["2025-05-21",1],["2025-05-22",1],["2025-05-23",1],["2025-05-26",1],["2025-05-27",1],["2025-05-30",1],["2025-06-01",1],["2025-06-03",1],["2025-06-06",1],["2025-06-09",1],["2025-06-10",1],["2025-06-13",1],["2025-06-15",1],["2025-06-17",1],["2025-07-05",1],["2025-07-06",1],["2025-07-11",1],["2025-07-14",1],["2025-07-16",1],["2025-07-19",1],["2025-07-20",1],["2025-07-22",1],["2025-07-24",1],["2025-07-26",1],["2025-07-27",1],["2025-08-05",1],["2025-08-07",1],["2025-08-10",1],["2025-08-12",1],["2025-08-13",1],["2025-08-17",1],["2025-08-26",1],["2025-08-29",1],["2025-08-31",1],["2025-09-02",1],["2025-09-03",1],["2025-09-07",1],["2025-09-08",1],["2025-09-10",1],["2025-09-13",1],["2025-09-14",1],["2025-09-15",1],["2025-09-17",1],["2025-09-19",1],["2025-09-26",1],["2025-09-27",1],["2025-09-28",1],["2025-10-06",1],["2025-10-11",1],["2025-10-14",1],["2025-10-16",1],["2025-10-21",1],["2025-10-22",1],["2025-10-26",1],["2025-10-29",1],["2025-10-31",1],["2025-11-01",1],["2025-11-02",1],["2025-11-03",1],["2025-11-04",1],["2025-11-05",3],["2025-11-06",1],["2025-11-11",1],["2025-11-12",1],["2025-11-13",1],["2025-11-17",1],["2025-11-18",1],["2025-11-20",1],["2025-11-24",1],["2025-11-26",1],["2025-11-27",1],["2025-11-28",1],["2025-11-29",1],["2025-12-02",1],["2025-12-03",1],["2025-12-04",1],["2025-12-05",1],["2025-12-10",1],["2025-12-11",1],["2025-12-12",1],["2025-12-13",1],["2025-12-14",2],["2025-12-15",1],["2025-12-16",1],["2025-12-17",1],["2025-12-18",2],["2025-12-24",1],["2025-12-25",1],["2025-12-27",1],["2025-12-28",2],["2025-12-31",2],["2026-01-06",1],["2026-01-10",1],["2026-01-14",1],["2026-01-15",1],["2026-02-02",1],["2026-02-03",1],["2026-02-05",1],["2026-02-06",3],["2026-02-08",2],["2026-02-09",3],["2026-02-11",1],["2026-02-12",2],["2026-02-13",1],["2026-02-14",2],["2026-02-15",1],["2026-02-17",1]]);
                    const years = Object.keys(groupedData).reverse();
                    
                    var initYear = parseInt(heatmapChartDom.getAttribute('year')) || new Date().getFullYear();
                    const minYear = years[years.length - 1];
                    const maxYear = years[0];
                    if (initYear < minYear || initYear > maxYear) {
                        initYear = maxYear;
                    }
                    console.log('[hexo-graph]generateHeatmapChart|initYear:', initYear, 'minYear:', minYear, 'maxYear:', maxYear);
                    
                    heatmapChart.setOption({
                        grid: {},
                        tooltip: { 
                            position: 'top', 
                            formatter: params => `${params.value[0]}: ${params.value[1]} Articles` 
                        },
                        calendar: { 
                            top: '10%',
                            left: 'left', 
                            right: '8%',
                            range: initYear,
                            cellSize: cellSize, 
                            splitLine: { lineStyle: { color: '#E0E0E0', width: 1 } }, 
                            itemStyle: { borderWidth: 1, borderColor: '#E0E0E0' }, 
                            dayLabel: { show: false },
                            monthLabel: { show: true },
                            yearLabel: { show: false },
                        },
                        visualMap: { 
                            show: true,
                            right: '8%',
                            bottom: '5%',
                            type: 'piecewise',
                            orient: 'horizontal',
                            text: ['More', 'Less'],
                            min: 0,
                            max: Math.max(...groupedData[initYear].map(item => item[1])),
                            inRange: { color: ["#ACE7AE","#69C16D","#549F57"] }
                        },
                        legend: {
                            type: 'scroll',
                            icon: 'none',
                            data: years,
                            orient: 'vertical',
                            top: '5%',
                            right: 'right',
                            itemWidth: 20,
                            itemHeight: 20,
                            itemGap: 10,
                            pageIconSize: 10,
                            pageTextStyle: { fontSize: 14 },
                            selectedMode: 'single',
                        },
                        series: years.map(year => ({
                            type: 'heatmap',
                            coordinateSystem: 'calendar',
                            data: groupedData[year],
                            name: year,
                            emphasis: {
                                disabled: true,
                            },
                            silent: year !== initYear,
                        })),
                    });
                    
                    // init selected year
                    heatmapChart.dispatchAction({
                        type: 'legendSelect',
                        name: initYear,
                    });
                    
                    heatmapChart.on('legendselectchanged', function(params) {
                        console.log('[hexo-graph]generateHeatmapChart|legendselectchanged:', params);
                        const selectedYear = Object.keys(params.selected).find(key => params.selected[key]);
                        if (selectedYear && groupedData[selectedYear]) {
                            heatmapChart.setOption({
                                calendar: {
                                    range: selectedYear,
                                },
                                visualMap: {
                                    max: Math.max(...groupedData[selectedYear].map(item => item[1])),
                                },
                                series: years.map(year => ({
                                    type: 'heatmap',
                                    coordinateSystem: 'calendar',
                                    data: groupedData[year],
                                    name: year,
                                    emphasis: {
                                        disabled: true,
                                    },
                                    silent: year !== selectedYear,
                                })),
                            });
                        }
                    });
                    
                    heatmapChart.on('click', function (params) {
                        if (params.componentType === 'series') {
                            const [year, month] = params.value[0].split('-');
                            window.location.href = '/archives/' + year + '/' + month;
                        }
                    });
                }
            });
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const monthlyChartDom = document.getElementById('monthlyChart');
                if(monthlyChartDom){
                    const monthlyChart = echarts.init(monthlyChartDom, 'light');
                    monthlyChart.setOption({
                        xAxis: { 
                            type: 'category', 
                            data: ["2019-03","2019-04","2019-06","2019-07","2019-08","2020-01","2020-04","2020-09","2020-10","2020-11","2020-12","2021-01","2021-02","2021-03","2021-04","2021-05","2021-06","2021-07","2021-08","2021-09","2021-10","2021-11","2022-01","2022-02","2022-03","2022-04","2022-05","2022-06","2022-07","2022-08","2022-10","2022-11","2023-01","2023-02","2023-03","2023-04","2023-06","2023-07","2023-08","2024-01","2024-02","2024-03","2024-04","2024-05","2024-06","2024-07","2024-08","2024-10","2024-11","2024-12","2025-01","2025-02","2025-03","2025-04","2025-05","2025-06","2025-07","2025-08","2025-09","2025-10","2025-11","2025-12","2026-01","2026-02"], 
                            axisLabel: { fontSize: 14, fontWeight: 'bold', fontFamily: 'Microsoft YaHei, SimSun, serif' }
                        },
                        yAxis: { type: 'value', splitLine: { lineStyle: { type: 'dashed', color: '#ccc' } } },
                        series: [{
                            name: 'Articles',
                            type: 'line',
                            data: [2,1,1,3,4,1,1,1,4,2,4,4,1,4,4,3,5,6,5,2,1,1,6,5,4,5,9,2,6,5,3,3,6,3,6,2,1,7,1,3,7,2,3,6,2,1,5,1,1,2,2,7,3,4,16,8,11,9,13,9,19,22,4,19],
                            smooth: true,
                            lineStyle: { color: '#5470C6', width: 2 },
                            itemStyle: { color: '#5470C6' },
                            areaStyle: { color: 'rgba(84, 112, 198, 0.4)' },
                            symbolSize: 10,
                            label: {
                                show: true,
                                position: 'top',
                                formatter: params => params.value,
                                fontSize: 14,
                                color: '#000',
                                fontWeight: 'bold',
                                fontFamily: 'Microsoft YaHei, SimSun, serif'
                            }
                        }]
                    });

                    monthlyChart.on('click', function (params) {
                        const [year, month] = params.name.split('-');
                        window.location.href = '/archives/' + year + '/' + month;
                    });
                }
            })
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const tagsChartDom = document.getElementById('tagsChart');
                if(tagsChartDom){
                    const tagsChart = echarts.init(tagsChartDom, 'light');
                    tagsChart.setOption({
                        tooltip: { trigger: 'item', formatter: '{b}: {c} ({d}%)' },
                        series: [{
                            type: 'pie',
                            radius: '60%',
                            data: [{"name":"深度学习","value":174},{"name":"大模型","value":79},{"name":"人工智能","value":46},{"name":"机器学习","value":33},{"name":"笔记","value":33},{"name":"多模态","value":29},{"name":"数学","value":25},{"name":"图神经网络","value":17}],
                            label: {
                                position: 'outside',
                                formatter: '{b} {c} ({d}%)',
                                fontSize: 14,
                                fontWeight: 'bold',
                                fontFamily: 'Microsoft YaHei, SimSun, serif'
                            },
                            color: ["#5470C6","#91CC75","#FAC858","#EE6666","#73C0DE","#3BA272","#FC8452","#9A60B4"],
                            labelLine: { show: true }
                        }],
                        legend: {
                            bottom: '0',
                            left: 'center',
                            data: [{"name":"深度学习","value":174},{"name":"大模型","value":79},{"name":"人工智能","value":46},{"name":"机器学习","value":33},{"name":"笔记","value":33},{"name":"多模态","value":29},{"name":"数学","value":25},{"name":"图神经网络","value":17}].map(tag => tag.name),
                            textStyle: { fontSize: 14, fontWeight: 'bold', fontFamily: 'Microsoft YaHei, SimSun, serif' }
                        }
                    });

                    tagsChart.on('click', function (params) {
                        window.location.href = '/tags/' + params.name;
                    });
                }
            })
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const categoriesChartDom = document.getElementById('categoriesChart');
                if(categoriesChartDom){
                    const categoriesChart = echarts.init(categoriesChartDom, 'light');
                    categoriesChart.setOption({
                        xAxis: { type: 'value', splitLine: { lineStyle: { type: 'dashed', color: '#ccc' } } },
                        yAxis: { 
                            type: 'category', 
                            data: [].map(category => category.name).reverse(), 
                            axisLabel: { fontSize: 14, fontWeight: 'bold', fontFamily: 'Microsoft YaHei, SimSun, serif' }
                        },
                        series: [{
                            name: 'Category Count',
                            type: 'bar',
                            data: [].map(category => category.value).reverse(),
                            label: {
                                show: true,
                                position: 'right',
                                formatter: params => params.value,
                                fontSize: 14,
                                color: '#000',
                                fontWeight: 'bold',
                                fontFamily: 'Microsoft YaHei, SimSun, serif'
                            },
                            itemStyle: {
                                color: new echarts.graphic.LinearGradient(0, 0, 1, 0, [
                                    { offset: 0, color: '#91CC75' },
                                    { offset: 1, color: '#73C0DE' }
                                ])
                            }
                        }]
                    });

                    categoriesChart.on('click', function (params) {
                        window.location.href = '/categories/' + params.name;
                    });
                }
            });
        </script>
    
        
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                const categoriesTreeChartDom = document.getElementById('categoriesTreeChart');
                if(categoriesTreeChartDom){
                    const treeChart = echarts.init(categoriesTreeChartDom, 'light');
                    treeChart.setOption({
                        title: {
                            text: '操作提示：单击展开分类，双击进入具体分类页面',
                            textStyle: {
                                fontSize: 12,
                                color: '#999',
                                fontWeight: 'normal'
                            },
                            bottom: 0,
                            left: 'center'
                        },
                        tooltip: {
                            trigger: 'item',
                            triggerOn: 'mousemove'
                        },
                        series: [{
                            type: 'tree',
                            data: [{"name":"Categories","children":[],"count":0,"path":""}],
                            initialTreeDepth: -1,
                            top: '5%',
                            bottom: '10%',
                            left: '0%',
                            right: '0%',
                            symbolSize: 15,
                            layout: 'orthogonal',
                            orient: 'TB',
                            itemStyle: {
                                color: '#91CC75',
                                borderColor: '#73C0DE'
                            },
                            label: {
                                position: 'bottom',
                                verticalAlign: 'middle',
                                align: 'center',
                                fontSize: 14,
                                distance: 28,
                                formatter: function(params) {
                                    return params.data.name + (params.data.count ? ' (' + params.data.count + ')' : '');
                                }
                            },
                            leaves: {
                                label: {
                                    position: 'top',
                                    verticalAlign: 'middle',
                                    align: 'center'
                                }
                            },
                            emphasis: {
                                focus: 'descendant'
                            },
                            expandAndCollapse: true
                        }]
                    });

                    treeChart.on('dblclick', function (params) {
                        if (params.data && params.data.path) {
                            window.location.href = '/categories/' + params.data.path;
                        }
                    });
                }
            });
        </script>
    
    <script>
  $(document).ready(function() {
    $('figure.codeblock').find('.tab').click(function() {
        var $codeblock = $(this).parent().parent().parent();
        var $tab = $(this);
        // remove "active" css class on all tabs
        $tab.siblings().removeClass('active');
        // add "active" css class on the clicked tab
        $tab.addClass('active');
        // hide all tab contents
        $codeblock.find('.highlight').hide();
        // show only the right one
        $codeblock.find('.highlight.' + $tab.text()).show();
    });
  });
  </script></body>
</html>
