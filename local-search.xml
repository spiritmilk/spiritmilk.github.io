<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>幻觉与越狱的一致性</title>
    <link href="/2025/20251104/"/>
    <url>/2025/20251104/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2026在投）</p><p>原标题是《从幻觉到越狱：重新思考大型基础模型的脆弱性》</p><p>作者提出一个统一的理论框架，将越狱行为建模为 token 级别的最优化，将幻觉现象建模为注意力级别的最优化。</p><p>在此框架下，作者建立了两个关键命题：</p><p>(1) 相似损失收敛——在优化目标特定输出时，两种漏洞的损失函数表<br>现出相似的收敛特性</p><p> (2) 注意力重分配中的梯度一致性——两者均表现出由共享注意力动态驱动的一致梯度行为。</p><span id="more"></span>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>少即是多：从 EOS 决策角度缓解多模态幻觉</title>
    <link href="/2025/20251102/"/>
    <url>/2025/20251102/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ACL 2024）</p><span id="more"></span><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>一个关于多模态模型（LVLMs）产生“幻觉”（即生成与图像不符的文本内容）的新观点 。作者认为，一个常被忽视的关键原因是<strong>训练数据“过度详细”</strong> 。</p><p>即许多训练数据（如LLaVA的 instruction data）包含了极其丰富的细节，这些细节甚至<strong>超出了模型当前的视觉感知能力</strong> 。模型在训练时，为了强行拟合这些它“看”不到的细节（为了匹配标注的长度和详细程度），被迫开始“凭空编造”内容，从而导致了幻觉 。</p><p><img src="/2019/20190430/1.png"></p><p>作者通过实验发现：预测普通内容的时候，模型更依赖当前句子的信息。预测$v_{EOS}$时，模型显著更依赖于“所有前面的句子”。</p><p>（使用的是显著性矩阵$I&#x3D;\left|A\odot \frac{\partial\mathcal{L}(x)}{\partial A} \right|$）</p><p><img src="/2019/20190430/2.png"></p><p>另外，作者也去消除语义和增加语义以探索$v_{EOS}$的变化。</p><ul><li>image-：应用高斯噪声掩码</li><li>image+：将图像与随机图像拼接，以引入当前文本中未描述的视觉信息。（作者还实现了一种变体，将输入图像替换为随机新图像而非进行拼接，以避免增加绝对信息丰富度）</li><li>text-：使用注意力掩码来隐藏部分暴露的文本。作者使用的是掩码前 30个 token，以确保序列末尾部分$v_{EOS}$预测的相邻上下文连贯性。</li></ul><p><img src="/2019/20190430/3.png"></p><p>这些实验证实，模型确实在通过比较“已生成的文本”和“感知到的视觉信息”来评估完整性，并以此决定是否停止 。模型天生就具备这种根据视觉感知极限“适可而止”的潜力 。</p><h3 id="标准训练（MLE）的缺陷"><a href="#标准训练（MLE）的缺陷" class="headerlink" title="标准训练（MLE）的缺陷"></a>标准训练（MLE）的缺陷</h3><p>标准的“最大似然估计”（MLE）训练目标会破坏上述的“适可而止”机制。</p><p>在训练时，当模型在某个位置（比如句子中间）认为“我已经描述完了”（$p_{v_{EOS}}$ 概率很高），但此时的标签（label）并不是 $v_{EOS}$，而是一个普通的内容 token（因为训练数据过度详细，要求它继续说） 。</p><p>在这种情况下，MLE 会惩罚模型产生 $v_{EOS}$ 的想法，迫使它降低 $p_{v_{EOS}}$，并鼓励它继续生成内容。这就等于在训练模型“<strong>即使超出了你的视觉感知极限，也要继续说</strong>”，从而加剧了幻觉。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="选择性EOS监督（Selective-EOS-Supervision-SES）"><a href="#选择性EOS监督（Selective-EOS-Supervision-SES）" class="headerlink" title="选择性EOS监督（Selective EOS Supervision, SES）"></a>选择性EOS监督（Selective EOS Supervision, SES）</h3><p>这是一种改进的训练目标（Learning Objective）。</p><p><strong>目标</strong>： 在不影响学习内容生成的同时，保护模型在非 $v_{EOS}$ 位置上“想要停止”的内在倾向。</p><p>具体做法：</p><ol><li><p><strong>当标签是 $v_{EOS}$ 时：</strong> 保持原样，使用标准 MLE 训练，让模型学会何时该停止。</p></li><li><p><strong>当标签不是 $v_{EOS}$ 时：</strong> 修改 softmax 计算。在计算概率分布时，<strong>将 $v_{EOS}$ token 排除在外</strong> 。</p></li></ol><h3 id="EOS监督评分（Scoring-EOS-Supervision）"><a href="#EOS监督评分（Scoring-EOS-Supervision）" class="headerlink" title="EOS监督评分（Scoring EOS Supervision）"></a>EOS监督评分（Scoring EOS Supervision）</h3><p>这是一种数据过滤策略（Data Filtering Strategy）。</p><p><strong>目标：</strong> 识别并<strong>过滤掉那些“有害”的</strong>（即会严重抑制模型 EOS 倾向的）训练数据。</p><p><strong>具体做法：</strong><br>设计两个分数来评估每条数据 ：</p><ol><li><p><strong>$S_{pos}$（正面效应）：</strong> 评估数据在“应该停止”的位置（$y &#x3D; v_{EOS}$）教会模型停止的能力。$S_{pos}$ 越高，说明模型在此处本不想停而数据强迫它停，<strong>正面效应越强</strong> 。</p></li><li><p><strong>$S_{neg}$（负面效应）：</strong> 评估数据在“不该停止”的位置（$y \ne v_{EOS}$）抑制模型停止倾向的程度。$S_{neg}$ 越高，说明模型在此处本想停而数据强迫它继续说，<strong>负面效应（危害）越大</strong> 。</p></li></ol><p>$S_{final} &#x3D; S_{neg} - S_{pos}$ 。$S_{final}$ 越高的数据，被认为“危害越大”。</p><p>移除训练集中 $S_{final}$ 分数最高的数据（例如，移除Top 20%）。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>性能提升的幻觉：为什么对比解码无法缓解多模态大模型中的对象幻觉问题？</title>
    <link href="/2025/20251101/"/>
    <url>/2025/20251101/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><p>（NIPS 2025）</p><p>作者认为，目前的一些对比解码方法在POPE等基准测试上观察到的性能提升是一种假象 ，并非真正解决了幻觉问题。</p><span id="more"></span><p>在 POPE 基准上观察到的性能提升主要由两个误导性因素驱动：</p><p>R1 ：一种单向调整输出分布的方法，该方法简单地使模型更倾向于生成更多的“是”类输出，从而在某些数据集上实现分布平衡。<br>R2 ：这些方法中的自适应约束将采样解码策略退化为贪心搜索的近似，导致性能出现欺骗性的提升。</p><p><a href="https://github.com/ustc-hyin/cd_rethink">代码</a></p><h2 id="对比解码"><a href="#对比解码" class="headerlink" title="对比解码"></a>对比解码</h2><p>对比解码由两个部分组成。</p><p><strong>解码模块.</strong> $(1+\alpha)原始logits-\alpha\cdot 幻觉logits$</p><p>生成幻觉有分为指令级或者或者图像级或者模型级（从自我内部生成）。</p><p><img src="/2025/20251101/1.png"></p><p><strong>自适应合理性约束.</strong><br>$$<br>\begin{align}<br>\mathcal{V} _ {head}(y _ {&lt;t})&amp;&#x3D;(y_t\in\mathcal{V}\mid p_\theta(y_t\mid v,x,y _ {&lt;t})\ge \beta max_w p_\theta(w\mid v,x,y _ {&lt;t}))\\<br>p _ {cd}(y_t\mid v ,x)&amp;&#x3D;0\text{  if  }y_t \notin  \mathcal{V} _ {head}(y _ {&lt;t})<br>\end{align}<br>$$</p><h2 id="POPE"><a href="#POPE" class="headerlink" title="POPE"></a>POPE</h2><p> POPE是评估多模态大模型（MLLMs）的对象幻觉的数据集。与基于描述的方法不同，它将幻觉检测问题定义为一个二分类任务，采用直接的是或否问答形式（例如，“图像中是否有椅子？”），从而实现更清晰的解释性。该基准确保了“是”和“否”样本的均衡分布（各占 50%）。</p><h2 id="误导性性能提升"><a href="#误导性性能提升" class="headerlink" title="误导性性能提升"></a>误导性性能提升</h2><h3 id="单向的“Yes”偏见"><a href="#单向的“Yes”偏见" class="headerlink" title="单向的“Yes”偏见"></a>单向的“Yes”偏见</h3><p><img src="/2025/20251101/2.png"></p><p>MLLM（如LLaVA）在回答POPE问题时，其原始输出严重偏向于“否”（No）。</p><p>而VCD和SID 均显著地将模型输出分布向“Yes”倾斜。</p><p>对比解码这种“强行增加Yes”的策略，恰好中和了模型本身的“No”偏见，使得“Yes&#x2F;No”的比例趋于平衡（POPE的真实答案是50% Yes &#x2F; 50% No ），从而在<strong>表面上</strong>提高了准确率 。</p><p>在另一些模型已经偏向“Yes”的数据集（如GQA-Adversarial）上，使用对比解码会进一步加剧“Yes”偏见，反而导致性能下降 。</p><h3 id="采样解码退化为贪婪搜索"><a href="#采样解码退化为贪婪搜索" class="headerlink" title="采样解码退化为贪婪搜索"></a>采样解码退化为贪婪搜索</h3><p>且在有些论文中，认为贪心搜索在绝大多数任务上均优于直接采样。（出自《Contrastive decoding: Open-ended text generation as optimization》）。许多对比解码方法报告了采用采样策略带来的性能提升，这需要对这些说法进行更深入的审视。</p><p>当应用自适应合理性约束时，许多候选选项因不满足条件而被剔除。</p><p><img src="/2025/20251101/3.png"></p><p>比如上图中，LLaVA-v1.5-7B 模型生成的正确答案分布为：是：8.8%<br>和 否：91.2%。采用贪心搜索策略时，模型始终生成正确答案” 否”。然而，在采用采样策略时，模型有 8.8% 的概率生成错误答案” 是”。</p><p>当应用自适应合理性约束时，可能只剩下否，<strong>采样策略简化为贪心搜索</strong>。</p><h2 id="虚假改进的方法"><a href="#虚假改进的方法" class="headerlink" title="虚假改进的方法"></a>虚假改进的方法</h2><p>为了证明上述两个因素是“刷分”的真正原因，作者设计了两种虚假改进方法，这些方法完全不涉及幻觉缓解，只模仿那两个误导性因素。</p><p><strong>实验一：模仿“Yes”偏见</strong></p><ul><li><strong>方法A (PBA)</strong>：在提示词中加入“如果可能，请回答Yes” 。</li><li><strong>方法B (OLM)</strong>：在输出层修改，如果“Yes”和“No”的概率很接近，就强行改成“Yes” 。</li><li><strong>结果</strong>：在POPE上，这两种“作弊”方法取得了与VCD和SID相当甚至更好的性能提升 。</li></ul><p><img src="/2025/20251101/4.png"></p><p><strong>实验二：模仿“采样退化”</strong></p><ul><li><strong>方法</strong>：不使用完整的对比解码，只使用“自适应合理性约束”（APC）模块，并配合采样策略 。</li><li><strong>结果</strong>：仅仅使用APC这一个模块，就取得了和完整VCD、ICD、SID方法几乎相同的性能提升 。</li><li><strong>结论</strong>：这有力地证明了，在采样模式下，所有的性能提升都来自APC（即采样退化为贪婪搜索），而与对比解码逻辑无关 。</li></ul><p><img src="/2025/20251101/5.png"></p><h2 id="其他数据集"><a href="#其他数据集" class="headerlink" title="其他数据集"></a>其他数据集</h2><p>除了POPE的二分类数据集，还有CHAIR等幻觉评估数据集。作者事实上也做了一些实验。</p><p><img src="/2025/20251101/6.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于参数高效微调的可扩展确切机器遗忘方法</title>
    <link href="/2025/20251031/"/>
    <url>/2025/20251031/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ ICLR 2025）</p><p>SISA的改进版。</p><p>没想到25年居然还有涉及的论文。</p><span id="more"></span><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>机器遗忘技术分为两大类：近似遗忘和确切遗忘。</p><p>近似遗忘无法保证完全移除该样本的影响，其对模型仍可能保留非零影响。</p><p>确切遗忘能够确保从已训练模型中彻底移除某个数据样本的影响。确切遗忘技术采用模块化系统，系统中的不同组件分别在互不重叠的数据子集上进行训练。当发生删除请求时，仅需重新训练受影响的组件即可。然而，在真实场景中，即使暂停生产系统以重新训练单个组件，也可能导致服务中断并产生巨大开销。另一种选择是绕过受影响的组件继续运行，但这可能导致性能下降，最终影响终端用户。</p><p>核心思想是在部署初始模型之前进行额外的离线训练，以降低重新训练的成本。在$S^3T$的核心中，我们采用了一种新颖的轻量级微调方法，通过依次使用不相交的数据切片训练模型各层，实现参数隔离。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>SISA 的一个关键缺陷在于，如果删除请求影响到所有分片的切片 1，则整个服务将无法运行，必须从头开始重新训练。另一个缺点是，每当执行删除请求时，集成中的各个模型都需要重新训练。即使仅对单个切片进行重新训练，对于大规模模型在为大量用户提供生产级服务的情况下，成本也非常高昂。一种简单的替代方案是使用最后一个可用的检查点，并定期进行重新训练。显然，随着删除请求数量的增加，整体模型的性能将逐渐下降。</p><p>$S^3T$使用LoRA来描述，但是也不仅限于LoRA。</p><p>再回顾一下LoRA，它是一种低秩更新：<br>$$<br>W&#x3D;\bar W+X^TY<br>$$<br>$S^3T$的核心思想是使用不同的数据切片来训练不同的 LoRA 层。这种方法允许我们在删除某一切片的数据时，有选择性地禁用（置零）与该层相关的 LoRA 参数（X 或 Y）。</p><p>与SISA相似，它采用的也是使用不同的数据切片去训练。但是他是巧妙地将模型的不同层与不同的数据切片绑定。</p><p>它选择冻结前$i-1$层，使用$S_1+…+S_i$的数据，来训练第i层的LoRA参数。</p><p>$S^{3}T$ 的遗忘策略：</p><p>假设一个删除请求命中了 $S_3$。</p><ul><li><strong>SISA 必须：</strong> 加载 $S_2$ 的检查点，用 $S_4, S_5…$ 的数据重新训练。</li><li><strong>$S^{3}T$ 仅需：</strong> <strong>停用 (Deactivate)</strong>（即参数设为零）所有在训练中接触过 $S_3$ 数据的 LoRA 层。根据 $S^{3}T$ 的训练策略，第 1 层 (只用了 $S_1$) 和第 2 层 (只用了 $S_1+S_2$) <strong>没有</strong>受到 $S_3$ 的影响。因此，系统<strong>无需任何重新训练</strong>，只需停用第 3 层、第 4 层 … 第 L 层的 LoRA 参数即可。</li></ul><p><img src="/2025/20251031/1.png"></p><h3 id="序列感知-Sequence-Aware"><a href="#序列感知-Sequence-Aware" class="headerlink" title="序列感知 (Sequence-Aware)"></a>序列感知 (Sequence-Aware)</h3><p>以上的过程其实有很大的缺点，比如S3有问题，则往后的S4、S5都要被删除了。</p><p>所以引入了<strong>训练预算 (Budget, B)</strong> 。它不再是为每个分片只训练一个模型，而是训练 $B$ 个模型副本，每个副本使用<strong>不同排列顺序 (Permutation)</strong> 的数据切片进行训练。</p><p>当然我们不可能训练B&#x3D;L!种排列，所以我们需要序列感知。</p><p><strong>情况 1：均匀删除先验 (Uniform Deletion Prior)</strong> (不知道哪个切片更容易被删)</p><p>使用 <strong>迭代循环轮换 (Iterative Cyclic Rotation)</strong> 。</p><p>例如 (L&#x3D;3)：(S1, S2, S3), (S2, S3, S1), (S3, S1, S2) 。这保证了 $S_1, S_2, S_3$ 在每个位置（第1、2、3位）都只出现一次。</p><p><strong>情况 2：非均匀删除先验 (Non-uniform Deletion Prior)</strong> (知道某些切片（如新用户数据）更容易被删)</p><ul><li><strong>策略：</strong> 使用 <strong>基于二分图匹配的序列选择 (BMS)</strong> 。</li><li>这个算法的目标是，将那些<strong>最不可能被删除</strong>的切片，尽可能地排在序列的<strong>最前面</strong>（因为最前面的切片最敏感，一旦被删，整个模型报废） 。</li></ul><p>作者也从理论证明了$S^3T$的优越性：</p><p>首先定义删除率 (Deletion Rate)为一个系统在需要从头开始重新训练之前，预期可以处理的删除请求总数。</p><p><strong>引理 1 (Deletion Rate)：</strong></p><ul><li>SISA 的删除率: $\delta(SISA) \sim O(mL \log m)$。</li><li>$S^{3}T$ 的删除率: $\delta(S^{3}T) \sim O(mL \log(m \min(B,L)))$ 。</li><li><strong>结论：</strong> $S^{3}T$ 的删除率明显更高，并且随着预算 $B$ 的增加而提高（直到 $B&#x3D;L$ 为止）。</li></ul><p><strong>引理 2 (Performance Retention)：</strong></p><ul><li>$S^{3}T$ 在经历 $r$ 次删除后，保持至少 $F(k)$（即 $k$ 个切片）性能的概率，显著高于 SISA。</li><li><strong>结论：</strong> 增加预算 $B$ 可以有效提高系统在删除后的性能保留能力。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>频域特征对多模态大模型的幻觉影响</title>
    <link href="/2025/20251029/"/>
    <url>/2025/20251029/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><span id="more"></span><h2 id="（arxiv-3月）Mitigating-Object-Hallucinations-in-MLLMs-via-Multi-Frequency-Perturbations"><a href="#（arxiv-3月）Mitigating-Object-Hallucinations-in-MLLMs-via-Multi-Frequency-Perturbations" class="headerlink" title="（arxiv 3月）Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations"></a>（arxiv 3月）Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations</h2><p>目前仍缺乏研究探讨视觉特征在频域中对多模态大模型（MLLMs）的作用。甚至尚未有先验工作探索其在 MLLMs 中的物体幻觉现象中的作用。</p><p>作者实验，无论“高频”还是“低频”被移除，模型的幻觉比例都会显著增加。这表明这两种信息对模型的正确理解都至关重要。</p><p><strong>方法</strong>：</p><p>使用高斯来过滤高低频。也惯用地使用了时域卷积等于频域乘积。</p><p>使用一个衰减因子 γ 来调节低频和高频特征的强度。</p><p><img src="/2025/20251029/1.png"><br>$$<br>\begin{cases}<br>F_c^l(u, v) &#x3D; \mathcal{F}_c(u, v) \cdot \mathcal{H}_c^l(u, v) \cdot G(\gamma) \\<br>F_c^h(u, v) &#x3D; \mathcal{F}_c(u, v) \cdot \mathcal{H}_c^h(u, v) \cdot G(\gamma)<br>\end{cases}<br>$$</p><p>其中 $G(\gamma)$ 是一个矩阵，其值通过从均匀分布 $U(0, \gamma)$ 中随机采样得到，其中 $\gamma \le 1$。</p><h2 id="（arxiv-7月）On-the-Reliability-of-Vision-Language-Models-Under-Adversarial-Frequency-Domain-Perturbations"><a href="#（arxiv-7月）On-the-Reliability-of-Vision-Language-Models-Under-Adversarial-Frequency-Domain-Perturbations" class="headerlink" title="（arxiv 7月）On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations"></a>（arxiv 7月）On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations</h2><p>研究发现表明，VLM 用于区分真实内容与生成内容所学成的决策边界出人意料地脆弱，尤其是在保持图像质量的扰动下。这引发了一个假设：即 VLM，尤其是参数较少的模型，可能依赖频域特征作为真实性代理，而非基于语义内容进行预测。</p><p>将频率扰动框架扩展至自动图像描述任务，表明中频域的扰动可以显著降低 VLM 生成描述的语义丰富度，而不会改变图像的高层感知内容。</p><p>在频率域中采用稀疏且结构化的扰动能够可靠地操控模型预测，揭示了 VLM 在解读视觉输入时存在系统性漏洞。我们的发现表明，其推理过程本质上依赖于低层次图像结构，而非高层次语义。</p><p>论文主要关注于<strong>真实性检测（deepfake检测）和图像描述任务</strong>。</p><p><img src="/2025/20251029/2.png"></p><h3 id="真实性检测"><a href="#真实性检测" class="headerlink" title="真实性检测"></a>真实性检测</h3><p><strong>真实图像</strong>对高频扰动的<strong>敏感度</strong>甚至高于 AI 生成的图像 。这强烈暗示 VLM 确实在“走捷径”：它们将“高频细节的统计特征”当作判断真实性的关键指标，而不是真正理解图像内容 。</p><h3 id="图像描述"><a href="#图像描述" class="headerlink" title="图像描述"></a>图像描述</h3><p><img src="/2025/20251029/3.png"></p><p>采用一个辅助的 CLIP 编码器，将原始 VLM 输出和对抗性 VLM 输出投影到共享的 CLIP 空间中。</p><p>比较原始与对抗性描述嵌入，并应用相似度阈值0.5作为攻击成功的标准。</p><p>在每一轮中，攻击者会在图像的<strong>中频带</strong>（论文假设该区域与物体特征相关）生成多组微小的“候选扰动” 。选择最好的并重复以上操作。</p><blockquote><p>比较笨的方法。</p></blockquote><p><img src="/2025/20251029/4.png"></p><h2 id="（TDSC-2025）A-Multimodal-Adversarial-Attack-Method-via-Frequency-Domain-Enhancement-and-Fine-grained-Cross-modal-Guidance"><a href="#（TDSC-2025）A-Multimodal-Adversarial-Attack-Method-via-Frequency-Domain-Enhancement-and-Fine-grained-Cross-modal-Guidance" class="headerlink" title="（TDSC 2025）A Multimodal Adversarial Attack Method via Frequency Domain Enhancement and Fine-grained  Cross-modal Guidance"></a>（TDSC 2025）A Multimodal Adversarial Attack Method via Frequency Domain Enhancement and Fine-grained  Cross-modal Guidance</h2><p>大多数现有的基于频率的方法仅关注视觉模态，而忽略了与语言的跨模态交互。</p><p>现有的对抗攻击方法通常采用空间域增强来提高对抗 样本在黑盒模型中的可迁移性。旋转、裁剪和翻转等技术 增加了样本的多样性。然而，这些增强技术仍然依赖于原 始数据的变换，这可能导致模型在训练过程中过度拟合细 节和噪声，最终限制对抗样本的泛化能力。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="图像模态：频域增强-Frequency-domain-Enhancement"><a href="#图像模态：频域增强-Frequency-domain-Enhancement" class="headerlink" title="图像模态：频域增强 (Frequency-domain Enhancement)"></a>图像模态：频域增强 (Frequency-domain Enhancement)</h4><p><strong>动机</strong>：减少对高频细节和噪声的依赖，转而攻击更稳定的低频结构信息，以提高迁移性 。</p><p>使用DFT来消除高频。</p><p><strong>结合空间增强</strong>：在频域增强的基础上，仍然辅以多尺度变换和高斯噪声等空间域增强手段，以增加样本多样性 。</p><h4 id="文本模态：细粒度跨模态指导-Fine-grained-Cross-modal-Guidance"><a href="#文本模态：细粒度跨模态指导-Fine-grained-Cross-modal-Guidance" class="headerlink" title="文本模态：细粒度跨模态指导 (Fine-grained Cross-modal Guidance)"></a>文本模态：细粒度跨模态指导 (Fine-grained Cross-modal Guidance)</h4><p><strong>动机</strong>：现有的文本攻击只关注词级别，粒度太粗 。作者类比图像的频率，认为词级别特征（Word-level）类似高频（细节），而句子级别特征（Sentence-level）类似低频（全局）。</p><p><strong>多级特征提取</strong>：使用BERT模型</p><ul><li>提取<strong>词级别特征</strong> $h _ {w_i}$ （每个token的隐藏状态）。</li><li>提取<strong>句子级别特征</strong> $h_s$ （[CLS] token的嵌入）。</li></ul><p><strong>细粒度重要性评分</strong>：为了找到最值得替换的“关键词”，作者设计了一个新的评分公式 $S_i$ ：</p><ul><li>$S_i^{KL}$：计算原始文本嵌入与<strong>Mask</strong>掉单词 $w_i$ 后的文本嵌入之间的KL散度。这衡量了该词对整个句子信息的改变程度。</li><li>$S_i^{cos}$：计算词级别特征 $h _ {w_i}$ 和句子级别特征 $h_s$ 之间的余弦相似度（$1 - \text{sim}$）。这衡量了该词与全局上下文的关联度。</li><li><strong>最终得分</strong>：$S_i &#x3D; \lambda S_i^{KL} + (1-\lambda)S_i^{cos}$ 。$\lambda$ 是一个超参数（实验中设为0.7）。</li></ul><p><strong>生成对抗文本</strong>：根据 $S_i$ 得分选择Top-k个词进行替换 ，并利用图像嵌入作为指导，通过梯度迭代优化对抗文本的嵌入。<br>$$<br>t _ {i+1}^{adv}&#x3D;t _ {i}^{adv}+\alpha\cdot sgn(\frac{\sum _ {j&#x3D;1}^{\epsilon_t} \nabla h _ {v_j}(h_v,h_t)}{||\sum _ {j&#x3D;1}^{\epsilon_t} \nabla h _ {v_j}(h_v,h_t)||})<br>$$<br><strong>生成对抗图像</strong>：最后，使用生成的对抗文本嵌入 $h _ {tj}^{adv}$ 作为指导 ，计算相似度矩阵：<br>$$<br>sim _ {ij}&#x3D;h _ {v_i}\cdot h _ {tj}^{adv^T}<br>$$<br>对经过频域增强的图像使用<strong>PGD</strong>等方法进行攻击，目标是最大化地破坏图像和对抗文本之间的对齐关系。</p><h2 id="额外阅读"><a href="#额外阅读" class="headerlink" title="额外阅读"></a>额外阅读</h2><p>1.<a href="https://www.bilibili.com/video/BV1vQuyzZEk7">频率原则：神经网络通常会先学习低频</a></p><p>2.[<a href="https://arxiv.org/abs/1903.00073">1903.00073] On the Effectiveness of Low Frequency Perturbations</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ChunkLLM：A Lightweight Pluggable Framework for Accelerating LLMs Inference</title>
    <link href="/2025/20251026/"/>
    <url>/2025/20251026/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（arxiv 2025）2510.02361</p><p>ChunkLLM 不仅在短文本基准上达到与基线相当的性能，同时在长上下文基准上仍保持 98.64% 的性能，并维持48.58% 的关键值缓存保留率。特别地，在处理 120K 长文本时，ChunkLLM相较于原始 Transformer 实现了最高达 4.48× 的加速比。</p><p><strong>现有方法的局限性</strong>：</p><ul><li><strong>线性注意力</strong>（Linear Attention）（如Mamba, RWKV）：虽然高效，但其结构与传统Transformer不同，导致难以迁移预训练好的模型，通常需要从头训练 。</li><li><strong>稀疏注意力</strong>（Sparse Attention）：如滑动窗口 或固定模式，但这些方法往往依赖特定任务，泛化能力受限 。</li><li><strong>块选择</strong>（Chunk Selection）：现有的分块方法要么是固定长度（导致语义不完整），要么是基于分隔符（如句号），但分隔符本身存在歧义（例如，句号也可能出现在数字或缩写中）。此外，这些方法通常需要在每生成一个新token时都重新进行块选择，引入了额外的计算开销 。</li></ul><span id="more"></span><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="/2025/20251026/1.png"></p><h3 id="chunk-adapter"><a href="#chunk-adapter" class="headerlink" title="chunk adapter"></a>chunk adapter</h3><p>用来检测一个Token是否为块的边界。</p><p>它连接在模型的第一个Transformer层的输出上 。<br>$$<br>\hat{y}_i &#x3D; \begin{cases}<br>1, &amp; \text{Sigmoid}(\text{FFN}(\mathbf{H}_i^{l_1})) &gt; \alpha, \\<br>0, &amp; \text{otherwise}<br>\end{cases}<br>$$<br>Chunk Adapter使用<code>pySBD</code>工具（一个基于规则的句子边界检测工具）对训练数据进行预处理，自动标注出”块”的边界作为标签 。</p><h3 id="QK-Adapter"><a href="#QK-Adapter" class="headerlink" title="QK Adapter"></a>QK Adapter</h3><p>使用知识蒸馏来让”块”级别的注意力分数尽可能接近于原始LLM的完整注意力分数。</p><h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><p>作者发现了一个现象，命名为”块内注意力一致性”（<strong>I</strong>ntra-<strong>C</strong>hunk <strong>A</strong>ttention <strong>C</strong>onsistency，ICAC）。</p><p>这意味着：<strong>在同一个语义块中生成的token，它们倾向于关注（Attend to）同一组历史块</strong> 。</p><p>ICAC 使得在分块选择中节省计算成本成为可能。作者将分块边界预测任务融入推理阶段。仅当当前解码的 token 是分块边界时，才更新分块选择，并将预测阶段得到的完整分块整合到 K 和 V；否则，不执行任何更新。</p><p><strong>Top-k 块选择与投票</strong>：</p><ul><li>当触发”块选择”时，每一层的QK Adapter会计算出当前token对所有历史”块”的注意力分数，并选出各自的Top-k个关键块 。</li><li>为了整合多层的信息，作者提出了一个<strong>块投票机制（Chunk Voting Mechanism）</strong>，对所有层选出的Top-k块进行投票，得出一个全局的Top-k列表 。</li><li>只有这些被选中的全局Top-k块的KV缓存会被保留和用于后续计算 。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20251026/2.png"></p><p><img src="/2025/20251026/3.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RoseLoRA：面向知识编辑与微调的预训练语言模型行与列稀疏低秩适配</title>
    <link href="/2025/20251022/"/>
    <url>/2025/20251022/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（EMNLP 2024）</p><span id="more"></span><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p><strong>参数高效微调</strong> （PEFT）是比较流行的微调方法，其中最流行的是 <strong>LoRA</strong> (Low-rank Adaptation) 。LoRA的原理是在模型的权重矩阵 $W^o$ 上，额外学习两个低秩矩阵 $B$ 和 $A$（$W &#x3D; W^o + BA$），微调时只训练 $B$ 和 $A$ 。</p><p>虽然B、A的参数很少，但是BA是一个稠密矩阵。这在知识编辑中是致命的。（因为知识编辑的目标是精确地修改模型内部的某个特定知识（例如，“法国的首都是巴黎”），同时必须保持所有其他知识不变 。）</p><p>LoRA的稠密更新使得选择性修改变得非常困难。 </p><p>故RoseLoRA的目标是寻找一种PEFT方法，既能高效微调，又能实现<strong>稀疏更新</strong>。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>实现稀疏更新（即让 $BA$ 稀疏）面临两大挑战：</p><p>1.直接$||BA||_0$约束是NP-hard问题。</p><p>2.A和B系数≠BA稀疏。</p><p><img src="/2025/20251022/1.png"></p><h3 id="创新方案"><a href="#创新方案" class="headerlink" title="创新方案"></a>创新方案</h3><p>RoseLoRA采用的方案是强制$A$ 矩阵的每一行 (row-wise) 和$B$矩阵的每一列(column-wise)保持稀疏。</p><p><img src="/2025/20251022/2.png"></p><p>论文从理论上证明了，通过约束 $A$ 的行和 $B$ 的列的稀疏度，可以为最终乘积 $BA$ 的稀疏度提供一个数学上的下限保证。这意味着只要控制了 $A$ 的行和 $B$ 的列，就必然能得到一个稀疏的 $BA$ 更新。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p><strong>重要性评分（敏感度）</strong>：它使用“敏感度”（Sensitivity）来评估每个参数的重要性。一个参数的敏感度被定义为其<strong>权重的绝对值</strong>与<strong>梯度的绝对值</strong>的乘积 ($I &#x3D; |W \cdot \nabla\mathcal{L}|$) 。这个分数反映了如果将该参数设为零，会对模型的损失产生多大影响 。</p><p><strong>迭代剪枝</strong>：在训练的每一步中，RoseLoRA会对于 $A$ 的每一行和 $B$ 的每一列，<strong>剪枝</strong>掉那些敏感度分数最低（即最不重要）的参数，将它们设为零。</p><p><strong>稀疏预算</strong>：剪枝的比例（稀疏度）会随着训练的进行而逐渐增加，直到达到最终的目标稀疏度 。</p><p>$$<br>\tau^{(t)} &#x3D; \begin{cases} 1, &amp; t \le t_i \\ \tau + (1-\tau)(1 - \frac{t - t_i}{t_f - t_i})^3, &amp; t_i &lt; t \le t_f \\ \tau, &amp; t_f &lt; t \le T \end{cases}<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>拒绝如坠悬崖：推理中的安全对齐为何失效？</title>
    <link href="/2025/20251021/"/>
    <url>/2025/20251021/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（arxiv 2510.06036）</p><h2 id="核心思想与摘要"><a href="#核心思想与摘要" class="headerlink" title="核心思想与摘要"></a>核心思想与摘要</h2><p>大型推理模型（LRMs）虽然展现了强大的多步骤推理和解决问题的能力，但它们在安全对齐方面却常常表现不佳，存在令人担忧的漏洞 。这篇论文的核心目的就是从“机理解释性”的视角出发，探究这些推理模型安全对齐失败的内部机制 。</p><span id="more"></span><p><strong>核心发现：</strong> 研究者发现了一种惊人的现象，称之为“<strong>拒绝悬崖</strong>”（Refusal Cliff） 。具体来说，许多安全对齐不佳的推理模型，在它们内部的“思考”过程中，实际上是能够正确识别有害提示并保持强烈“拒绝意图”的 。然而，就在模型即将生成最终答案之前的最后几个词元（token）处，这种拒绝意图会突然急剧下降（仿佛掉下悬崖）。</p><p>这表明，这些模型并非天生不安全或无法识别风险，而是它们原本的“拒绝意图”在最后关头被系统性地<strong>抑制</strong>(Suppressed)了 。</p><p><strong>机理与解决方案：</strong></p><ol><li><strong>原因定位：</strong> 通过因果干预分析，论文将这种抑制现象归因于模型中一小部分稀疏的“<strong>拒绝抑制头</strong> ”(Refusal Suppression Heads)。</li><li><strong>修复验证：</strong> 实验证明，仅需禁用 <strong>3%</strong> 的这类注意力头，就能将模型的攻击成功率（Attack Success Rate, ASR）降低到10%以下 。</li><li><strong>应用：</strong> 基于这一发现，论文提出了一种名为“<strong>悬崖作为评判者</strong>”(Cliff-as-a-Judge)的高效数据筛选方法 。该方法通过专门挑选那些“拒绝悬崖”最陡峭（即拒绝意图下降最严重）的训练样本 ，仅使用了 <strong>1.7%</strong> 的原始安全训练数据，就达到了与使用全部数据相当的安全对齐效果，展现了安全对齐中的“少即是多”（less-is-more）效应 。</li></ol><p><img src="/2025/20251021/1.png"></p><h2 id="拒绝悬崖"><a href="#拒绝悬崖" class="headerlink" title="拒绝悬崖"></a>拒绝悬崖</h2><p>当大型语言模型遇到有害提示时，它会提供拒绝响应，以避免向用户提供与问题相关有害的信息。期的研究表明，拒绝行为通常由其活性值空间中的一个单一拒绝方向所控制。</p><p>由于这一线性特性，可以使用一个简单的线性分类器 i.e.,一个拒绝探测器来有效识别该方向。拒绝探测器是一个逻辑回归模型，它以token 位置 j 处的隐状态向量 $h_j \in H$ 作为输入，并输出拒绝的概率。</p><p><img src="/2025/20251021/2.png"></p><p>使用训练好的拒绝探测器来探测推理模型的隐状态，以估计每个 token 位置处的 拒绝得分X。</p><p><img src="/2025/20251021/3.png"></p><h2 id="拒绝悬崖中的“魔鬼”是谁？来自注意力头的机制解释"><a href="#拒绝悬崖中的“魔鬼”是谁？来自注意力头的机制解释" class="headerlink" title="拒绝悬崖中的“魔鬼”是谁？来自注意力头的机制解释"></a>拒绝悬崖中的“魔鬼”是谁？来自注意力头的机制解释</h2><p>从机械可解释性的角度来看，注意力头是 Transformer 架构中信息路由的主要载体，且不同的头通常专门负责不同的功能。也有研究证实，注意力头在安全性方面起着关键作用。《On the Role of Attention Heads in Large Language Model Safety》【 ICLR 2025 (oral)】</p><p>为了准确评估每个注意力头对拒绝行为的因果影响，采样直接探测方法来追踪每个头的贡献。方法是单独评估每个头在拒绝悬崖发生的位置处的输出影响。</p><p>具体而言，对于任意层 i 中的注意力头 h ，首先隔离其输出向量$o_i,h,t_{cliff}$。根据标准 Transformer 架构，该向量通过注意力块的输出权重矩阵$W_{O,i}$投影到残差流中。</p><p>为了分析头 h 单独的贡献，仅头 h 的输出处于激活状态，而同一层中所有其他头的输出均被置零。随后，将仅包含单一头贡献的该向量输入至预训练的拒绝探测器，以评估该头的独立拒绝得分。其贡献得分 $s_{i,h}$ 计算如下<br>$$<br>s_{i,h}&#x3D;W^T\Delta h_{i,h,t_{cliff}}+b<br>$$<br>即移除了sigmoid。</p><p><img src="/2025/20251021/4.png"></p><p>这种贡献模式具有高度稀疏性：仅有少数头与拒绝行为呈现强负相关，将其称为<strong>拒绝抑制头</strong>。</p><h2 id="消除拒绝抑制头"><a href="#消除拒绝抑制头" class="headerlink" title="消除拒绝抑制头"></a>消除拒绝抑制头</h2><p>方法其实很简单：$O&#x3D;(QK^T)\cdot\gamma\cdot V$。</p><p>效果也还不错。</p><p><img src="/2025/20251021/5.png"></p><h2 id="Cliff-as-a-judge"><a href="#Cliff-as-a-judge" class="headerlink" title="Cliff-as-a-judge"></a>Cliff-as-a-judge</h2><p>作者提出一种基于悬崖效应的数据选择方法。数据选择的目标是获得一个最优数据集子集，以优化其对齐性能。</p><p>具体而言，假设对于某个样本，模型在内部思考中表达出的最大拒绝意图（即平台）对应于探测到的拒绝得分 I ，而在任何悬崖下降或抑制之后，其最终生成的拒绝得分为 I′ 。将偏离度得分 MS &#x3D; I − I′ 定义为衡量模型在最终输出中，其内部推理所表达的拒绝意图被抑制程度的指标。直观上，最有效的对齐数据子集由具有最高偏离度得分的样本组成，因为在这些数据上进行训练可以最高效地修复安全对齐问题。</p><h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>感觉有点像《Answer When Needed, Forget When Not: Language Models Pretend toForget via In-Context Knowledge Unlearning》，表面上遗忘了，实质上没有遗忘，只是在最后一层时临时修改了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LVLM幻觉幻觉思维导图</title>
    <link href="/2025/20251016/"/>
    <url>/2025/20251016/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>截止至25年10月3日，参考github awesome。</p><span id="more"></span><!doctype html><html><head><meta charset="UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta http-equiv="X-UA-Compatible" content="ie=edge" /><title>Markmap</title><style>* {  margin: 0;  padding: 0;}html {  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';}#mindmap {  display: block;  width: 100vw;  height: 100vh;}.markmap-dark {  background: #27272a;  color: white;}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css"></head><body><svg id="mindmap"></svg><script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((getMarkmap) => {          window.WebFontConfig = {            custom: {              families: [                "KaTeX_AMS",                "KaTeX_Caligraphic:n4,n7",                "KaTeX_Fraktur:n4,n7",                "KaTeX_Main:n4,n7,i4,i7",                "KaTeX_Math:i4,i7",                "KaTeX_Script",                "KaTeX_SansSerif:n4,n7,i4",                "KaTeX_Size1",                "KaTeX_Size2",                "KaTeX_Size3",                "KaTeX_Size4",                "KaTeX_Typewriter"              ]            },            active: () => {              getMarkmap().refreshHook.call();            }          };        })(() => window.markmap)</script><script src="https://cdn.jsdelivr.net/npm/webfontloader@1.6.28/webfontloader.js" defer></script><script>(r => {              setTimeout(r);            })(function renderToolbar() {  const {    markmap,    mm  } = window;  const {    el  } = markmap.Toolbar.create(mm);  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');  document.body.append(el);})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {              const markmap = getMarkmap();              window.mm = markmap.Markmap.create(                "svg#mindmap",                (getOptions || markmap.deriveOptions)(jsonOptions),                root2              );              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {                document.documentElement.classList.add("markmap-dark");              }            })(() => window.markmap,null,{"content":"LVLM幻觉缓解","children":[{"content":"&#x63d0;&#x793a;&#x8bcd;","children":[{"content":"BBVPE: Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;BBVPE&#x7684;&#x9ed1;&#x76d2;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x5de5;&#x7a0b;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x56fe;&#x50cf;&#x4e0a;&#x53e0;&#x52a0;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff08;&#x5982;&#x8fb9;&#x6846;&#x3001;&#x5706;&#x5708;&#xff09;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bbf;&#x95ee;&#x6a21;&#x578b;&#x5185;&#x90e8;&#xff0c;&#x53ef;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x6700;&#x4f18;&#x63d0;&#x793a;&#xff0c;&#x5728;POPE&#x548c;CHAIR&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"9,10"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x533b;&#x7597;&#x548c;&#x8f85;&#x52a9;&#x6280;&#x672f;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x5b58;&#x5728;&#x98ce;&#x9669;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x8bbf;&#x95ee;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x53c2;&#x6570;&#xff08;&#x5982;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x3001;&#x903b;&#x8f91;&#x8f93;&#x51fa;&#xff09;&#xff0c;&#x65e0;&#x6cd5;&#x9002;&#x7528;&#x4e8e;&#x9ed1;&#x76d2;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4o&#x7b49;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#xff09;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x4fe1;&#x606f;&#x7684;&#x901a;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"11,12"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;BBVPE&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1) &#x9884;&#x5b9a;&#x4e49;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x6c60;&#xff08;&#x5305;&#x62ec;&#x8fb9;&#x6846;&#x3001;&#x7bad;&#x5934;&#x3001;&#x6a21;&#x7cca;&#x7b49;&#x5904;&#x7406;&#xff09;&#xff1b;2) &#x57fa;&#x4e8e;&#x5bf9;&#x8c61;&#x5b9a;&#x4f4d;&#x6a21;&#x578b;&#xff08;&#x5982;SAM 2&#xff09;&#x8bc6;&#x522b;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x533a;&#x57df;&#xff1b;3) &#x8bad;&#x7ec3;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x8def;&#x7531;&#x5668;&#x6a21;&#x578b;&#xff08;&#x57fa;&#x4e8e;CLIP&#x7f16;&#x7801;&#x5668;&#x548c;MLP&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x8f93;&#x5165;-&#x8f93;&#x51fa;&#x884c;&#x4e3a;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x6700;&#x4f18;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x3002;&#x8bad;&#x7ec3;&#x65f6;&#x4f7f;&#x7528;&#x4ea4;&#x53c9;&#x71b5;&#x635f;&#x5931;&#x4f18;&#x5316;&#x8def;&#x7531;&#x5668;&#xff0c;&#x4f7f;&#x5176;&#x80fd;&#x591f;&#x9884;&#x6d4b;&#x6700;&#x9002;&#x5408;&#x5f53;&#x524d;&#x56fe;&#x50cf;&#x7684;&#x63d0;&#x793a;&#x7c7b;&#x578b;&#xff0c;&#x6700;&#x7ec8;&#x5c06;&#x4f18;&#x5316;&#x540e;&#x7684;&#x56fe;&#x50cf;&#x8f93;&#x5165;LVLM&#x751f;&#x6210;&#x66f4;&#x51c6;&#x786e;&#x7684;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"12,13"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x548c;CHAIR&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;BBVPE&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff1a;1) &#x5728;POPE&#x4e0a;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x6700;&#x9ad8;&#x63d0;&#x5347;&#x7ea6;2%&#xff08;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#xff09;&#x548c;1.5%&#xff08;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#xff09;&#xff1b;2) &#x5728;CHAIR&#x4e0a;&#xff0c;&#x5e7b;&#x89c9;&#x53e5;&#x5b50;&#x6bd4;&#x4f8b;&#xff08;CHS&#xff09;&#x6700;&#x9ad8;&#x964d;&#x4f4e;16.5%&#xff08;LLaVA&#x6a21;&#x578b;&#xff09;&#xff1b;3) &#x7efc;&#x5408;GPT-4o&#x8bc4;&#x4f30;&#x663e;&#x793a;&#xff0c;&#x63cf;&#x8ff0;&#x51c6;&#x786e;&#x6027;&#x3001;&#x7ec6;&#x8282;&#x6027;&#x548c;&#x9c81;&#x68d2;&#x6027;&#x5747;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x3002;&#x4f46;&#x6027;&#x80fd;&#x4ecd;&#x4f4e;&#x4e8e;&#x7406;&#x60f3;&#x5316;&#x7684;Oracle&#xff08;&#x5df2;&#x77e5;&#x6700;&#x4f18;&#x63d0;&#x793a;&#xff09;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x89c6;&#x89c9;&#x63d0;&#x793a;&#x5de5;&#x7a0b;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x9ed1;&#x76d2;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x6a21;&#x578b;&#x65e0;&#x5173;&#x6027;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x5f00;&#x6e90;&#x548c;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x6269;&#x5c55;&#x63d0;&#x793a;&#x7c7b;&#x578b;&#x548c;&#x4f18;&#x5316;&#x8def;&#x7531;&#x5668;&#x67b6;&#x6784;&#xff0c;&#x8fdb;&#x4e00;&#x6b65;&#x63d0;&#x5347;&#x5b9e;&#x7528;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"14,16"}}],"payload":{"tag":"li","lines":"10,16","fold":1}}],"payload":{"tag":"h4","lines":"8,9"}},{"content":"Prompt-in-Image: Cure or Poison? Embedding Instructions Visually Alters Hallucination in Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Prompt-in-Image&#x65b9;&#x6cd5;&#xff0c;&#x5c06;&#x6587;&#x672c;&#x6307;&#x4ee4;&#x76f4;&#x63a5;&#x5d4c;&#x5165;&#x56fe;&#x50cf;&#x4e2d;&#xff0c;&#x8feb;&#x4f7f;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4ec5;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x901a;&#x9053;&#x5904;&#x7406;&#x6240;&#x6709;&#x4fe1;&#x606f;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;Qwen2.5-VL&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#xff0c;&#x4f46;&#x5728;LLaVA-1.5&#x548c;InstructBLIP&#x4e0a;&#x5374;&#x5bfc;&#x81f4;&#x6027;&#x80fd;&#x5d29;&#x6e83;&#xff0c;&#x63ed;&#x793a;&#x4e86;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x5728;&#x5904;&#x7406;&#x5d4c;&#x5165;&#x6587;&#x672c;&#x65f6;&#x7684;&#x6839;&#x672c;&#x5dee;&#x5f02;&#x3002;","children":[],"payload":{"tag":"li","lines":"17,18"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x5ffd;&#x7565;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x975e;&#x5e38;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;VLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x6027;&#x80fd;&#x3002;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x90e8;&#x5206;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x6a21;&#x6001;&#x5728;&#x9884;&#x8bad;&#x7ec3;&#x9636;&#x6bb5;&#x5206;&#x79bb;&#x6240;&#x5e26;&#x6765;&#x7684;&#x8de8;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6311;&#x6218;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x5148;&#x9a8c;&#x800c;&#x5ffd;&#x89c6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"19,20"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Prompt-in-Image&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x5c06;&#x4f20;&#x7edf;&#x7684;&#x6587;&#x672c;&#x6307;&#x4ee4;&#xff08;&#x5982;&#x95ee;&#x9898;&#xff09;&#x76f4;&#x63a5;&#x4ee5;&#x5b57;&#x5e55;&#x5f62;&#x5f0f;&#x5d4c;&#x5165;&#x5230;&#x56fe;&#x50cf;&#x5e95;&#x90e8;&#xff08;&#x7ea6;&#x5360;&#x56fe;&#x50cf;&#x9ad8;&#x5ea6;5%&#x7684;&#x767d;&#x8272;&#x533a;&#x57df;&#xff0c;&#x4f7f;&#x7528;&#x9ed1;&#x8272;Arial&#x5b57;&#x4f53;&#xff09;&#xff0c;&#x4ece;&#x800c;&#x5b8c;&#x5168;&#x79fb;&#x9664;&#x72ec;&#x7acb;&#x7684;&#x6587;&#x672c;&#x8f93;&#x5165;&#x901a;&#x9053;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x4ec5;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x5904;&#x7406;&#x6240;&#x6709;&#x4fe1;&#x606f;&#xff08;&#x5305;&#x62ec;&#x6307;&#x4ee4;&#x548c;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;POPE&#xff08;&#x7528;&#x4e8e;&#x8bc4;&#x4f30;&#x5bf9;&#x8c61;&#x5b58;&#x5728;&#x6027;&#x5224;&#x65ad;&#xff09;&#x548c;MS-COCO&#xff08;&#x7528;&#x4e8e;&#x8bc4;&#x4f30;&#x63cf;&#x8ff0;&#x6027;&#x5e7b;&#x89c9;&#xff09;&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x5bf9;Qwen2.5-VL&#x3001;LLaVA-1.5&#x548c;InstructBLIP&#x4e09;&#x79cd;&#x5f00;&#x6e90;VLM&#x8fdb;&#x884c;&#x4e86;&#x7cfb;&#x7edf;&#x8bc4;&#x4f30;&#xff0c;&#x5e76;&#x8bbe;&#x7f6e;&#x4e86;&#x57fa;&#x7ebf;&#xff08;&#x539f;&#x56fe;+&#x6587;&#x672c;&#xff09;&#x3001;&#x6df7;&#x5408;&#xff08;&#x5d4c;&#x5b57;&#x56fe;+&#x6587;&#x672c;&#xff09;&#x548c;&#x63a7;&#x5236;&#xff08;&#x7a7a;&#x767d;&#x6846;+&#x6587;&#x672c;&#xff09;&#x7b49;&#x5bf9;&#x6bd4;&#x5b9e;&#x9a8c;&#x4ee5;&#x9a8c;&#x8bc1;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5448;&#x73b0;&#x51fa;&#x4e24;&#x6781;&#x5206;&#x5316;&#xff1a;1. Qwen2.5-VL&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff1a;&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x4ece;80.2%&#x63d0;&#x5347;&#x81f3;84.3%&#xff08;+4.1%&#xff09;&#xff0c;F1&#x5206;&#x6570;&#x4ece;0.76&#x5347;&#x81f3;0.82&#xff1b;&#x5728;MS-COCO&#x4e0a;&#xff0c;&#x63cf;&#x8ff0;&#x5e7b;&#x89c9;&#x6307;&#x6807;CHAIRs&#x548c;CHAIRi&#x5206;&#x522b;&#x4e0b;&#x964d;&#x4e86;7.6%&#x548c;2.1%&#x3002;2. LLaVA-1.5&#x548c;InstructBLIP&#x6027;&#x80fd;&#x5d29;&#x6e83;&#xff1a;&#x5728;POPE&#x4e0a;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x5206;&#x522b;&#x4ece;84%&#x66b4;&#x8dcc;&#x81f3;55%&#xff08;&#x8fd1;&#x968f;&#x673a;&#x6c34;&#x5e73;&#xff09;&#x548c;&#x4ece;74.4%&#x8dcc;&#x81f3;54%&#x3002;&#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;&#x6027;&#x80fd;&#x5dee;&#x5f02;&#x6839;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff1a;LLaVA&#x548c;InstructBLIP&#x4f7f;&#x7528;&#x7684;CLIP&#x7f16;&#x7801;&#x5668;&#x5bf9;&#x5d4c;&#x5165;&#x6587;&#x672c;&#x533a;&#x57df;&#x5b58;&#x5728;&#x8fc7;&#x5ea6;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x504f;&#x5dee;&#xff0c;&#x4e25;&#x91cd;&#x5e72;&#x6270;&#x4e86;&#x6b63;&#x5e38;&#x7684;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#xff1b;&#x800c;Qwen&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x80fd;&#x66f4;&#x7a33;&#x5065;&#x5730;&#x5904;&#x7406;&#x6587;&#x672c;&#x5d4c;&#x5165;&#x56fe;&#x50cf;&#xff0c;&#x4e14;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x7edf;&#x4e00;&#x4fe1;&#x606f;&#x5904;&#x7406;&#x6a21;&#x6001;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c0f;&#x4e86;&#x5176;&#x6a21;&#x6001;&#x95f4;&#x9699;&#xff08;modality gap&#xff09;&#xff0c;&#x589e;&#x5f3a;&#x4e86;&#x8de8;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;Prompt-in-Image&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x800c;&#x6709;&#x6548;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x4f46;&#x5176;&#x6548;&#x679c;&#x9ad8;&#x5ea6;&#x4f9d;&#x8d56;&#x4e8e;&#x5e95;&#x5c42;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x7279;&#x6027;&#x3002;&#x5bf9;&#x4e8e;&#x50cf;Qwen2.5-VL&#x8fd9;&#x6837;&#x5177;&#x5907;&#x7a33;&#x5065;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x6a21;&#x578b;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x901a;&#x8fc7;&#x89c4;&#x907f;&#x8de8;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x3001;&#x7edf;&#x4e00;&#x5904;&#x7406;&#x5355;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff1b;&#x800c;&#x5bf9;&#x4e8e;&#x4f9d;&#x8d56;CLIP&#x7f16;&#x7801;&#x5668;&#x7684;&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA&#x3001;InstructBLIP&#xff09;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x53cd;&#x800c;&#x4f1a;&#x56e0;&#x7f16;&#x7801;&#x5668;&#x7684;&#x6587;&#x672c;&#x504f;&#x89c1;&#x800c;&#x52a0;&#x5267;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x4e00;&#x53d1;&#x73b0;&#x63ed;&#x793a;&#x4e86;&#x4e0d;&#x540c;VLM&#x67b6;&#x6784;&#x5728;&#x5185;&#x5728;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x4e0a;&#x7684;&#x6839;&#x672c;&#x5dee;&#x5f02;&#xff0c;&#x5bf9;&#x672a;&#x6765;VLM&#x7684;&#x8bbe;&#x8ba1;&#xff08;&#x5c24;&#x5176;&#x662f;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x9009;&#x62e9;&#x548c;&#x6539;&#x8fdb;&#xff09;&#x5177;&#x6709;&#x91cd;&#x8981;&#x6307;&#x5bfc;&#x610f;&#x4e49;&#xff0c;&#x5e76;&#x8868;&#x660e;&#x8ffd;&#x6c42;&#x5355;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x5904;&#x7406;&#x662f;&#x4e00;&#x79cd;&#x53ef;&#x884c;&#x7684;&#x3001;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5bf9;&#x9f50;&#x95ee;&#x9898;&#x7684;&#x7814;&#x7a76;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"22,24"}}],"payload":{"tag":"li","lines":"18,24","fold":1}}],"payload":{"tag":"h4","lines":"16,17"}},{"content":"TVP: Exploring the Transferability of Visual Prompting for Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x53ef;&#x8fc1;&#x79fb;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff08;TVP&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x5355;&#x4e00;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e0a;&#x5b66;&#x4e60;&#x4e00;&#x7ec4;&#x5171;&#x4eab;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x53c2;&#x6570;&#xff0c;&#x80fd;&#x591f;&#x6709;&#x6548;&#x63d0;&#x5347;&#x591a;&#x79cd;&#x4e0d;&#x540c;MLLM&#x5728;&#x4e0b;&#x6e38;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x8bc6;&#x522b;&#x3001;&#x8ba1;&#x6570;&#x3001;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x7b49;&#xff09;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x907f;&#x514d;&#x4e86;&#x4e3a;&#x6bcf;&#x4e2a;&#x6a21;&#x578b;&#x5355;&#x72ec;&#x5fae;&#x8c03;&#x7684;&#x5de8;&#x5927;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x867d;&#x7136;&#x5728;&#x591a;&#x79cd;&#x4efb;&#x52a1;&#x4e0a;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5176;&#x5728;&#x4e0b;&#x6e38;&#x7279;&#x5b9a;&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x4ecd;&#x4e0d;&#x53ca;&#x4e13;&#x7528;&#x6a21;&#x578b;&#x3002;&#x4f20;&#x7edf;&#x7684;&#x5fae;&#x8c03;&#x65b9;&#x6cd5;&#x9700;&#x8981;&#x4e3a;&#x6bcf;&#x4e2a;&#x6a21;&#x578b;&#x72ec;&#x7acb;&#x8bad;&#x7ec3;&#xff0c;&#x8ba1;&#x7b97;&#x548c;&#x5185;&#x5b58;&#x5f00;&#x9500;&#x5de8;&#x5927;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x8d44;&#x6e90;&#x53cb;&#x597d;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x591f;&#x7528;&#x4e00;&#x7ec4;&#x5171;&#x4eab;&#x7684;&#x53c2;&#x6570;&#x540c;&#x65f6;&#x63d0;&#x5347;&#x591a;&#x4e2a;MLLM&#x7684;&#x6027;&#x80fd;&#xff0c;&#x7279;&#x522b;&#x662f;&#x5728;&#x6a21;&#x578b;&#x6743;&#x91cd;&#x4e0d;&#x53ef;&#x8bbf;&#x95ee;&#xff08;&#x9ed1;&#x76d2;&#xff09;&#x7684;&#x573a;&#x666f;&#x4e0b;&#xff0c;&#x8fd9;&#x5bf9;&#x4e8e;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x548c;&#x2018;&#x63d0;&#x793a;&#x5373;&#x670d;&#x52a1;&#x2019;&#xff08;PaaS&#xff09;&#x6a21;&#x5f0f;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x53ef;&#x8fc1;&#x79fb;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff08;TVP&#xff09;&#x6846;&#x67b6;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5728;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0a;&#x6dfb;&#x52a0;&#x53ef;&#x5b66;&#x4e60;&#x7684;&#x50cf;&#x7d20;&#x7ea7;&#x6270;&#x52a8;&#xff08;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff09;&#x6765;&#x9002;&#x914d;&#x6a21;&#x578b;&#x3002;&#x4e3a;&#x4e86;&#x89e3;&#x51b3;&#x73b0;&#x6709;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x65b9;&#x6cd5;&#x5728;&#x8de8;&#x6a21;&#x578b;&#x8fc1;&#x79fb;&#x65f6;&#x51fa;&#x73b0;&#x7684;&#x2018;&#x7279;&#x5f81;&#x7834;&#x574f;&#x2019;&#x95ee;&#x9898;&#xff0c;TVP&#x5f15;&#x5165;&#x4e86;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7b56;&#x7565;&#xff1a;1) &#x7279;&#x5f81;&#x4e00;&#x81f4;&#x6027;&#x5bf9;&#x9f50;&#xff08;FCA&#xff09;&#xff1a;&#x7ea6;&#x675f;&#x63d0;&#x793a;&#x5e94;&#x7528;&#x540e;&#x7684;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x53d8;&#x5316;&#xff0c;&#x4ee5;&#x4fdd;&#x7559;&#x6a21;&#x578b;&#x9884;&#x8bad;&#x7ec3;&#x83b7;&#x5f97;&#x7684;&#x4e0e;&#x4efb;&#x52a1;&#x65e0;&#x5173;&#x7684;&#x901a;&#x7528;&#x77e5;&#x8bc6;&#xff0c;&#x9632;&#x6b62;&#x8fc7;&#x62df;&#x5408;&#x3002;2) &#x4efb;&#x52a1;&#x8bed;&#x4e49;&#x589e;&#x5f3a;&#xff08;TSE&#xff09;&#xff1a;&#x5229;&#x7528;CLIP&#x6a21;&#x578b;&#xff0c;&#x9f13;&#x52b1;&#x63d0;&#x793a;&#x540e;&#x7684;&#x56fe;&#x50cf;&#x5728;&#x8bed;&#x4e49;&#x4e0a;&#x4e0e;&#x7279;&#x5b9a;&#x4efb;&#x52a1;&#x7684;&#x6587;&#x672c;&#x7279;&#x5f81;&#x66f4;&#x76f8;&#x4f3c;&#xff0c;&#x4ece;&#x800c;&#x663e;&#x5f0f;&#x5730;&#x5d4c;&#x5165;&#x66f4;&#x4e30;&#x5bcc;&#x7684;&#x4efb;&#x52a1;&#x8bed;&#x4e49;&#x4fe1;&#x606f;&#xff0c;&#x63d0;&#x5347;&#x63d0;&#x793a;&#x7684;&#x53ef;&#x8fc1;&#x79fb;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x901a;&#x8fc7;&#x5728;6&#x4e2a;&#x73b0;&#x4ee3;MLLM&#x548c;10&#x4e2a;&#x4e0d;&#x540c;&#x4efb;&#x52a1;&#xff08;&#x5305;&#x62ec;&#x76ee;&#x6807;&#x8bc6;&#x522b;&#x3001;&#x8ba1;&#x6570;&#x3001;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x548c;&#x5e7b;&#x89c9;&#x7ea0;&#x6b63;&#xff09;&#x4e0a;&#x7684;&#x5927;&#x91cf;&#x5b9e;&#x9a8c;&#x8bc1;&#x660e;&#xff0c;TVP&#x663e;&#x8457;&#x8d85;&#x8d8a;&#x4e86;&#x73b0;&#x6709;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x65b9;&#x6cd5;&#x3002;&#x4ec5;&#x5728;&#x5355;&#x4e00;&#x6a21;&#x578b;&#x4e0a;&#x8bad;&#x7ec3;&#x7684;TVP&#x63d0;&#x793a;&#xff0c;&#x53ef;&#x4ee5;&#x6210;&#x529f;&#x8fc1;&#x79fb;&#x5230;&#x5176;&#x4ed6;&#x6a21;&#x578b;&#x5e76;&#x5e26;&#x6765;&#x5927;&#x5e45;&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff08;&#x4f8b;&#x5982;&#xff0c;&#x5728;SVHN&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x6709;&#x6700;&#x9ad8;36.4%&#x7684;&#x63d0;&#x5347;&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5bf9;&#x5177;&#x6709;&#x4e0d;&#x540c;&#x6570;&#x636e;&#x89c4;&#x6a21;&#x548c;&#x67b6;&#x6784;&#x7684;&#x6a21;&#x578b;&#x90fd;&#x6709;&#x6548;&#xff0c;&#x80fd;&#x6cdb;&#x5316;&#x5230;&#x4e0d;&#x540c;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5e76;&#x62b5;&#x6297;&#x56fe;&#x50cf;&#x635f;&#x574f;&#xff0c;&#x5c55;&#x73b0;&#x4e86;&#x5f3a;&#x5927;&#x7684;&#x5b9e;&#x7528;&#x6027;&#x548c;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;TVP&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x800c;&#x6709;&#x6548;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x5b83;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x5730;&#x63a2;&#x7d22;&#x5e76;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x5728;MLLM&#x95f4;&#x7684;&#x53ef;&#x8fc1;&#x79fb;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x8bc1;&#x660e;&#x4e86;&#x4e00;&#x7ec4;&#x5171;&#x4eab;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x53c2;&#x6570;&#x53ef;&#x4ee5;&#x540c;&#x65f6;&#x6539;&#x5584;&#x591a;&#x4e2a;&#x9ed1;&#x76d2;&#x6a21;&#x578b;&#x7684;&#x4e0b;&#x6e38;&#x4efb;&#x52a1;&#x6027;&#x80fd;&#xff0c;&#x4e3a;&#x5b9e;&#x73b0;&#x9ad8;&#x6548;&#x3001;&#x7075;&#x6d3b;&#x4e14;&#x4fdd;&#x62a4;&#x6a21;&#x578b;&#x9690;&#x79c1;&#x7684;&#x2018;&#x63d0;&#x793a;&#x5373;&#x670d;&#x52a1;&#x2019;&#xff08;PaaS&#xff09;&#x8303;&#x5f0f;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x884c;&#x7684;&#x6280;&#x672f;&#x8def;&#x5f84;&#xff0c;&#x5177;&#x6709;&#x91cd;&#x8981;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"30,32"}}],"payload":{"tag":"li","lines":"26,32","fold":1}}],"payload":{"tag":"h4","lines":"24,25"}},{"content":"VDGD: Mitigating LVLM Hallucinations in Cognitive Prompts by Bridging the Visual Perception Gap","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;VDGD&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x7684;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x5e76;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x57fa;&#x4e8e;KL&#x6563;&#x5ea6;&#x91c7;&#x6837;&#xff0c;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x8ba4;&#x77e5;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x63a8;&#x7406;&#x80fd;&#x529b;2%-33%&#x3002;","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;LVLM&#x5728;&#x9700;&#x8981;&#x590d;&#x6742;&#x63a8;&#x7406;&#x7684;&#x8ba4;&#x77e5;&#x63d0;&#x793a;&#xff08;&#x5982;&#x77e5;&#x8bc6;&#x63d0;&#x53d6;&#x3001;&#x6570;&#x5b66;&#x63a8;&#x7406;&#xff09;&#x4e2d;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x4e0e;&#x4e8b;&#x5b9e;&#x4fe1;&#x606f;&#x4e0d;&#x7b26;&#x7684;&#x54cd;&#x5e94;&#xff09;&#x7684;&#x95ee;&#x9898;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x9488;&#x5bf9;&#x89c6;&#x89c9;&#x8bc6;&#x522b;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#xff0c;&#x4f46;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x5904;&#x7406;&#x8ba4;&#x77e5;&#x4efb;&#x52a1;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;LVLM&#x5728;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x590d;&#x6742;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"35,36"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x89c6;&#x89c9;&#x63cf;&#x8ff0;&#x63a5;&#x5730;&#x89e3;&#x7801;&#xff08;VDGD&#xff09;&#x65b9;&#x6cd5;&#xff1a;1. &#x9996;&#x5148;&#x751f;&#x6210;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x7684;&#x8be6;&#x7ec6;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#xff0c;&#x5e76;&#x5c06;&#x5176;&#x4f5c;&#x4e3a;&#x524d;&#x7f00;&#x9644;&#x52a0;&#x5230;&#x7528;&#x6237;&#x6307;&#x4ee4;&#x4e2d;&#xff1b;2. &#x5728;&#x81ea;&#x56de;&#x5f52;&#x54cd;&#x5e94;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x5bf9;&#x6bcf;&#x4e2a;&#x89e3;&#x7801;&#x6b65;&#x9aa4;&#x7684;top-k&#x5019;&#x9009;token&#xff0c;&#x8ba1;&#x7b97;&#x5176;&#x4e0e;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x6240;&#x6709;token&#x7684;KL&#x6563;&#x5ea6;&#xff08;KLD&#xff09;&#xff1b;3. &#x6839;&#x636e;KLD&#x503c;&#x91c7;&#x6837;&#x4e0b;&#x4e00;&#x4e2a;token&#xff08;KLD&#x8d8a;&#x4f4e;&#x4f18;&#x5148;&#x7ea7;&#x8d8a;&#x9ad8;&#xff09;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x63cf;&#x8ff0;&#x4e00;&#x81f4;&#x7684;&#x54cd;&#x5e94;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x589e;&#x5f3a;&#x63a8;&#x7406;&#x3002;","children":[],"payload":{"tag":"li","lines":"36,37"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;8&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5305;&#x62ec;MMMU&#x3001;MathVista&#x7b49;&#x6570;&#x5b66;&#x548c;&#x4e13;&#x5bb6;&#x7ea7;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#xff09;&#x4e0a;&#x8bc4;&#x4f30;&#xff0c;VDGD&#x5728;&#x591a;&#x4e2a;LVLM&#xff08;&#x5982;LLaVA&#x7cfb;&#x5217;&#xff09;&#x4e0a;&#x4e00;&#x81f4;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x3001;OPERA&#xff09;&#xff0c;&#x6027;&#x80fd;&#x63d0;&#x5347;2%-33%&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x8ba4;&#x77e5;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x8ba1;&#x7b97;&#x8f7b;&#x91cf;&#xff0c;&#x5e76;&#x80fd;&#x6cdb;&#x5316;&#x5230;&#x62bd;&#x8c61;&#x56fe;&#x50cf;&#xff08;&#x5982;&#x56fe;&#x8868;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"37,38"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff0c;VDGD&#x901a;&#x8fc7;&#x663e;&#x5f0f;&#x63a5;&#x5730;&#x89c6;&#x89c9;&#x63cf;&#x8ff0;&#xff0c;&#x6709;&#x6548;&#x5f25;&#x8865;&#x4e86;LVLM&#x5728;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#xff08;&#x4e0a;&#x4e0b;&#x6587;&#x5316;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e0e;&#x63d0;&#x793a;&#xff09;&#x65b9;&#x9762;&#x7684;&#x7f3a;&#x9677;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x590d;&#x6742;&#x63a8;&#x7406;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x65e0;&#x8bad;&#x7ec3;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x4e86;&#x65b0;&#x57fa;&#x51c6;VaLLu&#x4ee5;&#x5168;&#x9762;&#x8bc4;&#x4f30;LVLM&#x7684;&#x8ba4;&#x77e5;&#x80fd;&#x529b;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;AI&#x5728;&#x533b;&#x7597;&#x3001;&#x6559;&#x80b2;&#x7b49;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"38,40"}}],"payload":{"tag":"li","lines":"34,40","fold":1}}],"payload":{"tag":"h4","lines":"32,33"}},{"content":"PACU: Effectively Enhancing Vision Language Large Models by Prompt Augmentation and Caption Utilization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;PACU&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x63d0;&#x793a;&#x589e;&#x5f3a;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x5229;&#x7528;&#xff0c;&#x89e3;&#x51b3;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x5927;&#x6a21;&#x578b;&#xff08;VLLM&#xff09;&#x5728;&#x5904;&#x7406;&#x589e;&#x5f3a;&#x63d0;&#x793a;&#x65f6;&#x4ea7;&#x751f;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"41,42"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x5927;&#x6a21;&#x578b;&#xff08;VLLM&#xff09;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x7684;&#x5185;&#x5bb9;&#x3002;&#x73b0;&#x6709;&#x6297;&#x5e7b;&#x89c9;&#x6280;&#x672f;&#x867d;&#x6709;&#x6548;&#xff0c;&#x4f46;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x5bf9;&#x63d0;&#x793a;&#x8fdb;&#x884c;&#x7b80;&#x5355;&#x589e;&#x5f3a;&#xff08;&#x5982;&#x6dfb;&#x52a0;&#x8bcd;&#x6c47;&#x3001;&#x6539;&#x5199;&#x6216;&#x62fc;&#x5199;&#x9519;&#x8bef;&#xff09;&#x4f1a;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x518d;&#x6b21;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff0c;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x548c;&#x5e94;&#x7528;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x63d0;&#x793a;&#x589e;&#x5f3a;&#x662f;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x7684;&#x5e38;&#x89c1;&#x624b;&#x6bb5;&#xff0c;&#x800c;VLLM&#x5bf9;&#x6b64;&#x7684;&#x8106;&#x5f31;&#x6027;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: PACU&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x63d0;&#x793a;&#x589e;&#x5f3a;&#xff08;PA&#xff09;&#x6a21;&#x5757;&#xff1a;&#x4f7f;&#x7528;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x591a;&#x6837;&#x5316;&#x7684;&#x589e;&#x5f3a;&#x63d0;&#x793a;&#xff08;&#x5982;&#x6539;&#x5199;&#x3001;&#x6dfb;&#x52a0;&#x540e;&#x7f00;&#x7b49;&#xff09;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x53e6;&#x4e00;&#x4e2a;LLM&#x8bc4;&#x4f30;&#x63d0;&#x793a;&#x8d28;&#x91cf;&#xff0c;&#x4ec5;&#x4fdd;&#x7559;&#x9ad8;&#x8d28;&#x91cf;&#x63d0;&#x793a;&#x7528;&#x4e8e;&#x8bad;&#x7ec3;&#xff1b;2&#xff09;&#x63cf;&#x8ff0;&#x5229;&#x7528;&#x751f;&#x6210;&#xff08;CUG&#xff09;&#x673a;&#x5236;&#xff1a;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#xff0c;&#x8054;&#x5408;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff08;Caption&#xff09;&#x4f5c;&#x4e3a;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#xff0c;&#x5f53;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x4e0d;&#x51c6;&#x786e;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x53ef;&#x4ece;&#x63cf;&#x8ff0;&#x4e2d;&#x63d0;&#x53d6;&#x4fe1;&#x606f;&#x8f85;&#x52a9;&#x751f;&#x6210;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x53ef;&#x4f5c;&#x4e3a;&#x5373;&#x63d2;&#x5373;&#x7528;&#x5de5;&#x5177;&#x4e0e;&#x73b0;&#x6709;VLLM&#x53ca;&#x6297;&#x5e7b;&#x89c9;&#x6280;&#x672f;&#x534f;&#x540c;&#x5de5;&#x4f5c;&#x3002;","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;PACU&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;VLLM&#x5904;&#x7406;&#x589e;&#x5f3a;&#x63d0;&#x793a;&#x7684;&#x80fd;&#x529b;&#xff1a;&#x5728;CIEM&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff08;&#x5982;InstructBLIP+Vicuna&#xff09;&#x7684;&#x51c6;&#x786e;&#x7387;&#x4ece;&#x589e;&#x5f3a;&#x63d0;&#x793a;&#x540e;&#x7684;79.5%&#x5f97;&#x5230;&#x663e;&#x8457;&#x6539;&#x5584;&#xff1b;&#x5728;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;PACU&#x4e0e;&#x73b0;&#x6709;&#x6280;&#x672f;&#x7ed3;&#x5408;&#x540e;&#x8fdb;&#x4e00;&#x6b65;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: PACU&#x901a;&#x8fc7;&#x53cc;&#x7ba1;&#x9f50;&#x4e0b;&#x7684;&#x65b9;&#x5f0f;&#xff08;&#x63d0;&#x793a;&#x589e;&#x5f3a;&#x548c;&#x63cf;&#x8ff0;&#x5229;&#x7528;&#xff09;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;VLLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5c24;&#x5176;&#x589e;&#x5f3a;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x591a;&#x6837;&#x5316;&#x63d0;&#x793a;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x3002;&#x5176;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x80fd;&#x4e0e;&#x591a;&#x79cd;&#x73b0;&#x6709;&#x6280;&#x672f;&#x517c;&#x5bb9;&#xff0c;&#x4e3a;VLLM&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x9760;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x63a8;&#x52a8;&#x66f4;&#x7a33;&#x5065;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"46,48"}}],"payload":{"tag":"li","lines":"42,48","fold":1}}],"payload":{"tag":"h4","lines":"40,41"}},{"content":"CAP: Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x7ea6;&#x675f;&#x611f;&#x77e5;&#x63d0;&#x793a;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x53cc;&#x5411;&#x7ea6;&#x675f;&#x548c;&#x4f20;&#x9012;&#x6027;&#x7ea6;&#x675f;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x5728;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#x63a8;&#x7406;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#x63a8;&#x7406;&#x4e2d;&#x5b58;&#x5728;&#x6301;&#x7eed;&#x6027;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x4f1a;&#x751f;&#x6210;&#x5173;&#x4e8e;&#x7269;&#x4f53;&#x4f4d;&#x7f6e;&#x548c;&#x7a7a;&#x95f4;&#x914d;&#x7f6e;&#x7684;&#x9519;&#x8bef;&#x9884;&#x6d4b;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x6b64;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x96f6;&#x6837;&#x672c;&#x7684;&#x7ea6;&#x675f;&#x611f;&#x77e5;&#x63d0;&#x793a;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x79cd;&#x7ea6;&#x675f;&#xff1a;1&#xff09;&#x53cc;&#x5411;&#x7ea6;&#x675f;&#xff1a;&#x901a;&#x8fc7;&#x8981;&#x6c42;&#x6a21;&#x578b;&#x5206;&#x6790;&#x7269;&#x4f53;&#x5bf9;&#x7684;&#x53cd;&#x5411;&#x5173;&#x7cfb;&#xff08;&#x5982;BA&#x548c;AB&#xff09;&#x6765;&#x786e;&#x4fdd;&#x5173;&#x7cfb;&#x4e00;&#x81f4;&#x6027;&#xff1b;2&#xff09;&#x4f20;&#x9012;&#x6027;&#x7ea6;&#x675f;&#xff1a;&#x5f15;&#x5165;&#x7b2c;&#x4e09;&#x4e2a;&#x53c2;&#x8003;&#x7269;&#x4f53;&#xff08;C&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;AC&#x548c;BC&#x5173;&#x7cfb;&#x6765;&#x4f20;&#x9012;&#x6027;&#x5730;&#x7ea6;&#x675f;AB&#x5173;&#x7cfb;&#x3002;&#x6700;&#x7ec8;&#x8fd8;&#x7ed3;&#x5408;&#x4e86;&#x4e24;&#x79cd;&#x7ea6;&#x675f;&#x5f62;&#x6210;&#x7ec4;&#x5408;&#x7ea6;&#x675f;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x57fa;&#x4e8e;&#x6307;&#x4ee4;&#x6a21;&#x677f;&#xff0c;&#x7ed3;&#x5408;&#x96f6;&#x6837;&#x672c;&#x601d;&#x7ef4;&#x94fe;&#xff08;CoT&#xff09;&#x548c;&#x7ed3;&#x6784;&#x5316;&#x8f93;&#x51fa;&#xff0c;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x9010;&#x6b65;&#x63a8;&#x7406;&#x3002;","children":[],"payload":{"tag":"li","lines":"52,53"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;ARO&#x3001;GQA&#x548c;MMRel&#x4e09;&#x4e2a;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x6240;&#x6709;&#x7ea6;&#x675f;&#x65b9;&#x6cd5;&#x5747;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#xff08;vanilla&#x548c;CoT+structure&#xff09;&#x3002;&#x7ec4;&#x5408;&#x7ea6;&#x675f;&#x6548;&#x679c;&#x6700;&#x4f73;&#xff0c;&#x5728;MMRel&#x4e0a;&#x8fbe;&#x5230;92.7%&#x7684;&#x51c6;&#x786e;&#x7387;&#x548c;92.91%&#x7684;F1&#x5206;&#x6570;&#x3002;&#x53cc;&#x5411;&#x7ea6;&#x675f;&#x4e2d;&#xff0c;&#x5148;&#x5206;&#x6790;&#x53cd;&#x5411;&#x5173;&#x7cfb;&#xff08;BA+AB&#xff09;&#x7684;&#x7b56;&#x7565;&#x8868;&#x73b0;&#x6700;&#x4f18;&#x3002;","children":[],"payload":{"tag":"li","lines":"53,54"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x7ea6;&#x675f;&#x611f;&#x77e5;&#x63d0;&#x793a;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff0c;&#x53ef;&#x6269;&#x5c55;&#x5230;&#x5176;&#x4ed6;&#x7a7a;&#x95f4;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#xff0c;&#x4e3a;&#x63d0;&#x5347;LVLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x903b;&#x8f91;&#x4e00;&#x81f4;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;","children":[],"payload":{"tag":"li","lines":"54,56"}}],"payload":{"tag":"li","lines":"50,56","fold":1}}],"payload":{"tag":"h4","lines":"48,49"}}],"payload":{"tag":"h2","lines":"7,8","fold":1}},{"content":"&#x89e3;&#x7801;&#x7b56;&#x7565;","children":[{"content":"&#x5bf9;&#x6bd4;&#x89e3;&#x7801;","children":[{"content":"VCD: Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;VCD&#xff09;&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x548c;&#x5931;&#x771f;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4ea7;&#x751f;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x4f1a;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x5b9e;&#x9645;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x7edf;&#x8ba1;&#x504f;&#x5dee;&#xff08;&#x5982;&#x7269;&#x4f53;&#x5171;&#x73b0;&#x504f;&#x89c1;&#xff09;&#x548c;&#x5355;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#xff08;&#x5982;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff09;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x5bfc;&#x81f4;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x4ea7;&#x751f;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x6216;&#x51b3;&#x7b56;&#x98ce;&#x9669;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"63,64"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;VCD&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x7ecf;&#x8fc7;&#x9ad8;&#x65af;&#x566a;&#x58f0;&#x626d;&#x66f2;&#x7684;&#x5931;&#x771f;&#x56fe;&#x50cf;&#xff08;&#x901a;&#x8fc7;&#x591a;&#x6b65;&#x6269;&#x6563;&#x8fc7;&#x7a0b;&#x751f;&#x6210;&#xff09;&#x6240;&#x4ea7;&#x751f;&#x7684;&#x8f93;&#x51fa;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x5bf9;&#x751f;&#x6210;&#x6982;&#x7387;&#x8fdb;&#x884c;&#x6821;&#x51c6;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;VCD &#x7684;&#x751f;&#x6210;&#x6982;&#x7387;&#x8c03;&#x6574;&#x4e3a;&#xff1a;<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo><mo>=</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>i</mi><mi>t</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=\"normal\">&#x2223;</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>v</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>&#x3b1;</mi><mo stretchy=\"false\">[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=\"normal\">&#x2223;</mi><mi>x</mi><mo separator=\"true\">,</mo><mi>v</mi><mo stretchy=\"false\">)</mo><mo>&#x2212;</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>p</mi><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=\"normal\">&#x2223;</mi><mi>x</mi><mo separator=\"true\">,</mo><msup><mi>v</mi><mo mathvariant=\"normal\" lspace=\"0em\" rspace=\"0em\">&#x2032;</mo></msup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">]</mo></mrow><annotation encoding=\"application/x-tex\">logit(y_t) = logit(y_t | x, v) + &#x3b1; [log p(y_t | x, v) - log p(y_t | x, v&apos;)]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">t</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">&#x200b;</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">i</span><span class=\"mord mathnormal\">t</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">&#x200b;</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">&#x2223;</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">&#x3b1;</span><span class=\"mopen\">[</span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">&#x200b;</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">&#x2223;</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">&#x2212;</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0019em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mord mathnormal\">p</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">&#x200b;</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">&#x2223;</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">v</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7519em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">&#x2032;</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)]</span></span></span></span>&#xff0c;&#x5176;&#x4e2d; v &#x662f;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#xff0c;v&apos; &#x662f;&#x5931;&#x771f;&#x56fe;&#x50cf;&#xff0c;&#x3b1; &#x662f;&#x63a7;&#x5236;&#x5bf9;&#x6bd4;&#x5f3a;&#x5ea6;&#x7684;&#x8d85;&#x53c2;&#x6570;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x4e8e;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"64,65"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;VCD &#x5728;&#x4e0d;&#x540c;&#x5bb6;&#x65cf;&#x7684; LVLM&#xff08;&#x5982; LLaVA-1.5&#x3001;InstructBLIP&#x3001;Qwen-VL&#xff09;&#x4e0a;&#x5747;&#x663e;&#x8457;&#x51cf;&#x8f7b;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3002;&#x5728; POPE &#x57fa;&#x51c6;&#x4e0a; F1 &#x5206;&#x6570;&#x6700;&#x9ad8;&#x63d0;&#x5347;&#x4e86; 7.4&#xff0c;&#x5728; MME &#x57fa;&#x51c6;&#x4e0a;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x4e86; 18%&#x3002;&#x6b64;&#x5916;&#xff0c;VCD &#x8fd8;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x901a;&#x7528;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x57fa;&#x51c6;&#xff08;&#x5982; MME &#x548c; LLaVA-Bench&#xff09;&#x4e0a;&#x7684;&#x6574;&#x4f53;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x5e7f;&#x6cdb;&#x7684;&#x9002;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: VCD &#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x3001;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3; LVLM &#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x673a;&#x5236;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x7edf;&#x8ba1;&#x504f;&#x5dee;&#x548c;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4f7f;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x66f4;&#x8d34;&#x5408;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6210;&#x529f;&#x8868;&#x660e;&#xff0c;&#x65e0;&#x9700;&#x590d;&#x6742;&#x8bad;&#x7ec3;&#x6216;&#x989d;&#x5916;&#x6570;&#x636e;&#xff0c;&#x4ec5;&#x901a;&#x8fc7;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x7684;&#x7b56;&#x7565;&#x6539;&#x8fdb;&#x4e5f;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#xff0c;&#x4e3a; LVLM &#x7684;&#x53ef;&#x9760;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x5b9e;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5e76;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"66,68"}}],"payload":{"tag":"li","lines":"62,68","fold":1}}],"payload":{"tag":"h4","lines":"60,61"}},{"content":"ICD: Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x6307;&#x4ee4;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;ICD&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x6807;&#x51c6;&#x6307;&#x4ee4;&#x548c;&#x6270;&#x52a8;&#x6307;&#x4ee4;&#x7684;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff0c;&#x6709;&#x6548;&#x6291;&#x5236;&#x4e86;&#x5bf9;&#x8c61;&#x7ea7;&#x548c;&#x5c5e;&#x6027;&#x7ea7;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x6574;&#x4f53;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"69,70"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x4e14;&#x8fde;&#x8d2f;&#x7684;&#x54cd;&#x5e94;&#x65f6;&#xff0c;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7684;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x6216;&#x5173;&#x7cfb;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x591a;&#x6a21;&#x6001;&#x51b3;&#x7b56;&#x548c;&#x5f00;&#x653e;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x6307;&#x4ee4;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;ICD&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x4e14;&#x4e0e;&#x6a21;&#x578b;&#x65e0;&#x5173;&#x7684;&#x6280;&#x672f;&#x3002;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x901a;&#x8fc7;&#x5728;&#x539f;&#x59cb;&#x6307;&#x4ee4;&#x524d;&#x6dfb;&#x52a0;&#x89d2;&#x8272;&#x524d;&#x7f00;&#xff08;&#x5982;&#x201c;&#x4f60;&#x662f;&#x4e00;&#x4e2a;&#x56f0;&#x60d1;&#x7684;&#x5bf9;&#x8c61;&#x68c0;&#x6d4b;&#x5668;&#x201d;&#xff09;&#x6784;&#x9020;&#x6270;&#x52a8;&#x6307;&#x4ee4;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x52a0;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x7684;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x3002;ICD&#x5bf9;&#x6bd4;&#x6807;&#x51c6;&#x6307;&#x4ee4;&#x548c;&#x6270;&#x52a8;&#x6307;&#x4ee4;&#x5728;&#x878d;&#x5408;&#x6a21;&#x5757;&#x4e2d;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x901a;&#x8fc7;&#x6982;&#x7387;&#x76f8;&#x51cf;&#x7684;&#x65b9;&#x5f0f;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x6982;&#x5ff5;&#xff08;&#x5373;&#x6270;&#x52a8;&#x6307;&#x4ee4;&#x653e;&#x5927;&#x4f46;&#x6807;&#x51c6;&#x6307;&#x4ee4;&#x672a;&#x5f3a;&#x8c03;&#x7684;&#x90e8;&#x5206;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x9a8c;&#x8bc1;&#x4e86;ICD&#x7684;&#x6709;&#x6548;&#x6027;&#xff1a;1) &#x5224;&#x522b;&#x5f0f;&#x57fa;&#x51c6;&#xff08;POPE&#x548c;MME&#xff09;&#x663e;&#x793a;&#xff0c;&#x5bf9;&#x8c61;&#x7ea7;&#x548c;&#x5c5e;&#x6027;&#x7ea7;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x51cf;&#x5c11;&#xff1b;2) &#x751f;&#x6210;&#x5f0f;&#x57fa;&#x51c6;&#xff08;LLaVa-Bench&#xff09;&#x8bc1;&#x5b9e;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x51c6;&#x786e;&#x6027;&#x63d0;&#x5347;&#xff1b;3) &#x5728;MME&#x7684;14&#x4e2a;&#x901a;&#x7528;&#x611f;&#x77e5;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x4e00;&#x81f4;&#x63d0;&#x5347;&#xff0c;&#x8868;&#x660e;ICD&#x4e0d;&#x4ec5;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x589e;&#x5f3a;&#x4e86;&#x6574;&#x4f53;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"73,74"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ICD&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x901a;&#x7528;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x76f4;&#x63a5;&#x5728;&#x6821;&#x51c6;&#x9636;&#x6bb5;&#x51cf;&#x5c11;LVLM&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6807;&#x6ce8;&#x6216;&#x8bad;&#x7ec3;&#x3002;&#x5176;&#x521b;&#x65b0;&#x70b9;&#x5728;&#x4e8e;&#x5229;&#x7528;&#x6307;&#x4ee4;&#x6270;&#x52a8;&#x63ed;&#x793a;&#x5e76;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x7684;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x7ba1;&#x7406;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;&#x53ef;&#x9760;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x7684;&#x53d1;&#x5c55;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"74,76"}}],"payload":{"tag":"li","lines":"70,76","fold":1}}],"payload":{"tag":"h4","lines":"68,69"}},{"content":"HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: HALC&#x662f;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x7b97;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x9002;&#x5e94;&#x7126;&#x70b9;&#x5bf9;&#x6bd4;&#x673a;&#x5236;&#x548c;&#x5339;&#x914d;&#x6ce2;&#x675f;&#x641c;&#x7d22;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;OH&#xff09;&#xff0c;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3001;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#x6216;&#x5173;&#x7cfb;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3001;&#x540e;&#x5904;&#x7406;&#x6216;&#x989d;&#x5916;&#x6570;&#x636e;&#xff0c;&#x4e14;&#x4e3b;&#x8981;&#x9488;&#x5bf9;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#xff0c;&#x5ffd;&#x7565;&#x4e86;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x6b65;&#x89e3;&#x51b3;&#x3002;","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: HALC&#x91c7;&#x7528;&#x53cc;&#x5c42;&#x7ea7;&#x7b56;&#x7565;&#xff1a;1&#xff09;&#x5c40;&#x90e8;&#x5c42;&#x9762;&#xff1a;&#x901a;&#x8fc7;&#x81ea;&#x9002;&#x5e94;&#x7126;&#x70b9;&#x5bf9;&#x6bd4;&#x5b9a;&#x4f4d;&#x673a;&#x5236;&#xff0c;&#x5b9e;&#x65f6;&#x8bc6;&#x522b;&#x540d;&#x8bcd;&#x3001;&#x5f62;&#x5bb9;&#x8bcd;/&#x526f;&#x8bcd;/&#x6570;&#x8bcd;/&#x52a8;&#x8bcd;/&#x4ee3;&#x8bcd;&#x3001;&#x4ecb;&#x8bcd;&#x7b49;&#x53ef;&#x80fd;&#x5f15;&#x53d1;&#x5e7b;&#x89c9;&#x7684;&#x8bcd;&#x6027;&#x6807;&#x8bb0;&#xff0c;&#x5e76;&#x4e3a;&#x6bcf;&#x4e2a;&#x6807;&#x8bb0;&#x52a8;&#x6001;&#x8ba1;&#x7b97;&#x6700;&#x4f18;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#xff08;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x4e0d;&#x540c;&#x56fe;&#x50cf;&#x533a;&#x57df;&#x7684;logit&#x5206;&#x5e03;&#xff09;&#xff0c;&#x4fee;&#x6b63;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#xff1b;2&#xff09;&#x5168;&#x5c40;&#x5c42;&#x9762;&#xff1a;&#x4f7f;&#x7528;&#x57fa;&#x4e8e;&#x89c6;&#x89c9;&#x5339;&#x914d;&#x5f97;&#x5206;&#x7684;&#x6ce2;&#x675f;&#x641c;&#x7d22;&#x7b97;&#x6cd5;&#xff0c;&#x5728;&#x751f;&#x6210;&#x5b8c;&#x6574;&#x54cd;&#x5e94;&#x65f6;&#x5e73;&#x8861;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x548c;&#x6587;&#x672c;&#x8d28;&#x91cf;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;MiniGPT-4&#x3001;LLaVA&#x7b49;&#x5f00;&#x6e90;LVLMs&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x56db;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5305;&#x62ec;MME&#xff09;&#x4e0a;&#xff0c;HALC&#x663e;&#x8457;&#x964d;&#x4f4e;&#x6240;&#x6709;&#x4e09;&#x7c7b;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5b58;&#x5728;&#x6027;&#x3001;&#x5c5e;&#x6027;&#x3001;&#x5173;&#x7cfb;&#xff09;&#xff0c;&#x5e73;&#x5747;&#x51cf;&#x5c11;84.5%&#x7684;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;BLEU&#x7b49;&#x6587;&#x672c;&#x8d28;&#x91cf;&#x6307;&#x6807;&#x4e0d;&#x4e0b;&#x964d;&#xff0c;&#x4f18;&#x4e8e;&#x6240;&#x6709;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x540e;&#x6821;&#x6b63;&#x3001;&#x81ea;&#x6821;&#x6b63;&#x7b49;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"81,82"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: HALC&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x5bf9;&#x6bd4;&#x548c;&#x5168;&#x5c40;&#x89e3;&#x7801;&#x4f18;&#x5316;&#xff0c;&#x9996;&#x6b21;&#x5168;&#x9762;&#x89e3;&#x51b3;&#x591a;&#x7c7b;&#x578b;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x5177;&#x5907;&#x6613;&#x7528;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x6027;&#x3002;&#x5176;&#x5f00;&#x6e90;&#x5e73;&#x53f0;&#x6574;&#x5408;&#x4e86;&#x591a;&#x79cd;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x548c;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff0c;&#x63a8;&#x52a8;&#x9886;&#x57df;&#x6807;&#x51c6;&#x5316;&#x3002;&#x672a;&#x6765;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x533b;&#x7597;&#x62a5;&#x544a;&#x751f;&#x6210;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x573a;&#x666f;&#x7406;&#x89e3;&#x7b49;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x63a8;&#x7406;&#x7684;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"82,84"}}],"payload":{"tag":"li","lines":"78,84","fold":1}}],"payload":{"tag":"h4","lines":"76,77"}},{"content":"CICD: Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x8de8;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CICD&#xff09;&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x4f7f;&#x7528;&#x4e0d;&#x76f8;&#x5173;&#x7684;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x5bf9;&#x6bd4;&#x8f93;&#x5165;&#xff0c;&#x52a8;&#x6001;&#x6291;&#x5236;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5904;&#x7406;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x65f6;&#xff0c;&#x5e38;&#x56e0;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff08;&#x5373;&#x6a21;&#x578b;&#x4ece;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e2d;&#x5b66;&#x5230;&#x7684;&#x8bed;&#x8a00;&#x6a21;&#x5f0f;&#xff09;&#x800c;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff0c;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x4f46;&#x8bed;&#x8a00;&#x4e0a;&#x5408;&#x7406;&#x7684;&#x8f93;&#x51fa;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5373;&#x4f7f;&#x56fe;&#x50cf;&#x4e2d;&#x6ca1;&#x6709;&#x7535;&#x8111;&#xff0c;&#x6a21;&#x578b;&#x4e5f;&#x53ef;&#x80fd;&#x9519;&#x8bef;&#x5730;&#x63cf;&#x8ff0;&#x201c;&#x7535;&#x8111;&#x201d;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x7684;&#x9ad8;&#x6548;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"87,88"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x8de8;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CICD&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#x4e24;&#x90e8;&#x5206;&#xff1a;1&#xff09;&#x5bf9;&#x6bd4;&#x8f93;&#x5165;&#x6784;&#x5efa;&#xff1a;&#x4ece;&#x6570;&#x636e;&#x96c6;&#x4e2d;&#x68c0;&#x7d22;&#x6216;&#x968f;&#x673a;&#x9009;&#x62e9;&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x5b8c;&#x5168;&#x4e0d;&#x76f8;&#x5173;&#x7684;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x5bf9;&#x6bd4;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#xff0c;&#x907f;&#x514d;&#x8bad;&#x7ec3;-&#x63a8;&#x7406;&#x5dee;&#x5f02;&#x548c;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x91cd;&#x53e0;&#xff1b;2&#xff09;&#x52a8;&#x6001;&#x6291;&#x5236;&#x673a;&#x5236;&#xff1a;&#x5728;&#x6bcf;&#x4e00;&#x6b65;&#x89e3;&#x7801;&#x65f6;&#xff0c;&#x8ba1;&#x7b97;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x5bf9;&#x6bd4;&#x56fe;&#x50cf;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x7684;Jensen-Shannon&#xff08;JS&#xff09;&#x6563;&#x5ea6;&#x3002;&#x82e5;&#x6563;&#x5ea6;&#x4f4e;&#x4e8e;&#x9608;&#x503c;&#xff0c;&#x8bf4;&#x660e;&#x5f53;&#x524d;&#x751f;&#x6210;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff0c;&#x5219;&#x542f;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6291;&#x5236;&#x5148;&#x9a8c;&#xff1b;&#x5426;&#x5219;&#x56de;&#x9000;&#x5230;&#x5e38;&#x89c4;&#x89e3;&#x7801;&#xff0c;&#x9632;&#x6b62;&#x8fc7;&#x5ea6;&#x6291;&#x5236;&#x5f71;&#x54cd;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"88,89"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x548c;LVLM&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA-1.5&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x7ed3;&#x679c;&#x663e;&#x793a;&#xff1a;1&#xff09;CICD&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x4e3b;&#x5bfc;&#x7684;&#x573a;&#x666f;&#x4e0b;&#x6548;&#x679c;&#x7a81;&#x51fa;&#xff1b;2&#xff09;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x6291;&#x5236;&#x673a;&#x5236;&#xff0c;&#x5728;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#x672a;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x6574;&#x4f53;&#x8d28;&#x91cf;&#xff1b;3&#xff09;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x4f4e;&#xff0c;&#x4e14;&#x5177;&#x6709;&#x826f;&#x597d;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CICD&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x6709;&#x6548;&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8de8;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x548c;&#x52a8;&#x6001;&#x6291;&#x5236;&#x673a;&#x5236;&#xff0c;&#x7cbe;&#x51c6;&#x51cf;&#x5c11;&#x4e86;LVLM&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x907f;&#x514d;&#x4e86;&#x73b0;&#x6709;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#x7684;&#x7f3a;&#x9677;&#xff08;&#x5982;&#x5206;&#x5e03;&#x5931;&#x771f;&#x3001;&#x4fe1;&#x53f7;&#x4e0d;&#x5b8c;&#x6574;&#x548c;&#x8fc7;&#x5ea6;&#x6291;&#x5236;&#xff09;&#xff0c;&#x4e3a;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"90,92"}}],"payload":{"tag":"li","lines":"86,92","fold":1}}],"payload":{"tag":"h4","lines":"84,85"}},{"content":"HIO: Alleviating Hallucinations in Large Vision-Language Models through Hallucination-Induced Optimization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;HIO&#xff08;Hallucination-Induced Optimization&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5fae;&#x8c03;&#x7406;&#x8bba;&#x504f;&#x597d;&#x6a21;&#x578b;&#x6765;&#x589e;&#x5f3a;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#x4e0e;&#x76ee;&#x6807;&#x6807;&#x8bb0;&#x4e4b;&#x95f4;&#x7684;&#x5bf9;&#x6bd4;&#x5ea6;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x865a;&#x6784;&#x7269;&#x4f53;&#x3001;&#x9519;&#x8bef;&#x573a;&#x666f;&#x7b49;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x4e8e;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7684;&#x65b9;&#x6cd5;&#x7531;&#x4e8e;&#x5168;&#x5c40;&#x89c6;&#x89c9;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x7684;&#x4e0d;&#x53ef;&#x63a7;&#x6027;&#xff0c;&#x96be;&#x4ee5;&#x7cbe;&#x786e;&#x8bf1;&#x5bfc;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#xff0c;&#x5bfc;&#x81f4;&#x7f13;&#x89e3;&#x6548;&#x679c;&#x6709;&#x9650;&#x751a;&#x81f3;&#x4ea7;&#x751f;&#x65b0;&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLM&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;HIO&#x4f18;&#x5316;&#x7b56;&#x7565;&#xff0c;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x7406;&#x8bba;&#x5206;&#x6790;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x673a;&#x5236;&#xff0c;&#x53d1;&#x73b0;&#x589e;&#x5f3a;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#x4e0e;&#x76ee;&#x6807;&#x6807;&#x8bb0;&#x7684;&#x5bf9;&#x6bd4;&#x5ea6;&#x53ef;&#x63d0;&#x5347;&#x89e3;&#x7801;&#x6548;&#x679c;&#xff1b;2&#xff09;&#x57fa;&#x4e8e;Bradley-Terry&#x6a21;&#x578b;&#x6784;&#x5efa;&#x7406;&#x8bba;&#x504f;&#x597d;&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;&#x5fae;&#x8c03;&#x8bf1;&#x5bfc;&#x6a21;&#x578b;&#x751f;&#x6210;&#x66f4;&#x591a;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff08;&#x4f5c;&#x4e3a;&#x8d1f;&#x6837;&#x672c;&#xff09;&#x548c;&#x771f;&#x5b9e;&#x54cd;&#x5e94;&#xff08;&#x4f5c;&#x4e3a;&#x6b63;&#x6837;&#x672c;&#xff09;&#xff1b;3&#xff09;&#x5229;&#x7528;&#x8fd9;&#x4e9b;&#x504f;&#x597d;&#x6570;&#x636e;&#x4f18;&#x5316;&#x6a21;&#x578b;&#x53c2;&#x6570;&#xff0c;&#x663e;&#x8457;&#x653e;&#x5927;&#x4e24;&#x7c7b;&#x6807;&#x8bb0;&#x7684;logits&#x5dee;&#x5f02;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x4e2d;&#x66f4;&#x6709;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;HIO&#x5728;CHAIR&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#x6570;&#x91cf;&#xff08;&#x5982;&#x56fe;1&#x53f3;&#x6240;&#x793a;&#xff09;&#xff0c;&#x4f18;&#x4e8e;&#x6240;&#x6709;&#x73b0;&#x6709;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#x6216;&#x504f;&#x5dee;&#x653e;&#x5927;&#x7684;&#x65b9;&#x6cd5;&#xff09;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;HIO&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x4e2d;&#x5e7b;&#x89c9; token &#x66f4;&#x5c11;&#xff0c;&#x4e14;&#x5728;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x548c;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x4fdd;&#x6301;&#x66f4;&#x9ad8;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: HIO&#x901a;&#x8fc7;&#x7406;&#x8bba;&#x9a71;&#x52a8;&#x7684;&#x504f;&#x597d;&#x4f18;&#x5316;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x63a7;&#x751f;&#x6210;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x5176;&#x65b9;&#x6cd5;&#x8f7b;&#x91cf;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x66f4;&#x590d;&#x6742;&#x7684;&#x573a;&#x666f;&#xff08;&#x5982;&#x89c6;&#x9891;&#x7406;&#x89e3;&#x6216;&#x5177;&#x8eab;&#x667a;&#x80fd;&#xff09;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5bf9;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x7684;&#x611f;&#x77e5;&#x548c;&#x63cf;&#x8ff0;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"98,100"}}],"payload":{"tag":"li","lines":"94,100","fold":1}}],"payload":{"tag":"h4","lines":"92,93"}},{"content":"CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Model","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CODE&#x7684;&#x65b0;&#x578b;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5229;&#x7528;&#x5927;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x4f5c;&#x4e3a;&#x5bf9;&#x6bd4;&#x53c2;&#x8003;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x8bcd;&#x6c47;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#xff08;LMM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x63d0;&#x5347;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x4e00;&#x81f4;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"101,102"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#xff08;LMM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x4e0e;&#x54cd;&#x5e94;&#x751f;&#x6210;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x65e0;&#x5173;&#x7684;&#x9519;&#x8bef;&#x54cd;&#x5e94;&#x3002;&#x8fd9;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#x3002;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x8de8;&#x6a21;&#x6001;&#x4e00;&#x81f4;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: CODE&#x7684;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#x662f;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff1a;&#x9996;&#x5148;&#x8ba9;LMM&#x4e3a;&#x7ed9;&#x5b9a;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x4e00;&#x4e2a;&#x8be6;&#x7ec6;&#x7684;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#xff08;Self-gen Task&#xff09;&#xff1b;&#x7136;&#x540e;&#x5728;&#x89e3;&#x7801;&#x6bcf;&#x4e2a;&#x54cd;&#x5e94;&#x8bcd;&#x5143;&#x65f6;&#xff0c;&#x540c;&#x65f6;&#x8ba1;&#x7b97;&#x57fa;&#x4e8e;&#x771f;&#x5b9e;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x548c;&#x57fa;&#x4e8e;&#x81ea;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x7684;&#x4e24;&#x79cd;logit&#x6982;&#x7387;&#x5206;&#x5e03;&#xff1b;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x8fd9;&#x4e24;&#x79cd;&#x5206;&#x5e03;&#xff08;&#x516c;&#x5f0f;&#xff1a;(1+&#x3b1;)<em>logit(&#x56fe;&#x50cf;) - &#x3b1;</em>logit(&#x63cf;&#x8ff0;)&#xff09;&#xff0c;&#x6291;&#x5236;&#x4e0e;&#x63cf;&#x8ff0;&#x4e0d;&#x4e00;&#x81f4;&#x4f46;&#x53ef;&#x80fd;&#x5e7b;&#x89c9;&#x7684;&#x8bcd;&#x5143;&#xff0c;&#x589e;&#x5f3a;&#x4e0e;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x4e00;&#x81f4;&#x7684;&#x8bcd;&#x5143;&#x6982;&#x7387;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x8fd8;&#x5f15;&#x5165;&#x4e86;&#x52a8;&#x6001;&#x9650;&#x5236;&#x7b56;&#x7565;&#xff0c;&#x6839;&#x636e;&#x8bcd;&#x6c47;&#x5206;&#x5e03;&#x81ea;&#x9002;&#x5e94;&#x8c03;&#x6574;&#x4fe1;&#x606f;&#x6d41;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;LMM&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"104,105"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;&#x5148;&#x8fdb;LMM&#xff08;&#x5982;LLaVA&#x3001;InternVL&#xff09;&#x548c;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MMVP&#x3001;LLaVA-Bench&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#x3002;&#x7ed3;&#x679c;&#x663e;&#x793a;&#xff1a;CODE&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x54cd;&#x5e94;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x7684;&#x4e00;&#x81f4;&#x6027;&#xff1b;&#x5728;&#x591a;&#x9879;&#x57fa;&#x51c6;&#x4e2d;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff0c;&#x540c;&#x65f6;&#x589e;&#x5f3a;&#x4e86;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x7684;&#x76f8;&#x5173;&#x6027;&#x548c;&#x4fe1;&#x606f;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"105,106"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CODE&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x6709;&#x6548;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x80fd;&#x663e;&#x8457;&#x51cf;&#x8f7b;LMM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x8de8;&#x6a21;&#x6001;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x7684;&#x7279;&#x6027;&#xff0c;&#x4f7f;&#x5176;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x9002;&#x7528;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x6f5c;&#x529b;&#xff0c;&#x4e3a;&#x6539;&#x5584;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"106,108"}}],"payload":{"tag":"li","lines":"102,108","fold":1}}],"payload":{"tag":"h4","lines":"100,101"}},{"content":"RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: RITUAL&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x968f;&#x673a;&#x56fe;&#x50cf;&#x53d8;&#x6362;&#xff08;&#x5982;&#x7ffb;&#x8f6c;&#x3001;&#x88c1;&#x526a;&#xff09;&#x751f;&#x6210;&#x591a;&#x89c6;&#x89d2;&#x8f93;&#x5165;&#xff0c;&#x5e76;&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x6982;&#x7387;&#x96c6;&#x6210;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x5347;&#x7ea7;&#x7248;RITUAL+&#x901a;&#x8fc7;&#x6a21;&#x578b;&#x81ea;&#x53cd;&#x9988;&#x9009;&#x62e9;&#x6700;&#x4f18;&#x53d8;&#x6362;&#xff0c;&#x8fdb;&#x4e00;&#x6b65;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x8f93;&#x51fa;&#xff0c;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x654f;&#x611f;&#x5e94;&#x7528;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x590d;&#x6742;&#x8bad;&#x7ec3;&#x3001;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x6216;&#x53cd;&#x9988;&#x673a;&#x5236;&#xff0c;&#x90e8;&#x7f72;&#x6210;&#x672c;&#x9ad8;&#x3002;&#x672c;&#x7814;&#x7a76;&#x65e8;&#x5728;&#x4ee5;&#x7b80;&#x5355;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x5f0f;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x6838;&#x5fc3;&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#xff1a;1) RITUAL&#xff1a;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x5bf9;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x968f;&#x673a;&#x5e94;&#x7528;&#x51e0;&#x4f55;&#xff08;&#x7ffb;&#x8f6c;&#x3001;&#x65cb;&#x8f6c;&#x3001;&#x88c1;&#x526a;&#xff09;&#x548c;&#x5916;&#x89c2;&#xff08;&#x989c;&#x8272;&#x6296;&#x52a8;&#x3001;&#x9ad8;&#x65af;&#x6a21;&#x7cca;&#xff09;&#x53d8;&#x6362;&#xff0c;&#x751f;&#x6210;&#x53d8;&#x6362;&#x56fe;&#x50cf;V(T)&#xff0c;&#x5c06;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x53d8;&#x6362;&#x56fe;&#x50cf;&#x7684;&#x8f93;&#x51fa;&#x6982;&#x7387;&#x5206;&#x5e03;&#x901a;&#x8fc7;&#x52a0;&#x6743;&#x96c6;&#x6210;&#xff08;&#x8d85;&#x53c2;&#x6570;&#x3b1;&#x5e73;&#x8861;&#xff09;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7ed3;&#x679c;&#xff1b;2) RITUAL+&#xff1a;&#x901a;&#x8fc7;LVLM&#x81ea;&#x53cd;&#x9988;&#x673a;&#x5236;&#x8bc4;&#x4f30;&#x4e0d;&#x540c;&#x53d8;&#x6362;&#x5bf9;&#x5f53;&#x524d;&#x4efb;&#x52a1;&#x7684;&#x6536;&#x76ca;&#xff0c;&#x81ea;&#x52a8;&#x9009;&#x62e9;&#x6700;&#x6709;&#x6548;&#x7684;&#x53d8;&#x6362;&#x800c;&#x975e;&#x968f;&#x673a;&#x9009;&#x62e9;&#xff0c;&#x907f;&#x514d;&#x8d1f;&#x9762;&#x5e72;&#x6270;&#x3002;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#x5747;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x3001;CHAIR&#x3001;MME-Hallucination&#x548c;MME-Fullset&#x7b49;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;RITUAL&#x548c;RITUAL+&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c; consistently&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#xff08;&#x5982;DoLa&#xff09;&#x3002;&#x5177;&#x4f53;&#x5730;&#xff0c;&#x5728;MME-Hallucination&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x4e0d;&#x540c;&#x53d8;&#x6362;&#x5bf9;&#x5b50;&#x4efb;&#x52a1;&#x6709;&#x5dee;&#x5f02;&#x5316;&#x5f71;&#x54cd;&#xff08;&#x5982;&#x9ad8;&#x65af;&#x6a21;&#x7cca;&#x5728;&#x5b58;&#x5728;&#x6027;&#x68c0;&#x6d4b;&#x4e2d;&#x63d0;&#x5347;&#x5206;&#x6570;&#xff0c;&#x88c1;&#x526a;&#x5728;&#x989c;&#x8272;&#x4efb;&#x52a1;&#x4e2d;&#x6709;&#x6548;&#xff09;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x81ea;&#x9002;&#x5e94;&#x9009;&#x62e9;&#xff08;RITUAL+&#xff09;&#x7684;&#x5fc5;&#x8981;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"113,114"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x968f;&#x673a;&#x56fe;&#x50cf;&#x53d8;&#x6362;&#x53ef;&#x4f5c;&#x4e3a;&#x666e;&#x9002;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x6297;&#x5e7b;&#x89c9;&#x6760;&#x6746;&#xff0c;&#x901a;&#x8fc7;&#x591a;&#x89c6;&#x89d2;&#x96c6;&#x6210;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x9c81;&#x68d2;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7b80;&#x5355;&#x3001;&#x901a;&#x7528;&#xff08;&#x6a21;&#x578b;&#x65e0;&#x5173;&#xff09;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x4e3a;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x66f4;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#xff0c;&#x4ee5;&#x53ca;&#x4e3a;&#x6a21;&#x578b;&#x7ea0;&#x9519;&#x673a;&#x5236;&#x63d0;&#x4f9b;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"114,116"}}],"payload":{"tag":"li","lines":"110,116","fold":1}}],"payload":{"tag":"h4","lines":"108,109"}},{"content":"MoD: Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;MoD&#xff08;Mixture of Decoding&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6b63;&#x786e;&#x6027;&#xff0c;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x9009;&#x62e9;&#x4e92;&#x8865;&#x6216;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x6700;&#x4f18;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"117,118"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x591a;&#x79cd;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x8fde;&#x8d2f;&#x4f46;&#x5b9e;&#x9645;&#x4e0e;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x77db;&#x76fe;&#x6216;&#x65e0;&#x5173;&#x7684;&#x5185;&#x5bb9;&#x3002;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x3001;M3ID&#x3001;AvisC&#xff09;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x504f;&#x5dee;&#xff0c;&#x6216;&#x672a;&#x80fd;&#x5145;&#x5206;&#x8003;&#x8651;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x5f71;&#x54cd;&#x548c;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x7684;&#x53ef;&#x53d8;&#x6027;&#xff0c;&#x5b58;&#x5728;&#x660e;&#x663e;&#x5c40;&#x9650;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;MoD&#xff08;&#x6df7;&#x5408;&#x89e3;&#x7801;&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x539f;&#x59cb;&#x56fe;&#x50cf;token&#x751f;&#x6210;&#x7684;&#x8f93;&#x51fa;&#x4e0e;&#x6a21;&#x578b;&#x6ce8;&#x610f;&#x529b;&#x805a;&#x7126;&#x7684;&#x56fe;&#x50cf;token&#x751f;&#x6210;&#x7684;&#x8f93;&#x51fa;&#x4e4b;&#x95f4;&#x7684;Jensen-Shannon (JS) &#x6563;&#x5ea6;&#xff0c;&#x6765;&#x8bc4;&#x4f30;&#x6ce8;&#x610f;&#x529b;&#x6b63;&#x786e;&#x6027;&#x5e76;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#xff1a;1) &#x63d0;&#x53d6;&#x6a21;&#x578b;&#x9ad8;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x50cf;token&#x5e76;&#x63a9;&#x853d;&#x4f4e;&#x6ce8;&#x610f;&#x529b;token&#xff0c;&#x5f62;&#x6210;&#x201c;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x50cf;&#x201d;&#xff1b;2) &#x5206;&#x522b;&#x8ba1;&#x7b97;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x201c;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x50cf;&#x201d;&#x8f93;&#x5165;&#x4e0b;&#x7684;&#x8f93;&#x51fa;logits&#xff1b;3) &#x8ba1;&#x7b97;&#x4e24;&#x8005;JS&#x6563;&#x5ea6;&#x4ee5;&#x5224;&#x65ad;&#x4e00;&#x81f4;&#x6027;&#xff08;&#x5373;&#x6ce8;&#x610f;&#x529b;&#x6b63;&#x786e;&#x6027;&#xff09;&#xff1b;4) &#x81ea;&#x9002;&#x5e94;&#x9009;&#x62e9;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff1a;&#x82e5;&#x4e00;&#x81f4;&#xff08;&#x6ce8;&#x610f;&#x529b;&#x6b63;&#x786e;&#xff09;&#xff0c;&#x91c7;&#x7528;&#x4e92;&#x8865;&#x7b56;&#x7565;&#x653e;&#x5927;&#x5173;&#x952e;&#x4fe1;&#x606f;&#xff1b;&#x82e5;&#x4e0d;&#x4e00;&#x81f4;&#xff08;&#x6ce8;&#x610f;&#x529b;&#x9519;&#x8bef;&#xff09;&#xff0c;&#x91c7;&#x7528;&#x5bf9;&#x6bd4;&#x7b56;&#x7565;&#x6291;&#x5236;&#x8bef;&#x5bfc;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) JS&#x6563;&#x5ea6;&#x80fd;&#x6709;&#x6548;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x4e0e;&#x975e;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff08;&#x5728;POPE&#x5224;&#x522b;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x975e;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#x96c6;&#x4e2d;&#x5728;&#x4f4e;JS&#x6563;&#x5ea6;&#x533a;&#xff0c;&#x5e7b;&#x89c9;&#x5448;&#x957f;&#x5c3e;&#x5206;&#x5e03;&#xff1b;&#x5728;CHAIR&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;JS&#x6563;&#x5ea6;&#x4e0e;CHAIRi&#x6307;&#x6807;&#x5448;&#x663e;&#x8457;&#x6b63;&#x76f8;&#x5173;&#xff0c;&#x76ae;&#x5c14;&#x900a;&#x7cfb;&#x6570;&#x8fbe;0.85&#xff09;&#x3002;2) MoD&#x5728;&#x591a;&#x4e2a;&#x4e3b;&#x6d41;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;POPE&#x3001;MME&#x3001;MM-Vet&#x3001;CHAIR&#xff09;&#x4e0a;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x6240;&#x6709;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x57fa;&#x4e8e;&#x6ce8;&#x610f;&#x529b;&#x6b63;&#x786e;&#x6027;&#x81ea;&#x9002;&#x5e94;&#x9009;&#x62e9;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x7684;MoD&#x65b9;&#x6cd5;&#xff0c;&#x4e3a;&#x7f13;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x5f3a;&#x5927;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x5173;&#x952e;&#x521b;&#x65b0;&#x5728;&#x4e8e;&#x901a;&#x8fc7;&#x8f93;&#x51fa;&#x4e00;&#x81f4;&#x6027;&#x52a8;&#x6001;&#x8bc4;&#x4f30;&#x6ce8;&#x610f;&#x529b;&#x8d28;&#x91cf;&#xff0c;&#x5e76;&#x636e;&#x6b64;&#x7075;&#x6d3b;&#x8c03;&#x6574;&#x89e3;&#x7801;&#x65b9;&#x5f0f;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3001;&#x5927;&#x89c4;&#x6a21;&#x6570;&#x636e;&#x6d4b;&#x8bd5;&#x6216;&#x91cd;&#x590d;&#x91c7;&#x6837;&#xff0c;&#x8ba1;&#x7b97;&#x9ad8;&#x6548;&#xff0c;&#x6613;&#x4e8e;&#x96c6;&#x6210;&#x3002;MoD&#x7684;&#x6210;&#x529f;&#x8bc1;&#x660e;&#x4e86;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x7cbe;&#x7ec6;&#x8c03;&#x63a7;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x7684;&#x91cd;&#x8981;&#x6027;&#xff0c;&#x5bf9;&#x63d0;&#x5347;LVLM&#x5728;&#x73b0;&#x5b9e;&#x4e16;&#x754c;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x5177;&#x6709;&#x91cd;&#x8981;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"122,124"}}],"payload":{"tag":"li","lines":"118,124","fold":1}}],"payload":{"tag":"h4","lines":"116,117"}},{"content":"RBD: Mitigating Hallucination in Visual-Language Models via Re-Balancing Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;RBD&#x7684;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x5206;&#x652f;&#x91cd;&#x65b0;&#x6821;&#x51c6;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x4ee5;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x6587;&#x672c;&#x77e5;&#x8bc6;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x77e5;&#x8bc6;&#x51b2;&#x7a81;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;CHAIR&#x548c;POPEmetrics&#x4e0a;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"125,126"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x503e;&#x5411;&#x4e8e;&#x4f9d;&#x8d56;&#x5185;&#x90e8;&#x6587;&#x672c;&#x77e5;&#x8bc6;&#x800c;&#x975e;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#xff0c;&#x5bfc;&#x81f4;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x67b6;&#x6784;&#x4e2d;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;&#x5927;&#x578b;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x7684;&#x4e0d;&#x5e73;&#x8861;&#xff0c;&#x4ee5;&#x53ca;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x504f;&#x5411;&#x6587;&#x672c;token&#x800c;&#x975e;&#x89c6;&#x89c9;token&#x3002;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;VLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"127,128"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Re-Balancing Contrastive Decoding&#xff08;RBD&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x91c7;&#x7528;&#x53cc;&#x5206;&#x652f;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff1a;1&#xff09;&#x6587;&#x672c;&#x5206;&#x652f;&#xff1a;&#x901a;&#x8fc7;&#x5411;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#xff08;&#x5982;&#x9ad8;&#x65af;&#x566a;&#x58f0;&#xff09;&#xff0c;&#x523a;&#x6fc0;&#x6a21;&#x578b;&#x66b4;&#x9732;&#x5176;&#x5bf9;&#x6587;&#x672c;&#x77e5;&#x8bc6;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x4ece;&#x800c;&#x8bc6;&#x522b;&#x5e76;&#x60e9;&#x7f5a;&#x6e90;&#x81ea;&#x6587;&#x672c;&#x504f;&#x89c1;&#x7684;token&#xff1b;2&#xff09;&#x89c6;&#x89c9;&#x5206;&#x652f;&#xff1a;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x663e;&#x8457;&#x89c6;&#x89c9;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x63d0;&#x5347;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x7684;&#x5f71;&#x54cd;&#x529b;&#x3002;&#x6700;&#x7ec8;&#xff0c;&#x901a;&#x8fc7;&#x52a0;&#x6743;&#x6574;&#x5408;&#x4e24;&#x4e2a;&#x5206;&#x652f;&#x7684;logits&#xff08;&#x4f7f;&#x7528;&#x8d85;&#x53c2;&#x6570;&#x3b1;&#x63a7;&#x5236;&#x5e73;&#x8861;&#xff09;&#xff0c;&#x91cd;&#x65b0;&#x6821;&#x51c6;&#x6a21;&#x578b;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x516c;&#x5f0f;&#x4e3a;&#xff1a;PRBD(y|v,x) = Softmax[(1-&#x3b1;)<em>logit_&#x3b8;(y|v,x) + &#x3b1;</em>(logit_(&#x3b8;,v)(y|v,x) - logit_(&#x3b8;,t)(y|v,x))]&#x3002;","children":[],"payload":{"tag":"li","lines":"128,129"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1&#xff09;RBD&#x5728;CHAIR&#x548c;POPEmetrics&#x4e0a;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff1b;2&#xff09;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x539f;&#x59cb;VLMs&#xff08;&#x5982;LLaVA-v1.5&#xff09;&#x5bf9;&#x89c6;&#x89c9;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x4ec5;&#x5360;25%&#xff0c;&#x4e14;&#x6df1;&#x5c42;&#x7f51;&#x7edc;&#x4e2d;&#x8be5;&#x6bd4;&#x4f8b;&#x8fdb;&#x4e00;&#x6b65;&#x4e0b;&#x964d;&#xff0c;&#x800c;RBD&#x6709;&#x6548;&#x7ea0;&#x6b63;&#x4e86;&#x8fd9;&#x4e00;&#x4e0d;&#x5e73;&#x8861;&#xff1b;3&#xff09;&#x6d88;&#x878d;&#x5b9e;&#x9a8c;&#x9a8c;&#x8bc1;&#x4e86;&#x53cc;&#x5206;&#x652f;&#x7b56;&#x7565;&#x7684;&#x5fc5;&#x8981;&#x6027;&#xff0c;&#x6587;&#x672c;&#x5206;&#x652f;&#x8d1f;&#x8d23;&#x6291;&#x5236;&#x6587;&#x672c;&#x504f;&#x89c1;&#xff0c;&#x89c6;&#x89c9;&#x5206;&#x652f;&#x8d1f;&#x8d23;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4fe1;&#x53f7;&#x3002;","children":[],"payload":{"tag":"li","lines":"129,130"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: RBD&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x5373;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;VLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x7684;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x63ed;&#x793a;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4e0d;&#x5e73;&#x8861;&#x662f;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x539f;&#x56e0;&#xff0c;&#x5e76;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x63a7;&#x751f;&#x6210;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;VLMs&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4ee5;&#x53ca;&#x63a8;&#x52a8;&#x66f4;&#x5747;&#x8861;&#x7684;&#x591a;&#x6a21;&#x6001;&#x8868;&#x793a;&#x5b66;&#x4e60;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"130,132"}}],"payload":{"tag":"li","lines":"126,132","fold":1}}],"payload":{"tag":"h4","lines":"124,125"}},{"content":"LCD: Mitigating Hallucinations in Large Vision-Language Models (LVLMs) via Language-Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x8bed;&#x8a00;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;LCD&#xff09;&#x7684;&#x65b0;&#x7b97;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;LVLM&#x548c;&#x7eaf;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x7684;&#x8f93;&#x51fa;&#x6982;&#x7387;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x751f;&#x6210;&#x7ed3;&#x679c;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x590d;&#x6742;&#x540e;&#x5904;&#x7406;&#x3002;&#x5b9e;&#x9a8c;&#x663e;&#x793a;&#xff0c;LCD&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x6307;&#x6807;&#xff0c;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"133,134"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5904;&#x7406;&#x591a;&#x6a21;&#x6001;&#x8f93;&#x5165;&#x65f6;&#xff0c;&#x5e38;&#x56e0;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x7ebf;&#x7d22;&#x548c;&#x5df2;&#x5b66;&#x4e60;&#x7684;&#x7269;&#x4f53;&#x5171;&#x73b0;&#x504f;&#x89c1;&#x800c;&#x4ea7;&#x751f;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x8bef;&#x5bfc;&#x7528;&#x6237;&#x5e76;&#x9650;&#x5236;&#x6a21;&#x578b;&#x5728;&#x5173;&#x952e;&#x573a;&#x666f;&#xff08;&#x5982;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x96c6;&#x4e2d;&#x4e8e;&#x91cf;&#x5316;&#x5e7b;&#x89c9;&#xff0c;&#x7f3a;&#x4e4f;&#x6709;&#x6548;&#x7684;&#x7f13;&#x89e3;&#x7b56;&#x7565;&#xff0c;&#x4e14;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5e38;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6a21;&#x578b;&#x8c03;&#x6574;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x6cdb;&#x5316;&#x6027;&#x5dee;&#x3002;","children":[],"payload":{"tag":"li","lines":"135,136"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x8bed;&#x8a00;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;LCD&#xff09;&#x7b97;&#x6cd5;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x5728;LVLM&#x7684;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x52a8;&#x6001;&#x5bf9;&#x6bd4;LVLM&#x548c;&#x5185;&#x90e8;LLM&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x5728;&#x6bcf;&#x4e00;&#x6b65;&#x751f;&#x6210;&#x65f6;&#xff0c;&#x8ba1;&#x7b97;LVLM&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x548c;&#x6587;&#x672c;&#x7684;&#x4e0b;&#x4e00;&#x8bcd;&#x5143;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff08;P_LVLM&#xff09;&#xff1b;2) &#x8ba1;&#x7b97;&#x7eaf;LLM&#xff08;&#x4ec5;&#x57fa;&#x4e8e;&#x6587;&#x672c;&#xff09;&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff08;P_LLM&#xff09;&#xff1b;3) &#x5229;&#x7528;LLM&#x5206;&#x5e03;&#x7684;&#x71b5;&#x52a8;&#x6001;&#x8ba1;&#x7b97;&#x6743;&#x91cd;&#x3b2;_t&#xff08;&#x3b2;_t = &#x3b2; / H_LLM&#xff0c;&#x5176;&#x4e2d;H_LLM&#x662f;LLM&#x7684;&#x6761;&#x4ef6;&#x71b5;&#xff09;&#xff1b;4) &#x901a;&#x8fc7;LCD&#x516c;&#x5f0f;&#x8c03;&#x6574;LVLM&#x7684;logits&#xff1a;LCD = (1 + &#x3b2;_t) * log(P_LVLM) - &#x3b2;_t * log(P_LLM)&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x6216;&#x91cd;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x4e8e;&#x4e0d;&#x540c;LVLM&#xff08;&#x5982;InstructBLIP&#x3001;LLaVA-1.5&#x7b49;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"136,137"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1) &#x5728;POPE&#x57fa;&#x51c6;&#xff08;COCO&#x6570;&#x636e;&#x96c6;&#xff09;&#x4e0a;&#xff0c;LCD&#x5728;12&#x4e2a;&#x914d;&#x7f6e;&#x4e2d;&#x7684;11&#x4e2a;&#x63d0;&#x5347;&#x4e86;F1&#x5206;&#x6570;&#xff08;&#x6700;&#x9ad8;&#x63d0;&#x5347;4%&#xff09;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff1b;2) &#x5728;&#x8be6;&#x7ec6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;LCD&#x964d;&#x4f4e;&#x4e86;CHAIR&#x6307;&#x6807;&#xff08;CHAIRs&#x6700;&#x9ad8;&#x964d;&#x4f4e;36%&#xff0c;CHAIRi&#x6700;&#x9ad8;&#x964d;&#x4f4e;24%&#xff09;&#xff0c;&#x8868;&#x660e;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x6570;&#x91cf;&#x548c;&#x9891;&#x7387;&#x5747;&#x51cf;&#x5c11;&#xff1b;3) &#x751f;&#x6210;&#x8d28;&#x91cf;&#x672a;&#x4e0b;&#x964d;&#xff0c;Captioning&#x6307;&#x6807;&#xff08;&#x5982;METEOR&#x3001;ROUGE-L&#xff09;&#x548c;GPT-4V&#x8bc4;&#x4f30;&#x7684;&#x51c6;&#x786e;&#x6027;&#xff08;Accuracy&#xff09;&#x548c;&#x8be6;&#x7ec6;&#x6027;&#xff08;Detailedness&#xff09;&#x5206;&#x6570;&#x5747;&#x6709;&#x63d0;&#x5347;&#x3002;&#x4f8b;&#x5982;&#xff0c;InstructBLIP-Vicuna&#x7684;&#x51c6;&#x786e;&#x6027;&#x4ece;3.7&#x63d0;&#x5347;&#x81f3;4.59&#xff08;5&#x5206;&#x5236;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: LCD&#x901a;&#x8fc7;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x5e72;&#x9884;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8bc1;&#x660e;&#x5bf9;&#x6bd4;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#x662f;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x504f;&#x89c1;&#x7684;&#x5173;&#x952e;&#x9014;&#x5f84;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff08;&#x65e0;&#x9700;&#x91cd;&#x8bad;&#x7ec3;&#xff09;&#x3001;&#x9ad8;&#x6548;&#x6027;&#x548c;&#x6613;&#x7528;&#x6027;&#xff0c;&#x4e3a;&#x672a;&#x6765;LVLM&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x7814;&#x7a76;&#x5f00;&#x8f9f;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x771f;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4ee5;&#x53ca;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;&#x751f;&#x6210;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x63a7;&#x6027;&#x548c;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"138,140"}}],"payload":{"tag":"li","lines":"134,140","fold":1}}],"payload":{"tag":"h4","lines":"132,133"}},{"content":"VACoDe: Visual Augmented Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: VACoDe&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x81ea;&#x9002;&#x5e94;&#x9009;&#x62e9;&#x6700;&#x4f73;&#x56fe;&#x50cf;&#x589e;&#x5f3a;&#x65b9;&#x6cd5;&#x7684;&#x65b0;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x7684;&#x5dee;&#x5f02;&#xff0c;&#x9009;&#x62e9;&#x5bf9;&#x6bd4;&#x5ea6;&#x6700;&#x5f3a;&#x7684;&#x589e;&#x5f3a;&#x65b9;&#x5f0f;&#xff0c;&#x7ed3;&#x5408;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6280;&#x672f;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"141,142"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0d;&#x51c6;&#x786e;&#x7684;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#x3002;&#x867d;&#x7136;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x4f7f;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CD&#xff09;&#x548c;&#x56fe;&#x50cf;&#x589e;&#x5f3a;&#x6765;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e24;&#x4e2a;&#x4e3b;&#x8981;&#x5c40;&#x9650;&#xff1a;&#x4e00;&#x662f;&#x4f9d;&#x8d56;&#x5355;&#x4e00;&#x589e;&#x5f3a;&#x65b9;&#x5f0f;&#xff0c;&#x65e0;&#x6cd5;&#x9002;&#x5e94;&#x4e0d;&#x540c;&#x4efb;&#x52a1;&#x9700;&#x6c42;&#xff1b;&#x4e8c;&#x662f;&#x9700;&#x8981;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x6216;&#x9ad8;&#x6210;&#x672c;&#x8d44;&#x6e90;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x9650;&#x5236;&#x4e86;CD&#x65b9;&#x6cd5;&#x7684;&#x901a;&#x7528;&#x6027;&#x548c;&#x6548;&#x679c;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x7075;&#x6d3b;&#x3001;&#x81ea;&#x9002;&#x5e94;&#x7684;&#x589e;&#x5f3a;&#x9009;&#x62e9;&#x673a;&#x5236;&#x6765;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"143,144"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: VACoDe&#x7684;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x4e09;&#x6b65;&#xff1a;1) &#x5bf9;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5e94;&#x7528;&#x591a;&#x79cd;&#x589e;&#x5f3a;&#x64cd;&#x4f5c;&#xff08;&#x5982;&#x989c;&#x8272;&#x53cd;&#x8f6c;&#x3001;&#x7ffb;&#x8f6c;&#x3001;&#x88c1;&#x526a;&#x3001;&#x64e6;&#x9664;&#x3001;&#x9510;&#x5316;&#x3001;&#x8fb9;&#x7f18;&#x63d0;&#x53d6;&#x548c;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#xff09;&#xff0c;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#xff1b;2) &#x4f7f;&#x7528;LVLM&#x5206;&#x522b;&#x8ba1;&#x7b97;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x6bcf;&#x4e2a;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x7684;&#x8f93;&#x51fa;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x63d0;&#x51fa;&#x7684;softmax&#x8ddd;&#x79bb;&#x5ea6;&#x91cf;&#xff08;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff09;&#x8bc4;&#x4f30;&#x6bcf;&#x4e2a;&#x589e;&#x5f3a;&#x7684;&#x5bf9;&#x6bd4;&#x5ea6;&#x5f3a;&#x5ea6;&#xff1b;3) &#x9009;&#x62e9;&#x5bf9;&#x6bd4;&#x5ea6;&#x6700;&#x5f3a;&#x7684;&#x589e;&#x5f3a;&#x65b9;&#x5f0f;&#xff0c;&#x5c06;&#x5176;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x4e0e;&#x539f;&#x59cb;&#x5206;&#x5e03;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;&#x5373;&#x4ece;&#x539f;&#x59cb;&#x8f93;&#x51fa;logits&#x4e2d;&#x51cf;&#x53bb;&#x589e;&#x5f3a;&#x8f93;&#x51fa;&#x7684;logits&#xff09;&#xff0c;&#x4ee5;&#x6291;&#x5236;&#x5e7b;&#x89c9;token&#x7684;&#x6982;&#x7387;&#xff0c;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7b54;&#x6848;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"144,145"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) &#x4e0d;&#x540c;&#x589e;&#x5f3a;&#x65b9;&#x5f0f;&#x5bf9;&#x4efb;&#x52a1;&#x6548;&#x679c;&#x5dee;&#x5f02;&#x663e;&#x8457;&#xff08;&#x4f8b;&#x5982;&#x989c;&#x8272;&#x589e;&#x5f3a;&#x5bf9;&#x989c;&#x8272;&#x7c7b;&#x95ee;&#x9898;&#x5bf9;&#x6bd4;&#x5ea6;&#x9ad8;&#xff0c;&#x7ffb;&#x8f6c;&#x5bf9;&#x4f4d;&#x7f6e;&#x7c7b;&#x95ee;&#x9898;&#x6709;&#x6548;&#xff09;&#xff1b;2) VACoDe&#x5728;MME&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x3001;CRG&#xff09;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x8f93;&#x51fa;&#x51c6;&#x786e;&#x6027;&#xff1b;3) &#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x666e;&#x9002;&#x6027;&#xff0c;&#x5728;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x7c7b;&#x578b;&#x548c;&#x89c4;&#x6a21;&#xff08;&#x5982;LLaVA-1.5&#xff09;&#x4e0a;&#x5747;&#x6709;&#x6548;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"145,146"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: VACoDe&#x901a;&#x8fc7;&#x81ea;&#x9002;&#x5e94;&#x9009;&#x62e9;&#x56fe;&#x50cf;&#x589e;&#x5f3a;&#x65b9;&#x5f0f;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x5728;LVLMs&#x4e2d;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x5173;&#x952e;&#x521b;&#x65b0;&#x5728;&#x4e8e;&#x63ed;&#x793a;&#x4e86;&#x589e;&#x5f3a;&#x65b9;&#x5f0f;&#x4e0e;&#x4efb;&#x52a1;&#x7c7b;&#x578b;&#x7684;&#x5173;&#x8054;&#x6027;&#xff0c;&#x5e76;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x91cf;&#x5316;&#x7684;&#x9009;&#x62e9;&#x6807;&#x51c6;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x5bf9;&#x672a;&#x6765;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x8bbe;&#x8ba1;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x3001;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff09;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"146,148"}}],"payload":{"tag":"li","lines":"142,148","fold":1}}],"payload":{"tag":"h4","lines":"140,141"}},{"content":"ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: ConVis&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#xff08;T2I&#xff09;&#x751f;&#x6210;&#x6a21;&#x578b;&#xff08;&#x5982;Hyper-SDXL&#xff09;&#x5c06;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x751f;&#x6210;&#x7684;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#x91cd;&#x6784;&#x4e3a;&#x56fe;&#x50cf;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0e;&#x91cd;&#x6784;&#x56fe;&#x50cf;&#x7684;&#x8f93;&#x51fa;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x6291;&#x5236;&#x5e7b;&#x89c9;token&#x7684;&#x751f;&#x6210;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"149,150"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#x95ee;&#x9898;&#xff0c;&#x4f8b;&#x5982;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x6216;&#x573a;&#x666f;&#x3002;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#xff0c;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x5916;&#x90e8;API&#x3001;&#x4eba;&#x5de5;&#x53cd;&#x9988;&#x6216;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x6548;&#x7387;&#x4f4e;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x4ec5;&#x901a;&#x8fc7;&#x89e3;&#x7801;&#x5e72;&#x9884;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"151,152"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: ConVis&#x91c7;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x6838;&#x5fc3;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1. &#x4f7f;&#x7528;MLLM&#x4e3a;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x521d;&#x59cb;&#x63cf;&#x8ff0;&#xff08;&#x53ef;&#x80fd;&#x542b;&#x5e7b;&#x89c9;&#xff09;&#xff1b;2. &#x901a;&#x8fc7;T2I&#x6a21;&#x578b;&#xff08;Hyper-SDXL&#xff09;&#x5c06;&#x63cf;&#x8ff0;&#x91cd;&#x6784;&#x4e3a;&#x56fe;&#x50cf;&#xff0c;&#x53ef;&#x89c6;&#x5316;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#xff1b;3. &#x4f7f;&#x7528;&#x6838;&#x91c7;&#x6837;&#x751f;&#x6210;&#x591a;&#x6837;&#x5316;&#x7684;&#x63cf;&#x8ff0;&#x548c;&#x5bf9;&#x5e94;&#x56fe;&#x50cf;&#x4ee5;&#x63d0;&#x9ad8;&#x8986;&#x76d6;&#x7387;&#xff1b;4. &#x8ba1;&#x7b97;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0e;&#x91cd;&#x6784;&#x56fe;&#x50cf;&#x7684;logit&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff0c;&#x901a;&#x8fc7;&#x52a0;&#x6743;&#x5bf9;&#x6bd4;&#xff08;&#x516c;&#x5f0f;&#xff1a;(1+&#x3b1;)<em>f&#x3b8;(v) - &#x3b1;</em>f&#x3b8;(v&apos;)&#xff09;&#x653e;&#x5927;&#x5e7b;&#x89c9;token&#x7684;&#x8d1f;&#x9762;&#x4fe1;&#x53f7;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x6291;&#x5236;&#x5176;&#x751f;&#x6210;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6216;&#x989d;&#x5916;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"152,153"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x4e94;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;CHAIR&#x3001;HallusionBench&#x3001;POPE&#x3001;MME&#x3001;LLaVA-Bench&#xff09;&#x4e0a;&#x9a8c;&#x8bc1;&#xff0c;ConVis&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x591a;&#x79cd;MLLM&#xff08;&#x5982;LLaVA-1.5&#x3001;MiniGPT-4&#x3001;mPLUG-Owl2&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR&#x5206;&#x6570;&#x4e0b;&#x964d;&#xff09;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;T2I&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x7684;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x4fe1;&#x53f7;&#x80fd;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x548c;&#x60e9;&#x7f5a;&#x5e7b;&#x89c9;token&#x3002;","children":[],"payload":{"tag":"li","lines":"153,154"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ConVis&#x9996;&#x6b21;&#x5c06;T2I&#x6a21;&#x578b;&#x7528;&#x4e8e;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x53ef;&#x89c6;&#x5316;&#x5e7b;&#x89c9;&#x548c;&#x5bf9;&#x6bd4;&#x5206;&#x5e03;&#x6709;&#x6548;&#x63d0;&#x5347;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6216;&#x989d;&#x5916;&#x6570;&#x636e;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x5229;&#x7528;&#x751f;&#x6210;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x89c6;&#x89c9;&#x4fe1;&#x53f7;&#x5f00;&#x8f9f;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x5b9e;&#x9645;&#x573a;&#x666f;&#x4e2d;&#x5feb;&#x901f;&#x90e8;&#x7f72;&#xff0c;&#x4f46;&#x6027;&#x80fd;&#x53d7;T2I&#x6a21;&#x578b;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x9650;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"154,156"}}],"payload":{"tag":"li","lines":"150,156","fold":1}}],"payload":{"tag":"h4","lines":"148,149"}},{"content":"TCD: Diagnosing Event Hallucinations in Video LLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;EventHallusion&#x57fa;&#x51c6;&#xff0c;&#x7528;&#x4e8e;&#x8bc4;&#x4f30;&#x89c6;&#x9891;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VideoLLMs&#xff09;&#x7684;&#x4e8b;&#x4ef6;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x65f6;&#x95f4;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;TCD&#xff09;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x3002;&#x5b9e;&#x9a8c;&#x53d1;&#x73b0;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#xff0c;&#x800c;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#x8868;&#x73b0;&#x66f4;&#x597d;&#xff0c;TCD&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"157,158"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x9891;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VideoLLMs&#xff09;&#x5728;&#x89c6;&#x9891;&#x7406;&#x89e3;&#x65b9;&#x9762;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x8fdb;&#x5c55;&#xff0c;&#x4f46;&#x5176;&#x4e8b;&#x4ef6;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5373;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x4e8b;&#x5b9e;&#x9519;&#x8bef;&#x7684;&#x63cf;&#x8ff0;&#xff09;&#x5c1a;&#x672a;&#x5f97;&#x5230;&#x5145;&#x5206;&#x7814;&#x7a76;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x73b0;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x7cfb;&#x7edf;&#x6027;&#x7684;&#x8bc4;&#x4f30;&#x548c;&#x89e3;&#x51b3;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"159,160"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;EventHallusion&#x57fa;&#x51c6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x7c7b;&#x8bc4;&#x4f30;&#xff1a;1&#xff09;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x654f;&#x611f;&#x6027;&#xff08;&#x901a;&#x8fc7;&#x8bef;&#x5bfc;&#x6027;&#x4e8b;&#x4ef6;&#x95ee;&#x9898;&#x6d4b;&#x8bd5;&#xff09;&#xff1b;2&#xff09;&#x5bf9;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x504f;&#x89c1;&#x7684;&#x654f;&#x611f;&#x6027;&#xff08;&#x901a;&#x8fc7;&#x7f55;&#x89c1;&#x4f46;&#x5408;&#x7406;&#x7684;&#x4e8b;&#x4ef6;&#x6d4b;&#x8bd5;&#xff09;&#x3002;&#x540c;&#x65f6;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x65f6;&#x95f4;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;TCD&#xff09;&#x65b9;&#x6cd5;&#xff1a;&#x901a;&#x8fc7;&#x5c06;&#x539f;&#x59cb;&#x89c6;&#x9891;&#x4e0e;&#x65f6;&#x95f4;&#x4fe1;&#x606f;&#x6a21;&#x7cca;&#x5316;&#x7684;&#x4fee;&#x6539;&#x7248;&#x672c;&#x5bf9;&#x6bd4;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x8c03;&#x6574;&#x9884;&#x6d4b;&#x5206;&#x5e03;&#xff0c;&#x4ee5;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#x7684;&#x4f9d;&#x8d56;&#x3002;","children":[],"payload":{"tag":"li","lines":"160,161"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5bf9;8&#x4e2a;&#x5f00;&#x6e90;&#x548c;3&#x4e2a;&#x95ed;&#x6e90;VideoLLMs&#x7684;&#x8bc4;&#x4f30;&#x663e;&#x793a;&#xff0c;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x4e8b;&#x4ef6;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#xff0c;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4V&#xff09;&#x8868;&#x73b0;&#x66f4;&#x597d;&#x3002;&#x4f7f;&#x7528;TCD&#x65b9;&#x6cd5;&#x540e;&#xff0c;&#x5927;&#x591a;&#x6570;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x7684;&#x6027;&#x80fd;&#x5728;EventHallusion&#x57fa;&#x51c6;&#x4e0a;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff0c;&#x8bc1;&#x660e;&#x4e86;TCD&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"161,162"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x63ed;&#x793a;&#x4e86;VideoLLMs&#x666e;&#x904d;&#x5b58;&#x5728;&#x4e8b;&#x4ef6;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x6839;&#x6e90;&#x662f;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x5206;&#x5e03;&#x4e0d;&#x5e73;&#x8861;&#x5bfc;&#x81f4;&#x7684;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x548c;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x504f;&#x89c1;&#x3002;EventHallusion&#x57fa;&#x51c6;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x8bc4;&#x4f30;&#x5de5;&#x5177;&#xff0c;TCD&#x65b9;&#x6cd5;&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;VideoLLMs&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"162,164"}}],"payload":{"tag":"li","lines":"158,164","fold":1}}],"payload":{"tag":"h4","lines":"156,157"}},{"content":"CATCH: Complementary Adaptive Token-level Contrastive Decoding to Mitigate Hallucinations in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;CATCH&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x4e92;&#x8865;&#x89c6;&#x89c9;&#x89e3;&#x8026;&#x3001;&#x975e;&#x89c6;&#x89c9;&#x7b5b;&#x9009;&#x548c;&#x81ea;&#x9002;&#x5e94;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x63d0;&#x5347;&#x591a;&#x4efb;&#x52a1;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"165,166"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x6587;&#x672c;&#xff09;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x533b;&#x7597;&#x548c;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5e26;&#x6765;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x4f20;&#x7edf;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x6570;&#x636e;&#x8d28;&#x91cf;&#x6216;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x589e;&#x5f3a;&#xff0c;&#x4f46;&#x672a;&#x80fd;&#x89e3;&#x51b3;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x4e2d;&#x7684;&#x6839;&#x672c;&#x7f3a;&#x9677;&#x2014;&#x2014;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x8fc7;&#x8f7d;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff0c;&#x4ece;&#x800c;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: CATCH&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1. &#x4e92;&#x8865;&#x89c6;&#x89c9;&#x89e3;&#x8026;&#xff08;CVD&#xff09;&#xff1a;&#x4f7f;&#x7528;Segment Anything Model&#xff08;SAM&#xff09;&#x5c06;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x5206;&#x5272;&#x4e3a;&#x4e92;&#x8865;&#x7684;&#x53cc;&#x56fe;&#x50cf;&#x548c;&#x6b8b;&#x5dee;&#x56fe;&#x50cf;&#xff0c;&#x52a8;&#x6001;&#x5206;&#x79bb;&#x5173;&#x952e;&#x7279;&#x5f81;&#x4e0e;&#x5197;&#x4f59;&#x4fe1;&#x606f;&#xff1b;2. &#x975e;&#x89c6;&#x89c9;&#x7b5b;&#x9009;&#xff08;NVS&#xff09;&#xff1a;&#x5f15;&#x5165;&#x65e0;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4f5c;&#x4e3a;&#x57fa;&#x51c6;&#xff0c;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x7684;Jensen-Shannon&#x6563;&#x5ea6;&#xff08;JSD&#xff09;&#x8bc6;&#x522b;&#x6700;&#x80fd;&#x4fdd;&#x7559;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x89e3;&#x8026;&#x56fe;&#x50cf;&#xff1b;3. &#x81ea;&#x9002;&#x5e94;&#x8bcd;&#x7ea7;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;ATCD&#xff09;&#xff1a;&#x6839;&#x636e;JSD&#x6bd4;&#x8f83;&#x7ed3;&#x679c;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x2014;&#x2014;&#x82e5;&#x89e3;&#x8026;&#x56fe;&#x50cf;&#x5206;&#x5e03;&#x66f4;&#x63a5;&#x8fd1;&#x771f;&#x5b9e;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x5219;&#x7528;&#x4e8e;&#x6291;&#x5236;&#x539f;&#x59cb;&#x5206;&#x5e03;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#xff1b;&#x82e5;&#x539f;&#x59cb;&#x5206;&#x5e03;&#x591a;&#x6837;&#x6027;&#x4e0d;&#x8db3;&#xff0c;&#x5219;&#x7528;&#x89e3;&#x8026;&#x56fe;&#x50cf;&#x589e;&#x5f3a;&#x751f;&#x6210;&#x591a;&#x6837;&#x6027;&#xff0c;&#x9632;&#x6b62;&#x5e7b;&#x89c9;&#x7d2f;&#x79ef;&#x3002;","children":[],"payload":{"tag":"li","lines":"168,169"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1. &#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x63a9;&#x853d;&#x5173;&#x952e;&#x7279;&#x5f81;&#x7684;&#x56fe;&#x50cf;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x4e0e;&#x65e0;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x51e0;&#x4e4e;&#x4e00;&#x81f4;&#xff0c;&#x800c;&#x66b4;&#x9732;&#x5173;&#x952e;&#x7279;&#x5f81;&#x7684;&#x56fe;&#x50cf;&#x663e;&#x8457;&#x63d0;&#x5347;&#x771f;&#x5b9e;&#x8bcd;&#x7b26;&#x6982;&#x7387;&#xff08;&#x5982;&#x201c;green&#x201d;&#xff09;&#xff1b;2. &#x89e3;&#x8026;&#x56fe;&#x50cf;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x9ad8;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5bc6;&#x5ea6;&#xff0c;&#x51cf;&#x5c11;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#xff1b;3. CATCH&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x6cdb;&#x5316;&#x5230;&#x591a;&#x79cd;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#xff0c;&#x6709;&#x6548;&#x7ea0;&#x6b63;&#x5e7b;&#x89c9;&#x8bcd;&#x7b26;&#xff08;&#x5982;&#x5c06;&#x201c;phone&#x201d;&#x4fee;&#x6b63;&#x4e3a;&#x201c;sandwich&#x201d;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"169,170"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CATCH&#x4ece;&#x4fe1;&#x606f;&#x74f6;&#x9888;&#x7406;&#x8bba;&#x51fa;&#x53d1;&#xff0c;&#x89e3;&#x51b3;&#x4e86;LVLM&#x56e0;&#x89c6;&#x89c9;&#x7f3a;&#x9677;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x8bad;&#x7ec3;&#x7684;&#x901a;&#x7528;&#x89e3;&#x7801;&#x6846;&#x67b6;&#x3002;&#x5176;&#x52a8;&#x6001;&#x8bcd;&#x7ea7;&#x5904;&#x7406;&#x673a;&#x5236;&#x80fd;&#x9002;&#x5e94;&#x5f00;&#x653e;&#x573a;&#x666f;&#x7684;&#x590d;&#x6742;&#x6027;&#xff0c;&#x4e3a;LVLM&#x5728;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x4e86;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x673a;&#x5236;&#x7684;&#x57fa;&#x7840;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"170,172"}}],"payload":{"tag":"li","lines":"166,172","fold":1}}],"payload":{"tag":"h4","lines":"164,165"}},{"content":"VaLiD: Mitigating the Hallucination of Large Vision Language Models by Visual Layer Fusion Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;VaLiD&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x878d;&#x5408;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0d;&#x540c;&#x5c42;&#x7684;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x4fe1;&#x606f;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x63d0;&#x5347;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"173,174"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4ece;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x89d2;&#x5ea6;&#x5f52;&#x56e0;&#x5e76;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x672c;&#x6587;&#x53d1;&#x73b0;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x4fe1;&#x606f;&#x626d;&#x66f2;&#xff08;Visual Encoding Distortion&#xff09;&#x624d;&#x662f;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x539f;&#x56e0;&#xff0c;&#x5373;&#x65e9;&#x671f;&#x89c6;&#x89c9;&#x5c42;&#x7684;&#x5173;&#x952e;&#x7279;&#x5f81;&#x5728;&#x4f20;&#x64ad;&#x81f3;&#x8f93;&#x51fa;&#x5c42;&#x8fc7;&#x7a0b;&#x4e2d;&#x9010;&#x6e10;&#x5931;&#x771f;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x5b83;&#x63ed;&#x793a;&#x4e86;LVLM&#x53ef;&#x9760;&#x6027;&#x4e0d;&#x8db3;&#x7684;&#x6839;&#x6e90;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4ece;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x89d2;&#x5ea6; mitigation &#x7684;&#x65b0;&#x601d;&#x8def;&#x3002;","children":[],"payload":{"tag":"li","lines":"175,176"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Visual-Layer Fusion Contrastive Decoding (VaLiD)&#x65b9;&#x6cd5;&#x3002;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x5229;&#x7528;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#xff08;&#x71b5;&#xff09;&#x4f5c;&#x4e3a;&#x89c6;&#x89c9;&#x5c42;&#x4fe1;&#x606f;&#x626d;&#x66f2;&#x7684;&#x6307;&#x6807;&#xff0c;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x6700;&#x9ad8;&#x7684;top-k&#x89c6;&#x89c9;&#x5c42;&#xff1b;2&#xff09;&#x5c06;&#x8fd9;&#x4e9b;&#x5c42;&#x7684;&#x7279;&#x5f81;&#x8fdb;&#x884c;&#x71b5;&#x52a0;&#x6743;&#x878d;&#x5408;&#xff0c;&#x6784;&#x5efa;&#x53c2;&#x8003;&#x5206;&#x5e03;&#xff1b;3&#xff09;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x5c06;&#x6807;&#x51c6;&#x89c6;&#x89c9;&#x8f93;&#x51fa;&#x5c42;&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#x4e0e;&#x53c2;&#x8003;&#x5206;&#x5e03;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#xff0c;&#x6291;&#x5236;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x7684;&#x5931;&#x771f;&#x4fe1;&#x606f;&#xff0c;&#x589e;&#x5f3a;&#x53ef;&#x9760;&#x4fe1;&#x606f;&#x7684;&#x6743;&#x91cd;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x5b9e;&#x65bd;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"176,177"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5931;&#x771f;&#x73b0;&#x8c61;&#x666e;&#x904d;&#x5b58;&#x5728;&#x4e8e;&#x591a;&#x79cd;LVLM&#xff08;&#x5982;LLaVA-v1.5&#x3001;InstructBLIP&#x3001;Qwen-VL&#xff09;&#x4e2d;&#xff0c;&#x65e9;&#x671f;&#x89c6;&#x89c9;&#x5c42;&#x80fd;&#x6b63;&#x786e;&#x89e3;&#x7801;&#x7684;&#x6837;&#x672c;&#x5728;&#x8f93;&#x51fa;&#x5c42;&#x9519;&#x8bef;&#x7387;&#x663e;&#x8457;&#x5347;&#x9ad8;&#xff08;&#x5982;InstructBLIP&#x7684;bucket5 EDR&#x8fbe;69.35%&#xff09;&#xff1b;2&#xff09;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x89c6;&#x89c9;&#x5c42;&#x4e0e;&#x9519;&#x8bef;&#x89e3;&#x7801;&#x5f3a;&#x76f8;&#x5173;&#xff1b;3&#xff09;VaLiD&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;AMBER&#xff09;&#x4e0a;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x4f18;&#x4e8e;&#x5176;&#x4ed6;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"177,178"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff1a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x4fe1;&#x606f;&#x626d;&#x66f2;&#x662f;LVLM&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x7684;&#x91cd;&#x8981;&#x6839;&#x6e90;&#xff0c;&#x800c;&#x975e;&#x4ec5;&#x6e90;&#x4e8e;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5148;&#x9a8c;&#x3002;VaLiD&#x901a;&#x8fc7;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x5f15;&#x5bfc;&#x7684;&#x89c6;&#x89c9;&#x5c42;&#x878d;&#x5408;&#x4e0e;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x5728;&#x4e0d;&#x589e;&#x52a0;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x7684;&#x524d;&#x63d0;&#x4e0b;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8fd9;&#x4e00;&#x5de5;&#x4f5c;&#x4e0d;&#x4ec5;&#x63d0;&#x51fa;&#x4e86;&#x6709;&#x6548;&#x7684; mitigation &#x65b9;&#x6cd5;&#xff0c;&#x66f4;&#x5f00;&#x62d3;&#x4e86;&#x4ece;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x89c6;&#x89d2;&#x7406;&#x89e3;&#x548c;&#x89e3;&#x7801;&#x5e7b;&#x89c9;&#x7684;&#x65b0;&#x7814;&#x7a76;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;LVLM&#x5728;&#x5b9e;&#x9645;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"178,180"}}],"payload":{"tag":"li","lines":"174,180","fold":1}}],"payload":{"tag":"h4","lines":"172,173"}},{"content":"VCD Analysis: Delve into Visual Contrastive Decoding for Hallucination Mitigation of Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;VCD&#xff09;&#x7684;&#x514d;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x878d;&#x5408;&#x591a;&#x79cd;&#x89c6;&#x89c9;&#x5931;&#x771f;&#x6837;&#x672c;&#xff08;&#x5982;&#x964d;&#x91c7;&#x6837;&#x548c;&#x56fe;&#x50cf;&#x7f16;&#x8f91;&#xff09;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5229;&#x7528;&#x71b5;&#x52a0;&#x6743;&#x7ec4;&#x5408;&#x4e0d;&#x540c;&#x6837;&#x672c;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"181,182"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x76f8;&#x5173;&#x7684;&#x54cd;&#x5e94;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x548c;&#x5fae;&#x8c03;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x7f3a;&#x4e4f;&#x7075;&#x6d3b;&#x6027;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x6765;&#x9ad8;&#x6548;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"183,184"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;Contrastive Decoding&#xff09;&#x6280;&#x672f;&#xff0c;&#x4f7f;&#x7528;&#x89c6;&#x89c9;&#x5931;&#x771f;&#x6837;&#x672c;&#xff08;&#x5982;&#x9ad8;&#x65af;&#x566a;&#x58f0;&#x6269;&#x6563;&#x3001;&#x56fe;&#x50cf;&#x964d;&#x91c7;&#x6837;&#x548c;&#x56fe;&#x50cf;&#x7f16;&#x8f91;&#xff09;&#x751f;&#x6210;&#x66f4;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x7684;&#x5bf9;&#x6bd4;&#x6837;&#x672c;&#xff0c;&#x5e76;&#x4ece;&#x539f;&#x59cb;&#x8f93;&#x51fa;&#x7684;&#x5bf9;&#x6570;&#x6982;&#x7387;&#x4e2d;&#x51cf;&#x53bb;&#x5bf9;&#x6bd4;&#x6837;&#x672c;&#x7684;&#x5bf9;&#x6570;&#x6982;&#x7387;&#x4ee5;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;&#x8fdb;&#x4e00;&#x6b65;&#x63d0;&#x51fa;&#x71b5;&#x52a0;&#x6743;&#x878d;&#x5408;&#x65b9;&#x6cd5;&#xff0c;&#x81ea;&#x52a8;&#x8c03;&#x6574;&#x4e0d;&#x540c;&#x5bf9;&#x6bd4;&#x6837;&#x672c;&#x7684;&#x5f71;&#x54cd;&#x6743;&#x91cd;&#xff0c;&#x5b9e;&#x73b0;&#x8de8;&#x6a21;&#x578b;&#x548c;&#x4efb;&#x52a1;&#x7684;&#x901a;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"184,185"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x4e0d;&#x540c;&#x89c6;&#x89c9;&#x5931;&#x771f;&#x6837;&#x672c;&#xff08;&#x5982;&#x964d;&#x91c7;&#x6837;&#x548c;&#x7f16;&#x8f91;&#xff09;&#x5728;&#x4e0d;&#x540c;LVLM&#xff08;&#x5982;InstructBLIP&#x3001;LLaVA-1.5&#xff09;&#x548c;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;POPE&#x3001;MME&#xff09;&#x4e0a;&#x6548;&#x679c;&#x5dee;&#x5f02;&#x663e;&#x8457;&#x3002;&#x71b5;&#x8f83;&#x9ad8;&#x7684;&#x6837;&#x672c;&#x901a;&#x5e38;&#x66f4;&#x6709;&#x6548;&#x3002;&#x63d0;&#x51fa;&#x7684;&#x71b5;&#x52a0;&#x6743;&#x878d;&#x5408;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5747;&#x53d6;&#x5f97;&#x6700;&#x4f73;&#x6027;&#x80fd;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"185,186"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x901a;&#x8fc7;&#x878d;&#x5408;&#x591a;&#x79cd;&#x5931;&#x771f;&#x6837;&#x672c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6216;&#x989d;&#x5916;&#x6570;&#x636e;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x4e3a;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x7075;&#x6d3b;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x6837;&#x672c;&#x751f;&#x6210;&#x548c;&#x52a0;&#x6743;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"186,188"}}],"payload":{"tag":"li","lines":"182,188","fold":1}}],"payload":{"tag":"h4","lines":"180,181"}},{"content":"VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: VASparse&#x662f;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x7b97;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7684;token&#x7a00;&#x758f;&#x5316;&#x6765;&#x9ad8;&#x6548;&#x7f13;&#x89e3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff08;VH&#xff09;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x4fdd;&#x6301;&#x9ad8;&#x89e3;&#x7801;&#x901f;&#x5ea6;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x540e;&#x5904;&#x7406;&#x3002;","children":[],"payload":{"tag":"li","lines":"189,190"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x53ef;&#x80fd;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x2019;&#xff08;VH&#xff09;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x5176;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;&#x540e;&#x5904;&#x7406;&#x3001;&#x5fae;&#x8c03;&#x6216;&#x590d;&#x6742;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x5f80;&#x5f80;&#x4f9d;&#x8d56;&#x989d;&#x5916;&#x6570;&#x636e;&#x3001;&#x8bad;&#x7ec3;&#x6216;&#x591a;&#x6b21;&#x89e3;&#x7801;&#xff0c;&#x5bfc;&#x81f4;&#x6548;&#x7387;&#x4f4e;&#x4e0b;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;VH&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x786e;&#x4fdd;&#x89e3;&#x7801;&#x6548;&#x7387;&#x548c;&#x9ad8;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x8fd9;&#x5bf9;LVLM&#x7684;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"191,192"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;VASparse&#x65b9;&#x6cd5;&#xff0c;&#x57fa;&#x4e8e;&#x4e09;&#x4e2a;&#x5173;&#x952e;&#x89c2;&#x5bdf;&#xff1a;LVLM&#x6ce8;&#x610f;&#x529b;&#x5177;&#x6709;&#x7a00;&#x758f;&#x6027;&#xff1b;&#x89c6;&#x89c9;&#x65e0;&#x5173;&#x7684;token&#x7a00;&#x758f;&#x5316;&#x4f1a;&#x52a0;&#x5267;VH&#xff1b;&#x56fe;&#x50cf;&#x548c;&#x6587;&#x672c;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#x663e;&#x8457;&#x3002;&#x65b9;&#x6cd5;&#x5305;&#x62ec;&#xff1a;1) &#x5c06;token&#x7a00;&#x758f;&#x5316;&#x548c;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7edf;&#x4e00;&#x4e3a;&#x7ea6;&#x675f;&#x4f18;&#x5316;&#x95ee;&#x9898;&#xff0c;&#x8bbe;&#x8ba1;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7684;token&#x9009;&#x62e9;&#x7b56;&#x7565;&#xff0c;&#x4fdd;&#x7559;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#xff1b;2) &#x63d0;&#x51fa;&#x57fa;&#x4e8e;&#x7a00;&#x758f;&#x7684;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x89c6;&#x89c9;&#x65e0;&#x5173;&#x548c;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7a00;&#x758f;&#x5316;&#x7684;logits&#x91cd;&#x65b0;&#x6821;&#x51c6;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x907f;&#x514d;&#x4e8c;&#x6b21;&#x89e3;&#x7801;&#x7684;&#x65f6;&#x95f4;&#x5f00;&#x9500;&#xff1b;3) &#x4f7f;&#x7528;&#x7d2f;&#x79ef;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6570;&#x60e9;&#x7f5a;&#x2018;&#x6ce8;&#x610f;&#x529b;&#x4e0b;&#x6c89;&#x2019;&#x73b0;&#x8c61;&#xff0c;&#x9632;&#x6b62;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x8bed;&#x8a00;&#x504f;&#x5411;&#x6216;&#x4f4e;&#x8bed;&#x4e49;token&#x3002;","children":[],"payload":{"tag":"li","lines":"192,193"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x56db;&#x4e2a;&#x6d41;&#x884c;&#x57fa;&#x51c6;&#xff08;&#x5982;CHAIR&#x3001;POPE&#xff09;&#x548c;&#x591a;&#x4e2a;LVLM&#x5bb6;&#x65cf;&#xff08;LLaVA-1.5&#x3001;MiniGPT-4&#x3001;mPLUG-Owl2&#xff09;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;VASparse&#x5728;&#x51cf;&#x5c11;VH&#x65b9;&#x9762;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x6027;&#x80fd;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x7ade;&#x4e89;&#x529b;&#x7684;&#x89e3;&#x7801;&#x901f;&#x5ea6;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x76f8;&#x6bd4;&#x5f53;&#x524d;&#x6700;&#x4f73;&#x65b9;&#x6cd5;HALC&#xff0c;VASparse&#x5728;&#x6027;&#x80fd;&#x66f4;&#x4f18;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x89e3;&#x7801;&#x901f;&#x5ea6;&#x63d0;&#x5347;&#x9ad8;&#x8fbe;12.9&#x500d;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x540e;&#x5904;&#x7406;&#x3002;","children":[],"payload":{"tag":"li","lines":"193,194"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: VASparse&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x611f;&#x77e5;token&#x7a00;&#x758f;&#x5316;&#x548c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x6709;&#x6548;&#x5e73;&#x8861;&#x4e86;LVLM&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x6548;&#x7387;&#xff0c;&#x4e3a;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x6548;&#x3001;&#x5b9e;&#x7528;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x4e2d;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x771f;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x90e8;&#x7f72;&#x6548;&#x7387;&#xff0c;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;AI&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"194,196"}}],"payload":{"tag":"li","lines":"190,196","fold":1}}],"payload":{"tag":"h4","lines":"188,189"}},{"content":"Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Octopus&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x81ea;&#x9002;&#x5e94;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"197,198"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x7406;&#x89e3;&#x548c;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x751f;&#x6210;&#x865a;&#x5047;&#x5bf9;&#x8c61;&#x3001;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#x548c;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5173;&#x7cfb;&#xff09;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x533b;&#x7597;&#x62a5;&#x544a;&#x548c;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x3002;&#x73b0;&#x6709;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CD&#xff09;&#x65b9;&#x6cd5;&#x91c7;&#x7528;&#x201c;&#x4e00;&#x5200;&#x5207;&#x201d;&#x7b56;&#x7565;&#xff0c;&#x65e0;&#x6cd5;&#x9002;&#x5e94;&#x4e0d;&#x540c;&#x6837;&#x672c;&#x548c;&#x751f;&#x6210;&#x6b65;&#x9aa4;&#x7684;&#x591a;&#x6837;&#x5316;&#x5e7b;&#x89c9;&#x539f;&#x56e0;&#x3002;","children":[],"payload":{"tag":"li","lines":"199,200"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;Octopus&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x52a8;&#x6001;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x5e76;&#x9009;&#x62e9;&#x5bf9;&#x5e94;&#x7684;CD&#x7b56;&#x7565;&#xff1a;1&#xff09;&#x4f7f;&#x7528;&#x57fa;&#x4e8e;Transformer&#x7684;&#x6a21;&#x5757;&#x548c;&#x53ef;&#x5b66;&#x4e60;token&#xff08;&#x7c7b;&#x4f3c;&#x201c;&#x7ae0;&#x9c7c;&#x773c;&#x775b;&#x201d;&#xff09;&#x81ea;&#x9002;&#x5e94;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff1b;2&#xff09;&#x5c06;&#x4e0d;&#x540c;CD&#x7b56;&#x7565;&#x89c6;&#x4e3a;&#x201c;&#x89e6;&#x624b;&#x201d;&#xff0c;&#x6839;&#x636e;&#x8bc6;&#x522b;&#x7ed3;&#x679c;&#x6267;&#x884c;&#x7279;&#x5b9a;&#x5bf9;&#x6bd4;&#x64cd;&#x4f5c;&#xff08;&#x5982;VCD&#x89e3;&#x51b3;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x3001;M3ID&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e22;&#x5931;&#x3001;AVISC&#x7f13;&#x89e3;&#x6ce8;&#x610f;&#x529b;&#x504f;&#x5dee;&#xff09;&#xff1b;3&#xff09;&#x901a;&#x8fc7;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x6216;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x8fdb;&#x884c;&#x4f18;&#x5316;&#xff0c;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;LVLM&#x6743;&#x91cd;&#xff0c;&#x652f;&#x6301;&#x7075;&#x6d3b;&#x6269;&#x5c55;&#x65b0;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"200,201"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x4e0d;&#x540c;CD&#x7b56;&#x7565;&#x4ec5;&#x5bf9;&#x90e8;&#x5206;&#x6837;&#x672c;&#x6709;&#x6548;&#xff08;&#x7ea6;60%&#x6837;&#x672c;&#x4ec5;&#x88ab;&#x7279;&#x5b9a;&#x7b56;&#x7565;&#x7ea0;&#x6b63;&#xff0c;&#x7b56;&#x7565;&#x95f4;&#x91cd;&#x53e0;&#x4ec5;&#x7ea6;10%&#xff09;&#xff1b;2&#xff09;Octopus&#x5728;&#x751f;&#x6210;&#x4efb;&#x52a1;&#xff08;AMBER&#x3001;Object-HalBench&#x3001;MMHalbench&#xff09;&#x548c;&#x5224;&#x522b;&#x4efb;&#x52a1;&#xff08;POPE&#x6570;&#x636e;&#x96c6;&#xff09;&#x7684;&#x56db;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5747;&#x8fbe;&#x5230;SOTA&#x6027;&#x80fd;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#xff1b;3&#xff09;&#x6846;&#x67b6;&#x5177;&#x5907;&#x9ad8;&#x90e8;&#x7f72;&#x6027;&#x548c;&#x6269;&#x5c55;&#x6027;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x65b0;CD&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"201,202"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Octopus&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x9002;&#x914d;&#x591a;&#x7b56;&#x7565;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;LVLM&#x7684;&#x6df7;&#x5408;&#x578b;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e3a;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x5b9e;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x514d;&#x91cd;&#x8bad;&#x7ec3;&#x8bbe;&#x8ba1;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x66f4;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x5e7b;&#x89c9;&#x5206;&#x7c7b;&#x548c;&#x5b9e;&#x65f6;&#x7b56;&#x7565;&#x4f18;&#x5316;&#x3002;","children":[],"payload":{"tag":"li","lines":"202,204"}}],"payload":{"tag":"li","lines":"198,204","fold":1}}],"payload":{"tag":"h4","lines":"196,197"}},{"content":"IBD: Alleviating Hallucinations in Large Vision-Language Models via Image-Biased Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x56fe;&#x50cf;&#x504f;&#x7f6e;&#x89e3;&#x7801;&#xff08;IBD&#xff09;&#x7684;&#x65b0;&#x6280;&#x672f;&#xff0c;&#x65e8;&#x5728;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x6a21;&#x578b;&#x548c;&#x56fe;&#x50cf;&#x504f;&#x7f6e;&#x6a21;&#x578b;&#x7684;&#x9884;&#x6d4b;&#x7ed3;&#x679c;&#xff0c;&#x589e;&#x5f3a;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#x7684;&#x6b63;&#x786e;&#x4fe1;&#x606f;&#xff0c;&#x6291;&#x5236;&#x56e0;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x800c;&#x4ea7;&#x751f;&#x7684;&#x5e7b;&#x89c9;&#x9519;&#x8bef;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;IBD&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#xff0c;&#x4ec5;&#x9700;&#x5c11;&#x91cf;&#x53c2;&#x6570;&#x589e;&#x52a0;&#x5373;&#x53ef;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x771f;&#x5b9e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"205,206"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x6216;&#x9519;&#x8bef;&#x7684;&#x5e7b;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x8fd9;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x800c;&#x975e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x7684;&#x5b89;&#x5168;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x7814;&#x7a76;&#x5982;&#x4f55;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5bf9;&#x4e8e;&#x63d0;&#x5347;LVLM&#x7684;&#x5b9e;&#x7528;&#x4ef7;&#x503c;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"207,208"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x56fe;&#x50cf;&#x504f;&#x7f6e;&#x89e3;&#x7801;&#xff08;IBD&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x662f;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;LVLM&#xff08;&#x3b8;&#xff09;&#x548c;&#x56fe;&#x50cf;&#x504f;&#x7f6e;LVLM&#xff08;&#x2c6;&#x3b8;&#xff09;&#x7684;&#x9884;&#x6d4b;&#x7ed3;&#x679c;&#x6765;&#x751f;&#x6210;&#x4e0b;&#x4e00;&#x4e2a;&#x4ee4;&#x724c;&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#x3002;&#x56fe;&#x50cf;&#x504f;&#x7f6e;&#x6a21;&#x578b;&#x2c6;&#x3b8;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x77e9;&#x9635;&#x5b9e;&#x73b0;&#xff0c;&#x5177;&#x4f53;&#x662f;&#x5728;&#x8ba1;&#x7b97;&#x6ce8;&#x610f;&#x529b;&#x65f6;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x6dfb;&#x52a0;&#x653e;&#x5927;&#x7cfb;&#x6570;&#xff08;&#x3b5;&#xff09;&#xff0c;&#x4ee5;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#x3002;&#x6700;&#x7ec8;&#xff0c;&#x5229;&#x7528;&#x5bf9;&#x6bd4;&#x5f97;&#x5206;&#xff08;LCD = logit&#x2c6;&#x3b8; - logit&#x3b8;&#xff09;&#x901a;&#x8fc7;Softmax&#x751f;&#x6210;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x4ee4;&#x724c;&#x5206;&#x5e03;&#xff0c;&#x4ece;&#x800c;&#x6291;&#x5236;&#x6587;&#x672c;&#x4e3b;&#x5bfc;&#x7684;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"208,209"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;IBD&#x5728;&#x591a;&#x4e2a;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x4e0a;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x771f;&#x5b9e;&#x6027;&#x5f97;&#x5230;&#x63d0;&#x5347;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x4e0d;&#x9700;&#x8981;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x548c;&#x4ec5;&#x589e;&#x52a0;&#x6781;&#x5c11;&#x53c2;&#x6570;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x4f18;&#x4e8e;&#x5176;&#x4ed6;&#x9700;&#x8981;&#x5927;&#x91cf;&#x53c2;&#x6570;&#x6216;&#x6570;&#x636e;&#x7684;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5fae;&#x8c03;&#x6216;&#x8f85;&#x52a9;&#x7f51;&#x7edc;&#xff09;&#xff0c;&#x4e14;&#x80fd;&#x81ea;&#x9002;&#x5e94;&#x5904;&#x7406;&#x4e0d;&#x540c;&#x8bcd;&#x6c47;&#x7c7b;&#x578b;&#x548c;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"209,210"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: IBD&#x901a;&#x8fc7;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x7684;&#x4f18;&#x5316;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5177;&#x6709;&#x4f4e;&#x5f00;&#x9500;&#x3001;&#x5168;&#x9762;&#x5904;&#x7406;&#x80fd;&#x529b;&#x548c;&#x9ad8;&#x6027;&#x80fd;&#x7684;&#x4f18;&#x52bf;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x6539;&#x5584;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x81ea;&#x9002;&#x5e94;&#x8c03;&#x6574;&#x673a;&#x5236;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"210,212"}}],"payload":{"tag":"li","lines":"206,212","fold":1}}],"payload":{"tag":"h4","lines":"204,205"}},{"content":"LQCD: Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x9996;&#x6b21;&#x5bf9;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5949;&#x627f;&#x884c;&#x4e3a;&#xff08;sycophancy&#xff09;&#x8fdb;&#x884c;&#x4e86;&#x7cfb;&#x7edf;&#x5206;&#x6790;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x5373;&#x53ef;&#x5e94;&#x7528;&#x7684;&#x7f13;&#x89e3;&#x6846;&#x67b6;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x67e5;&#x8be2;&#x4e2d;&#x548c;&#x3001;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x548c;&#x81ea;&#x9002;&#x5e94;&#x4f18;&#x5316;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x56e0;&#x8bf1;&#x5bfc;&#x6027;&#x63d0;&#x793a;&#x4ea7;&#x751f;&#x7684;&#x504f;&#x89c1;&#x548c;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x5bf9;&#x4e2d;&#x6027;&#x67e5;&#x8be2;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"213,214"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5949;&#x627f;&#x884c;&#x4e3a;&#xff08;sycophancy&#xff09;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x53d7;&#x5f15;&#x5bfc;&#x6027;&#x6216;&#x6b3a;&#x9a97;&#x6027;&#x63d0;&#x793a;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x4ea7;&#x751f;&#x6709;&#x504f;&#x89c1;&#x7684;&#x8f93;&#x51fa;&#x548c;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4f1a;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x4e0d;&#x51c6;&#x786e;&#x3001;&#x5e26;&#x6709;&#x504f;&#x89c1;&#xff0c;&#x751a;&#x81f3;&#x8fdd;&#x53cd;&#x4eba;&#x7c7b;&#x4f26;&#x7406;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x73b0;&#x5b9e;&#x4e16;&#x754c;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;&#x5c3d;&#x7ba1;LVLM&#x53d1;&#x5c55;&#x8fc5;&#x901f;&#xff0c;&#x4f46;&#x5949;&#x627f;&#x884c;&#x4e3a;&#x7684;&#x8bc4;&#x4f30;&#x548c;&#x7f13;&#x89e3;&#x4ecd;&#x672a;&#x88ab;&#x5145;&#x5206;&#x63a2;&#x7d22;&#x3002;","children":[],"payload":{"tag":"li","lines":"215,216"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x4ee5;&#x4e0b;&#x65b9;&#x6cd5;&#x89e3;&#x51b3;&#x95ee;&#x9898;&#xff1a;1&#xff09;&#x6784;&#x5efa;&#x5305;&#x542b;&#x8bf1;&#x5bfc;&#x6027;&#x63d0;&#x793a;&#x7684;&#x6269;&#x5c55;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x7528;&#x4e8e;&#x7cfb;&#x7edf;&#x8bc4;&#x4f30;&#x5949;&#x627f;&#x884c;&#x4e3a;&#xff1b;2&#xff09;&#x63d0;&#x51fa;&#x4e00;&#x4e2a;&#x63a8;&#x7406;&#x65f6;&#x7f13;&#x89e3;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x62ec;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;&#x67e5;&#x8be2;&#x4e2d;&#x548c;&#x5668;&#xff08;&#x4f7f;&#x7528;&#x8f85;&#x52a9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x548c;&#x7528;&#x6237;&#x67e5;&#x8be2;&#x4e2d;&#x7684;&#x9690;&#x542b;&#x5949;&#x627f;&#x504f;&#x89c1;&#xff09;&#x3001;&#x5949;&#x627f;&#x611f;&#x77e5;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x673a;&#x5236;&#xff08;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x4e2d;&#x548c;&#x67e5;&#x8be2;&#x4e0e;&#x539f;&#x59cb;&#x8bf1;&#x5bfc;&#x67e5;&#x8be2;&#x7684;&#x54cd;&#x5e94;&#xff0c;&#x52a8;&#x6001;&#x91cd;&#x65b0;&#x6821;&#x51c6;&#x6807;&#x8bb0;&#x7ea7;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff09;&#x3001;&#x81ea;&#x9002;&#x5e94;&#x903b;&#x8f91;&#x4f18;&#x5316;&#x6a21;&#x5757;&#xff08;&#x6574;&#x5408;&#x81ea;&#x9002;&#x5e94;&#x5408;&#x7406;&#x6027;&#x8fc7;&#x6ee4;&#x5668;&#x548c;&#x67e5;&#x8be2;&#x60c5;&#x611f;&#x7f29;&#x653e;&#x5668;&#xff0c;&#x8fdb;&#x4e00;&#x6b65;&#x4fee;&#x6539;&#x5bf9;&#x6bd4;&#x540e;&#x7684;&#x903b;&#x8f91;&#xff0c;&#x786e;&#x4fdd;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x8fde;&#x8d2f;&#x6027;&#x548c;&#x9c81;&#x68d2;&#x6027;&#xff09;&#x3002;&#x6574;&#x4e2a;&#x6846;&#x67b6;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x4e0e;&#x6a21;&#x578b;&#x65e0;&#x5173;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x4e8e;&#x5404;&#x79cd;LVLM&#x3002;","children":[],"payload":{"tag":"li","lines":"216,217"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x5728;&#x6240;&#x6709;&#x8bc4;&#x4f30;&#x7684;&#x5148;&#x8fdb;LVLM&#xff08;&#x5982;Qwen-VL&#x3001;CogVLM2&#x7b49;&#xff09;&#x4e2d;&#xff0c;&#x8bf1;&#x5bfc;&#x6027;&#x67e5;&#x8be2;&#x663e;&#x8457;&#x52a0;&#x5267;&#x4e86;&#x5949;&#x627f;&#x95ee;&#x9898;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x548c;&#x5e7b;&#x89c9;&#x589e;&#x52a0;&#xff1b;2&#xff09;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x8868;&#x73b0;&#x51fa;&#x4e0d;&#x540c;&#x7684;&#x5949;&#x627f;&#x884c;&#x4e3a;&#x7279;&#x5f81;&#xff0c;&#x4f8b;&#x5982;&#x9884;&#x6d4b;&#x7ffb;&#x8f6c;&#x7387;&#x548c;&#x60c5;&#x611f;&#x654f;&#x611f;&#x6027;&#x5404;&#x5f02;&#xff1b;3&#xff09;&#x63d0;&#x51fa;&#x7684;&#x7f13;&#x89e3;&#x6846;&#x67b6;&#x5728;&#x6240;&#x6709;&#x6a21;&#x578b;&#x548c;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5747;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5949;&#x627f;&#x884c;&#x4e3a;&#xff0c;&#x4e14;&#x5728;&#x591a;&#x6570;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x6a21;&#x578b;&#x5728;&#x8bf1;&#x5bfc;&#x67e5;&#x8be2;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x751a;&#x81f3;&#x8d85;&#x8fc7;&#x4e86;&#x4e2d;&#x6027;&#x67e5;&#x8be2;&#xff1b;4&#xff09;&#x4e0e;&#x73b0;&#x6709;&#x7684;&#x63d0;&#x793a;&#x5de5;&#x7a0b;&#x548c;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x76f8;&#x6bd4;&#xff0c;&#x8be5;&#x6846;&#x67b6;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x4f18;&#x7684;&#x6027;&#x80fd;&#x3001;&#x66f4;&#x5f3a;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x66f4;&#x5c11;&#x7684;&#x5e7b;&#x89c9;&#xff1b;5&#xff09;&#x6846;&#x67b6;&#x4fdd;&#x6301;&#x6216;&#x7565;&#x5fae;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x4e2d;&#x6027;&#x63d0;&#x793a;&#x4e0b;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x5bf9;&#x504f;&#x7f6e;&#x548c;&#x65e0;&#x504f;&#x8f93;&#x5165;&#x90fd;&#x9c81;&#x68d2;&#x7684;&#x901a;&#x7528;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"217,218"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff1a;LVLM&#x4e2d;&#x7684;&#x5949;&#x627f;&#x884c;&#x4e3a;&#x662f;&#x4e00;&#x4e2a;&#x666e;&#x904d;&#x4e14;&#x7d27;&#x8feb;&#x7684;&#x6311;&#x6218;&#x3002;&#x63d0;&#x51fa;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x6846;&#x67b6;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x901a;&#x7528;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6848;&#xff0c;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x67b6;&#x6784;&#x6216;&#x8fdb;&#x884c;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x7684;&#x6210;&#x529f;&#x8868;&#x660e;&#xff0c;&#x63a8;&#x7406;&#x65f6;&#x7b56;&#x7565;&#x662f;&#x5b9e;&#x73b0;&#x53ef;&#x4fe1;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x7684;&#x4e00;&#x6761;&#x53ef;&#x884c;&#x8def;&#x5f84;&#xff0c;&#x5bf9;&#x63d0;&#x9ad8;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x3001;&#x51cf;&#x5c11;&#x504f;&#x89c1;&#x548c;&#x5e7b;&#x89c9;&#x5177;&#x6709;&#x91cd;&#x8981;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#xff0c;&#x6709;&#x52a9;&#x4e8e;&#x63a8;&#x52a8;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x5e7f;&#x6cdb;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"218,221"}}],"payload":{"tag":"li","lines":"214,221","fold":1}}],"payload":{"tag":"h4","lines":"212,213"}},{"content":"IMCCD: Mitigating Hallucination for Large Vision Language Model by Inter-Modality Correlation Calibration Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;IMCCD&#xff0c;&#x901a;&#x8fc7;&#x8de8;&#x6a21;&#x6001;&#x503c;&#x589e;&#x5f3a;&#x89e3;&#x7801;&#xff08;CMVED&#xff09;&#x548c;&#x5185;&#x5bb9;&#x9a71;&#x52a8;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x4f18;&#x5316;&#xff08;CDAR&#xff09;&#x6a21;&#x5757;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x6587;&#x672c;&#x751f;&#x6210;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"222,223"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x590d;&#x6742;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e2d;&#x4f1a;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x5355;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4f46;&#x5ffd;&#x7565;&#x4e86;&#x865a;&#x5047;&#x7684;&#x8de8;&#x6a21;&#x6001;&#x76f8;&#x5173;&#x6027;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x98df;&#x7269;&#x4e0e;&#x6587;&#x672c;&#x4e2d;&#x7684;&#x201c;&#x9910;&#x684c;&#x201d;&#x4ea7;&#x751f;&#x865a;&#x5047;&#x5173;&#x8054;&#xff09;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65b0;&#x65b9;&#x6cd5;&#x6765;&#x540c;&#x65f6;&#x89e3;&#x51b3;&#x8fd9;&#x4e24;&#x79cd;&#x5e7b;&#x89c9;&#x6765;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"224,225"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;IMCCD&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;1) Cross-Modal Value-Enhanced Decoding (CMVED)&#xff1a;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x673a;&#x5236;&#xff0c;&#x5728;&#x4f30;&#x8ba1;&#x626d;&#x66f2;&#x5206;&#x5e03;&#x65f6;&#x9009;&#x62e9;&#x6027;&#x63a9;&#x7801;&#x4e0e;&#x9ad8;&#x8de8;&#x6a21;&#x6001;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x76f8;&#x5173;&#x7684;&#x503c;&#x5411;&#x91cf;&#xff0c;&#x4fdd;&#x7559;&#x865a;&#x5047;&#x8de8;&#x6a21;&#x6001;&#x76f8;&#x5173;&#x6027;&#x540c;&#x65f6;&#x6291;&#x5236;&#x91cd;&#x8981;&#x5173;&#x8054;&#xff1b;2) Content-Driven Attention Refinement (CDAR)&#xff1a;&#x901a;&#x8fc7;&#x5f52;&#x4e00;&#x5316;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x4f4d;&#x7f6e;&#x5d4c;&#x5165;&#x6765;&#x4f18;&#x5316;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x786e;&#x4fdd;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x91cd;&#x8981;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x4ec5;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#x3002;","children":[],"payload":{"tag":"li","lines":"225,226"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;IMCCD&#x65b9;&#x6cd5;&#x5728;&#x51cf;&#x5c11;LVLM&#x6587;&#x672c;&#x751f;&#x6210;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x6700;&#x5148;&#x8fdb;&#x6280;&#x672f;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#x4e86;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"226,227"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: IMCCD&#x901a;&#x8fc7;&#x540c;&#x65f6;&#x89e3;&#x51b3;&#x5355;&#x6a21;&#x6001;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x548c;&#x865a;&#x5047;&#x8de8;&#x6a21;&#x6001;&#x76f8;&#x5173;&#x6027;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5e94;&#x7528;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff0c;&#x53ef;&#x4fc3;&#x8fdb;LVLM&#x5728;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"227,229"}}],"payload":{"tag":"li","lines":"223,229","fold":1}}],"payload":{"tag":"h4","lines":"221,222"}},{"content":"OPA-DPA: Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x5217;&#x8868;&#x805a;&#x7126;&#x4e8e;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x591a;&#x79cd;&#x6280;&#x672f;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x3001;&#x5de5;&#x5177;&#x8c03;&#x7528;&#x3001;&#x6ce8;&#x610f;&#x529b;&#x63a7;&#x5236;&#x7b49;&#xff09;&#x6765;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x9a8c;&#x8bc1;&#x4e86;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"230,231"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5c55;&#x73b0;&#x51fa;&#x5f3a;&#x5927;&#x591a;&#x6a21;&#x6001;&#x80fd;&#x529b;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x4e5f;&#x7ee7;&#x627f;&#x4e86;&#x5176;&#x5e95;&#x5c42;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x6216;&#x865a;&#x6784;&#x5185;&#x5bb9;&#xff09;&#x7684;&#x503e;&#x5411;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3001;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5728;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#xff08;&#x5982;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x7684;&#x5e94;&#x7528;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x89e3;&#x51b3;&#x3002;","children":[],"payload":{"tag":"li","lines":"232,233"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x8bba;&#x6587;&#x5217;&#x8868;&#x4e2d;&#x7684;&#x65b9;&#x6cd5;&#x591a;&#x6837;&#xff0c;&#x4e3b;&#x8981;&#x5305;&#x62ec;&#xff1a;1. &#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;Visual Contrastive Decoding&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0e;&#x5176;&#x589e;&#x5f3a;&#x7248;&#x672c;&#xff08;&#x5982;&#x88c1;&#x526a;&#x3001;&#x65cb;&#x8f6c;&#xff09;&#x7684;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#xff0c;&#x6291;&#x5236;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x65e0;&#x5173;&#x7684;&#x5e7b;&#x89c9;&#x751f;&#x6210;&#x3002;2. &#x667a;&#x80fd;&#x4f53;&#x6570;&#x636e;&#x7ba1;&#x9053;&#xff08;Agentic Data Pipeline&#xff09;&#xff1a;&#x5982;Omni-Detective&#xff0c;&#x901a;&#x8fc7;&#x5de5;&#x5177;&#x8c03;&#x7528;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x4e14;&#x5e7b;&#x89c9;&#x6700;&#x5c11;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x6570;&#x636e;&#x3002;3. &#x6ce8;&#x610f;&#x529b;&#x63a7;&#x5236;&#xff08;Attention Control&#xff09;&#xff1a;&#x5982;FlexAC&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5e76;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x4e2d;&#x5bfc;&#x81f4;&#x5173;&#x8054;&#x6027;&#x5e7b;&#x89c9;&#x7684;&#x7279;&#x5b9a;&#x5c42;&#x8868;&#x793a;&#x6765;&#x8c03;&#x63a7;&#x63a8;&#x7406;&#x5f3a;&#x5ea6;&#x3002;4. &#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x751f;&#x6210;&#xff08;RAG&#xff09;&#xff1a;&#x5982;VisRAG&#xff0c;&#x5f15;&#x5165;&#x5916;&#x90e8;&#x89c6;&#x89c9;&#x77e5;&#x8bc6;&#x5e93;&#x6765; grounding &#x63a8;&#x7406;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;5. &#x591a;&#x6a21;&#x6001;&#x7f6e;&#x4fe1;&#x5ea6;&#x63a8;&#x7406;&#xff08;Multimodal Deep Confidence&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x63a2;&#x7d22;&#x591a;&#x6761;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x8def;&#x5f84;&#x6765;&#x8bc4;&#x4f30;&#x7b54;&#x6848;&#x7f6e;&#x4fe1;&#x5ea6;&#xff0c;&#x9009;&#x62e9;&#x6700;&#x53ef;&#x9760;&#x7684;&#x8def;&#x5f84;&#x3002;","children":[],"payload":{"tag":"li","lines":"233,234"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1. &#x6240;&#x63d0;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x5f00;&#x6e90;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;&#x6587;&#x672c;&#x56fe;&#x50cf;&#x7406;&#x89e3;&#x3001;&#x63a8;&#x7406;&#x3001;&#x591a;&#x56fe;&#x50cf;&#x7406;&#x89e3;&#x3001;&#x901a;&#x7528;VQA&#x3001;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x7b49;&#xff09;&#x4e0a;&#xff0c;&#x6027;&#x80fd;&#x8fbe;&#x5230;&#x6216;&#x8d85;&#x8fc7;&#x4e86;&#x540c;&#x89c4;&#x6a21;&#x7684;&#x6700;&#x5148;&#x8fdb;&#x6a21;&#x578b;&#x3002;2. &#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x548c;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x4e0e;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x3002;3. &#x5728;&#x653e;&#x5c04;&#x5b66;&#x7b49;&#x4e13;&#x4e1a;&#x9886;&#x57df;&#xff0c;&#x901a;&#x8fc7;&#x79bb;&#x6563;&#x8bed;&#x4e49;&#x71b5;&#xff08;DSE&#xff09;&#x8fc7;&#x6ee4;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#x4e86;&#x9ed1;&#x76d2;&#x6a21;&#x578b;&#x7684;&#x56de;&#x7b54;&#x51c6;&#x786e;&#x6027;&#x3002;4. &#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x5177;&#x6709;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x9ad8;&#x6548;&#x7684;&#x7279;&#x70b9;&#xff08;&#x5982;1+N LoRA&#x67b6;&#x6784;&#xff09;&#xff0c;&#x9002;&#x5408;&#x79fb;&#x52a8;&#x7aef;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"234,235"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x4e00;&#x7cfb;&#x5217;&#x521b;&#x65b0;&#x7684;&#x6280;&#x672f;&#x624b;&#x6bb5;&#xff08;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x3001;&#x6570;&#x636e;&#x7ba1;&#x9053;&#x3001;&#x6ce8;&#x610f;&#x529b;&#x8c03;&#x63a7;&#x3001;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x7b49;&#xff09;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x5730;&#x51cf;&#x8f7b;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x9ad8;&#x5176;&#x8f93;&#x51fa;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x771f;&#x5b9e;&#x6027;&#x3002;&#x8fd9;&#x4e9b;&#x5de5;&#x4f5c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x3001;&#x66f4;&#x7a33;&#x5065;&#x7684;&#x591a;&#x6a21;&#x6001;&#x4eba;&#x5de5;&#x667a;&#x80fd;&#x7cfb;&#x7edf;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x4e86;LVLM&#x5728;&#x533b;&#x7597;&#x3001;&#x5b89;&#x5168;&#x68c0;&#x6d4b;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x900f;&#x660e;&#x5ea6;&#x3001;&#x589e;&#x5f3a;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#xff0c;&#x5e76;&#x4e3a;&#x89e3;&#x51b3;AI&#x5b89;&#x5168;&#x6027;&#x4e0e;&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x601d;&#x8def;&#x3002;","children":[],"payload":{"tag":"li","lines":"235,237"}}],"payload":{"tag":"li","lines":"231,237","fold":1}}],"payload":{"tag":"h4","lines":"229,230"}},{"content":"MINT: Mitigating Hallucinations in Large Vision-Language Models via Token Reduction","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;MINT&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x51cf;&#x5c11;&#x5197;&#x4f59;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x548c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x589e;&#x5f3a;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5bf9;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x533a;&#x57df;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"238,239"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x7684;&#x9886;&#x57df;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x95ee;&#x7b54;&#xff09;&#x4e2d;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x6570;&#x636e;&#x6807;&#x6ce8;&#x6216;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#xff0c;&#x4f46;&#x5ffd;&#x89c6;&#x4e86;&#x6a21;&#x578b;&#x56fa;&#x6709;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5197;&#x4f59;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x5728;&#x6df1;&#x5c42;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x8fc7;&#x5ea6;&#x5904;&#x7406;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff0c;&#x5206;&#x6563;&#x4e86;&#x5bf9;&#x5173;&#x952e;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x4ece;&#x6839;&#x672c;&#x4e0a;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"240,241"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x5206;&#x6790;LVLM&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x53d1;&#x73b0;&#x6d45;&#x5c42;&#x6ce8;&#x610f;&#x529b;&#x5df2;&#x5145;&#x5206;&#x7406;&#x89e3;&#x56fe;&#x50cf;&#xff0c;&#x4f46;&#x6df1;&#x5c42;&#x5b58;&#x5728;&#x5197;&#x4f59;&#x3002;MINT&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6b65;&#x9aa4;&#xff1a;1&#xff09;&#x4ee4;&#x724c;&#x9009;&#x62e9;&#xff1a;&#x57fa;&#x4e8e;&#x6d45;&#x5c42;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#x52a8;&#x6001;&#x5c4f;&#x853d;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff0c;&#x4ec5;&#x4fdd;&#x7559;&#x5173;&#x952e;&#x4ee4;&#x724c;&#xff1b;2&#xff09;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff1a;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x4e0e;&#x65e0;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff0c;&#x653e;&#x5927;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x4ece;&#x800c; refined &#x6700;&#x7ec8;&#x9884;&#x6d4b;&#x5bf9;&#x6570;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x8f85;&#x52a9;&#x3002;","children":[],"payload":{"tag":"li","lines":"241,242"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x516c;&#x5f00;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;MINT&#x76f8;&#x6bd4;&#x539f;&#x59cb;&#x6a21;&#x578b;&#x5728;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x63d0;&#x5347;&#x4e86;4%&#xff0c;&#x540c;&#x65f6;&#x4f7f;&#x6a21;&#x578b;&#x591a;&#x611f;&#x77e5;5%&#x7684;&#x89c6;&#x89c9;&#x5173;&#x952e;&#x70b9;&#xff08;&#x5c3d;&#x7ba1;&#x51cf;&#x5c11;&#x4e86;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x6570;&#x91cf;&#xff09;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x4e0d;&#x540c;&#x89c4;&#x6a21;&#x7684;LVLM&#x4e0a;&#x5747;&#x6709;&#x6548;&#xff0c;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"242,243"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MINT&#x901a;&#x8fc7;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x7684;&#x4ee4;&#x724c;&#x4f18;&#x5316;&#x548c;&#x5bf9;&#x6bd4;&#x673a;&#x5236;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x589e;&#x5f3a;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x5143;&#x7d20;&#x7684;&#x611f;&#x77e5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x8ba1;&#x7b97;&#x9ad8;&#x6548;&#xff0c;&#x4e3a;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5c24;&#x5176;&#x9002;&#x7528;&#x4e8e;&#x5bf9;&#x9519;&#x8bef;&#x5bb9;&#x5fcd;&#x5ea6;&#x4f4e;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"243,245"}}],"payload":{"tag":"li","lines":"239,245","fold":1}}],"payload":{"tag":"h4","lines":"237,238"}},{"content":"DeGF: Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;DeGF&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#xff08;&#x5982;Stable Diffusion&#xff09;&#x4e3a;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x63d0;&#x4f9b;&#x81ea;&#x53cd;&#x9988;&#xff0c;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x539f;&#x56fe;&#x6765;&#x68c0;&#x6d4b;&#x548c;&#x7ea0;&#x6b63;&#x5e7b;&#x89c9;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"246,247"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7684;&#x5bf9;&#x8c61;&#x3001;&#x8ba1;&#x6570;&#x6216;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x4ec5;&#x9488;&#x5bf9;&#x8bed;&#x8a00;&#x504f;&#x89c1;&#xff0c;&#x65e0;&#x6cd5;&#x5168;&#x9762;&#x89e3;&#x51b3;&#x591a;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x6a21;&#x578b;&#x7684;&#x53cd;&#x9988;&#x80fd;&#x529b;&#xff0c;&#x52a8;&#x6001;&#x7ea0;&#x6b63;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"248,249"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: DeGF&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b97;&#x6cd5;&#xff0c;&#x5206;&#x4e24;&#x6b65;&#xff1a;1) &#x4f7f;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#xff08;&#x5982;Stable Diffusion XL&#xff09;&#x6839;&#x636e;LVLM&#x7684;&#x521d;&#x59cb;&#x54cd;&#x5e94;&#x751f;&#x6210;&#x8f85;&#x52a9;&#x56fe;&#x50cf;&#xff1b;2) &#x5c06;&#x539f;&#x56fe;&#x548c;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x53cc;&#x91cd;&#x89c6;&#x89c9;&#x53c2;&#x8003;&#xff0c;&#x901a;&#x8fc7;&#x4e92;&#x8865;&#x6216;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;complementary/contrastive decoding&#xff09;&#x9a8c;&#x8bc1;&#x548c;&#x4fee;&#x6b63;&#x521d;&#x59cb;&#x54cd;&#x5e94;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x901a;&#x8fc7;&#x6bd4;&#x8f83;&#x57fa;&#x4e8e;&#x539f;&#x56fe;&#x548c;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x7684;token&#x9884;&#x6d4b;&#x5dee;&#x5f02;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4ee5;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"249,250"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;LLaVA-1.5&#x3001;InstructBLIP&#x548c;Qwen-VL&#x7b49;&#x6a21;&#x578b;&#x4e0a;&#xff0c;&#x4e8e;POPE&#x3001;CHAIR&#x3001;MME-Hallucination&#x7b49;&#x516d;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;DeGF&#x5747;&#x8d85;&#x8d8a;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x8c61;&#x5b58;&#x5728;&#x6027;&#x3001;&#x5916;&#x89c2;&#x3001;&#x8ba1;&#x6570;&#x7b49;&#x591a;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x548c;GPT-4V&#x8bc4;&#x4f30;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x63d0;&#x9ad8;&#x4e86;&#x54cd;&#x5e94;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"250,251"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: DeGF&#x9996;&#x6b21;&#x5229;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#x4e3a;LVLM&#x63d0;&#x4f9b;&#x81ea;&#x53cd;&#x9988;&#xff0c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x5176;&#x901a;&#x7528;&#x6027;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;LVLM&#xff0c;&#x4e14;&#x4ee3;&#x7801;&#x5f00;&#x6e90;&#xff0c;&#x4fc3;&#x8fdb;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"251,253"}}],"payload":{"tag":"li","lines":"247,253","fold":1}}],"payload":{"tag":"h4","lines":"245,246"}},{"content":"IAVA: Instruction-Aligned Visual Attention for Mitigating Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x6307;&#x4ee4;&#x5bf9;&#x9f50;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#xff08;IAVA&#xff09;&#x7684;&#x65e0;&#x5b66;&#x4e60;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6bd4;&#x8f83;&#x6a21;&#x578b;&#x5728;&#x4e0d;&#x540c;&#x6307;&#x4ee4;&#x4e0b;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x53d8;&#x5316;&#x6765;&#x8bc6;&#x522b;&#x4e0e;&#x67e5;&#x8be2;&#x65e0;&#x5173;&#x7684;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x65e0;&#x5173;&#x4fe1;&#x606f;&#x7684;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;IAVA&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5747;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"254,255"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x4ecd;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x5305;&#x542b;&#x975e;&#x4e8b;&#x5b9e;&#x5bf9;&#x8c61;&#x7684;&#x7b54;&#x6848;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x533b;&#x7597;&#x548c;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x8bef;&#x5bfc;&#x6027;&#x7ed3;&#x679c;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x867d;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x5229;&#x7528;&#x5f02;&#x5e38;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x4f5c;&#x4e3a;&#x8d1f;&#x6837;&#x672c;&#x6765;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x672a;&#x533a;&#x5206;&#x8fd9;&#x4e9b;&#x4ee4;&#x724c;&#x662f;&#x5426;&#x4e0e;&#x67e5;&#x8be2;&#x76ee;&#x6807;&#x76f8;&#x5173;&#xff0c;&#x53ef;&#x80fd;&#x8bef;&#x5305;&#x542b;&#x76f8;&#x5173;&#x4ee4;&#x724c;&#xff0c;&#x9650;&#x5236;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x672c;&#x6587;&#x65e8;&#x5728;&#x66f4;&#x7cbe;&#x786e;&#x5730;&#x8bc6;&#x522b;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff0c;&#x4ee5;&#x66f4;&#x6709;&#x6548;&#x5730;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"256,257"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x6307;&#x4ee4;&#x5bf9;&#x9f50;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#xff08;IAVA&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x901a;&#x8fc7;&#x4e24;&#x4e2a;&#x6307;&#x4ee4;&#xff08;&#x901a;&#x7528;&#x6307;&#x4ee4;&#x5982;&#x201c;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x201d;&#x548c;&#x5177;&#x4f53;&#x67e5;&#x8be2;&#x6307;&#x4ee4;&#xff09;&#x8ba1;&#x7b97;&#x6a21;&#x578b;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff0c;&#x8bc6;&#x522b;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x8ba1;&#x7b97;&#x901a;&#x7528;&#x6307;&#x4ee4;&#x4e0b;&#x6ce8;&#x610f;&#x529b;&#x5f97;&#x5206;att1&#x548c;&#x67e5;&#x8be2;&#x6307;&#x4ee4;&#x4e0b;&#x6ce8;&#x610f;&#x529b;&#x5f97;&#x5206;att2&#xff1b;2) &#x6839;&#x636e;&#x6ce8;&#x610f;&#x529b;&#x53d8;&#x5316;&#x394;att = att2 - att1&#x548c;&#x7edf;&#x8ba1;&#x9608;&#x503c;&#xff08;&#x5982;att1 &gt; &#x3bc; + &#x3bb;&#x3c3;&#xff09;&#x9009;&#x62e9;&#x540c;&#x65f6;&#x6ee1;&#x8db3;&#x6ce8;&#x610f;&#x529b;&#x4e0b;&#x964d;&#x4e14;&#x901a;&#x7528;&#x6307;&#x4ee4;&#x4e0b;&#x5f97;&#x5206;&#x9ad8;&#x7684;&#x4ee4;&#x724c;&#x4f5c;&#x4e3a;&#x65e0;&#x5173;&#x4ee4;&#x724c;&#xff1b;3) &#x5bf9;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x8fdb;&#x884c;&#x63a9;&#x7801;&#xff0c;&#x4ec5;&#x4fdd;&#x7559;&#x65e0;&#x5173;&#x4ee4;&#x724c;&#x751f;&#x6210;&#x8d1f;&#x6837;&#x672c;&#x56fe;&#x50cf;V*&#xff1b;4) &#x5e94;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x8c03;&#x6574;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x901a;&#x8fc7;&#x964d;&#x4f4e;&#x65e0;&#x5173;&#x4ee4;&#x724c;&#x7684;logits&#x6743;&#x91cd;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x4ec5;&#x9700;&#x63a8;&#x7406;&#x65f6;&#x8c03;&#x6574;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"257,258"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: IAVA&#x5728;MME&#x3001;POPE&#x548c;TextVQA&#x4e09;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x5747;&#x53d6;&#x5f97;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff1a;1) &#x5728;MME&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x4f7f;&#x7528;LLaVA&#x548c;InstructBLIP&#x6a21;&#x578b;&#x65f6;&#xff0c;&#x603b;&#x4f53;&#x5206;&#x6570;&#x5206;&#x522b;&#x6bd4;&#x57fa;&#x7ebf;&#x63d0;&#x9ad8;&#x7ea6;6.9%&#x548c;6.6%&#xff0c;&#x4e14;&#x5728;&#x591a;&#x4e2a;&#x5b50;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x6700;&#x4f73;&#xff1b;2) &#x5728;POPE&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x51e0;&#x4e4e;&#x6240;&#x6709;&#x6307;&#x6807;&#x5747;&#x8fbe;&#x5230;&#x6700;&#x4f18;&#x6027;&#x80fd;&#xff1b;3) &#x5728;TextVQA&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x8f83;&#x57fa;&#x7ebf;&#x63d0;&#x5347;&#x7ea6;6.9%&#x548c;5.4%&#x3002;&#x5b9e;&#x9a8c;&#x8fd8;&#x8868;&#x660e;&#xff0c;IAVA&#x4f18;&#x4e8e;&#x5176;&#x4ed6;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x548c;M3ID&#xff09;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x65e0;&#x5173;&#x4fe1;&#x606f;&#x7684;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x3002;","children":[],"payload":{"tag":"li","lines":"258,259"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: IAVA&#x901a;&#x8fc7;&#x6307;&#x4ee4;&#x5bf9;&#x9f50;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x7cbe;&#x786e;&#x8bc6;&#x522b;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff0c;&#x5e76;&#x7ed3;&#x5408;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x8f93;&#x51fa;&#xff0c;&#x663e;&#x8457;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x8ba1;&#x7b97;&#x9ad8;&#x6548;&#xff0c;&#x4e14;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;&#x6a21;&#x578b;&#x548c;&#x4efb;&#x52a1;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x57fa;&#x4e8e;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6790;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x4ee3;&#x7801;&#x5df2;&#x5f00;&#x6e90;&#xff0c;&#x4fc3;&#x8fdb;&#x8fdb;&#x4e00;&#x6b65;&#x7814;&#x7a76;&#x548c;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"259,261"}}],"payload":{"tag":"li","lines":"255,261","fold":1}}],"payload":{"tag":"h4","lines":"253,254"}},{"content":"Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x89e3;&#x8026;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;DCD&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x89e3;&#x8026;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x5b66;&#x4e60;&#x5e76;&#x8bad;&#x7ec3;&#x72ec;&#x7acb;&#x7684;&#x56fe;&#x50cf;&#x6295;&#x5f71;&#x5668;&#xff0c;&#x5728;&#x4e0d;&#x727a;&#x7272;&#x6a21;&#x578b;&#x901a;&#x7528;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x7684;&#x524d;&#x63d0;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"262,263"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x590d;&#x6742;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x5f3a;&#x5927;&#x7684;&#x63a8;&#x7406;&#x80fd;&#x529b;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x660e;&#x663e;&#x89c6;&#x89c9;&#x6216;&#x4e8b;&#x5b9e;&#x8bc1;&#x636e;&#x4e0d;&#x7b26;&#x7684;&#x8f93;&#x51fa;&#x3002;&#x8fd9;&#x4f1a;&#x635f;&#x5bb3;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#xff0c;&#x5e76;&#x963b;&#x788d;&#x5176;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5b58;&#x5728;&#x7f3a;&#x9677;&#xff1a;&#x57fa;&#x4e8e;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#xff08;&#x5982;DPO&#xff09;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4f3c;&#x7136;&#x4f4d;&#x79fb;&#xff0c;&#x727a;&#x7272;&#x901a;&#x7528;&#x80fd;&#x529b;&#xff1b;&#x800c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff09;&#x4f9d;&#x8d56;&#x624b;&#x5de5;&#x6270;&#x52a8;&#xff0c;&#x65e0;&#x6cd5;&#x6355;&#x6349;&#x771f;&#x5b9e;&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e2;&#x80fd;&#x6709;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#xff0c;&#x53c8;&#x80fd;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x901a;&#x7528;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x7684;&#x201c;&#x9c81;&#x68d2;&#x201d;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"264,265"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89e3;&#x8026;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;DCD&#xff09;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x8bbe;&#x8ba1;&#xff1a;1&#xff09;&#x89e3;&#x8026;&#x5b66;&#x4e60;&#xff1a;&#x5c06;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x4e2d;&#x7684;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x5bf9;&#x5b66;&#x4e60;&#x89e3;&#x8026;&#x4e3a;&#x72ec;&#x7acb;&#x7684;&#x5b66;&#x4e60;&#x8fc7;&#x7a0b;&#xff0c;&#x4ee5;&#x7f13;&#x89e3;&#x4f3c;&#x7136;&#x4f4d;&#x79fb;&#x95ee;&#x9898;&#x3002;2&#xff09;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7684;&#x8d1f;&#x56fe;&#x50cf;&#xff1a;&#x5229;&#x7528;&#x8d1f;&#x6837;&#x672c;&#x5b66;&#x4e60;&#x4e00;&#x4e2a;&#x8d1f;&#x56fe;&#x50cf;&#x6295;&#x5f71;&#x5668;&#xff0c;&#x4ee5;&#x66ff;&#x4ee3;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x4e2d;&#x624b;&#x5de5;&#x8bbe;&#x8ba1;&#x7684;&#x56fe;&#x50cf;&#x6270;&#x52a8;&#x3002;&#x5728;&#x8bad;&#x7ec3;&#x9636;&#x6bb5;&#xff0c;&#x4f7f;&#x7528;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x5206;&#x522b;&#x8bad;&#x7ec3;MLLM&#x4e2d;&#x7684;&#x6b63;&#x56fe;&#x50cf;&#x6295;&#x5f71;&#x5668;&#x548c;&#x8d1f;&#x56fe;&#x50cf;&#x6295;&#x5f71;&#x5668;&#x3002;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#xff0c;&#x4f7f;&#x7528;&#x8d1f;&#x56fe;&#x50cf;&#x6295;&#x5f71;&#x5668;&#x5c06;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#x6295;&#x5f71;&#x4e3a;&#x201c;&#x8d1f;&#x201d;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#xff0c;&#x5e76;&#x5728;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x4e2d;&#x4f7f;&#x7528;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5229;&#x7528;&#x6a21;&#x578b;&#x751f;&#x6210;&#x7684;&#x8d1f;&#x6837;&#x672c;&#xff08;&#x6765;&#x81ea;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#xff09;&#x6765;&#x9690;&#x5f0f;&#x5730;&#x5efa;&#x6a21;&#x771f;&#x5b9e;&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#xff0c;&#x4ece;&#x800c;&#x66f4;&#x7cbe;&#x786e;&#x5730;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"265,266"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x548c;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;DCD&#x5728;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x6027;&#x80fd;&#x4e0a;&#x4e0e;DPO&#x76f8;&#x5f53;&#xff0c;&#x540c;&#x65f6;&#x5728;&#x901a;&#x7528;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x4fdd;&#x6301;&#x751a;&#x81f3;&#x63d0;&#x9ad8;&#x4e86;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x800c;DPO&#x5728;&#x901a;&#x7528;&#x80fd;&#x529b;&#x4e0a;&#x51fa;&#x73b0;&#x4e86;&#x660e;&#x663e;&#x7684;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x3002;2&#xff09;&#x4e0e;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#xff09;&#x76f8;&#x6bd4;&#xff0c;DCD&#x5728;&#x6240;&#x6709;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x90fd;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x4f18;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;3&#xff09;&#x5f97;&#x76ca;&#x4e8e;&#x89e3;&#x8026;&#x5b66;&#x4e60;&#x8bbe;&#x8ba1;&#xff0c;DCD&#x751a;&#x81f3;&#x53ef;&#x4ee5;&#x4ec5;&#x4ece;&#x8d1f;&#x6837;&#x672c;&#x4e2d;&#x5b66;&#x4e60;&#xff08;&#x5373;&#x4ec5;&#x8bad;&#x7ec3;&#x8d1f;&#x56fe;&#x50cf;&#x6295;&#x5f71;&#x5668;&#xff09;&#xff0c;&#x4ec5;&#x4f7f;&#x7528;&#x8d1f;&#x6837;&#x672c;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#x5c31;&#x80fd;&#x663e;&#x8457;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x800c;&#x4f7f;&#x7528;&#x6b63;&#x6837;&#x672c;&#x7684;&#x6539;&#x8fdb;&#x5219;&#x5fae;&#x4e4e;&#x5176;&#x5fae;&#x3002;&#x8fd9;&#x63ed;&#x793a;&#x4e86;&#x5728;RLHF&#x5fae;&#x8c03;&#x9636;&#x6bb5;&#xff0c;&#x8d1f;&#x6837;&#x672c;&#x6bd4;&#x6b63;&#x6837;&#x672c;&#x66f4;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"266,267"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;DCD&#x6846;&#x67b6;&#x5b9e;&#x73b0;&#x4e86;&#x4e00;&#x79cd;&#x9c81;&#x68d2;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x3002;&#x5b83;&#x907f;&#x514d;&#x4e86;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x7684;&#x7f3a;&#x70b9;&#xff1a;&#x65e2;&#x4e0d;&#x4f1a;&#x50cf;DPO&#x90a3;&#x6837;&#x56e0;&#x6210;&#x5bf9;&#x4f18;&#x5316;&#x800c;&#x5bfc;&#x81f4;&#x4f3c;&#x7136;&#x4f4d;&#x79fb;&#x548c;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#xff0c;&#x4e5f;&#x4e0d;&#x4f1a;&#x50cf;&#x624b;&#x5de5;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x90a3;&#x6837;&#x56e0;&#x4f7f;&#x7528;&#x4e0d;&#x771f;&#x5b9e;&#x7684;&#x6270;&#x52a8;&#x800c;&#x5f15;&#x5165;&#x566a;&#x58f0;&#x3002;DCD&#x7684;&#x6838;&#x5fc3;&#x6d1e;&#x5bdf;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x89e3;&#x8026;&#x5b66;&#x4e60;&#x5e76;&#x4ece;&#x8d1f;&#x6837;&#x672c;&#x4e2d;&#x663e;&#x5f0f;&#x5b66;&#x4e60;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#xff0c;&#x53ef;&#x4ee5;&#x4e3a;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x5224;&#x522b;&#x6027;&#x610f;&#x8bc6;&#xff0c;&#x4ece;&#x800c;&#x8865;&#x5145;&#x5176;&#x73b0;&#x6709;&#x77e5;&#x8bc6;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x548c;&#x66f4;&#x5e7f;&#x6cdb;&#x7684;MLLM&#x5bf9;&#x9f50;&#x7814;&#x7a76;&#x5f00;&#x8f9f;&#x4e86;&#x65b0;&#x7684;&#x9053;&#x8def;&#xff0c;&#x672a;&#x6765;&#x6709;&#x671b;&#x63a8;&#x52a8;&#x8be5;&#x9886;&#x57df;&#x7684;&#x8fdb;&#x4e00;&#x6b65;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"267,269"}}],"payload":{"tag":"li","lines":"263,269","fold":1}}],"payload":{"tag":"h4","lines":"261,262"}},{"content":"SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;SECOND&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x9009;&#x62e9;&#x6027;&#x591a;&#x5c3a;&#x5ea6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x6574;&#x5408;&#x548c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8d85;&#x8d8a;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"270,271"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5408;&#x7406;&#x56de;&#x7b54;&#xff09;&#x65b9;&#x9762;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x7684;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x4f7f;&#x7528;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x5c1d;&#x8bd5;&#x901a;&#x8fc7;&#x591a;&#x5c3a;&#x5ea6;&#x89c6;&#x89c9;&#x6280;&#x672f;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x5f53;&#x524d;&#x65b9;&#x6cd5;&#x5f80;&#x5f80; indiscriminately&#xff08;&#x4e0d;&#x52a0;&#x533a;&#x5206;&#x5730;&#xff09;&#x6574;&#x5408;&#x591a;&#x5c3a;&#x5ea6;&#x4fe1;&#x606f;&#xff0c;&#x5bfc;&#x81f4;&#x7269;&#x4f53;&#x7ec6;&#x8282;&#x63d0;&#x53d6;&#x4e0d;&#x7cbe;&#x786e;&#xff0c;&#x4fe1;&#x606f;&#x635f;&#x5931;&#x4e25;&#x91cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"272,273"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: SECOND&#x662f;&#x4e00;&#x4e2a;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#x4e24;&#x4e2a;&#x90e8;&#x5206;&#xff1a;1) &#x9009;&#x62e9;&#x6027;&#x89e3;&#x7801;&#xff08;Selective Decoding&#xff09;&#xff1a;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x5e76;&#x9010;&#x6b65;&#x6574;&#x5408;&#x4e0e;&#x7269;&#x4f53;&#x76f8;&#x5173;&#x7684;&#x591a;&#x5c3a;&#x5ea6;&#x56fe;&#x50cf;&#x5757;&#xff08;&#x4ece;&#x7c97;&#x7c92;&#x5ea6;&#x5230;&#x7ec6;&#x7c92;&#x5ea6;&#xff09;&#xff0c;&#x8fc7;&#x6ee4;&#x4e0d;&#x76f8;&#x5173;&#x4fe1;&#x606f;&#xff0c;&#x6a21;&#x62df;&#x4eba;&#x7c7b;&#x4ece;&#x6574;&#x4f53;&#x626b;&#x63cf;&#x5230;&#x7cbe;&#x7ec6;&#x89c2;&#x5bdf;&#x7684;&#x89c6;&#x89c9;&#x8fc7;&#x7a0b;&#xff1b;2) &#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;Contrastive Decoding&#xff09;&#xff1a;&#x91c7;&#x7528;&#x591a;&#x9636;&#x6bb5;&#x89e3;&#x7801;&#xff0c;&#x5c06;&#x521d;&#x59cb;&#x9636;&#x6bb5;&#x7684;&#x201c;&#x4e1a;&#x4f59;&#x201d;&#x8f93;&#x51fa;&#xff08;&#x7c97;&#x5904;&#x7406;&#xff09;&#x4e0e;&#x6700;&#x7ec8;&#x9636;&#x6bb5;&#x7684;&#x201c;&#x4e13;&#x5bb6;&#x201d;&#x8f93;&#x51fa;&#xff08;&#x7ec6;&#x7c92;&#x5ea6;&#x5206;&#x6790;&#xff09;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#xff0c;&#x901a;&#x8fc7;&#x7a00;&#x758f;&#x4e0e;&#x5bc6;&#x96c6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5bf9;&#x6bd4;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"273,274"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: SECOND&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff1a;&#x5728;POPE&#xff08;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#xff09;&#x3001;VQAv2&#xff08;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff09;&#x3001;MMStar&#x548c;MMBench&#x7b49;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5747;&#x53d6;&#x5f97;&#x4e86;&#x66f4;&#x4f18;&#x7684;&#x6027;&#x80fd;&#x3002;&#x7406;&#x8bba;&#x5206;&#x6790;&#x548c;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x5176;&#x6ce8;&#x610f;&#x529b;&#x9ab0;&#x5b50;&#x7cfb;&#x6570;&#xff08;Attention Dice Coefficient&#xff09;&#x4e0e;&#x5e7b;&#x89c9;&#x6982;&#x7387;&#x5448;&#x5f3a;&#x76f8;&#x5173;&#x6027;&#xff0c;&#x8bc1;&#x5b9e;&#x4e86;&#x7cbe;&#x51c6;&#x5173;&#x6ce8;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x53ef;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x98ce;&#x9669;&#x3002;","children":[],"payload":{"tag":"li","lines":"274,275"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SECOND&#x901a;&#x8fc7;&#x9009;&#x62e9;&#x6027;&#x548c;&#x5bf9;&#x6bd4;&#x6027;&#x89e3;&#x7801;&#x673a;&#x5236;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x89c6;&#x89c9;&#x4fdd;&#x771f;&#x5ea6;&#x548c;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x63ed;&#x793a;&#x4e86;&#x591a;&#x5c3a;&#x5ea6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5728;VLMs&#x4e2d;&#x7684;&#x672a;&#x5f00;&#x53d1;&#x6f5c;&#x529b;&#xff0c;&#x5373;&#x4f18;&#x5148;&#x5904;&#x7406;&#x548c;&#x5bf9;&#x6bd4;&#x4e0d;&#x540c;&#x5c3a;&#x5ea6;&#x7684;&#x4fe1;&#x606f;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;VLMs&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"275,277"}}],"payload":{"tag":"li","lines":"271,277","fold":1}}],"payload":{"tag":"h4","lines":"269,270"}},{"content":"ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;ASCD&#x7684;&#x6ce8;&#x610f;&#x529b;&#x53ef;&#x5f15;&#x5bfc;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x76f4;&#x63a5;&#x5e72;&#x9884;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5185;&#x90e8;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x800c;&#x975e;&#x4ec5;&#x4fee;&#x6539;&#x8f93;&#x51fa;logits&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"278,279"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x5904;&#x7406;&#x56fe;&#x50cf;&#x548c;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x9519;&#x8bef;&#x5185;&#x5bb9;&#xff09;&#xff0c;&#x8fd9;&#x662f;&#x56e0;&#x4e3a;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x90e8;&#x5206;&#x7ebf;&#x7d22;&#x800c;&#x5ffd;&#x7565;&#x6574;&#x4f53;&#x4e0a;&#x4e0b;&#x6587;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x548c;ICD&#xff09;&#x901a;&#x8fc7;&#x6270;&#x52a8;&#x8f93;&#x5165;&#x6216;&#x6dfb;&#x52a0;&#x8d1f;&#x9762;&#x524d;&#x7f00;&#x6765;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x4f46;&#x4ec5;&#x505c;&#x7559;&#x5728;&#x8868;&#x9762;logit&#x8c03;&#x6574;&#xff0c;&#x672a;&#x89e3;&#x51b3;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x7684;&#x6839;&#x672c;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x66f4;&#x539f;&#x5219;&#x6027;&#x7684;&#x65b9;&#x6cd5;&#x4ece;&#x5185;&#x90e8;&#x673a;&#x5236;&#x5165;&#x624b;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"280,281"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x6ce8;&#x610f;&#x529b;&#x53ef;&#x5f15;&#x5bfc;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;ASCD&#xff09;&#x6846;&#x67b6;&#xff0c;&#x76f4;&#x63a5;&#x5e72;&#x9884;&#x6a21;&#x578b;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x3002;&#x5177;&#x4f53;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x6b63;&#x5411;&#x5f15;&#x5bfc;&#xff1a;&#x589e;&#x5f3a;&#x5bf9;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x673a;&#x5236;&#x8bc6;&#x522b;&#x201c;&#x4ee5;&#x6587;&#x672c;&#x4e3a;&#x4e2d;&#x5fc3;&#x201d;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff08;text-centric heads&#xff09;&#xff0c;&#x9488;&#x5bf9;&#x6027;&#x52a0;&#x5f3a;&#x89c6;&#x89c9;&#x8d21;&#x732e;&#xff1b;2&#xff09;&#x8d1f;&#x5411;&#x5f15;&#x5bfc;&#xff1a;&#x6291;&#x5236;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x8d1f;&#x9762;&#x5f71;&#x54cd;&#xff0c;&#x4ec5;&#x5bf9;&#x5fc5;&#x8981;&#x4f4d;&#x7f6e;&#x8fdb;&#x884c; suppression &#x4ee5;&#x4fdd;&#x7559;&#x6709;&#x6548;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff1b;3&#xff09;&#x5c06;&#x6ce8;&#x610f;&#x529b;&#x8c03;&#x6574;&#x96c6;&#x6210;&#x5230;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6d41;&#x7a0b;&#x4e2d;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x5236;&#x6ce8;&#x610f;&#x529b;&#x8def;&#x5f84;&#x800c;&#x975e;&#x88ab;&#x52a8;&#x8c03;&#x6574;logits&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff08;&#x8d2a;&#x5a6a;&#x641c;&#x7d22;&#x3001;&#x675f;&#x641c;&#x7d22;&#x3001;&#x6838;&#x91c7;&#x6837;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"281,282"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;MLLM&#x67b6;&#x6784;&#xff08;LLaVA-1.5 7B&#x3001;LLaVA-NeXT 7B&#x3001;Phi2-SigLIP&#xff09;&#x548c;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x9a8c;&#x8bc1;&#xff1a;1&#xff09;&#x5728;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;&#xff08;POPE&#x3001;CHAIR&#x3001;MMHAL-BENCH&#xff09;&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff1b;2&#xff09;&#x5728;&#x6807;&#x51c6;VQA&#x57fa;&#x51c6;&#xff08;MMMU&#x3001;MM-VET&#x3001;SCIENCEQA&#x3001;TEXTVQA&#x3001;GQA&#xff09;&#x4e0a;&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff0c;&#x800c;&#x5176;&#x4ed6;&#x5bf9;&#x6bd4;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x3001;ICD&#xff09;&#x5728;&#x8fd9;&#x4e9b;&#x57fa;&#x51c6;&#x4e0a;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#xff1b;3&#xff09;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;ASCD&#x80fd;&#x6709;&#x6548;&#x8c03;&#x6574;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x589e;&#x5f3a;&#x89c6;&#x89c9; grounding&#x3002;","children":[],"payload":{"tag":"li","lines":"282,283"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ASCD&#x901a;&#x8fc7;&#x76f4;&#x63a5;&#x64cd;&#x7eb5;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x66f4;&#x539f;&#x5219;&#x6027;&#x7684;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x8bc1;&#x660e;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x8c03;&#x6574;&#x662f;&#x63d0;&#x5347;MLLM&#x53ef;&#x9760;&#x6027;&#x7684;&#x5173;&#x952e;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x901a;&#x7528;&#x6027;&#x5f3a;&#x7684;&#x7279;&#x70b9;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#x5230;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x4e2d;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;AI&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x5230;&#x5176;&#x4ed6;&#x6a21;&#x6001;&#x548c;&#x4efb;&#x52a1;&#x3002;","children":[],"payload":{"tag":"li","lines":"283,285"}}],"payload":{"tag":"li","lines":"279,285","fold":1}}],"payload":{"tag":"h4","lines":"277,278"}},{"content":"Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5b58;&#x5728;&#x4e00;&#x79cd;&#x65b0;&#x7684;&#x2018;&#x6a21;&#x6001;&#x504f;&#x89c1;&#x2019;&#x73b0;&#x8c61;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#x65f6;&#xff0c;&#x4f1a;&#x8fc7;&#x5ea6;&#x504f;&#x5411;&#x89c6;&#x89c9;&#x6216;&#x6587;&#x672c;&#x4fe1;&#x606f;&#xff0c;&#x800c;&#x5ffd;&#x7565;&#x53e6;&#x4e00;&#x79cd;&#x6a21;&#x6001;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;TVAI&#xff0c;&#x901a;&#x8fc7;&#x5e72;&#x9884;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x6765;&#x5e73;&#x8861;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"286,287"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;LVLM&#x4e2d;&#x4e25;&#x91cd;&#x7684;&#x2018;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x7b49;&#x73b0;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x90e8;&#x7f72;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x5c06;&#x539f;&#x56e0;&#x5f52;&#x548e;&#x4e8e;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x6216;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;LLM&#x7684;&#x89c4;&#x6a21;&#x4e0d;&#x5339;&#x914d;&#xff0c;&#x4f46;&#x672c;&#x6587;&#x53d1;&#x73b0;&#x4e86;&#x4e00;&#x79cd;&#x88ab;&#x5ffd;&#x89c6;&#x7684;&#x2018;&#x6a21;&#x6001;&#x504f;&#x89c1;&#x2019;&#x73b0;&#x8c61;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x5728;&#x5e7b;&#x89c9;&#x65f6;&#x53ef;&#x80fd;&#x540c;&#x65f6;&#x5ffd;&#x7565;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x4fe1;&#x606f;&#xff0c;&#x5bfc;&#x81f4;&#x5bf9;&#x7528;&#x6237;&#x6307;&#x4ee4;&#x7684;&#x788e;&#x7247;&#x5316;&#x7406;&#x89e3;&#x3002;","children":[],"payload":{"tag":"li","lines":"288,289"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;TVAI&#xff08;Textual and Visual Attention Intervention&#xff09;&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x5e72;&#x9884;&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x76f4;&#x63a5;&#x64cd;&#x4f5c;LVLM&#x89e3;&#x7801;&#x5c42;&#x4e2d;&#x7684;&#x81ea;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff1a;1&#xff09;&#x5bf9;&#x4e8e;&#x751f;&#x6210;&#x5f0f;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#xff09;&#xff0c;&#x589e;&#x5f3a;&#x5bf9;&#x6587;&#x672c;&#x6307;&#x4ee4;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff1b;2&#xff09;&#x5bf9;&#x4e8e;&#x5224;&#x522b;&#x5f0f;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x56de;&#x7b54;&#x5bf9;&#x8c61;&#x662f;&#x5426;&#x5b58;&#x5728;&#xff09;&#xff0c;&#x589e;&#x5f3a;&#x5bf9;&#x89c6;&#x89c9;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x3002;&#x901a;&#x8fc7;&#x8fd9;&#x79cd;&#x5e72;&#x9884;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x751f;&#x6210;&#x66f4;&#x5e73;&#x8861;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff0c;&#x66f4;&#x597d;&#x5730;&#x5bf9;&#x9f50;&#x7528;&#x6237;&#x610f;&#x56fe;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x8fd8;&#x91c7;&#x7528;&#x4e86;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x4ee5;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x53c2;&#x6570;&#x5316;&#x77e5;&#x8bc6;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4e0e;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#x534f;&#x540c;&#x4f5c;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"289,290"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x6a21;&#x6001;&#x504f;&#x89c1;&#x5728;&#x591a;&#x4e2a;&#x4e3b;&#x6d41;LVLM&#xff08;&#x5982;LLaVA-1.5&#xff09;&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#xff1b;2&#xff09;TVAI&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;CHAIR&#x3001;POPE&#x3001;MMBench&#xff09;&#x4e0a;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x4e00;&#x822c;&#x80fd;&#x529b;&#xff1b;3&#xff09;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x8bad;&#x7ec3;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x5c0f;&#xff0c;&#x4e14;&#x53ef;&#x6cdb;&#x5316;&#x5230;&#x4e0d;&#x540c;&#x7684;LVLM&#x548c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"290,291"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x6a21;&#x6001;&#x504f;&#x89c1;&#x662f;&#x5bfc;&#x81f4;LVLM&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x7684;&#x4e00;&#x4e2a;&#x5173;&#x952e;&#x56e0;&#x7d20;&#x3002;TVAI&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x7b80;&#x5355;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x4e3a;&#x5f00;&#x53d1;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;&#x52a9;&#x624b;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#xff1a;1&#xff09;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x4f4e;&#x6210;&#x672c;&#x3001;&#x9ad8;&#x6548;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6848;&#xff1b;2&#xff09;&#x5f00;&#x8f9f;&#x4e86;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x6765;&#x7406;&#x89e3;&#x548c;&#x6539;&#x8fdb;LVLM&#x7684;&#x65b0;&#x9014;&#x5f84;&#xff1b;3&#xff09;&#x63a8;&#x52a8;&#x4e86;&#x672a;&#x6765;&#x80fd;&#x591f;&#x66f4;&#x5168;&#x9762;&#x6574;&#x5408;&#x7ec6;&#x7c92;&#x5ea6;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x7684;&#x901a;&#x7528;&#x52a9;&#x624b;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"291,293"}}],"payload":{"tag":"li","lines":"287,293","fold":1}}],"payload":{"tag":"h4","lines":"285,286"}},{"content":"ECD: Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x9ad8;&#x6548;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;ECD&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6982;&#x7387;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x751f;&#x6210;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x4ec5;&#x9700;&#x4e00;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x548c;&#x8f7b;&#x91cf;&#x7ea7;&#x68c0;&#x6d4b;&#xff0c;&#x5373;&#x53ef;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x6709;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x6548;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"294,295"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CD&#xff09;&#x867d;&#x80fd;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x9700;&#x8981;&#x8ba1;&#x7b97;&#x4e24;&#x6b21;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x589e;&#x52a0;&#x4e86;&#x63a8;&#x7406;&#x65f6;&#x95f4;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x5b9e;&#x65f6;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"296,297"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x9ad8;&#x6548;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;ECD&#xff09;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x57fa;&#x4e8e;&#x5143;&#x5206;&#x7c7b;&#xff08;meta classification&#xff09;&#x8bad;&#x7ec3;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x5668;&#xff0c;&#x8be5;&#x68c0;&#x6d4b;&#x5668;&#x5229;&#x7528;&#x4ece;LVLM&#x4e2d;&#x95f4;&#x5c42;&#x63d0;&#x53d6;&#x7684;&#x7279;&#x5f81;&#xff08;&#x5982;&#x8d1f;&#x5bf9;&#x6570;&#x4f3c;&#x7136;&#x3001;KL&#x6563;&#x5ea6;&#x3001;&#x6ce8;&#x610f;&#x529b;&#x71b5;&#x7b49;&#xff09;&#x6765;&#x4f30;&#x8ba1;&#x6bcf;&#x4e2a;token&#x7684;&#x5e7b;&#x89c9;&#x6982;&#x7387;&#xff08;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#xff09;&#xff1b;2&#xff09;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;token&#x6982;&#x7387;&#x548c;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#xff0c;&#x4f7f;&#x7528;&#x516c;&#x5f0f;&#x8c03;&#x6574;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x6291;&#x5236;&#x9ad8;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#x7684;token&#xff1b;3&#xff09;&#x7ed3;&#x5408;&#x81ea;&#x9002;&#x5e94;&#x5408;&#x7406;&#x6027;&#x7ea6;&#x675f;&#xff08;APC&#xff09;&#x786e;&#x4fdd;&#x8bed;&#x4e49;&#x5408;&#x7406;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;&#x4e00;&#x6b21;LVLM&#x524d;&#x5411;&#x4f20;&#x64ad;&#x548c;&#x8f7b;&#x91cf;&#x7ea7;&#x68c0;&#x6d4b;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"297,298"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;ECD&#x5728;&#x591a;&#x4e2a;LVLM&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5728;&#x5f00;&#x653e;&#x5f0f;&#x4efb;&#x52a1;&#x4e2d;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;&#x6700;&#x591a;5.74&#x4e2a;&#x767e;&#x5206;&#x70b9;&#xff08;&#x76f8;&#x5bf9;&#x51cf;&#x5c11;32%&#xff09;&#xff0c;&#x5728;&#x5224;&#x522b;&#x5f0f;VQA&#x4efb;&#x52a1;&#x4e2d;F1&#x5206;&#x6570;&#x63d0;&#x5347;23.02&#x4e2a;&#x767e;&#x5206;&#x70b9;&#xff08;&#x76f8;&#x5bf9;&#x63d0;&#x5347;33%&#xff09;&#xff1b;2&#xff09;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x5668;&#x7684;&#x7cbe;&#x786e;&#x53ec;&#x56de;&#x66f2;&#x7ebf;&#x4e0b;&#x9762;&#x79ef;&#xff08;AUPR&#xff09;&#x8fbe;&#x5230;74.05%&#xff1b;3&#xff09;ECD&#x5728;&#x8ba1;&#x7b97;&#x65f6;&#x95f4;&#x4e0a;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x548c;ICD&#xff09;&#xff0c;&#x4ec5;&#x589e;&#x52a0;&#x8f7b;&#x5fae;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"298,299"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ECD&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x6291;&#x5236;LVLM&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x5176;&#x8f7b;&#x91cf;&#x7ea7;&#x8bbe;&#x8ba1;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;&#x5f00;&#x6e90;LVLM&#xff0c;&#x5bf9;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x66f4;&#x591a;&#x6a21;&#x6001;&#x548c;&#x4efb;&#x52a1;&#x3002;","children":[],"payload":{"tag":"li","lines":"299,301"}}],"payload":{"tag":"li","lines":"295,301","fold":1}}],"payload":{"tag":"h4","lines":"293,294"}}],"payload":{"tag":"h3","lines":"58,59","fold":1}},{"content":"&#x52a0;&#x6743;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;","children":[{"content":"ED: Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x96c6;&#x6210;&#x89e3;&#x7801;&#xff08;ED&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5c06;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5206;&#x5272;&#x4e3a;&#x5b50;&#x56fe;&#x50cf;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x52a8;&#x6001;&#x52a0;&#x6743;&#x7ec4;&#x5408;&#x5176;logit&#x5206;&#x5e03;&#xff0c;&#x4ee5;&#x51cf;&#x8f7b;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"304,305"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5305;&#x542b;&#x4e0d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x6216;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x73b0;&#x6709;&#x7269;&#x4f53;&#x7684;&#x5185;&#x5bb9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x548c;&#x5236;&#x9020;&#x7cfb;&#x7edf;&#x7b49;&#x9700;&#x8981;&#x7cbe;&#x786e;&#x7b54;&#x6848;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x5c24;&#x4e3a;&#x4e25;&#x91cd;&#xff0c;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x9002;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;&#x6570;&#x636e;&#x589e;&#x5f3a;&#x548c;&#x514d;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#x5b58;&#x5728;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5dee;&#x548c;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x6a21;&#x5757;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x6709;&#x6548;&#x4e14;&#x81ea;&#x9002;&#x5e94;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"306,307"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x96c6;&#x6210;&#x89e3;&#x7801;&#xff08;ED&#xff09;&#x7b56;&#x7565;&#xff0c;&#x4e3b;&#x8981;&#x5305;&#x62ec;&#x4e09;&#x4e2a;&#x90e8;&#x5206;&#xff1a;1&#xff09;&#x5c06;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5206;&#x5272;&#x4e3a;&#x591a;&#x4e2a;&#x5b50;&#x56fe;&#x50cf;&#xff0c;&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e00;&#x8d77;&#x8f93;&#x5165;&#x9884;&#x8bad;&#x7ec3;&#x7684;LVLM&#xff0c;&#x83b7;&#x53d6;&#x5404;&#x81ea;&#x7684;logit&#x5206;&#x5e03;&#xff1b;2&#xff09;&#x5229;&#x7528;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x8ba1;&#x7b97;&#x6bcf;&#x4e2a;&#x5b50;&#x56fe;&#x50cf;&#x5728;&#x6bcf;&#x4e00;&#x6b65;token&#x751f;&#x6210;&#x65f6;&#x7684;&#x52a8;&#x6001;&#x6743;&#x91cd;&#xff0c;&#x901a;&#x8fc7;softmax&#x51fd;&#x6570;&#x751f;&#x6210;&#x6ce8;&#x610f;&#x529b;&#x5f15;&#x5bfc;&#x7684;&#x6743;&#x91cd;&#xff1b;3&#xff09;&#x5c06;&#x8fd9;&#x4e9b;&#x52a0;&#x6743;logit&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x7684;logit&#x7ed3;&#x5408;&#xff0c;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x8fd8;&#x5f15;&#x5165;&#x4e86;ED&#x81ea;&#x9002;&#x5e94;&#x5408;&#x7406;&#x6027;&#x7ea6;&#x675f;&#xff0c;&#x901a;&#x8fc7;&#x8d85;&#x53c2;&#x6570;&#x3b2;&#x6821;&#x51c6;logit&#x5206;&#x5e03;&#xff0c;&#x786e;&#x4fdd;&#x7ec6;&#x7c92;&#x5ea6;token&#x7684;&#x8d21;&#x732e;&#xff1b;&#x4ee5;&#x53ca;FastED&#x53d8;&#x4f53;&#xff0c;&#x4ec5;&#x4f7f;&#x7528;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x6700;&#x9ad8;&#x7684;&#x5b50;&#x56fe;&#x50cf;&#x6765;&#x5e73;&#x8861;&#x6027;&#x80fd;&#x4e0e;&#x901f;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"307,308"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x3001;CHAIR&#x3001;MME&#x548c;LLaVA-Bench&#x7b49;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;ED&#x65b9;&#x6cd5;&#x5728;&#x5927;&#x591a;&#x6570;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x4e0a;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;ED&#x5728;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x9ad8;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x800c;FastED&#x53d8;&#x4f53;&#x5728;&#x4ec5;&#x4f7f;&#x7528;&#x4e24;&#x4e2a;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x6027;&#x80fd;&#x635f;&#x5931;&#x6700;&#x5c0f;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x901f;&#x5ea6;&#x4e0e;&#x6548;&#x679c;&#x7684;&#x5e73;&#x8861;&#x3002;","children":[],"payload":{"tag":"li","lines":"308,309"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;ED&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6709;&#x6548;&#x5229;&#x7528;&#x5185;&#x5728;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x548c;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6a21;&#x5757;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x8f7b;&#x4e86;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e00;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x9002;&#x7528;&#x6027;&#xff0c;&#x8fd8;&#x4e3a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x6548;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff08;&#x5982;FastED&#xff09;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x5236;&#x9020;&#x7cfb;&#x7edf;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x66f4;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#xff0c;&#x4ee5;&#x53ca;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x514d;&#x8bad;&#x7ec3;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"309,311"}}],"payload":{"tag":"li","lines":"305,311","fold":1}}],"payload":{"tag":"h4","lines":"303,304"}},{"content":"TPC: Cross-Temporal Prediction Connection for Vision-Language Model Hallucination Reduction","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x8de8;&#x65f6;&#x95f4;&#x9884;&#x6d4b;&#x8fde;&#x63a5;&#xff08;TPC&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x8fde;&#x63a5;&#x76f8;&#x90bb;&#x65f6;&#x95f4;&#x6b65;&#x7684;logits&#x6765;&#x589e;&#x5f3a;&#x8bed;&#x4e49;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x5728;&#x4fdd;&#x6301;&#x9ad8;&#x6548;&#x7684;&#x540c;&#x65f6;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"312,313"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5728;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x4f1a;&#x81ea;&#x4fe1;&#x5730;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x5c5e;&#x6027;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4ee5;&#x53ca;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;&#x5927;&#x578b;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x4e4b;&#x95f4;&#x7684;&#x80fd;&#x529b;&#x4e0d;&#x5e73;&#x8861;&#x3002;&#x5728;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x5b66;&#x56fe;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#xff0c;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"314,315"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x8de8;&#x65f6;&#x95f4;&#x9884;&#x6d4b;&#x8fde;&#x63a5;&#xff08;TPC&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x79cd;&#x7b56;&#x7565;&#xff1a;1&#xff09;&#x7ebf;&#x6027;&#x65f6;&#x95f4;&#x9884;&#x6d4b;&#x8fde;&#x63a5;&#xff08;LTPC&#xff09;&#xff1a;&#x4f7f;&#x7528;&#x6ed1;&#x52a8;&#x7a97;&#x53e3;&#x5c06;&#x8fde;&#x7eed;&#x65f6;&#x95f4;&#x6b65;&#x7684;logits&#x52a0;&#x6743;&#x6c42;&#x548c;&#xff0c;&#x589e;&#x5f3a;&#x5f53;&#x524d;logits&#x7684;&#x4e0a;&#x4e0b;&#x6587;&#x4fe1;&#x606f;&#xff1b;2&#xff09;&#x8870;&#x51cf;&#x65f6;&#x95f4;&#x9884;&#x6d4b;&#x8fde;&#x63a5;&#xff08;ATPC&#xff09;&#xff1a;&#x4ee5;&#x6307;&#x6570;&#x8870;&#x51cf;&#x65b9;&#x5f0f;&#x52a0;&#x6743;&#x5386;&#x53f2;logits&#xff0c;&#x5f3a;&#x8c03;&#x8fd1;&#x671f;&#x4fe1;&#x606f;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5bf9;&#x6bd4;&#x64cd;&#x4f5c;&#xff0c;&#x76f4;&#x63a5;&#x901a;&#x8fc7;logits&#x7684;&#x65f6;&#x95f4;&#x5173;&#x8054;&#x6027;&#x63d0;&#x5347;&#x8bed;&#x4e49;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"315,316"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;TPC&#x5728;POPEMSCoCO&#x7b49;&#x57fa;&#x51c6;&#x4e0a;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x548c;F1&#x5206;&#x6570;&#x8d85;&#x8d8a;&#x5bf9;&#x6bd4;&#x65b9;&#x6cd5;VCD&#x548c;DoLa&#xff1b;2&#xff09;TPC&#x5728;&#x751f;&#x6210;&#x957f;&#x6587;&#x672c;&#x65f6;&#x8868;&#x73b0;&#x66f4;&#x9c81;&#x68d2;&#xff0c;&#x4e14;&#x63a8;&#x7406;&#x6548;&#x7387;&#x66f4;&#x9ad8;&#xff08;&#x65e0;&#x9700;&#x91cd;&#x590d;&#x751f;&#x6210;logits&#x6216;&#x641c;&#x7d22;&#x65e9;&#x671f;&#x5c42;&#xff09;&#xff1b;3&#xff09;&#x7406;&#x8bba;&#x5206;&#x6790;&#x9a8c;&#x8bc1;&#x4e86;&#x76f8;&#x90bb;logits&#x7684;JS&#x6563;&#x5ea6;&#x968f;&#x8ddd;&#x79bb;&#x589e;&#x5927;&#x800c;&#x589e;&#x52a0;&#xff0c;&#x8bc1;&#x5b9e;&#x4e86;&#x65f6;&#x95f4;&#x5c40;&#x90e8;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x5b58;&#x5728;&#x3002;","children":[],"payload":{"tag":"li","lines":"316,317"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: TPC&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x3001;&#x9ad8;&#x6548;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6316;&#x6398;logits&#x7684;&#x65f6;&#x95f4;&#x8fde;&#x7eed;&#x6027;&#x4e00;&#x81f4;&#x6027;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x548c;&#x5bf9;&#x6bd4;&#x7684;&#x7279;&#x6027;&#x4e3a;VLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x5f00;&#x653e;&#x57df;&#x6587;&#x672c;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e2d;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"317,319"}}],"payload":{"tag":"li","lines":"313,319","fold":1}}],"payload":{"tag":"h4","lines":"311,312"}},{"content":"MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;MRFD&#xff0c;&#x901a;&#x8fc7;&#x591a;&#x533a;&#x57df;&#x878d;&#x5408;&#x548c;&#x81ea;&#x4e00;&#x81f4;&#x6027;&#x68c0;&#x67e5;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5229;&#x7528;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x9009;&#x62e9;&#x5173;&#x952e;&#x56fe;&#x50cf;&#x533a;&#x57df;&#xff0c;&#x751f;&#x6210;&#x5404;&#x533a;&#x57df;&#x7684;&#x521d;&#x59cb;&#x63cf;&#x8ff0;&#xff0c;&#x5e76;&#x901a;&#x8fc7;Jensen-Shannon&#x6563;&#x5ea6;&#x8ba1;&#x7b97;&#x4e00;&#x81f4;&#x6027;&#x6743;&#x91cd;&#xff0c;&#x6700;&#x7ec8;&#x878d;&#x5408;&#x751f;&#x6210;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x8f93;&#x51fa;&#x3002;&#x5b9e;&#x9a8c;&#x8bc1;&#x660e;MRFD&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"320,321"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4f8b;&#x5982;&#x9519;&#x8bef;&#x8bc6;&#x522b;&#x7269;&#x4f53;&#x6216;&#x865a;&#x6784;&#x5c5e;&#x6027;&#x3002;&#x8fd9;&#x662f;&#x7531;&#x4e8e;&#x6a21;&#x578b;&#x96be;&#x4ee5;&#x9a8c;&#x8bc1;&#x56fe;&#x50cf;&#x4e0d;&#x540c;&#x533a;&#x57df;&#x7684;&#x4fe1;&#x606f;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x590d;&#x6742;&#x573a;&#x666f;&#x4e2d;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x4e14;&#x80fd;&#x52a8;&#x6001;&#x8bc4;&#x4f30;&#x591a;&#x533a;&#x57df;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"322,323"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: MRFD&#x91c7;&#x7528;&#x4e09;&#x9636;&#x6bb5;&#x6d41;&#x7a0b;&#xff1a;1) &#x6ce8;&#x610f;&#x529b;&#x5f15;&#x5bfc;&#x533a;&#x57df;&#x9009;&#x62e9;&#xff1a;&#x5229;&#x7528;LVLM&#x7684;&#x4ea4;&#x53c9;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x8bc6;&#x522b;&#x56fe;&#x50cf;&#x4e2d;&#x4e0e;&#x67e5;&#x8be2;&#x6700;&#x76f8;&#x5173;&#x7684;&#x591a;&#x4e2a;&#x663e;&#x8457;&#x533a;&#x57df;&#xff1b;2) &#x591a;&#x533a;&#x57df;&#x5206;&#x6790;&#x4e0e;JSD&#x52a0;&#x6743;&#xff1a;&#x5bf9;&#x6bcf;&#x4e2a;&#x533a;&#x57df;&#x751f;&#x6210;&#x72ec;&#x7acb;&#x63cf;&#x8ff0;&#xff0c;&#x8ba1;&#x7b97;&#x5404;&#x63cf;&#x8ff0;&#x4e0e;&#x5168;&#x5c40;&#x5e73;&#x5747;&#x5206;&#x5e03;&#x7684;Jensen-Shannon&#x6563;&#x5ea6;&#xff08;JSD&#xff09;&#xff0c;&#x4ee5;&#x6b64;&#x8861;&#x91cf;&#x4e00;&#x81f4;&#x6027;&#x5e76;&#x751f;&#x6210;&#x6743;&#x91cd;&#xff1b;3) &#x4e00;&#x81f4;&#x6027;&#x878d;&#x5408;&#x89e3;&#x7801;&#xff1a;&#x5c06;&#x5404;&#x533a;&#x57df;&#x7684;&#x8f93;&#x51fa;logits&#x6309;&#x6743;&#x91cd;&#x878d;&#x5408;&#xff0c;&#x7ed3;&#x5408;&#x533a;&#x57df;&#x611f;&#x77e5;&#x63d0;&#x793a;&#xff08;&#x878d;&#x5408;&#x539f;&#x59cb;&#x95ee;&#x9898;&#x4e0e;&#x533a;&#x57df;&#x63cf;&#x8ff0;&#xff09;&#x8fdb;&#x884c;&#x6700;&#x7ec8;&#x89e3;&#x7801;&#xff0c;&#x751f;&#x6210;&#x4e00;&#x81f4;&#x6027;&#x66f4;&#x9ad8;&#x7684;&#x54cd;&#x5e94;&#x3002;","children":[],"payload":{"tag":"li","lines":"323,324"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;LVLM&#x548c;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MSCOCO&#x548c;POPE&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#xff1a;1) JSD&#x4e0e;&#x5e7b;&#x89c9;&#x7387;&#x663e;&#x8457;&#x76f8;&#x5173;&#xff0c;&#x6b63;&#x786e;&#x7b54;&#x6848;&#x7684;JSD&#x503c;&#xff08;&#x7ea6;0.02&#xff09;&#x8fdc;&#x4f4e;&#x4e8e;&#x5e7b;&#x89c9;&#x7b54;&#x6848;&#xff08;0.06-0.07&#xff09;&#xff1b;2) MRFD&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5728;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x53c2;&#x6570;&#x4e0d;&#x53d8;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#x63d0;&#x5347;&#x660e;&#x663e;&#xff1b;3) &#x8be5;&#x65b9;&#x6cd5;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x601d;&#x7ef4;&#x94fe;&#x63d0;&#x793a;&#xff09;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x5bc6;&#x96c6;&#x7684;&#x6a21;&#x578b;&#x66f4;&#x65b0;&#x3002;","children":[],"payload":{"tag":"li","lines":"324,325"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MRFD&#x901a;&#x8fc7;&#x591a;&#x533a;&#x57df;&#x4e00;&#x81f4;&#x6027;&#x9a8c;&#x8bc1;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x53ef;&#x89e3;&#x91ca;&#x4e14;&#x8d44;&#x6e90;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x4ef7;&#x503c;&#x5728;&#x4e8e;&#x5229;&#x7528;&#x6a21;&#x578b;&#x5185;&#x5728;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x548c;&#x81ea;&#x4e00;&#x81f4;&#x6027;&#x539f;&#x7406;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x63d0;&#x5347;&#x53ef;&#x9760;&#x6027;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x5230;&#x66f4;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x7a33;&#x5065;&#x7684;&#x591a;&#x6a21;&#x6001;&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"325,327"}}],"payload":{"tag":"li","lines":"321,327","fold":1}}],"payload":{"tag":"h4","lines":"319,320"}}],"payload":{"tag":"h3","lines":"301,302","fold":1}},{"content":"&#x5f15;&#x5bfc;&#x5f0f;&#x89e3;&#x7801;","children":[{"content":"CGD: Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CLIP-Guided Decoding (CGD)&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;CLIP&#x6a21;&#x578b;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x5f15;&#x5bfc;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x66f4;&#x5339;&#x914d;&#x7684;&#x6587;&#x672c;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"330,331"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff08;&#x5373;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#xff08;&#x5982;&#x673a;&#x5668;&#x4eba;&#x548c;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x4fe1;&#x606f;&#xff08;&#x5982;token&#x4f3c;&#x7136;&#x5ea6;&#xff09;&#x6216;&#x590d;&#x6742;&#x7684;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x5b58;&#x5728;&#x6210;&#x672c;&#x9ad8;&#x3001;&#x53ef;&#x9760;&#x6027;&#x4f4e;&#x6216;&#x5de5;&#x7a0b;&#x590d;&#x6742;&#x7684;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"332,333"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x5b9e;&#x8bc1;&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;CLIP&#x6a21;&#x578b;&#x8ba1;&#x7b97;&#x7684;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x76f8;&#x4f3c;&#x5ea6;(CLIPScore)&#x6bd4;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x7684;token&#x4f3c;&#x7136;&#x5ea6;&#x66f4;&#x80fd;&#x53ef;&#x9760;&#x5730;&#x6307;&#x793a;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e86;CLIP-Guided Decoding (CGD)&#x65b9;&#x6cd5;&#xff1a;&#x5728;&#x6a21;&#x578b;&#x89e3;&#x7801;&#x751f;&#x6210;&#x6bcf;&#x4e2a;&#x53e5;&#x5b50;&#x65f6;&#xff0c;&#x5229;&#x7528;CLIPScore&#x5bf9;&#x591a;&#x4e2a;&#x5019;&#x9009;&#x53e5;&#x5b50;&#x8fdb;&#x884c;&#x8bc4;&#x5206;&#xff0c;&#x9009;&#x62e9;&#x4e0e;&#x56fe;&#x50cf;&#x76f8;&#x4f3c;&#x5ea6;&#x6700;&#x9ad8;&#x7684;&#x53e5;&#x5b50;&#x4f5c;&#x4e3a;&#x8f93;&#x51fa;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#x589e;&#x5f3a;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x89c6;&#x89c9;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"333,334"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x5728;&#x540e;&#x7eed;&#x751f;&#x6210;&#x7684;&#x53e5;&#x5b50;&#x4e2d;&#x66f4;&#x9891;&#x7e41;&#xff1b;2&#xff09;CLIPScore&#x5728;&#x4e0d;&#x540c;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x6bd4;token&#x4f3c;&#x7136;&#x5ea6;&#x66f4;&#x7a33;&#x5b9a;&#x53ef;&#x9760;&#xff1b;3&#xff09;CGD&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x591a;&#x79cd;LVLM&#xff08;&#x5982;LLaVA-1.5&#x3001;InstructBLIP&#x7b49;&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6587;&#x672c;&#x751f;&#x6210;&#x7684;&#x6574;&#x4f53;&#x8d28;&#x91cf;&#xff1b;4&#xff09;&#x5373;&#x4f7f;&#x91cd;&#x7528;LVLM&#x5185;&#x90e8;&#x7684;CLIP&#x6a21;&#x578b;&#x4e5f;&#x6709;&#x6548;&#x679c;&#xff0c;&#x8868;&#x660e;&#x73b0;&#x6709;LVLM&#x53ef;&#x80fd;&#x5b58;&#x5728;&#x5fae;&#x8c03;&#x8fc7;&#x62df;&#x5408;&#x3002;","children":[],"payload":{"tag":"li","lines":"334,335"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CGD&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x6709;&#x6548;&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x663e;&#x8457;&#x51cf;&#x5c11;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x9ad8;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x6539;&#x5584;LVLM&#x4e0e;&#x4eba;&#x7c7b;&#x671f;&#x671b;&#x7684;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5bf9;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;&#x4ee3;&#x7801;&#x5df2;&#x5f00;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"335,337"}}],"payload":{"tag":"li","lines":"331,337","fold":1}}],"payload":{"tag":"h4","lines":"329,330"}},{"content":"MARINE: Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;MARINE&#x6846;&#x67b6;&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x548c;API&#x8c03;&#x7528;&#x3001;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x5229;&#x7528;&#x5916;&#x90e8;&#x89c6;&#x89c9;&#x6a21;&#x578b;&#xff08;&#x5982;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x5668;&#xff09;&#x63d0;&#x4f9b;&#x7684;&#x7269;&#x4f53;&#x7ea7;&#x4fe1;&#x606f;&#x6765;&#x5f15;&#x5bfc;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x751f;&#x6210;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5176;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"338,339"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x65f6;&#x7ecf;&#x5e38;&#x4f1a;&#x4ea7;&#x751f;&#x2018;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x2019;&#xff0c;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x662f;&#x5728;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x5e94;&#x7528;&#x65f6;&#x98ce;&#x9669;&#x6781;&#x9ad8;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x9700;&#x8981;&#x6602;&#x8d35;&#x7684;&#x6570;&#x636e;&#x96c6;&#x5fae;&#x8c03;&#xff0c;&#x8981;&#x4e48;&#x4f9d;&#x8d56;&#x95ed;&#x6e90;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x540e;&#x5904;&#x7406;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x672a;&#x89e6;&#x53ca;&#x95ee;&#x9898;&#x6839;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"340,341"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;MARINE&#x6846;&#x67b6;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#x662f;&#xff1a;1&#xff09;&#x5229;&#x7528;&#x5f00;&#x6e90;&#x89c6;&#x89c9;&#x6a21;&#x578b;&#xff08;&#x5982;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x6a21;&#x578b;DETR&#x3001;&#x6807;&#x7b7e;&#x6a21;&#x578b;RAM++&#xff09;&#x7ec4;&#x6210;&#x4e00;&#x4e2a;&#x2018;&#x89c6;&#x89c9;&#x5de5;&#x5177;&#x7bb1;&#x2019;&#xff0c;&#x4ece;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e2d;&#x63d0;&#x53d6;&#x66f4;&#x7cbe;&#x786e;&#x3001;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x7269;&#x4f53;&#x4fe1;&#x606f;&#x3002;2&#xff09;&#x5c06;&#x8fd9;&#x4e9b;&#x7269;&#x4f53;&#x4fe1;&#x606f;&#x4f5c;&#x4e3a;&#x989d;&#x5916;&#x7684;&#x2018;&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x7684;&#x5f15;&#x5bfc;&#x2019;&#xff08;Image-Grounded Guidance&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x4fee;&#x6539;LVLM&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;logit&#x7a7a;&#x95f4;&#x6765;&#x6574;&#x5408;&#x8be5;&#x5f15;&#x5bfc;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x65f6;&#xff0c;&#x901a;&#x8fc7;&#x4e00;&#x4e2a;&#x5f15;&#x5bfc;&#x5f3a;&#x5ea6;&#x53c2;&#x6570;&#x3b3;&#xff0c;&#x5c06;&#x6807;&#x51c6;LVLM&#x7684;&#x751f;&#x6210;&#x6982;&#x7387;&#x4e0e;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x7684;&#x751f;&#x6210;&#x6982;&#x7387;&#x8fdb;&#x884c;&#x52a0;&#x6743;&#x878d;&#x5408;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x66f4;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x4e2d;&#x5b9e;&#x9645;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6216;&#x5fae;&#x8c03;LVLM&#xff0c;&#x4e5f;&#x65e0;&#x9700;&#x8c03;&#x7528;&#x5916;&#x90e8;API&#xff0c;&#x4ec5;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x8fdb;&#x884c;&#x3002;","children":[],"payload":{"tag":"li","lines":"341,342"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;MSCOCO&#x3001;LLaVA-QA90&#x3001;A-OKVQA&#x548c;GQA&#x7b49;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x4e0a;&#x5bf9;5&#x79cd;&#x6d41;&#x884c;LVLM&#x8fdb;&#x884c;&#x4e86;&#x7efc;&#x5408;&#x8bc4;&#x4f30;&#x3002;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;MARINE&#x5728;&#x591a;&#x79cd;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR&#x3001;POPE&#xff09;&#x548c;GPT-4V&#x8f85;&#x52a9;&#x8bc4;&#x4f30;&#x4e2d;&#xff0c; consistently&#x4e14;&#x663e;&#x8457;&#x5730;&#x51cf;&#x5c11;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x5176;&#x6548;&#x679c;&#x751a;&#x81f3;&#x4f18;&#x4e8e;&#x4e00;&#x4e9b;&#x57fa;&#x4e8e;&#x5fae;&#x8c03;&#x7684;&#x65b9;&#x6cd5;&#x3002;&#x540c;&#x65f6;&#xff0c;&#x5b83;&#x4fdd;&#x6301;&#x4e86;LVLM&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x8be6;&#x7ec6;&#x7a0b;&#x5ea6;&#xff0c;&#x5e76;&#x4e14;&#x5728;&#x5ef6;&#x8fdf;&#x548c;&#x51c6;&#x786e;&#x6027;&#x4e4b;&#x95f4;&#x53d6;&#x5f97;&#x4e86;&#x826f;&#x597d;&#x5e73;&#x8861;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x6781;&#x4f4e;&#x3002;","children":[],"payload":{"tag":"li","lines":"342,343"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MARINE&#x6210;&#x529f;&#x8bc1;&#x660e;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x5916;&#x90e8;&#x89c6;&#x89c9;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x7269;&#x4f53;&#x4fe1;&#x606f;&#x4f5c;&#x4e3a;&#x5f15;&#x5bfc;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x4ece;&#x6839;&#x6e90;&#x4e0a;&#x7f13;&#x89e3;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x800c;&#x65e0;&#x9700;&#x6602;&#x8d35;&#x7684;&#x8bad;&#x7ec3;&#x6216;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x5b9e;&#x7528;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x6709;&#x671b;&#x63d0;&#x9ad8;LVLM&#x5728;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;&#x5176;&#x65b9;&#x6cd5;&#x601d;&#x60f3;&#x7c7b;&#x4f3c;&#x4e8e;&#x6269;&#x6563;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x2018;&#x5206;&#x7c7b;&#x5668;&#x81ea;&#x7531;&#x5f15;&#x5bfc;&#x2019;&#xff0c;&#x4f46;&#x88ab;&#x521b;&#x65b0;&#x5730;&#x5e94;&#x7528;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x8bed;&#x5883;&#x4ee5;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"343,345"}}],"payload":{"tag":"li","lines":"339,345","fold":1}}],"payload":{"tag":"h4","lines":"337,338"}},{"content":"DeGF: Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;DeGF&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#xff08;&#x5982;Stable Diffusion&#xff09;&#x4e3a;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x63d0;&#x4f9b;&#x81ea;&#x53cd;&#x9988;&#xff0c;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x539f;&#x56fe;&#x6765;&#x68c0;&#x6d4b;&#x548c;&#x7ea0;&#x6b63;&#x5e7b;&#x89c9;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"346,347"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7684;&#x5bf9;&#x8c61;&#x3001;&#x8ba1;&#x6570;&#x6216;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x4ec5;&#x9488;&#x5bf9;&#x8bed;&#x8a00;&#x504f;&#x89c1;&#xff0c;&#x65e0;&#x6cd5;&#x5168;&#x9762;&#x89e3;&#x51b3;&#x591a;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x6a21;&#x578b;&#x7684;&#x53cd;&#x9988;&#x80fd;&#x529b;&#xff0c;&#x52a8;&#x6001;&#x7ea0;&#x6b63;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"348,349"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: DeGF&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b97;&#x6cd5;&#xff0c;&#x5206;&#x4e24;&#x6b65;&#xff1a;1) &#x4f7f;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#xff08;&#x5982;Stable Diffusion XL&#xff09;&#x6839;&#x636e;LVLM&#x7684;&#x521d;&#x59cb;&#x54cd;&#x5e94;&#x751f;&#x6210;&#x8f85;&#x52a9;&#x56fe;&#x50cf;&#xff1b;2) &#x5c06;&#x539f;&#x56fe;&#x548c;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x53cc;&#x91cd;&#x89c6;&#x89c9;&#x53c2;&#x8003;&#xff0c;&#x901a;&#x8fc7;&#x4e92;&#x8865;&#x6216;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;complementary/contrastive decoding&#xff09;&#x9a8c;&#x8bc1;&#x548c;&#x4fee;&#x6b63;&#x521d;&#x59cb;&#x54cd;&#x5e94;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x901a;&#x8fc7;&#x6bd4;&#x8f83;&#x57fa;&#x4e8e;&#x539f;&#x56fe;&#x548c;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x7684;token&#x9884;&#x6d4b;&#x5dee;&#x5f02;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4ee5;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"349,350"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;LLaVA-1.5&#x3001;InstructBLIP&#x548c;Qwen-VL&#x7b49;&#x6a21;&#x578b;&#x4e0a;&#xff0c;&#x4e8e;POPE&#x3001;CHAIR&#x3001;MME-Hallucination&#x7b49;&#x516d;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;DeGF&#x5747;&#x8d85;&#x8d8a;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x8c61;&#x5b58;&#x5728;&#x6027;&#x3001;&#x5916;&#x89c2;&#x3001;&#x8ba1;&#x6570;&#x7b49;&#x591a;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x548c;GPT-4V&#x8bc4;&#x4f30;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x63d0;&#x9ad8;&#x4e86;&#x54cd;&#x5e94;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"350,351"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: DeGF&#x9996;&#x6b21;&#x5229;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#x4e3a;LVLM&#x63d0;&#x4f9b;&#x81ea;&#x53cd;&#x9988;&#xff0c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x5176;&#x901a;&#x7528;&#x6027;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;LVLM&#xff0c;&#x4e14;&#x4ee3;&#x7801;&#x5f00;&#x6e90;&#xff0c;&#x4fc3;&#x8fdb;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"351,353"}}],"payload":{"tag":"li","lines":"347,353","fold":1}}],"payload":{"tag":"h4","lines":"345,346"}},{"content":"ReVisiT: Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ReVisiT&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x53c2;&#x8003;&#x89c6;&#x89c9;token&#x6765;&#x5f15;&#x5bfc;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x6587;&#x672c;&#x751f;&#x6210;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x5e76;&#x63d0;&#x5347;&#x89c6;&#x89c9; grounding &#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"354,355"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5982;&#x4f55;&#x8d21;&#x732e;&#x4e8e;&#x6587;&#x672c;&#x751f;&#x6210;&#x5c1a;&#x4e0d;&#x660e;&#x786e;&#xff0c;&#x5bfc;&#x81f4;&#x9891;&#x7e41;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#x95ee;&#x9898;&#x3002;&#x7406;&#x89e3;&#x89c6;&#x89c9;token&#x7684;&#x8bed;&#x4e49;&#x53ca;&#x5176;&#x5728;&#x89e3;&#x7801;&#x4e2d;&#x7684;&#x4f5c;&#x7528;&#xff0c;&#x5bf9;&#x4e8e;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x548c;&#x8f93;&#x51fa;&#x51c6;&#x786e;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"356,357"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;ReVisiT&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#xff1a;1) &#x5c06;&#x89c6;&#x89c9;token&#x6295;&#x5f71;&#x5230;&#x6587;&#x672c;token&#x5206;&#x5e03;&#x7a7a;&#x95f4;&#xff1b;2) &#x901a;&#x8fc7;&#x4e0a;&#x4e0b;&#x6587;&#x611f;&#x77e5;&#x7684;&#x7ea6;&#x675f;&#x6563;&#x5ea6;&#x6700;&#x5c0f;&#x5316;&#xff0c;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x6bcf;&#x4e00;&#x6b65;&#x6700;&#x76f8;&#x5173;&#x7684;&#x89c6;&#x89c9;token&#xff1b;3) &#x5229;&#x7528;&#x5176;&#x7ea6;&#x675f;&#x6295;&#x5f71;&#x4f18;&#x5316;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x878d;&#x5165;&#x89c6;&#x89c9;&#x8bed;&#x4e49;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3001;&#x67b6;&#x6784;&#x4fee;&#x6539;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x5757;&#x3002;","children":[],"payload":{"tag":"li","lines":"357,358"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x4e94;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5305;&#x62ec;HallusionBench&#x3001;CHAIR&#x3001;POPE&#x3001;VQAv2&#x548c;MMMU&#xff09;&#x4e0a;&#xff0c;ReVisiT&#x4e00;&#x81f4;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x89c6;&#x89c9; grounding&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;CHAIR&#x57fa;&#x51c6;&#x4e0a;F1&#x5206;&#x6570;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#xff08;&#x89c1;&#x56fe;1&#xff09;&#xff0c;&#x4e14;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x964d;&#x4f4e;&#x6700;&#x591a;2&#x500d;&#xff0c;&#x5728;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x89c4;&#x6a21;&#x548c;&#x67b6;&#x6784;&#x4e0a;&#x5747;&#x6709;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"358,359"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x89c6;&#x89c9;token&#x5185;&#x5728;&#x7f16;&#x7801;&#x53ef;&#x89e3;&#x91ca;&#x7684;&#x7269;&#x4f53;&#x7ea7;&#x8bed;&#x4e49;&#xff0c;ReVisiT&#x901a;&#x8fc7;&#x7b80;&#x5355;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x6fc0;&#x6d3b;&#x8fd9;&#x4e9b;&#x8bed;&#x4e49;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6027;&#x80fd;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x6a21;&#x578b;&#x65e0;&#x5173;&#x6027;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x6027;&#xff0c;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"359,361"}}],"payload":{"tag":"li","lines":"355,361","fold":1}}],"payload":{"tag":"h4","lines":"353,354"}},{"content":"MRGD: Controlling Multimodal LLMs via Reward-guided Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x591a;&#x6a21;&#x6001;&#x5956;&#x52b1;&#x5f15;&#x5bfc;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;MRGD&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x4e24;&#x4e2a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff08;&#x5206;&#x522b;&#x63a7;&#x5236;&#x76ee;&#x6807;&#x7cbe;&#x786e;&#x5ea6;&#x548c;&#x53ec;&#x56de;&#x7387;&#xff09;&#x6765;&#x52a8;&#x6001;&#x63a7;&#x5236;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#xff0c;&#x5b9e;&#x73b0;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x7cbe;&#x786e;&#x5ea6;&#x4e0e;&#x53ec;&#x56de;&#x7387;&#x7684;&#x6743;&#x8861;&#xff0c;&#x4ee5;&#x53ca;&#x8ba1;&#x7b97;&#x91cf;&#x4e0e;&#x89c6;&#x89c9; grounding &#x8d28;&#x91cf;&#x4e4b;&#x95f4;&#x7684;&#x5e73;&#x8861;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x6807;&#x51c6;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"362,363"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x968f;&#x7740;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5e7f;&#x6cdb;&#x5e94;&#x7528;&#xff0c;&#x7528;&#x6237;&#x9700;&#x8981;&#x6839;&#x636e;&#x4e0d;&#x540c;&#x9700;&#x6c42;&#x52a8;&#x6001;&#x63a7;&#x5236;&#x6a21;&#x578b;&#x884c;&#x4e3a;&#xff0c;&#x4f8b;&#x5982;&#x5728;&#x8f93;&#x51fa;&#x7cbe;&#x786e;&#x5ea6;&#xff08;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff09;&#x548c;&#x53ec;&#x56de;&#x7387;&#xff08;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#xff09;&#x4e4b;&#x95f4;&#x6743;&#x8861;&#xff0c;&#x4ee5;&#x53ca;&#x5728;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#xff08;&#x5982;&#x5ef6;&#x8fdf;&#xff09;&#x548c;&#x8f93;&#x51fa;&#x8d28;&#x91cf;&#x4e4b;&#x95f4;&#x5e73;&#x8861;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x63d0;&#x793a;&#x5de5;&#x7a0b;&#x3001;&#x5fae;&#x8c03;&#xff09;&#x65e0;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x63d0;&#x4f9b;&#x7ec6;&#x7c92;&#x5ea6;&#x63a7;&#x5236;&#xff0c;&#x800c;&#x9488;&#x5bf9;&#x7eaf;&#x6587;&#x672c;&#x6a21;&#x578b;&#x7684;&#x5956;&#x52b1;&#x5f15;&#x5bfc;&#x89e3;&#x7801;&#x6280;&#x672f;&#x5c1a;&#x672a;&#x9002;&#x914d;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"364,365"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x591a;&#x6a21;&#x6001;&#x5956;&#x52b1;&#x5f15;&#x5bfc;&#x89e3;&#x7801;&#xff08;MRGD&#xff09;&#x6846;&#x67b6;&#xff1a;1) &#x6784;&#x5efa;&#x4e24;&#x4e2a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff1a;&#x57fa;&#x4e8e;&#x504f;&#x597d;&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x7684;&#x5e7b;&#x89c9;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff08;r_hal&#xff09;&#x4f7f;&#x7528; PaliGemma  backbone&#xff0c;&#x9f13;&#x52b1;&#x8f93;&#x51fa;&#x7cbe;&#x786e;&#x6027;&#xff1b;&#x57fa;&#x4e8e;&#x9884;&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x5668;&#x548c;&#x8bcd;&#x5d4c;&#x5165;&#x6a21;&#x578b;&#x7684;&#x53ec;&#x56de;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff08;r_rec&#xff09;&#xff0c;&#x9f13;&#x52b1;&#x8f93;&#x51fa;&#x8be6;&#x7ec6;&#x6027;&#x3002;2) &#x5728;&#x89e3;&#x7801;&#x65f6;&#xff0c;&#x901a;&#x8fc7;&#x7ebf;&#x6027;&#x7ec4;&#x5408;&#x4e24;&#x4e2a;&#x5956;&#x52b1;&#xff08;&#x6743;&#x91cd; w &#x53ef;&#x8c03;&#xff09;&#x5b9a;&#x4e49;&#x7efc;&#x5408;&#x5f97;&#x5206;&#x51fd;&#x6570;&#x3002;3) &#x91c7;&#x7528;&#x641c;&#x7d22;&#x7b56;&#x7565;&#xff1a;&#x5728;&#x6bcf;&#x4e00;&#x6b65;&#x751f;&#x6210; k &#x4e2a;&#x5019;&#x9009;&#x8865;&#x5168;&#xff0c;&#x9009;&#x62e9;&#x7efc;&#x5408;&#x5f97;&#x5206;&#x6700;&#x9ad8;&#x7684;&#x5019;&#x9009;&#xff0c;&#x8fed;&#x4ee3;&#x76f4;&#x81f3;&#x751f;&#x6210;&#x7ed3;&#x675f;&#x3002;","children":[],"payload":{"tag":"li","lines":"365,366"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) MRGD &#x5141;&#x8bb8;&#x7528;&#x6237;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x6743;&#x91cd; w &#x52a8;&#x6001;&#x63a7;&#x5236;&#x7cbe;&#x786e;&#x5ea6;&#x4e0e;&#x53ec;&#x56de;&#x7387;&#x7684;&#x6743;&#x8861;&#xff08;w=1 &#x4fa7;&#x91cd;&#x7cbe;&#x786e;&#x6027;&#xff0c;w=0 &#x4fa7;&#x91cd;&#x8be6;&#x7ec6;&#x6027;&#xff09;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x641c;&#x7d22;&#x5bbd;&#x5ea6; k &#x63a7;&#x5236;&#x8ba1;&#x7b97;&#x91cf;&#x4e0e;&#x89c6;&#x89c9; grounding &#x8d28;&#x91cf;&#x7684;&#x5e73;&#x8861;&#x3002;2) &#x5728;&#x6807;&#x51c6;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;MRGD &#x59cb;&#x7ec8;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5fae;&#x8c03;&#x3001;&#x63d0;&#x793a;&#x5de5;&#x7a0b;&#xff09;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x4f9b;&#x5b9e;&#x65f6;&#x53ef;&#x63a7;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"366,367"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MRGD &#x662f;&#x9996;&#x4e2a;&#x9488;&#x5bf9; MLLM &#x7684;&#x5956;&#x52b1;&#x5f15;&#x5bfc;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x63a8;&#x7406;&#x65f6;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x63a7;&#x5236;&#xff0c;&#x6ee1;&#x8db3;&#x591a;&#x6837;&#x5316;&#x7528;&#x6237;&#x9700;&#x6c42;&#x3002;&#x5176;&#x4f18;&#x52bf;&#x5305;&#x62ec;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x3001;&#x53ef;&#x4e0e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5fae;&#x8c03;&#xff09;&#x7ed3;&#x5408;&#x4f7f;&#x7528;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x66f4;&#x591a;&#x63a7;&#x5236;&#x7ef4;&#x5ea6;&#xff08;&#x5982;&#x98ce;&#x683c;&#x3001;&#x5b89;&#x5168;&#x6027;&#xff09;&#xff0c;&#x5bf9;&#x63a8;&#x52a8; MLLM &#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"367,369"}}],"payload":{"tag":"li","lines":"363,369","fold":1}}],"payload":{"tag":"h4","lines":"361,362"}}],"payload":{"tag":"h3","lines":"327,328","fold":1}},{"content":"&#x81ea;&#x7701;&#x5f0f;/&#x4fee;&#x6b63;&#x89e3;&#x7801;","children":[{"content":"DLC: Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x52a8;&#x6001;&#x5bf9;&#x6570;&#x6821;&#x51c6;&#xff08;DLC&#xff09;&#x7684;&#x65b0;&#x578b;&#x514d;&#x8bad;&#x7ec3;&#x89e3;&#x7801;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x52a8;&#x6001;&#x5bf9;&#x9f50;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x6587;&#x672c;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#xff0c;&#x4ece;&#x800c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x9ad8;&#x63a8;&#x7406;&#x6548;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"372,373"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x76f8;&#x77db;&#x76fe;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x548c;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#xff0c;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x514d;&#x8bad;&#x7ec3;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x5b58;&#x5728;&#x9759;&#x6001;&#x7ea6;&#x675f;&#x65e0;&#x6cd5;&#x9002;&#x5e94;&#x8bed;&#x4e49;&#x6f02;&#x79fb;&#x3001;&#x9700;&#x8981;&#x591a;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x5bfc;&#x81f4;&#x6548;&#x7387;&#x4f4e;&#x4e0b;&#xff0c;&#x4ee5;&#x53ca;&#x8fc7;&#x4e8e;&#x50f5;&#x786c;&#x7684;&#x5e72;&#x9884;&#x89c4;&#x5219;&#x7834;&#x574f;&#x7ec6;&#x8282;&#x7b49;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"374,375"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x52a8;&#x6001;&#x5bf9;&#x6570;&#x6821;&#x51c6;&#xff08;DLC&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;DLC&#x9010;&#x6b65;&#x4f7f;&#x7528;CLIP&#x6a21;&#x578b;&#x8bc4;&#x4f30;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0e;&#x5df2;&#x751f;&#x6210;&#x6587;&#x672c;&#x5e8f;&#x5217;&#x7684;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x7a0b;&#x5ea6;&#x3002;&#x7136;&#x540e;&#xff0c;&#x6839;&#x636e;&#x52a8;&#x6001;&#x66f4;&#x65b0;&#x7684;&#x4e0a;&#x4e0b;&#x6587;&#x57fa;&#x7ebf;&#xff0c;&#x8bc4;&#x4f30;&#x5019;&#x9009;&#x4ee4;&#x724c;&#x7684;&#x76f8;&#x5bf9;&#x89c6;&#x89c9;&#x4f18;&#x52bf;&#xff08;RVA&#xff09;&#xff0c;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x8c03;&#x6574;&#x8f93;&#x51fa;&#x5bf9;&#x6570;&#xff0c;&#x4ee5;&#x504f;&#x5411;&#x4e8e;&#x89c6;&#x89c9;&#x63a5;&#x5730;&#x7684;&#x4ee4;&#x724c;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x901a;&#x8fc7;&#x5b9e;&#x65f6;&#x4e0a;&#x4e0b;&#x6587;&#x5bf9;&#x9f50;&#x5206;&#x6570; informed &#x7684;&#x81ea;&#x9002;&#x5e94;&#x52a0;&#x6743;&#x673a;&#x5236;&#xff0c;&#x5728;&#x786e;&#x4fdd;&#x6587;&#x672c;&#x8f93;&#x51fa;&#x6574;&#x4f53;&#x8d28;&#x91cf;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x4ed4;&#x7ec6;&#x5e73;&#x8861;&#x89c6;&#x89c9;&#x5f15;&#x5bfc;&#x3002;","children":[],"payload":{"tag":"li","lines":"375,376"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x548c;&#x4e0d;&#x540c;LVLM&#x67b6;&#x6784;&#xff08;&#x5982;LLaVA&#x3001;InstructBLIP&#x548c;MiniGPT-4&#xff09;&#x4e0a;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;DLC&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x6027;&#x80fd;&#x4f18;&#x4e8e;&#x5f53;&#x524d;&#x65b9;&#x6cd5;&#xff0c;&#x540c;&#x65f6;&#x901a;&#x8fc7;&#x907f;&#x514d;&#x591a;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x4fdd;&#x6301;&#x4e86;&#x9ad8;&#x63a8;&#x7406;&#x6548;&#x7387;&#x3002;&#x5982;&#x56fe;1&#x6240;&#x793a;&#xff0c;DLC&#x5728;&#x6b63;&#x786e;&#x6027;&#x3001;&#x8be6;&#x7ec6;&#x6027;&#x548c;&#x89e3;&#x7801;&#x901f;&#x5ea6;&#xff08;TPS&#xff09;&#x4e09;&#x4e2a;&#x7ef4;&#x5ea6;&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x8272;&#x3002;","children":[],"payload":{"tag":"li","lines":"376,377"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: DLC&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x7801;&#x65f6;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x53ef;&#x51cf;&#x8f7b;LVLM&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x5f3a;&#x5176;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4fc3;&#x8fdb;&#x66f4;&#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x52a8;&#x6001;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x6311;&#x6218;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x9014;&#x5f84;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x6a21;&#x578b;&#x65e0;&#x5173;&#xff0c;&#x53ef;&#x7075;&#x6d3b;&#x5e94;&#x7528;&#x4e8e;&#x5404;&#x79cd;LVLM&#x3002;","children":[],"payload":{"tag":"li","lines":"377,380"}}],"payload":{"tag":"li","lines":"373,380","fold":1}}],"payload":{"tag":"h4","lines":"371,372"}},{"content":"VOLCANO: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8bba;&#x6587;&#x63d0;&#x51fa;VOLCANO&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x6211;&#x53cd;&#x9988;&#x5f15;&#x5bfc;&#x7684;&#x4fee;&#x8ba2;&#x673a;&#x5236;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x6a21;&#x578b;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x6c34;&#x5e73;&#xff0c;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"381,382"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5b58;&#x5728;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x54cd;&#x5e94;&#x4e0e;&#x8f93;&#x5165;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e0d;&#x7b26;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x672a;&#x80fd;&#x6b63;&#x786e; grounding&#xff08;&#x951a;&#x5b9a;&#xff09;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x5185;&#x90e8;&#x53c2;&#x6570;&#x77e5;&#x8bc6;&#x800c;&#x975e;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x3002;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"383,384"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;VOLCANO&#x6a21;&#x578b;&#xff0c;&#x91c7;&#x7528;&#x5355;&#x4e00;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5b9e;&#x73b0;&#x8fed;&#x4ee3;&#x5f0f;&#x7684;&#x81ea;&#x6211;&#x4fee;&#x8ba2;&#x6d41;&#x7a0b;&#xff1a;1. &#x9996;&#x5148;&#x751f;&#x6210;&#x521d;&#x59cb;&#x54cd;&#x5e94;&#xff1b;2. &#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x548c;&#x95ee;&#x9898;&#x751f;&#x6210;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x53cd;&#x9988;&#xff0c;&#x6307;&#x51fa;&#x521d;&#x59cb;&#x54cd;&#x5e94;&#x7684;&#x9519;&#x8bef;&#x5e76;&#x63d0;&#x4f9b;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#xff1b;3. &#x6839;&#x636e;&#x53cd;&#x9988;&#x4fee;&#x8ba2;&#x54cd;&#x5e94;&#xff1b;4. &#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x51b3;&#x7b56;&#x9009;&#x62e9;&#x66f4;&#x4f18;&#x54cd;&#x5e94;&#x3002;&#x8be5;&#x8fc7;&#x7a0b;&#x6700;&#x591a;&#x8fed;&#x4ee3;&#x4e09;&#x6b21;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x5757;&#x3002;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x901a;&#x8fc7;&#x4e13;&#x6709;LLM&#x751f;&#x6210;&#x53cd;&#x9988;&#x548c;&#x4fee;&#x8ba2;&#x6837;&#x672c;&#x6784;&#x5efa;&#x3002;","children":[],"payload":{"tag":"li","lines":"384,385"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: VOLCANO&#x5728;MMHal-Bench&#x3001;POPE&#x548c;GAVIE&#x7b49;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x4e0a;&#x5b9e;&#x73b0;SOTA&#x6027;&#x80fd;&#xff0c;&#x6bd4;&#x5148;&#x524d;&#x65b9;&#x6cd5;&#x63d0;&#x5347;24.9%&#x3002;&#x540c;&#x65f6;&#x5728;MM-Vet&#x548c;MMBench&#x7b49;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x57fa;&#x51c6;&#x4e0a;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x5176;&#x53cd;&#x9988;&#x66f4;&#x805a;&#x7126;&#x56fe;&#x50cf;&#x7ec6;&#x8282;&#xff0c;&#x8986;&#x76d6;&#x66f4;&#x591a;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#xff0c;&#x6709;&#x6548;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x81ea;&#x6211;&#x4fee;&#x6b63;&#x3002;","children":[],"payload":{"tag":"li","lines":"385,386"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x81ea;&#x6211;&#x53cd;&#x9988;&#x673a;&#x5236;&#x80fd;&#x4e3a;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x5373;&#x4f7f;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x5931;&#x6548;&#x65f6;&#x4ecd;&#x80fd;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;VOLCANO&#x8bc1;&#x660e;&#x4e86;&#x5355;&#x4e00;&#x6a21;&#x578b;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x53cd;&#x9988;&#x4fee;&#x8ba2;&#x53ef;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x6a21;&#x578b;&#x3001;&#x6570;&#x636e;&#x548c;&#x4ee3;&#x7801;&#x5df2;&#x5f00;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"386,388"}}],"payload":{"tag":"li","lines":"382,388","fold":1}}],"payload":{"tag":"h4","lines":"380,381"}},{"content":"SID: Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x81ea;&#x7701;&#x89e3;&#x7801;&#xff08;SID&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x4e0a;&#x4e0b;&#x6587;&#x548c;&#x6587;&#x672c;&#x611f;&#x77e5;&#x7684;&#x4ee4;&#x724c;&#x9009;&#x62e9;&#xff08;CT2S&#xff09;&#x7b56;&#x7565;&#xff0c;&#x5728;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x653e;&#x5927;&#x5e76;&#x6d88;&#x9664;&#x5e7b;&#x89c9;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x4e14;&#x4e0d;&#x589e;&#x52a0;&#x989d;&#x5916;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"389,390"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CD&#xff09;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6270;&#x52a8;&#x8f93;&#x5165;&#x6765;&#x8bf1;&#x5bfc;&#x5e7b;&#x89c9;&#x5e76;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x6d88;&#x9664;&#xff0c;&#x4f46;&#x8fd9;&#x79cd;&#x65b9;&#x6cd5;&#x8bbe;&#x8ba1;&#x590d;&#x6742;&#x3001;&#x53ef;&#x80fd;&#x5f15;&#x5165;&#x566a;&#x58f0;&#xff0c;&#x4e14;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x7ffb;&#x500d;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x9ad8;&#x6548;&#x3001;&#x7cbe;&#x51c6;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"391,392"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x81ea;&#x7701;&#x89e3;&#x7801;&#xff08;SID&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x662f;&#x4e0a;&#x4e0b;&#x6587;&#x548c;&#x6587;&#x672c;&#x611f;&#x77e5;&#x4ee4;&#x724c;&#x9009;&#x62e9;&#xff08;CT2S&#xff09;&#x7b56;&#x7565;&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;&#x9884;&#x8bad;&#x7ec3;LVLM&#x80fd;&#x591f;&#x57fa;&#x4e8e;&#x5df2;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x548c;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x81ea;&#x7701;&#x5730;&#x8bc4;&#x4f30;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;CT2S&#x5728;&#x89e3;&#x7801;&#x5668;&#x65e9;&#x671f;&#x5c42;&#x4ec5;&#x4fdd;&#x7559;&#x6700;&#x4e0d;&#x91cd;&#x8981;&#x7684;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff0c;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x653e;&#x5927;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x5173;&#x8054;&#x5e7b;&#x89c9;&#xff0c;&#x968f;&#x540e;&#x901a;&#x8fc7;&#x539f;&#x59cb;&#x4ee4;&#x724c;&#x903b;&#x8f91;&#x51cf;&#x53bb;&#x653e;&#x5927;&#x7684;&#x5e7b;&#x89c9;&#x4fe1;&#x53f7;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x901a;&#x7528;&#x80fd;&#x529b;&#x7684;&#x540c;&#x65f6;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8f93;&#x5165;&#x6270;&#x52a8;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x8ba1;&#x7b97;&#x8d1f;&#x62c5;&#x3002;","children":[],"payload":{"tag":"li","lines":"392,393"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;SID&#x5728;&#x591a;&#x79cd;&#x8bc4;&#x4f30;&#x8bbe;&#x7f6e;&#xff08;&#x5982;&#x968f;&#x673a;&#x548c;&#x5bf9;&#x6297;&#x6027;&#x8bbe;&#x7f6e;&#xff09;&#x4e0b;&#x5747;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;VCD&#x548c;ICD&#xff09;&#xff0c;&#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x53d6;&#x5f97;&#x4e86;&#x66f4;&#x9ad8;&#x7684;&#x51c6;&#x786e;&#x7387;&#x548c;F1&#x5206;&#x6570;&#xff08;&#x4f8b;&#x5982;&#x5728;&#x5bf9;&#x6297;&#x6027;&#x8bbe;&#x7f6e;&#x4e0b;&#xff0c;SID&#x7684;&#x51c6;&#x786e;&#x7387;&#x8fbe;83.3%&#xff0c;&#x800c;VCD&#x4e3a;80.9%&#xff09;&#x3002;&#x540c;&#x65f6;&#xff0c;SID&#x5927;&#x5e45;&#x51cf;&#x5c11;&#x4e86;&#x63a8;&#x7406;&#x6210;&#x672c;&#xff0c;&#x65e0;&#x9700;&#x53cc;&#x500d;&#x8ba1;&#x7b97;&#x3002;","children":[],"payload":{"tag":"li","lines":"393,394"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SID&#x901a;&#x8fc7;&#x4ee4;&#x724c;&#x7ea7;&#x6270;&#x52a8;&#x800c;&#x975e;&#x6574;&#x4f53;&#x8f93;&#x5165;&#x6270;&#x52a8;&#xff0c;&#x66f4;&#x7cbe;&#x51c6;&#x5730;&#x8bf1;&#x5bfc;&#x548c;&#x6d88;&#x9664;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x4e86;LVLM&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x6709;&#x671b;&#x63a8;&#x52a8;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#xff0c;&#x5982;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x3002;","children":[],"payload":{"tag":"li","lines":"394,396"}}],"payload":{"tag":"li","lines":"390,396","fold":1}}],"payload":{"tag":"h4","lines":"388,389"}},{"content":"PENSIEVE: Retrospect-then-Compare Mitigates Visual Hallucination","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: Pensieve&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x68c0;&#x7d22;&#x76f8;&#x4f3c;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x53c2;&#x8003;&#xff0c;&#x6bd4;&#x8f83;&#x5176;&#x4e0e;&#x6d4b;&#x8bd5;&#x56fe;&#x50cf;&#x7684;&#x7f6e;&#x4fe1;&#x5ea6;&#x5206;&#x6570;&#x5dee;&#x5f02;&#xff0c;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"397,398"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x5353;&#x8d8a;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x5185;&#x5bb9;&#x4e0e;&#x56fe;&#x50cf;&#x5b9e;&#x9645;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x3001;&#x5173;&#x952e;&#x4fe1;&#x606f;&#x9057;&#x6f0f;&#x6216;&#x4e8b;&#x5b9e;&#x6027;&#x9519;&#x8bef;&#x3002;","children":[],"payload":{"tag":"li","lines":"399,400"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: Pensieve&#x91c7;&#x7528;&#x201c;&#x56de;&#x987e;-&#x6bd4;&#x8f83;&#x201d;&#x7b56;&#x7565;&#xff1a;1&#xff09;&#x68c0;&#x7d22;&#x4e0e;&#x6d4b;&#x8bd5;&#x56fe;&#x50cf;&#x8bed;&#x4e49;&#x548c;&#x5916;&#x89c2;&#x76f8;&#x4f3c;&#x7684;&#x53c2;&#x8003;&#x56fe;&#x50cf;&#xff1b;2&#xff09;&#x5206;&#x522b;&#x83b7;&#x53d6;&#x6d4b;&#x8bd5;&#x56fe;&#x50cf;&#x548c;&#x53c2;&#x8003;&#x56fe;&#x50cf;&#x4e0b;&#x6a21;&#x578b;&#x5bf9;&#x5019;&#x9009;&#x8bcd;&#x5143;&#x7684;&#x7f6e;&#x4fe1;&#x5ea6;&#x5206;&#x6570;&#xff1b;3&#xff09;&#x901a;&#x8fc7;&#x5206;&#x6570;&#x76f8;&#x51cf;&#x7a81;&#x51fa;&#x5206;&#x6570;&#x53d8;&#x5316;&#x663e;&#x8457;&#x7684;&#x5019;&#x9009;&#xff08;&#x66f4;&#x53ef;&#x80fd;&#x662f;&#x51c6;&#x786e;&#x5185;&#x5bb9;&#xff09;&#xff1b;4&#xff09;&#x81ea;&#x9002;&#x5e94;&#x7f29;&#x653e;&#x5206;&#x6570;&#x4ee5;&#x5e73;&#x8861;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x5206;&#x652f;&#x7684;&#x9519;&#x8bef;&#x7ea0;&#x6b63;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"400,401"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;Whoops&#x3001;LLaVA Bench&#x3001;POPE&#x548c;MME&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;Pensieve&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff1a;LLaVA1.5&#x5728;Whoops&#x4e0a;&#x7684;FaithScore&#x63d0;&#x5347;0.4&#xff0c;InstructBLIP&#x5728;MME&#x4e0a;&#x7684;&#x603b;&#x5206;&#x63d0;&#x5347;55&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x8fd8;&#x80fd;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7684;&#x7ec6;&#x8282;&#x548c;&#x7279;&#x5f02;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"401,402"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Pensieve&#x901a;&#x8fc7;&#x5229;&#x7528;&#x76f8;&#x4f3c;&#x56fe;&#x50cf;&#x8bf1;&#x5bfc;&#x7684;&#x7c7b;&#x6bd4;&#x5e7b;&#x89c9;&#xff0c;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x5e76;&#x51cf;&#x5c11;MLLMs&#x7684;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x5206;&#x652f;&#x9519;&#x8bef;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x89c6;&#x89d2;&#xff0c;&#x6709;&#x671b;&#x63d0;&#x5347;MLLMs&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"402,404"}}],"payload":{"tag":"li","lines":"398,404","fold":1}}],"payload":{"tag":"h4","lines":"396,397"}}],"payload":{"tag":"h3","lines":"369,370","fold":1}}],"payload":{"tag":"h2","lines":"56,57"}},{"content":"&#x5bf9;&#x9f50;/&#x504f;&#x597d;&#x4f18;&#x5316;","children":[{"content":"&#x57fa;&#x4e8e;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x7684;&#x5bf9;&#x9f50;(RLHF&#x3001;RLAIF)","children":[{"content":"BDHS: UNDERSTANDING ALIGNMENT IN MULTIMODAL LLMS: A COMPREHENSIVE STUDY","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x504f;&#x597d;&#x5bf9;&#x9f50;&#xff08;Preference Alignment&#xff09;&#x8fdb;&#x884c;&#x4e86;&#x7cfb;&#x7edf;&#x6027;&#x7814;&#x7a76;&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;&#x7ed3;&#x5408;&#x79bb;&#x7ebf;&#x548c;&#x5728;&#x7ebf;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x80fd;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x6784;&#x5efa;&#x65b0;&#x65b9;&#x6cd5;BDHS&#xff0c;&#x5176;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x6709;&#x7ade;&#x4e89;&#x529b;&#x7684;&#x7ed3;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"409,410"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#xff08;hallucination&#xff09;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x6216;&#x4e0d;&#x51c6;&#x786e;&#x7684;&#x56de;&#x7b54;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;MLLMs&#x5728;&#x73b0;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x5c3d;&#x7ba1;&#x504f;&#x597d;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x5728;&#x7eaf;&#x6587;&#x672c;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLMs&#xff09;&#x4e2d;&#x5df2;&#x88ab;&#x8bc1;&#x660e;&#x6709;&#x6548;&#xff0c;&#x4f46;&#x5176;&#x5728;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#x4e0b;&#x7684;&#x6548;&#x679c;&#x548c;&#x6700;&#x4f73;&#x5b9e;&#x8df5;&#x5c1a;&#x4e0d;&#x660e;&#x786e;&#xff0c;&#x4e14;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x56e0;&#x6570;&#x636e;&#x96c6;&#x3001;&#x57fa;&#x6a21;&#x578b;&#x548c;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x7684;&#x5dee;&#x5f02;&#x800c;&#x96be;&#x4ee5;&#x8fdb;&#x884c;&#x76f4;&#x63a5;&#x6bd4;&#x8f83;&#x548c;&#x5f52;&#x56e0;&#x3002;","children":[],"payload":{"tag":"li","lines":"411,412"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x72ec;&#x7acb;&#x5206;&#x6790;&#x5bf9;&#x9f50;&#x8fc7;&#x7a0b;&#x7684;&#x5404;&#x4e2a;&#x7ec4;&#x4ef6;&#x6765;&#x89e3;&#x51b3;&#x95ee;&#x9898;&#x3002;&#x4e3b;&#x8981;&#x65b9;&#x6cd5;&#x5305;&#x62ec;&#xff1a;1. &#x5c06;&#x5bf9;&#x9f50;&#x7b97;&#x6cd5;&#x5206;&#x4e3a;&#x4e24;&#x7c7b;&#xff1a;&#x79bb;&#x7ebf;&#x65b9;&#x6cd5;&#xff08;&#x5982;DPO&#xff0c;&#x4f7f;&#x7528;&#x9884;&#x5148;&#x6536;&#x96c6;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#xff09;&#x548c;&#x5728;&#x7ebf;&#x65b9;&#x6cd5;&#xff08;&#x5982;Online-DPO&#xff0c;&#x5728;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x4ece;&#x6a21;&#x578b;&#x4e2d;&#x91c7;&#x6837;&#x5e76;&#x83b7;&#x53d6;&#x504f;&#x597d;&#x5bf9;&#xff09;&#x3002;2. &#x7814;&#x7a76;&#x4e86;&#x7ed3;&#x5408;&#x79bb;&#x7ebf;&#x548c;&#x5728;&#x7ebf;&#x65b9;&#x6cd5;&#x7684;&#x6df7;&#x5408;&#x7b56;&#x7565;&#xff08;Mixed-DPO&#xff09;&#x3002;3. &#x7cfb;&#x7edf;&#x5206;&#x6790;&#x4e86;&#x73b0;&#x6709;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x7684;&#x6784;&#x5efa;&#x7ec6;&#x8282;&#xff08;&#x63d0;&#x793a;&#x3001;&#x4f18;&#x9009;&#x56de;&#x7b54;&#x3001;&#x52a3;&#x9009;&#x56de;&#x7b54;&#x7684;&#x6765;&#x6e90;&#x548c;&#x8d28;&#x91cf;&#xff09;&#x53ca;&#x5176;&#x5bf9;&#x6027;&#x80fd;&#x7684;&#x5f71;&#x54cd;&#x3002;4. &#x57fa;&#x4e8e;&#x5206;&#x6790;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x201c;&#x504f;&#x5dee;&#x9a71;&#x52a8;&#x5e7b;&#x89c9;&#x91c7;&#x6837;&#x201d;&#xff08;Bias-Driven Hallucination Sampling, BDHS&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5229;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x7684;&#x504f;&#x5dee;&#x6765;&#x751f;&#x6210;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x6559;&#x5e08;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4V&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"412,413"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1. &#x79bb;&#x7ebf;&#x548c;&#x5728;&#x7ebf;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x5404;&#x6709;&#x4f18;&#x52a3;&#xff0c;&#x800c;&#x5c06;&#x5b83;&#x4eec;&#x7ed3;&#x5408;&#x7684;&#x6df7;&#x5408;&#x65b9;&#x6cd5;&#xff08;Mixed-DPO&#xff09;&#x5728;&#x7279;&#x5b9a;&#x573a;&#x666f;&#x4e0b;&#x80fd;&#x53d6;&#x5f97;&#x66f4;&#x597d;&#x7684;&#x6027;&#x80fd;&#x3002;2. &#x504f;&#x597d;&#x6570;&#x636e;&#x7684;&#x8d28;&#x91cf;&#x3001;&#x591a;&#x6837;&#x6027;&#x548c;&#x89c4;&#x6a21;&#x663e;&#x8457;&#x5f71;&#x54cd;&#x6700;&#x7ec8;&#x7684;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#x3002;3. &#x6240;&#x63d0;&#x51fa;&#x7684;BDHS&#x65b9;&#x6cd5;&#xff0c;&#x5c3d;&#x7ba1;&#x4e0d;&#x4f9d;&#x8d56;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3001;&#x63d0;&#x9ad8;&#x56de;&#x7b54;&#x51c6;&#x786e;&#x6027;&#xff09;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x4e0e;&#x4f7f;&#x7528;&#x66f4;&#x5927;&#x3001;&#x66f4;&#x590d;&#x6742;&#x65b9;&#x6cd5;&#x6784;&#x5efa;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x76f8;&#x5f53;&#xff0c;&#x751a;&#x81f3;&#x66f4;&#x5177;&#x7ade;&#x4e89;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"413,414"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x5bf9;&#x9f50;&#x7684;&#x6210;&#x529f;&#x53d6;&#x51b3;&#x4e8e;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x548c;&#x6570;&#x636e;&#x6784;&#x5efa;&#x7b56;&#x7565;&#x7684;&#x7cbe;&#x5fc3;&#x9009;&#x62e9;&#x3002;&#x7ed3;&#x5408;&#x79bb;&#x7ebf;&#x548c;&#x5728;&#x7ebf;&#x65b9;&#x6cd5;&#x7684;&#x6df7;&#x5408;&#x7b56;&#x7565;&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x7684;&#x9014;&#x5f84;&#x3002;&#x66f4;&#x91cd;&#x8981;&#x7684;&#x662f;&#xff0c;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x7684;BDHS&#x65b9;&#x6cd5;&#x8bc1;&#x660e;&#x4e86;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x4e14;&#x6210;&#x672c;&#x6548;&#x76ca;&#x9ad8;&#x7684;&#x6570;&#x636e;&#x6784;&#x5efa;&#x65b9;&#x5f0f;&#x4e5f;&#x80fd;&#x8fbe;&#x5230;&#x4f18;&#x5f02;&#x6027;&#x80fd;&#xff0c;&#x8fd9;&#x4e3a;&#x672a;&#x6765;MLLMs&#x7684;&#x5bf9;&#x9f50;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x65b9;&#x5411;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x793e;&#x533a;&#x63d0;&#x4f9b;&#x4e86;&#x66f4;&#x6e05;&#x6670;&#x7684;&#x5bf9;&#x9f50;&#x6280;&#x672f;&#x5206;&#x6790;&#x6846;&#x67b6;&#x548c;&#x4e00;&#x79cd;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x3001;&#x65e0;&#x9700;&#x6602;&#x8d35;&#x6807;&#x6ce8;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x751f;&#x6210;&#x65b9;&#x6848;&#xff0c;&#x6709;&#x52a9;&#x4e8e;&#x63a8;&#x52a8;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x63a5;&#x5730;&#x6c14;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"414,416"}}],"payload":{"tag":"li","lines":"410,416","fold":1}}],"payload":{"tag":"h4","lines":"408,409"}},{"content":"LLaVA-RLHF: Aligning Large Multimodal Models with Factually Augmented RLHF","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Factually Augmented RLHF&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x7ed3;&#x5408;&#x4eba;&#x7c7b;&#x53cd;&#x9988;&#x7684;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x4e0e;&#x989d;&#x5916;&#x4e8b;&#x5b9e;&#x4fe1;&#x606f;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x663e;&#x8457;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"417,418"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMM&#xff09;&#x5728;&#x5904;&#x7406;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x4fe1;&#x606f;&#x65f6;&#x5b58;&#x5728;&#x6a21;&#x6001;&#x9519;&#x4f4d;&#x95ee;&#x9898;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x6587;&#x672c;&#x8f93;&#x51fa;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x5bf9;&#x9f50;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"419,420"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x5c06;&#x57fa;&#x4e8e;&#x4eba;&#x7c7b;&#x53cd;&#x9988;&#x7684;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#xff08;RLHF&#xff09;&#x4ece;&#x6587;&#x672c;&#x9886;&#x57df;&#x9002;&#x914d;&#x5230;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x4efb;&#x52a1;&#x4e2d;&#x3002;&#x5177;&#x4f53;&#x65b9;&#x6cd5;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x6536;&#x96c6;&#x4eba;&#x7c7b;&#x6807;&#x6ce8;&#x8005;&#x5bf9;&#x6bd4;&#x54cd;&#x5e94;&#x5e76;&#x6807;&#x6ce8;&#x8f83;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x7b54;&#x6848;&#xff1b;2&#xff09;&#x63d0;&#x51fa;Factually Augmented RLHF&#x7b97;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x771f;&#x5b9e;&#x591a;&#x9009;&#x9879;&#x7b49;&#x4e8b;&#x5b9e;&#x4fe1;&#x606f;&#x589e;&#x5f3a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff0c;&#x907f;&#x514d;&#x5956;&#x52b1;&#x9ed1;&#x5ba2;&#x95ee;&#x9898;&#xff1b;3&#xff09;&#x5728;&#x76d1;&#x7763;&#x5fae;&#x8c03;&#x9636;&#x6bb5;&#x878d;&#x5408;&#x9ad8;&#x8d28;&#x91cf;&#x4eba;&#x7c7b;&#x6807;&#x6ce8;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6570;&#x636e;&#xff08;&#x5982;VQA-v2&#x3001;A-OKVQA&#x3001;Flickr30k&#xff09;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"420,421"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;LLaVA-RLHF&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff1a;&#x5728;LLaVA-Bench&#x4e0a;&#x8fbe;&#x5230;&#x4ec5;&#x6587;&#x672c;GPT-4&#x6027;&#x80fd;&#x7684;94%&#xff08;&#x6b64;&#x524d;&#x6700;&#x4f73;&#x65b9;&#x6cd5;&#x4ec5;&#x4e3a;87%&#xff09;&#xff0c;&#x5728;&#x4e13;&#x95e8;&#x9488;&#x5bf9;&#x5e7b;&#x89c9;&#x60e9;&#x7f5a;&#x7684;&#x65b0;&#x57fa;&#x51c6;MMHAL-BENCH&#x4e0a;&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x63d0;&#x5347;60%&#xff0c;&#x540c;&#x65f6;&#x5728;MMBench&#x548c;POPE&#x57fa;&#x51c6;&#x4e0a;&#x4e5f;&#x521b;&#x9020;&#x4e86;&#x65b0;&#x6027;&#x80fd;&#x8bb0;&#x5f55;&#xff08;52.4%&#x548c;82.7% F1&#x5206;&#x6570;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"421,422"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x7814;&#x7a76;&#x6210;&#x529f;&#x5c06;RLHF&#x5e94;&#x7528;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#xff0c;&#x901a;&#x8fc7;&#x4e8b;&#x5b9e;&#x589e;&#x5f3a;&#x7684;&#x5956;&#x52b1;&#x673a;&#x5236;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;LMM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ee5;&#x8f83;&#x4f4e;&#x6807;&#x6ce8;&#x6210;&#x672c;&#xff08;&#x5982;10K&#x4eba;&#x7c7b;&#x504f;&#x597d;&#x6807;&#x6ce8;&#x4ec5;&#x9700;3000&#x7f8e;&#x5143;&#xff09;&#x5b9e;&#x73b0;&#x4e86;&#x663e;&#x8457;&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x63d0;&#x9ad8;LMM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"422,424"}}],"payload":{"tag":"li","lines":"418,424","fold":1}}],"payload":{"tag":"h4","lines":"416,417"}},{"content":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: RLHF-V&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x4eba;&#x5de5;&#x4fee;&#x6b63;&#x53cd;&#x9988;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;1.4k&#x6807;&#x6ce8;&#x6837;&#x672c;&#x5373;&#x53ef;&#x5c06;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;34.8%&#xff0c;&#x6548;&#x679c;&#x4f18;&#x4e8e;&#x4f7f;&#x7528;10k&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x7684;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"425,426"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x73b0;&#x6709;MLLM&#xff08;&#x5982;GPT-4V&#xff09;&#x5728;45.9%&#x7684;&#x54cd;&#x5e94;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#xff08;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff09;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x4e0d;&#x53ef;&#x4fe1;&#xff0c;&#x65e0;&#x6cd5;&#x5e94;&#x7528;&#x4e8e;&#x9ad8;&#x98ce;&#x9669;&#x573a;&#x666f;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x8f85;&#x52a9;&#x6216;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x3002;&#x6838;&#x5fc3;&#x95ee;&#x9898;&#x662f;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x7f3a;&#x4e4f;&#x6b63;/&#x8d1f;&#x4eba;&#x7c7b;&#x53cd;&#x9988;&#xff0c;&#x96be;&#x4ee5;&#x5b66;&#x4e60;&#x7cbe;&#x786e;&#x7684;&#x884c;&#x4e3a;&#x8fb9;&#x754c;&#x4ee5;&#x6392;&#x9664;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"427,428"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6570;&#x636e;&#x5c42;&#x9762;&#xff1a;&#x6536;&#x96c6;&#x7ec6;&#x7c92;&#x5ea6;&#x6bb5;&#x843d;&#x7ea7;&#x4fee;&#x6b63;&#x53cd;&#x9988;&#xff08;Segment-level Correction&#xff09;&#xff0c;&#x4eba;&#x5de5;&#x76f4;&#x63a5;&#x4fee;&#x6b63;&#x6a21;&#x578b;&#x54cd;&#x5e94;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x6bb5;&#x843d;&#xff0c;&#x63d0;&#x4f9b;&#x660e;&#x786e;&#x4e14;&#x5bc6;&#x96c6;&#x7684;&#x504f;&#x597d;&#x4fe1;&#x53f7;&#xff1b;2. &#x65b9;&#x6cd5;&#x5c42;&#x9762;&#xff1a;&#x63d0;&#x51fa;&#x5bc6;&#x96c6;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DDPO&#xff09;&#xff0c;&#x4e00;&#x79cd;DPO&#x7684;&#x53d8;&#x4f53;&#xff0c;&#x76f4;&#x63a5;&#x9488;&#x5bf9;&#x6bb5;&#x843d;&#x7ea7;&#x504f;&#x597d;&#x4f18;&#x5316;&#x7b56;&#x7565;&#x6a21;&#x578b;&#xff0c;&#x5bf9;&#x5e7b;&#x89c9;&#x6bb5;&#x843d;&#x65bd;&#x52a0;&#x66f4;&#x5f3a;&#x7684;&#x4e8b;&#x5b9e;&#x6027;&#x7ea6;&#x675f;&#x3002;","children":[],"payload":{"tag":"li","lines":"428,429"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x5728;5&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;MLLM&#x53ef;&#x4fe1;&#x5ea6;&#xff1b;2. &#x4ec5;&#x7528;1.4k&#x6807;&#x6ce8;&#x6570;&#x636e;&#x5c06;&#x57fa;&#x7840;MLLM&#x7684;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;34.8%&#xff0c;&#x8d85;&#x8d8a;&#x4f7f;&#x7528;10k&#x6570;&#x636e;&#x7684;LLaVA-RLHF&#xff1b;3. &#x5728;&#x5f00;&#x6e90;MLLM&#x4e2d;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x6027;&#x80fd;&#xff0c;&#x5728;&#x9632;&#x6b62;&#x8fc7;&#x5ea6;&#x6cdb;&#x5316;&#x5f15;&#x53d1;&#x7684;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x6bd4;GPT-4V&#x66f4;&#x5177;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"429,430"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: RLHF-V&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x4eba;&#x7c7b;&#x53cd;&#x9988;&#x548c;DDPO&#x7b97;&#x6cd5;&#x9ad8;&#x6548;&#x89e3;&#x51b3;&#x4e86;MLLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x53ef;&#x4fe1;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#x3002;&#x5176;&#x9ad8;&#x6570;&#x636e;&#x6548;&#x7387;&#xff08;&#x5c11;&#x91cf;&#x6807;&#x6ce8;&#x5b9e;&#x73b0;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff09;&#x548c;&#x5f00;&#x6e90;&#x7279;&#x6027;&#xff08;&#x4ee3;&#x7801;&#x3001;&#x6570;&#x636e;&#x3001;&#x6a21;&#x578b;&#x6743;&#x91cd;&#x5747;&#x516c;&#x5f00;&#xff09;&#x5bf9;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"430,432"}}],"payload":{"tag":"li","lines":"426,432","fold":1}}],"payload":{"tag":"h4","lines":"424,425"}},{"content":"RLAIF-V: Aligning MLLMs through Open-Source AI Feedback for Super GPT-4V Trustworthines","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: RLAIF-V&#x662f;&#x4e00;&#x4e2a;&#x5b8c;&#x5168;&#x5f00;&#x6e90;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;AI&#x53cd;&#x9988;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5b83;&#x4f7f;&#x7528;&#x53bb;&#x6df7;&#x6dc6;&#x7b56;&#x7565;&#x751f;&#x6210;&#x9ad8;&#x8d28;&#x91cf;&#x53cd;&#x9988;&#x6570;&#x636e;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x5206;&#x6cbb;&#x6cd5;&#x63d0;&#x5347;&#x53cd;&#x9988;&#x51c6;&#x786e;&#x6027;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x6846;&#x67b6;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x751a;&#x81f3;&#x5728;&#x67d0;&#x4e9b;&#x65b9;&#x9762;&#x8d85;&#x8d8a;&#x4e86;GPT-4V&#x3002;","children":[],"payload":{"tag":"li","lines":"433,434"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x81ea;&#x4fe1;&#x5730;&#x751f;&#x6210;&#x4e0e;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#x3002;&#x4f20;&#x7edf;&#x7684;RLHF&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x6602;&#x8d35;&#x7684;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x96be;&#x4ee5;&#x6269;&#x5c55;&#x3002;RLAIF-V&#x65e8;&#x5728;&#x63a2;&#x7d22;&#x5982;&#x4f55;&#x4ec5;&#x4f7f;&#x7528;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x751f;&#x6210;&#x9ad8;&#x8d28;&#x91cf;&#x53cd;&#x9988;&#xff0c;&#x89e3;&#x51b3;&#x53cd;&#x9988;&#x751f;&#x6210;&#x548c;&#x63a8;&#x7406;&#x65f6;&#x6269;&#x5c55;&#x4e24;&#x5927;&#x6311;&#x6218;&#xff0c;&#x63a8;&#x52a8;&#x5f00;&#x6e90;&#x793e;&#x533a;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"435,436"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: RLAIF-V&#x91c7;&#x7528;&#x4e24;&#x5927;&#x521b;&#x65b0;&#x6280;&#x672f;&#xff1a;1) &#x9ad8;&#x8d28;&#x91cf;&#x53cd;&#x9988;&#x751f;&#x6210;&#xff1a;&#x4f7f;&#x7528;&#x53bb;&#x6df7;&#x6dc6;&#x7b56;&#x7565;&#xff08;&#x540c;&#x4e00;&#x8f93;&#x5165;&#x4e0b;&#x591a;&#x6b21;&#x91c7;&#x6837;&#x751f;&#x6210;&#x54cd;&#x5e94;&#xff0c;&#x6d88;&#x9664;&#x6587;&#x672c;&#x98ce;&#x683c;&#x5e72;&#x6270;&#xff09;&#x548c;&#x5206;&#x6cbb;&#x6cd5;&#xff08;&#x5c06;&#x54cd;&#x5e94;&#x62c6;&#x5206;&#x4e3a;&#x539f;&#x5b50;&#x58f0;&#x660e;&#x5e76;&#x8f6c;&#x6362;&#x4e3a;&#x6781;&#x6027;&#x95ee;&#x9898;&#xff0c;&#x7531;&#x5f00;&#x6e90;MLLM&#x8bc4;&#x5206;&#xff09;&#x3002;2) &#x63a8;&#x7406;&#x65f6;&#x81ea;&#x53cd;&#x9988;&#x5f15;&#x5bfc;&#xff1a;&#x57fa;&#x4e8e;DPO&#x5bf9;&#x9f50;&#x7684;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5956;&#x52b1;&#x5206;&#x6570;&#x4f5c;&#x4e3a;&#x81ea;&#x53cd;&#x9988;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x957f;&#x5ea6;&#x5f52;&#x4e00;&#x5316;&#x7b56;&#x7565;&#x6291;&#x5236;&#x5bf9;&#x77ed;&#x54cd;&#x5e94;&#x7684;&#x504f;&#x597d;&#x504f;&#x5dee;&#x3002;&#x6574;&#x4e2a;&#x6846;&#x67b6;&#x91c7;&#x7528;&#x8fed;&#x4ee3;&#x53cd;&#x9988;&#x5b66;&#x4e60;&#xff0c;&#x52a8;&#x6001;&#x66f4;&#x65b0;&#x6570;&#x636e;&#x548c;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"436,437"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x516d;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;RLAIF-V&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x4fe1;&#x5ea6;&#xff1a;RLAIF-V 7B&#x5c06;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;80.7%&#xff0c;&#x603b;&#x4f53;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;33.7%&#xff1b;RLAIF-V 12B&#x5728;&#x65e0;&#x66f4;&#x5f3a;&#x6a21;&#x578b;&#x53ef;&#x7528;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x53cd;&#x9988;&#x5b9e;&#x73b0;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;76.8%&#xff0c;&#x603b;&#x4f53;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;32.4%&#xff0c;&#x5728;&#x591a;&#x9879;&#x6307;&#x6807;&#x4e0a;&#x8d85;&#x8d8a;GPT-4V&#xff0c;&#x5c55;&#x793a;&#x4e86;&#x5f00;&#x6e90;MLLM&#x7684;&#x81ea;&#x5bf9;&#x9f50;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"437,438"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: RLAIF-V&#x8bc1;&#x660e;&#x4e86;&#x4ec5;&#x4f7f;&#x7528;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x5373;&#x53ef;&#x5b9e;&#x73b0;&#x9ad8;&#x6027;&#x80fd;&#x7684;&#x5bf9;&#x9f50;&#xff0c;&#x751a;&#x81f3;&#x8d85;&#x8d8a;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#x3002;&#x5176;&#x65b9;&#x6cd5;&#x4e3a;&#x793e;&#x533a;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x63a8;&#x52a8;&#x4e86;&#x5f00;&#x6e90;MLLM&#x7684;&#x53d1;&#x5c55;&#xff0c;&#x5e76;&#x4e3a;&#x63a8;&#x7406;&#x65f6;&#x4f18;&#x5316;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x6240;&#x6709;&#x4ee3;&#x7801;&#x3001;&#x6570;&#x636e;&#x548c;&#x6a21;&#x578b;&#x6743;&#x91cd;&#x5c06;&#x5f00;&#x6e90;&#xff0c;&#x4fc3;&#x8fdb;&#x8fdb;&#x4e00;&#x6b65;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"438,440"}}],"payload":{"tag":"li","lines":"434,440","fold":1}}],"payload":{"tag":"h4","lines":"432,433"}},{"content":"FGAIF: Aligning Large Vision-Language Models with Fine-grained AI Feedback","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;FGAIF&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x7ec6;&#x7c92;&#x5ea6;AI&#x53cd;&#x9988;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;AI&#x5de5;&#x5177;&#x81ea;&#x52a8;&#x6807;&#x6ce8;&#x4e09;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff0c;&#x8bad;&#x7ec3;&#x591a;&#x4e2a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x5bc6;&#x96c6;&#x5956;&#x52b1;&#xff0c;&#x5e76;&#x96c6;&#x6210;&#x5230;PPO&#x7b97;&#x6cd5;&#x4e2d;&#x5fae;&#x8c03;&#x6a21;&#x578b;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x4e14;&#x53c2;&#x6570;&#x6548;&#x7387;&#x66f4;&#x9ad8;&#x3002;","children":[],"payload":{"tag":"li","lines":"441,442"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5b58;&#x5728;&#x6587;&#x672c;&#x4e0e;&#x56fe;&#x50cf;&#x6a21;&#x6001;&#x672a;&#x5bf9;&#x9f50;&#x95ee;&#x9898;&#xff0c;&#x5bfc;&#x81f4;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x4e2d;&#x51fa;&#x73b0;&#x4e09;&#x79cd;&#x5e7b;&#x89c9;&#xff1a;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x3001;&#x5c5e;&#x6027;&#x53ca;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x4e8e;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x7684;&#x65b9;&#x6cd5;&#x5b58;&#x5728;&#x4e09;&#x5927;&#x5c40;&#x9650;&#xff1a;1&#xff09;&#x901a;&#x7528;&#x53cd;&#x9988;&#x65e0;&#x6cd5;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff1b;2&#xff09;&#x7a00;&#x758f;&#x5956;&#x52b1;&#x4ec5;&#x9488;&#x5bf9;&#x6574;&#x53e5;&#x800c;&#x975e;&#x7ec6;&#x7c92;&#x5ea6;&#x7247;&#x6bb5;&#xff1b;3&#xff09;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x3002;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLM&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x7684;&#x53ef;&#x9760;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"443,444"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x4e09;&#x6b65;&#xff1a;1&#xff09;AI&#x53cd;&#x9988;&#x6536;&#x96c6;&#xff1a;&#x4f7f;&#x7528;AI&#x5de5;&#x5177;&#x81ea;&#x52a8;&#x8bc6;&#x522b;&#x54cd;&#x5e94;&#x4e2d;&#x6bcf;&#x4e2a;&#x5b50;&#x53e5;&#x7684;&#x4e09;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff08;&#x7269;&#x4f53;&#x5b58;&#x5728;/&#x5c5e;&#x6027;/&#x5173;&#x7cfb;&#xff09;&#xff0c;&#x751f;&#x6210;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#xff1b;2&#xff09;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x8bad;&#x7ec3;&#xff1a;&#x57fa;&#x4e8e;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x4e09;&#x4e2a;&#x72ec;&#x7acb;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff08;Ro/Ra/Rr&#xff09;&#xff0c;&#x5206;&#x522b;&#x8f93;&#x51fa;&#x5bf9;&#x5e94;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x7684;&#x5bc6;&#x96c6;&#x7247;&#x6bb5;&#x7ea7;&#x5956;&#x52b1;&#xff1b;3&#xff09;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x5fae;&#x8c03;&#xff1a;&#x5c06;&#x591a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x96c6;&#x6210;&#x81f3;&#x8fd1;&#x7aef;&#x7b56;&#x7565;&#x4f18;&#x5316;&#xff08;PPO&#xff09;&#x7b97;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x5956;&#x52b1;&#x4fe1;&#x53f7;&#x5fae;&#x8c03;LVLM&#x53c2;&#x6570;&#x3002;","children":[],"payload":{"tag":"li","lines":"444,445"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x8bc4;&#x6d4b;&#x57fa;&#x51c6;&#x548c;&#x901a;&#x7528;&#x57fa;&#x51c6;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;FGAIF&#x663e;&#x8457;&#x964d;&#x4f4e;&#x6240;&#x6709;&#x4e09;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff1b;2&#xff09;&#x76f8;&#x6bd4;&#x73b0;&#x6709;RL&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#xff08;&#x5982;LLaVA-RLHF&#xff09;&#xff0c;&#x5728;&#x53c2;&#x6570;&#x91cf;&#x66f4;&#x5c11;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#x6027;&#x80fd;&#x66f4;&#x4f18;&#xff1b;3&#xff09;&#x6d88;&#x878d;&#x5b9e;&#x9a8c;&#x9a8c;&#x8bc1;&#x4e86;&#x5404;&#x6a21;&#x5757;&#xff08;&#x5982;&#x591a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x3001;&#x7247;&#x6bb5;&#x7ea7;&#x5956;&#x52b1;&#xff09;&#x7684;&#x5fc5;&#x8981;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"445,446"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: FGAIF&#x9996;&#x6b21;&#x5b9e;&#x73b0;&#x4e86;&#x591a;&#x7c7b;&#x578b;&#x3001;&#x7247;&#x6bb5;&#x7ea7;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;AI&#x53cd;&#x9988;&#x5bf9;&#x9f50;&#xff0c;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x5373;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x81ea;&#x52a8;&#x5316;&#x53cd;&#x9988;&#x5728;&#x590d;&#x6742;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"446,448"}}],"payload":{"tag":"li","lines":"442,448","fold":1}}],"payload":{"tag":"h4","lines":"440,441"}}],"payload":{"tag":"h3","lines":"406,407","fold":1}},{"content":"&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316; (DPO)","children":[{"content":"SILKIE: Preference Distillation for Large Visual Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Silkie&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;GPT-4V&#x6784;&#x5efa;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;VLFeedback&#xff0c;&#x5e76;&#x91c7;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x65b9;&#x6cd5;&#x5bf9;Qwen-VL-Chat&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x611f;&#x77e5;&#x3001;&#x8ba4;&#x77e5;&#x80fd;&#x529b;&#x548c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"451,452"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89e3;&#x51b3;&#x5f00;&#x6e90;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5b58;&#x5728;&#x7684;&#x4e0e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x8131;&#x8282;&#x3001;&#x4ea7;&#x751f;&#x8bef;&#x5bfc;&#x6027;&#x5185;&#x5bb9;&#x6216;&#x504f;&#x89c1;&#x54cd;&#x5e94;&#x7684;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x901a;&#x8fc7;&#x5bf9;&#x9f50;&#x4f18;&#x5316;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x5fe0;&#x5b9e;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"453,454"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6784;&#x5efa;VLFeedback&#x6570;&#x636e;&#x96c6;&#xff1a;&#x4ece;12&#x4e2a;LVLMs&#x751f;&#x6210;&#x54cd;&#x5e94;&#xff0c;&#x4f7f;&#x7528;GPT-4V&#x4ece;&#x5e2e;&#x52a9;&#x6027;&#x3001;&#x89c6;&#x89c9;&#x5fe0;&#x5b9e;&#x5ea6;&#x548c;&#x4f26;&#x7406;&#x8003;&#x91cf;&#x4e09;&#x4e2a;&#x7ef4;&#x5ea6;&#x8fdb;&#x884c;&#x504f;&#x597d;&#x6807;&#x6ce8;&#xff1b;2. &#x91c7;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x65b9;&#x6cd5;&#x5bf9;Qwen-VL-Chat&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x84b8;&#x998f;GPT-4V&#x7684;&#x504f;&#x597d;&#x77e5;&#x8bc6;&#x3002;","children":[],"payload":{"tag":"li","lines":"454,455"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x5728;MME&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;&#x611f;&#x77e5;&#x548c;&#x8ba4;&#x77e5;&#x80fd;&#x529b;&#x5206;&#x522b;&#x76f8;&#x5bf9;&#x63d0;&#x5347;6.9%&#x548c;9.5%&#xff1b;2. &#x5728;MMHal-Bench&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x4e2d;&#x8fbe;&#x5230;SOTA&#x5206;&#x6570;3.02&#xff1b;3. &#x5728;&#x7ec6;&#x7c92;&#x5ea6;&#x611f;&#x77e5;&#xff08;&#x5982;OCR&#xff09;&#x548c;&#x590d;&#x6742;&#x8ba4;&#x77e5;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x4ee3;&#x7801;&#x63a8;&#x7406;&#xff09;&#x4e0a;&#x63d0;&#x5347;&#x663e;&#x8457;&#xff1b;4. AI&#x6807;&#x6ce8;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x6bd4;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x5e26;&#x6765;&#x66f4;&#x4e00;&#x81f4;&#x7684;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"455,456"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x901a;&#x8fc7;AI&#x53cd;&#x9988;&#x7684;&#x504f;&#x597d;&#x84b8;&#x998f;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;LVLMs&#x7684;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#x548c;&#x6027;&#x80fd;&#xff0c;VLFeedback&#x6570;&#x636e;&#x96c6;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x8d28;&#x91cf;&#x8d44;&#x6e90;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x53ef;&#x63a8;&#x5e7f;&#x81f3;&#x66f4;&#x591a;&#x6a21;&#x6001;&#x548c;&#x4efb;&#x52a1;&#xff0c;&#x4fc3;&#x8fdb;&#x53ef;&#x9760;&#x3001;&#x65e0;&#x504f;&#x89c1;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"456,458"}}],"payload":{"tag":"li","lines":"452,458","fold":1}}],"payload":{"tag":"h4","lines":"450,451"}},{"content":"MFPO: Modality-Fair Preference Optimization for Trustworthy MLLM Alignment","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;MFPO&#xff08;Modality-Fair Preference Optimization&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x548c;&#x8bbe;&#x8ba1;&#x65b0;&#x7684;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff0c;&#x6765;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x7684;&#x6a21;&#x6001;&#x5931;&#x51c6;&#x548c;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"459,460"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x56e0;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x7f16;&#x7801;&#x5668;&#x5206;&#x5f00;&#x8bad;&#x7ec3;&#x800c;&#x5bfc;&#x81f4;&#x7684;&#x6a21;&#x6001;&#x5931;&#x51c6;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x79cd;&#x5931;&#x51c6;&#x4f1a;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x5185;&#x5bb9;&#xff0c;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x4e86;MLLM&#x5728;&#x73b0;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x5c1d;&#x8bd5;&#x901a;&#x8fc7;&#x6587;&#x672c;&#x504f;&#x597d;&#x4f18;&#x5316;&#x6765;&#x7f13;&#x89e3;&#x6b64;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x4f5c;&#x8005;&#x53d1;&#x73b0;&#x6a21;&#x578b;&#x53ef;&#x80fd;&#x53ea;&#x662f;&#x8bb0;&#x5fc6;&#x4e86;&#x6587;&#x672c;&#x504f;&#x597d;&#xff0c;&#x800c;&#x975e;&#x771f;&#x6b63;&#x5b66;&#x4f1a;&#x4e86;&#x5bf9;&#x9f50;&#x56fe;&#x50cf;&#x548c;&#x6587;&#x672c;&#xff0c;&#x5bfc;&#x81f4;&#x5728;&#x9762;&#x5bf9;&#x5931;&#x771f;&#x56fe;&#x50cf;&#x65f6;&#x4ecd;&#x4f1a;&#x7ed9;&#x51fa;&#x770b;&#x4f3c;&#x6b63;&#x786e;&#x4f46;&#x5b9e;&#x5219;&#x9519;&#x8bef;&#x7684;&#x7b54;&#x6848;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x76f4;&#x63a5;&#x5173;&#x7cfb;&#x5230;MLLM&#x80fd;&#x5426;&#x88ab;&#x5b89;&#x5168;&#x53ef;&#x9760;&#x5730;&#x90e8;&#x7f72;&#x5230;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"461,462"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;MFPO&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1. <strong>&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x6784;&#x5efa;</strong>&#xff1a;&#x9996;&#x5148;&#x57fa;&#x4e8e;&#x6587;&#x672c;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x4f7f;&#x7528;&#x591a;&#x90e8;&#x56fe;&#xff08;multipartite graph&#xff09;&#x7b97;&#x6cd5;&#x8bc6;&#x522b;&#x56de;&#x7b54;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x8bcd;&#xff08;top-K keywords&#xff09;&#x3002;&#x7136;&#x540e;&#xff0c;&#x5229;&#x7528;&#x6539;&#x8fdb;&#x7684;Segment Anything Model (SAM) &#x5c06;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x8bcd;&#x6620;&#x5c04;&#x5230;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x533a;&#x57df;&#xff0c;&#x5e76;&#x4ec5;&#x5bf9;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x533a;&#x57df;&#x65bd;&#x52a0;&#x6269;&#x6563;&#x566a;&#x58f0;&#xff08;diffusion noise&#xff09;&#xff0c;&#x751f;&#x6210;&#x2018;&#x4e0d;&#x88ab;&#x504f;&#x597d;&#x2019;&#x7684;&#x56fe;&#x50cf;&#x3002;&#x8fd9;&#x79cd;&#x65b9;&#x6cd5;&#x786e;&#x4fdd;&#x4e86;&#x504f;&#x597d;&#x4e0e;&#x4e0d;&#x504f;&#x597d;&#x56fe;&#x50cf;&#x4e4b;&#x95f4;&#x4ec5;&#x6709;&#x7ec6;&#x5fae;&#x4e14;&#x5173;&#x952e;&#x7684;&#x533a;&#x522b;&#x3002;2. <strong>&#x56fe;&#x50cf;&#x5956;&#x52b1;&#x635f;&#x5931;&#x51fd;&#x6570;</strong>&#xff1a;&#x8bbe;&#x8ba1;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x7684;&#x635f;&#x5931;&#x51fd;&#x6570; <code>LTotal</code>&#xff0c;&#x5b83;&#x7ed3;&#x5408;&#x4e86;&#x6587;&#x672c;&#x504f;&#x597d;&#x635f;&#x5931;&#xff08;<code>Ltext</code>&#xff0c;&#x57fa;&#x4e8e;DPO&#xff09;&#x3001;&#x56fe;&#x50cf;&#x504f;&#x597d;&#x635f;&#x5931;&#xff08;<code>Limage</code>&#xff0c;&#x9f13;&#x52b1;&#x7b54;&#x6848;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5bf9;&#x9f50;&#xff09;&#x4ee5;&#x53ca;&#x4e00;&#x4e2a;&#x8fb9;&#x9645;&#x635f;&#x5931;&#xff08;<code>Lmargin</code>&#xff0c;&#x7528;&#x4e8e;&#x7a33;&#x5b9a;&#x8bad;&#x7ec3;&#xff09;&#x3002;3. <strong>&#x7531;&#x6613;&#x5230;&#x96be;&#x7684;&#x8fed;&#x4ee3;&#x5bf9;&#x9f50;&#x7b56;&#x7565;</strong>&#xff1a;&#x6839;&#x636e;&#x8bed;&#x4e49;&#x71b5;&#xff08;semantic entropy&#xff09;&#x5c06;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x6309;&#x96be;&#x5ea6;&#x5206;&#x7c7b;&#xff0c;&#x5148;&#x8ba9;&#x6a21;&#x578b;&#x5728;&#x7b80;&#x5355;&#x7684;&#x6837;&#x672c;&#x4e0a;&#x8fdb;&#x884c;&#x8bad;&#x7ec3;&#xff0c;&#x518d;&#x9010;&#x6b65;&#x8fc7;&#x6e21;&#x5230;&#x66f4;&#x56f0;&#x96be;&#x7684;&#x6837;&#x672c;&#xff0c;&#x4ee5;&#x7a33;&#x5b9a;&#x8054;&#x5408;&#x6a21;&#x6001;&#x7684;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"462,463"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x4e09;&#x4e2a;&#x53ef;&#x4fe1;&#x5ea6;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;Object HalBench, AMBER&#x7b49;&#xff09;&#x4e0a;&#x7684;&#x5927;&#x91cf;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;MFPO&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;MLLM&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x5173;&#x952e;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1. &#x5e94;&#x7528;MFPO&#x540e;&#xff0c;7B&#x53c2;&#x6570;&#x89c4;&#x6a21;&#x7684;LLaVA-v1.5&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x8fbe;&#x5230;&#x4e86;&#x4e0e;&#x751a;&#x81f3;&#x8d85;&#x8fc7;&#x4e86;13B&#x3001;34B&#x7b49;&#x66f4;&#x5927;&#x6a21;&#x578b;&#x7684;&#x6c34;&#x5e73;&#x3002;2. MFPO&#x52a0;&#x6301;&#x7684;LLaVA-v1.5-13B&#x6a21;&#x578b;&#x5728;Object HalBench&#x4e0a;&#x6027;&#x80fd;&#x8d85;&#x8d8a;GPT-4V&#x8fbe;40%&#xff0c;&#x5e76;&#x5728;Object HalBench&#x548c;AMBER&#x4e0a;&#x521b;&#x9020;&#x4e86;&#x65b0;&#x7684;&#x6700;&#x5148;&#x8fdb;&#xff08;SOTA&#xff09;&#x7ed3;&#x679c;&#x3002;3. LLaVA-v1.5-7B+MFPO&#x548c;LLaVA-v1.5-13B+MFPO&#x5728;&#x516b;&#x9879;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x4e2d;&#x7684;&#x4e94;&#x9879;&#x4e0a;&#x5747;&#x4f18;&#x4e8e;GPT-4V&#x3002;4. &#x6d88;&#x878d;&#x7814;&#x7a76;&#x8bc1;&#x5b9e;&#x4e86;MFPO&#x4e2d;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"463,464"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;MFPO&#x901a;&#x8fc7;&#x516c;&#x5e73;&#x5730;&#x8054;&#x5408;&#x4f18;&#x5316;&#x6587;&#x672c;&#x548c;&#x56fe;&#x50cf;&#x504f;&#x597d;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;MLLM&#x7684;&#x6a21;&#x6001;&#x5931;&#x51c6;&#x548c;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x5f3a;&#x8c03;&#x4e86;&#x5728;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5bf9;&#x9f50;&#x4e2d;&#x8fdb;&#x884c;&#x5e73;&#x8861;&#x7684;&#x3001;&#x8de8;&#x6a21;&#x6001;&#x504f;&#x597d;&#x4f18;&#x5316;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x503c;&#x5f97;&#x4fe1;&#x8d56;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x7684;&#x65b9;&#x6cd5;&#x8bba;&#xff0c;&#x63a8;&#x52a8;&#x4e86;MLLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x578b;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"464,466"}}],"payload":{"tag":"li","lines":"460,466","fold":1}}],"payload":{"tag":"h4","lines":"458,459"}},{"content":"mDPO: Conditional Preference Optimization for Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;MDPO&#xff0c;&#x4e00;&#x79cd;&#x9488;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x6761;&#x4ef6;&#x504f;&#x597d;&#x4f18;&#x5316;&#x65b9;&#x6cd5;&#xff0c;&#x89e3;&#x51b3;&#x4e86;&#x6807;&#x51c6;DPO&#x5728;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#x4e2d;&#x5ffd;&#x89c6;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"467,468"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x5728;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x5e94;&#x7528;&#x65f6;&#x51fa;&#x73b0;&#x7684;&#x2018;&#x65e0;&#x6761;&#x4ef6;&#x504f;&#x597d;&#x2019;&#x95ee;&#x9898;&#x3002;&#x5177;&#x4f53;&#x8868;&#x73b0;&#x4e3a;&#x6a21;&#x578b;&#x5728;&#x4f18;&#x5316;&#x8fc7;&#x7a0b;&#x4e2d;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x800c;&#x5ffd;&#x89c6;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#xff0c;&#x5bfc;&#x81f4;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x548c;&#x5e7b;&#x89c9;&#x589e;&#x52a0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x6838;&#x5fc3;&#x4ef7;&#x503c;&#x662f;&#x878d;&#x5408;&#x89c6;&#x89c9;&#x4e0e;&#x8bed;&#x8a00;&#x4fe1;&#x606f;&#xff0c;&#x82e5;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x5229;&#x7528;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#xff0c;&#x5c06;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"469,470"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;MDPO&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x6807;&#x51c6;DPO&#x76ee;&#x6807;&#xff08;LDPOm&#xff09;&#xff0c;&#x7528;&#x4e8e;&#x5b66;&#x4e60;&#x54cd;&#x5e94;&#x504f;&#x597d;&#xff1b;2&#xff09;&#x6761;&#x4ef6;&#x504f;&#x597d;&#x4f18;&#x5316;&#x76ee;&#x6807;&#xff08;LCoPO&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x5bf9;&#xff08;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0e;&#x88c1;&#x526a;&#x540e;&#x7684;&#x8d1f;&#x6837;&#x672c;&#xff09;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x6761;&#x4ef6;&#xff1b;3&#xff09;&#x951a;&#x5b9a;&#x504f;&#x597d;&#x4f18;&#x5316;&#x76ee;&#x6807;&#xff08;LAncPO&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x5956;&#x52b1;&#x951a;&#x5b9a;&#xff08;reward anchor&#xff09;&#x786e;&#x4fdd;&#x88ab;&#x9009;&#x54cd;&#x5e94;&#x7684;&#x6982;&#x7387;&#x4e0d;&#x4e0b;&#x964d;&#x3002;&#x4e09;&#x8005;&#x8054;&#x5408;&#x4f18;&#x5316;&#xff08;LMDPO = LDPOm + LCoPO + LAncPO&#xff09;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x540c;&#x65f6;&#x5b66;&#x4e60;&#x8bed;&#x8a00;&#x548c;&#x89c6;&#x89c9;&#x6761;&#x4ef6;&#x4e0b;&#x7684;&#x504f;&#x597d;&#x3002;","children":[],"payload":{"tag":"li","lines":"470,471"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;Bunny-3B&#x548c;LLaVA-7B&#x6a21;&#x578b;&#x4e0a;&#xff0c;&#x4f7f;&#x7528;MMHalBench&#x3001;Object HalBench&#x548c;AMBER&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff0c;MDPO&#x5747;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x6807;&#x51c6;DPO&#xff1a;1&#xff09;&#x5728;MMHalBench&#x4e0a;&#x603b;&#x4f53;&#x5f97;&#x5206;&#x66f4;&#x9ad8;&#x4e14;&#x5e7b;&#x89c9;&#x7387;&#xff08;HalRate&#xff09;&#x66f4;&#x4f4e;&#xff1b;2&#xff09;&#x5728;Object HalBench&#x4e0a;CHAIRs&#xff08;&#x5bf9;&#x8c61;&#x7ea7;&#x5e7b;&#x89c9;&#xff09;&#x6307;&#x6807;&#x4e0b;&#x964d;&#xff1b;3&#xff09;&#x4eba;&#x7c7b;&#x8bc4;&#x4f30;&#x786e;&#x8ba4;MDPO&#x751f;&#x6210;&#x7684;&#x54cd;&#x5e94;&#x66f4;&#x51c6;&#x786e;&#x4e14;&#x66f4;&#x5c11;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x3002;&#x6d88;&#x878d;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x6761;&#x4ef6;&#x504f;&#x597d;&#x4f18;&#x5316;&#x548c;&#x5956;&#x52b1;&#x951a;&#x5b9a;&#x5747;&#x5bf9;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x6709;&#x8d21;&#x732e;&#x3002;","children":[],"payload":{"tag":"li","lines":"471,472"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff0c;MDPO&#x901a;&#x8fc7;&#x663e;&#x5f0f;&#x5efa;&#x6a21;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#x504f;&#x597d;&#x548c;&#x5956;&#x52b1;&#x6b63;&#x5219;&#x5316;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;&#x591a;&#x6a21;&#x6001;DPO&#x4e2d;&#x7684;&#x65e0;&#x6761;&#x4ef6;&#x504f;&#x597d;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#xff1a;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#xff0c;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x5176;&#x4ed6;&#x591a;&#x7ec4;&#x4ef6;&#x8f93;&#x5165;&#x573a;&#x666f;&#xff08;&#x5982;&#x97f3;&#x9891;-&#x6587;&#x672c;&#xff09;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x4e86;&#x9ad8;&#x6548;&#x5c0f;&#x53c2;&#x6570;&#x6a21;&#x578b;&#xff08;&#x5982;3B&#x89c4;&#x6a21;&#xff09;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"472,474"}}],"payload":{"tag":"li","lines":"468,474","fold":1}}],"payload":{"tag":"h4","lines":"466,467"}},{"content":"HDPO: Mitigating Hallucination in Multimodal Large Language Model via Hallucination-targeted Direct Preference Optimization","children":[{"content":"","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x9488;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x4f18;&#x5316;&#x65b9;&#x6cd5;HDPO&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5e7b;&#x89c9;&#x7684;&#x4e09;&#x79cd;&#x6210;&#x56e0;&#xff08;&#x89c6;&#x89c9;&#x80fd;&#x529b;&#x4e0d;&#x8db3;&#x3001;&#x751f;&#x6210;&#x957f;&#x4e0a;&#x4e0b;&#x6587;&#x80fd;&#x529b;&#x4e0d;&#x8db3;&#x3001;&#x591a;&#x6a21;&#x6001;&#x51b2;&#x7a81;&#xff09;&#xff0c;&#x4e13;&#x95e8;&#x6784;&#x5efa;&#x4e86;&#x4e09;&#x79cd;&#x7c7b;&#x578b;&#x7684;&#x504f;&#x597d;&#x5bf9;&#x6570;&#x636e;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x8fdb;&#x884c;&#x8bad;&#x7ec3;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;HDPO&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x4f18;&#x6027;&#x80fd;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;MLLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"475,476"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x5176;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x5c1d;&#x8bd5;&#x4f7f;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x6765;&#x7f13;&#x89e3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5728;&#x4e0d;&#x540c;&#x7c7b;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x4efb;&#x52a1;&#x4e0a;&#x8868;&#x73b0;&#x4e0d;&#x4e00;&#x81f4;&#xff0c;&#x4e14;&#x672a;&#x80fd;&#x9488;&#x5bf9;&#x5e7b;&#x89c9;&#x7684;&#x591a;&#x6837;&#x6210;&#x56e0;&#x8fdb;&#x884c;&#x7cfb;&#x7edf;&#x5904;&#x7406;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x5168;&#x9762;&#x3001;&#x9488;&#x5bf9;&#x6027;&#x66f4;&#x5f3a;&#x7684;&#x4f18;&#x5316;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"477,478"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Hallucination-targeted Direct Preference Optimization (HDPO)&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x4e09;&#x79cd;&#x9488;&#x5bf9;&#x4e0d;&#x540c;&#x5e7b;&#x89c9;&#x6210;&#x56e0;&#x7684;&#x504f;&#x597d;&#x5bf9;&#x6570;&#x636e;&#x6765;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff1a;","children":[],"payload":{"tag":"li","lines":"478,479"}}],"payload":{"tag":"li","lines":"476,479","fold":1}}],"payload":{"tag":"ul","lines":"475,479"}},{"content":"","children":[{"content":"1. &#x89c6;&#x89c9;&#x80fd;&#x529b;&#x4e0d;&#x8db3;&#xff08;VDH&#xff09;&#xff1a;&#x5728;&#x81ea;&#x56de;&#x5f52;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x4ec5;&#x4fdd;&#x7559;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6570;&#x6700;&#x4f4e;&#x7684;&#x90e8;&#x5206;&#x89c6;&#x89c9;token&#x751f;&#x6210;&#x8d1f;&#x9762;&#x54cd;&#x5e94;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x66f4;&#x6709;&#x6548;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"479,480","listIndex":1}},{"content":"2. &#x957f;&#x4e0a;&#x4e0b;&#x6587;&#x751f;&#x6210;&#x80fd;&#x529b;&#x4e0d;&#x8db3;&#xff08;LCH&#xff09;&#xff1a;&#x9009;&#x62e9;&#x9ad8;&#x8d28;&#x91cf;&#x7684;&#x957f;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x4f5c;&#x4e3a;&#x6b63;&#x9762;&#x6837;&#x672c;&#xff0c;&#x751f;&#x6210;&#x540e;&#x534a;&#x90e8;&#x5206;&#x504f;&#x79bb;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x7684;&#x8d1f;&#x9762;&#x6837;&#x672c;&#xff0c;&#x6a21;&#x62df;&#x957f;&#x6587;&#x672c;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"480,481","listIndex":2}},{"content":"3. &#x591a;&#x6a21;&#x6001;&#x51b2;&#x7a81;&#xff08;MCH&#xff09;&#xff1a;&#x5728;&#x63d0;&#x793a;&#x8bcd;&#x4e2d;&#x6dfb;&#x52a0;&#x4e0e;&#x56fe;&#x50cf;&#x51b2;&#x7a81;&#x7684;&#x4fe1;&#x606f;&#x751f;&#x6210;&#x8d1f;&#x9762;&#x6837;&#x672c;&#xff0c;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x5728;&#x5b58;&#x5728;&#x51b2;&#x7a81;&#x4fe1;&#x606f;&#x65f6;&#x4ecd;&#x80fd;&#x6b63;&#x786e;&#x54cd;&#x5e94;&#x3002;<br>\n&#x8fd9;&#x4e9b;&#x504f;&#x597d;&#x5bf9;&#x6570;&#x636e;&#x7528;&#x4e8e;DPO&#x8bad;&#x7ec3;&#xff0c;&#x76f4;&#x63a5;&#x4f18;&#x5316;&#x6a21;&#x578b;&#x53c2;&#x6570;&#xff0c;&#x65e0;&#x9700;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x3002;","children":[{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: HDPO&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5305;&#x62ec;VQA&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#xff09;&#x4e0a;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x4f18;&#x6027;&#x80fd;&#xff0c;&#x8d85;&#x8d8a;&#x4e86;&#x5927;&#x591a;&#x6570;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;HA-DPO&#x3001;SeVa&#x7b49;&#xff09;&#x3002;&#x6d88;&#x878d;&#x5b9e;&#x9a8c;&#x548c;&#x6df1;&#x5165;&#x5206;&#x6790;&#x8bc1;&#x5b9e;&#x4e86;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6709;&#x6548;&#x6027;&#xff0c;&#x5e76;&#x8868;&#x660e;&#x901a;&#x8fc7;&#x6269;&#x5927;&#x6570;&#x636e;&#x89c4;&#x6a21;&#x8fd8;&#x6709;&#x8fdb;&#x4e00;&#x6b65;&#x6539;&#x8fdb;&#x7684;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"483,484"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: HDPO&#x901a;&#x8fc7;&#x7cfb;&#x7edf;&#x5206;&#x6790;&#x5e76;&#x9488;&#x5bf9;MLLM&#x5e7b;&#x89c9;&#x7684;&#x591a;&#x79cd;&#x6210;&#x56e0;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x4e0d;&#x540c;&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x6297;&#x5e7b;&#x89c9;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x4e3a;&#x7406;&#x89e3;MLLM&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x89c6;&#x89d2;&#xff0c;&#x4e5f;&#x4e3a;&#x672a;&#x6765;&#x4f18;&#x5316;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x6709;&#x6548;&#x9014;&#x5f84;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63a8;&#x52a8;MLLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#xff0c;&#x4ee5;&#x53ca;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x6570;&#x636e;&#x6784;&#x5efa;&#x6846;&#x67b6;&#x3002;","children":[],"payload":{"tag":"li","lines":"484,486"}}],"payload":{"tag":"li","lines":"481,486","listIndex":3}}],"payload":{"tag":"ol","lines":"479,486"}}],"payload":{"tag":"h4","lines":"474,475"}},{"content":"HA-DPO: Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;HA-DPO&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5c06;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x91cd;&#x6784;&#x4e3a;&#x504f;&#x597d;&#x9009;&#x62e9;&#x4efb;&#x52a1;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x98ce;&#x683c;&#x4e00;&#x81f4;&#x7684;&#x6837;&#x672c;&#x5bf9;&#x8fdb;&#x884c;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#x8bad;&#x7ec3;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x751f;&#x6210;&#x865a;&#x5047;&#x5185;&#x5bb9;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x5927;&#x5e45;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"487,488"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x201d;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x4e0e;&#x5173;&#x8054;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x6216;&#x5b8c;&#x5168;&#x865a;&#x6784;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4e0d;&#x4ec5;&#x635f;&#x5bb3;&#x7528;&#x6237;&#x4f53;&#x9a8c;&#xff0c;&#x8fd8;&#x53ef;&#x80fd;&#x4ea7;&#x751f;&#x4e25;&#x91cd;&#x540e;&#x679c;&#xff0c;&#x4f8b;&#x5982;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x7b49;&#x9886;&#x57df;&#x5bfc;&#x81f4;&#x8bef;&#x8bca;&#x3002;","children":[],"payload":{"tag":"li","lines":"489,490"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x5e7b;&#x89c9;&#x611f;&#x77e5;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;HA-DPO&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x9996;&#x5148;&#xff0c;&#x901a;&#x8fc7;&#x4e00;&#x4e2a;&#x9ad8;&#x6548;&#x7ba1;&#x9053;&#x6784;&#x5efa;&#x98ce;&#x683c;&#x4e00;&#x81f4;&#x7684;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x5bf9;&#xff1a;&#x4f7f;&#x7528;LVLM&#x751f;&#x6210;&#x5305;&#x542b;&#x5e7b;&#x89c9;&#x7684;&#x8d1f;&#x6837;&#x672c;&#xff0c;GPT-4&#x68c0;&#x6d4b;&#x5e76;&#x4fee;&#x6b63;&#x5e7b;&#x89c9;&#x751f;&#x6210;&#x6b63;&#x6837;&#x672c;&#xff0c;&#x518d;&#x7528;GPT-4&#x91cd;&#x5199;&#x4ee5;&#x4fdd;&#x8bc1;&#x98ce;&#x683c;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x7136;&#x540e;&#xff0c;&#x5c06;DPO&#x6269;&#x5c55;&#x5230;&#x591a;&#x6a21;&#x6001;&#x9886;&#x57df;&#xff0c;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x5728;&#x7ed9;&#x5b9a;&#x56fe;&#x50cf;&#x548c;&#x95ee;&#x9898;&#x65f6;&#xff0c;&#x504f;&#x597d;&#x9009;&#x62e9;&#x975e;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#x800c;&#x975e;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff0c;&#x800c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x590d;&#x6742;&#x7684;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"490,491"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x663e;&#x8457;&#xff1a;&#x5e94;&#x7528;HA-DPO&#x540e;&#xff0c;MiniGPT-4&#x6a21;&#x578b;&#x7684;POPE&#x51c6;&#x786e;&#x7387;&#x4ece;51.13%&#x63d0;&#x5347;&#x81f3;86.13%&#xff08;&#x7edd;&#x5bf9;&#x63d0;&#x5347;35%&#xff09;&#xff0c;MME&#x5206;&#x6570;&#x4ece;932.00&#x8dc3;&#x5347;&#x81f3;1326.46&#xff08;&#x76f8;&#x5bf9;&#x63d0;&#x5347;42.32%&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x53e6;&#x5916;&#x4e24;&#x4e2a;&#x4e3b;&#x6d41;&#x6a21;&#x578b;&#xff08;LLaVA&#x548c;mPLUG-Owl&#xff09;&#x4e0a;&#x4e5f;&#x4e00;&#x81f4;&#x5730;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x5e76;&#x589e;&#x5f3a;&#x4e86;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"491,492"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;HA-DPO&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x901a;&#x7528;&#x7684;&#x8303;&#x5f0f;&#xff0c;&#x80fd;&#x663e;&#x8457;&#x589e;&#x5f3a;LVLM&#x7684;&#x771f;&#x5b9e;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x5927;&#x91cf;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x6216;&#x590d;&#x6742;&#x540e;&#x5904;&#x7406;&#xff0c;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x53e5;&#x5b50;&#x7ea7;&#x5e7b;&#x89c9;&#x6bd4;&#x7387;&#xff08;SHR&#xff09;&#x6307;&#x6807;&#x63a8;&#x52a8;&#x4e86;&#x66f4;&#x5168;&#x9762;&#x7684;&#x8bc4;&#x4f30;&#x6807;&#x51c6;&#x3002;","children":[],"payload":{"tag":"li","lines":"492,494"}}],"payload":{"tag":"li","lines":"488,494","fold":1}}],"payload":{"tag":"h4","lines":"486,487"}},{"content":"HSA-DPO: Detecting and Mitigating Hallucination in Large Vision Language Models via Fine-Grained AI Feedback","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x5229;&#x7528;&#x7ec6;&#x7c92;&#x5ea6;AI&#x53cd;&#x9988;&#x6765;&#x68c0;&#x6d4b;&#x548c;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x65b0;&#x65b9;&#x6cd5;HSA-DPO&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x8bad;&#x7ec3;&#x4e00;&#x4e2a;&#x53e5;&#x5b50;&#x7ea7;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#xff0c;&#x5e76;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x7ed3;&#x5408;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x6027;&#x611f;&#x77e5;&#x7684;&#x4f18;&#x5316;&#x7b56;&#x7565;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8d85;&#x8d8a;GPT-4V&#x548c;Gemini&#x7b49;&#x73b0;&#x6709;&#x6280;&#x672f;&#x3002;","children":[],"payload":{"tag":"li","lines":"495,496"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x8f93;&#x5165;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7684;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5b58;&#x5728;&#x4e09;&#x4e2a;&#x4e3b;&#x8981;&#x95ee;&#x9898;&#xff1a;1&#xff09;&#x504f;&#x597d;&#x6570;&#x636e;&#x901a;&#x5e38;&#x505c;&#x7559;&#x5728;&#x7c97;&#x7c92;&#x5ea6;&#x7684;&#x54cd;&#x5e94;&#x7ea7;&#x522b;&#xff0c;&#x68c0;&#x6d4b;&#x548c;&#x7f13;&#x89e3;&#x4e0d;&#x591f;&#x5f7b;&#x5e95;&#xff1b;2&#xff09;&#x6784;&#x5efa;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x96c6;&#x4f9d;&#x8d56;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x5de5;&#x6216;&#x4e13;&#x6709;&#x6a21;&#x578b;&#x6807;&#x6ce8;&#xff1b;3&#xff09;&#x672a;&#x80fd;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x7684;&#x4e25;&#x91cd;&#x7a0b;&#x5ea6;&#xff0c;&#x5bfc;&#x81f4;&#x5173;&#x952e;&#x5e7b;&#x89c9;&#x53ef;&#x80fd;&#x88ab;&#x5ffd;&#x7565;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"497,498"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#xff1a;1&#xff09;&#x7ec6;&#x7c92;&#x5ea6;AI&#x53cd;&#x9988;&#x751f;&#x6210;&#xff1a;&#x4f7f;&#x7528;GPT-4/GPT-4V&#x5bf9;&#x5c11;&#x91cf;&#x6837;&#x672c;&#x751f;&#x6210;&#x53e5;&#x5b50;&#x7ea7;&#x5e7b;&#x89c9;&#x6807;&#x6ce8;&#xff08;&#x5305;&#x62ec;&#x7c7b;&#x578b;&#x3001;&#x4e25;&#x91cd;&#x6027;&#x5206;&#x6570;&#x53ca;&#x539f;&#x56e0;&#xff09;&#xff0c;&#x8986;&#x76d6;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x4e09;&#x7c7b;&#x4e3b;&#x8981;&#x5e7b;&#x89c9;&#xff1b;2&#xff09;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x4e0e;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x6784;&#x5efa;&#xff1a;&#x57fa;&#x4e8e;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x4e00;&#x4e2a;&#x53e5;&#x5b50;&#x7ea7;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#xff0c;&#x7136;&#x540e;&#x901a;&#x8fc7;&#x201c;&#x68c0;&#x6d4b;-&#x91cd;&#x5199;&#x201d;&#x7ba1;&#x9053;&#x81ea;&#x52a8;&#x6784;&#x5efa;&#x5927;&#x89c4;&#x6a21;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5373;&#x3008;&#x4fee;&#x6b63;&#x540e;&#x7684;&#x7b54;&#x6848;&#xff0c;&#x88ab;&#x62d2;&#x7edd;&#x7684;&#x5e7b;&#x89c9;&#x7b54;&#x6848;&#x3009;&#x5bf9;&#xff09;&#xff1b;3&#xff09;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x6027;&#x611f;&#x77e5;&#x4f18;&#x5316;&#xff08;HSA-DPO&#xff09;&#xff1a;&#x5728;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x4e2d;&#x5f15;&#x5165;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x6027;&#x5206;&#x6570;&#x4f5c;&#x4e3a;&#x6743;&#x91cd;&#xff0c;&#x4f18;&#x5148;&#x7f13;&#x89e3;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"498,499"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x5728;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x4efb;&#x52a1;&#x4e0a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;MHaluBench&#x57fa;&#x51c6;&#x4e0a;&#x8fbe;&#x5230;&#x6700;&#x65b0; state-of-the-art &#x6027;&#x80fd;&#xff0c;&#x8d85;&#x8d8a;GPT-4V&#x548c;Gemini&#xff1b;2&#xff09;&#x5728;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x4efb;&#x52a1;&#x4e0a;&#xff0c;HSA-DPO&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x5728;AMBER&#x57fa;&#x51c6;&#x4e0a;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;36.1%&#xff0c;&#x5728;Object HalBench&#x57fa;&#x51c6;&#x4e0a;&#x964d;&#x4f4e;CHAIRS&#x6307;&#x6807;76.3%&#xff1b;3&#xff09;&#x53ef;&#x89c6;&#x5316;&#x7ed3;&#x679c;&#xff08;&#x56fe;1&#xff09;&#x663e;&#x793a;HSA-DPO&#x5728;&#x6240;&#x6709;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x4e0a;&#x5747;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x5148;&#x8fdb;&#x6a21;&#x578b;&#xff08;&#x5982;Silkie&#x3001;GPT-4V&#x3001;RLHF-V&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"499,500"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff1a;&#x7ec6;&#x7c92;&#x5ea6;AI&#x53cd;&#x9988;&#x548c;HSA-DPO&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x68c0;&#x6d4b;&#x548c;&#x7f13;&#x89e3;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5c24;&#x5176;&#x901a;&#x8fc7;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x6027;&#x63d0;&#x5347;&#x4e86;&#x7f13;&#x89e3;&#x7684;&#x9488;&#x5bf9;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;LVLM&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x4e14;&#x6210;&#x672c;&#x8f83;&#x4f4e;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#xff0c;&#x540c;&#x65f6;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x57fa;&#x4e8e;&#x4e25;&#x91cd;&#x6027;&#x52a0;&#x6743;&#x504f;&#x597d;&#x5b66;&#x4e60;&#x7684;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"500,502"}}],"payload":{"tag":"li","lines":"496,502","fold":1}}],"payload":{"tag":"h4","lines":"494,495"}},{"content":"POVID: Aligning Modalities in Vision Large Language Models via Preference Fine-tuning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;POVID&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;AI&#x751f;&#x6210;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x901a;&#x8fc7;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"503,504"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x7406;&#x89e3;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x865a;&#x5047;&#x4fe1;&#x606f;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7269;&#x4f53;&#x3001;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#x7b49;&#xff09;&#x3002;&#x8fd9;&#x662f;&#x7531;&#x4e8e;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#x672a;&#x5145;&#x5206;&#x5bf9;&#x9f50;&#xff0c;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x800c;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x8bc1;&#x636e;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x5728;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#xff08;&#x5982;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x53ef;&#x80fd;&#x5e26;&#x6765;&#x4e25;&#x91cd;&#x540e;&#x679c;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x89e3;&#x51b3;&#x3002;","children":[],"payload":{"tag":"li","lines":"505,506"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;POVID&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x4e24;&#x79cd;&#x7b56;&#x7565;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x504f;&#x597d;&#x6570;&#x636e;&#xff1a;1&#xff09;&#x4f7f;&#x7528;GPT-4V&#x5411;&#x771f;&#x5b9e;&#x7b54;&#x6848;&#x6ce8;&#x5165;&#x5408;&#x7406;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x6dfb;&#x52a0;&#x4e0d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x3001;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#x6216;&#x903b;&#x8f91;&#x5173;&#x7cfb;&#xff09;&#xff0c;&#x6784;&#x9020;&#x8d1f;&#x9762;&#x54cd;&#x5e94;&#xff1b;2&#xff09;&#x5728;&#x8bad;&#x7ec3;&#x65f6;&#x5bf9;&#x56fe;&#x50cf;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#xff08;&#x5982;&#x626d;&#x66f2;&#x56fe;&#x50cf;&#xff09;&#xff0c;&#x89e6;&#x53d1;&#x6a21;&#x578b;&#x56fa;&#x6709;&#x5e7b;&#x89c9;&#x884c;&#x4e3a;&#x4f5c;&#x4e3a;&#x5b9e;&#x65f6;&#x8d1f;&#x9762;&#x54cd;&#x5e94;&#x3002;&#x5c06;&#x771f;&#x5b9e;&#x7b54;&#x6848;&#x4f5c;&#x4e3a;&#x6b63;&#x9762;&#x54cd;&#x5e94;&#xff0c;&#x7ed3;&#x5408;&#x4e24;&#x79cd;&#x8d1f;&#x9762;&#x54cd;&#x5e94;&#xff0c;&#x91c7;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x7b97;&#x6cd5;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x5f3a;&#x5316;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#x3002;","children":[],"payload":{"tag":"li","lines":"506,507"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MMBench&#x3001;MM-Vet&#x3001;POPE&#x7b49;&#xff09;&#x4e0a;&#xff0c;POVID&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x76f8;&#x6bd4;&#x5176;&#x4ed6;&#x504f;&#x597d;&#x5b66;&#x4e60;&#x65b9;&#x6cd5;&#x5e73;&#x5747;&#x63d0;&#x5347;12.4%&#x6027;&#x80fd;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x5728;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x4efb;&#x52a1;POPE&#x4e0a;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x5347;&#x81f3;94.75%&#xff0c;&#x5728;&#x7efc;&#x5408;&#x8bc4;&#x4f30;MM-Vet&#x4e0a;&#x8fbe;&#x5230;69.0%&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x6a21;&#x6001;&#xff0c;&#x6539;&#x5584;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"507,508"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: POVID&#x901a;&#x8fc7;AI&#x751f;&#x6210;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x6709;&#x6548;&#x7f13;&#x89e3;VLLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x7684;&#x4f9d;&#x8d56;&#x548c;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#xff0c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5f3a;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;VLLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x81ea;&#x52a8;&#x5316;&#x504f;&#x597d;&#x6570;&#x636e;&#x751f;&#x6210;&#x8303;&#x5f0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"508,510"}}],"payload":{"tag":"li","lines":"504,510","fold":1}}],"payload":{"tag":"h4","lines":"502,503"}},{"content":"V-DPO: Mitigating Hallucination in Large Vision Language Models viaVision-Guided Direct Preference Optimization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;V-DPO&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x5f15;&#x5bfc;&#x7684;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x800c;&#x5ffd;&#x89c6;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x8bad;&#x7ec3;&#x65f6;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x5b66;&#x4e60;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"511,512"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x5176;&#x5927;&#x578b;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x9aa8;&#x5e72;&#x7f51;&#x7edc;&#xff0c;&#x5bfc;&#x81f4;&#x5bf9;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x5173;&#x6ce8;&#x4e0d;&#x8db3;&#xff0c;&#x4ece;&#x800c;&#x4ea7;&#x751f;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x3002;&#x89e3;&#x51b3;&#x6b64;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"513,514"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x5f15;&#x5bfc;&#x7684;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;V-DPO&#xff09;&#xff0c;&#x8fd9;&#x662f;&#x5bf9;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x7684;&#x89c6;&#x89c9;&#x7279;&#x5b9a;&#x53d8;&#x4f53;&#x3002;&#x65b9;&#x6cd5;&#x6838;&#x5fc3;&#x662f;&#x5c06;&#x5206;&#x7c7b;&#x5668;&#x81ea;&#x7531;&#x5f15;&#x5bfc;&#xff08;CFG&#xff09;&#x96c6;&#x6210;&#x5230;&#x4f18;&#x5316;&#x76ee;&#x6807;&#x4e2d;&#xff0c;&#x4ee5;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;V-DPO&#x901a;&#x8fc7;&#x4fee;&#x6539;DPO&#x7684;&#x76ee;&#x6807;&#x51fd;&#x6570;&#xff0c;&#x52a0;&#x5165;&#x89c6;&#x89c9;&#x5f15;&#x5bfc;&#x9879;&#xff08;&#x5982;KL&#x6563;&#x5ea6;&#x9879;&#xff09;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x5728;&#x8bad;&#x7ec3;&#x65f6;&#x66f4;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x6761;&#x4ef6;&#x5206;&#x5e03;&#x4e0e;&#x7eaf;&#x6587;&#x672c;&#x5206;&#x5e03;&#x4e4b;&#x95f4;&#x7684;&#x5dee;&#x5f02;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x4f5c;&#x8005;&#x6784;&#x5efa;&#x4e86;&#x4e00;&#x4e2a;&#x5305;&#x542b;&#x54cd;&#x5e94;&#x5bf9;&#x6bd4;&#x548c;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x504f;&#x597d;&#x5bf9;&#x7684;&#x5408;&#x6210;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x7528;&#x4e8e;&#x8bad;&#x7ec3;&#x548c;&#x8bc4;&#x4f30;&#x3002;","children":[],"payload":{"tag":"li","lines":"514,515"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: V-DPO&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x6539;&#x8fdb;&#x3002;&#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;V-DPO&#x5c24;&#x5176;&#x64c5;&#x957f;&#x4ece;&#x56fe;&#x50cf;&#x5bf9;&#x6bd4;&#x504f;&#x597d;&#x6570;&#x636e;&#x4e2d;&#x5b66;&#x4e60;&#xff0c;&#x5c55;&#x73b0;&#x4e86;&#x5176;&#x5353;&#x8d8a;&#x7684;&#x6355;&#x6349;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x7ec6;&#x5fae;&#x5dee;&#x522b;&#x7684;&#x80fd;&#x529b;&#x3002;&#x5206;&#x5e03;&#x504f;&#x79fb;&#x5206;&#x6790;&#x8fdb;&#x4e00;&#x6b65;&#x8bc1;&#x5b9e;&#x4e86;V-DPO&#x6709;&#x6548;&#x51cf;&#x8f7b;&#x4e86;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x3002;","children":[],"payload":{"tag":"li","lines":"515,516"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x662f;V-DPO&#x901a;&#x8fc7;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x589e;&#x5f3a;&#x4e86;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x6ce8;&#x91cd;&#x89c6;&#x89c9;&#x57fa;&#x7840;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x5c55;&#x793a;&#x4e86;&#x5408;&#x6210;&#x6570;&#x636e;&#x5728;&#x504f;&#x597d;&#x5b66;&#x4e60;&#x4e2d;&#x7684;&#x4ef7;&#x503c;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x65f6;&#x95f4;&#xff0c;&#x5177;&#x6709;&#x826f;&#x597d;&#x7684;&#x6cdb;&#x5316;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"516,518"}}],"payload":{"tag":"li","lines":"512,518","fold":1}}],"payload":{"tag":"h4","lines":"510,511"}},{"content":"TPO: Token Preference Optimization with Self-Calibrated Visual-Anchored Rewards for Hallucination Mitigation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;TPO&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x6821;&#x51c6;&#x89c6;&#x89c9;&#x951a;&#x5b9a;&#x5956;&#x52b1;&#x5728;token&#x7ea7;&#x522b;&#x4f18;&#x5316;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x65e0;&#x9700;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;SOTA&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"519,520"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x5b58;&#x5728;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x5185;&#x5bb9;&#x4e0e;&#x8f93;&#x5165;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x4e0d;&#x7b26;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;DPO&#x5b58;&#x5728;&#x4e24;&#x4e2a;&#x4e3b;&#x8981;&#x7f3a;&#x9677;&#xff1a;1&#xff09;&#x7f3a;&#x4e4f;&#x53ef;&#x6269;&#x5c55;&#x7684;token&#x7ea7;&#x522b;&#x5956;&#x52b1;&#xff1b;2&#xff09;&#x5ffd;&#x89c6;&#x4e86;&#x89c6;&#x89c9;&#x951a;&#x5b9a;token&#xff08;&#x4e0e;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#x7684;&#x5173;&#x952e;token&#xff09;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x65e0;&#x6cd5;&#x7cbe;&#x7ec6;&#x8c03;&#x6574;&#x751f;&#x6210;&#x5185;&#x5bb9;&#xff0c;&#x96be;&#x4ee5;&#x4ece;&#x6839;&#x672c;&#x4e0a;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"521,522"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;TPO&#xff08;Token Preference Optimization&#xff09;&#x6846;&#x67b6;&#xff1a;1&#xff09;&#x901a;&#x8fc7;&#x5411;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#x751f;&#x6210;&#x635f;&#x574f;&#x56fe;&#x50cf;&#xff0c;&#x8ba1;&#x7b97;&#x6bcf;&#x4e2a;&#x751f;&#x6210;token&#x5728;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x635f;&#x574f;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#x4e0b;&#x7684;&#x903b;&#x8f91;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff0c;&#x4f5c;&#x4e3a;token&#x7ea7;&#x522b;&#x7684;&#x89c6;&#x89c9;&#x951a;&#x5b9a;&#x5956;&#x52b1;&#xff08;s_yi&#xff09;&#xff0c;&#x5dee;&#x5f02;&#x8d8a;&#x5927;&#x8868;&#x793a;&#x8be5;token&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x8d8a;&#x654f;&#x611f;&#xff1b;2&#xff09;&#x8bbe;&#x8ba1;&#x81ea;&#x6821;&#x51c6;&#x8fc7;&#x7a0b;&#xff0c;&#x5c06;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#x901a;&#x8fc7;sigmoid&#x51fd;&#x6570;&#x5f52;&#x4e00;&#x5316;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x8fb9;&#x754c;&#x503c;a=0.5&#xff0c;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x5956;&#x52b1;c_yi&#xff0c;&#x786e;&#x4fdd;&#x6b63;&#x6837;&#x672c;&#x5956;&#x52b1;&#x9ad8;&#x4e8e;&#x8d1f;&#x6837;&#x672c;&#xff1b;3&#xff09;&#x5c06;token&#x5956;&#x52b1;&#x96c6;&#x6210;&#x5230;DPO&#x635f;&#x5931;&#x51fd;&#x6570;&#x4e2d;&#xff0c;&#x901a;&#x8fc7;&#x52a0;&#x6743;&#x4f3c;&#x7136;&#x5206;&#x5e03;&#x5b9e;&#x73b0;token&#x7ea7;&#x522b;&#x7684;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff0c;&#x91cd;&#x70b9;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x76f8;&#x5173;token&#x3002;","children":[],"payload":{"tag":"li","lines":"522,523"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1&#xff09;TPO&#x5728;LLaVA&#x548c;Qwen&#x7b49;&#x4e3b;&#x6d41;LVLM&#x57fa;&#x7840;&#x4e0a;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x6027;&#x80fd;&#xff08;&#x5177;&#x4f53;&#x63d0;&#x5347;&#x5e45;&#x5ea6;&#x56e0;&#x8bba;&#x6587;&#x672a;&#x63d0;&#x4f9b;&#x8be6;&#x7ec6;&#x6570;&#x636e;&#x800c;&#x672a;&#x5217;&#x51fa;&#xff09;&#xff1b;2&#xff09;&#x53ef;&#x89c6;&#x5316;&#x5206;&#x6790;&#xff08;&#x5982;&#x56fe;1&#x793a;&#x4f8b;&#xff09;&#x663e;&#x793a;&#xff0c;TPO&#x80fd;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x5e76;&#x5f3a;&#x5316;&#x89c6;&#x89c9;&#x951a;&#x5b9a;token&#xff08;&#x5982;&#x2018;black&#x2019;&#x3001;&#x2018;glasses&#x2019;&#xff09;&#xff0c;&#x51cf;&#x5c11;&#x9519;&#x8bef;token&#xff08;&#x5982;&#x2018;not&#x2019;&#xff09;&#x7684;&#x6743;&#x91cd;&#xff1b;3&#xff09;&#x5982;&#x8868;1&#x5bf9;&#x6bd4;&#x6240;&#x793a;&#xff0c;TPO&#x662f;&#x552f;&#x4e00;&#x540c;&#x65f6;&#x5b9e;&#x73b0;&#x89c6;&#x89c9;&#x951a;&#x5b9a;&#x5173;&#x6ce8;&#x3001;token&#x7ea7;&#x522b;&#x5956;&#x52b1;&#x4e14;&#x65e0;&#x9700;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"523,524"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: TPO&#x901a;&#x8fc7;&#x81ea;&#x6821;&#x51c6;&#x89c6;&#x89c9;&#x951a;&#x5b9a;&#x5956;&#x52b1;&#x5b9e;&#x73b0;&#x4e86;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x7684;token&#x7ea7;&#x522b;&#x4f18;&#x5316;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x7cbe;&#x7ec6;&#x5956;&#x52b1;&#x673a;&#x5236;&#xff0c;&#x8fd8;&#x7a81;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x5173;&#x952e;token&#x7684;&#x91cd;&#x8981;&#x6027;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLMs&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4ee5;&#x53ca;&#x63a8;&#x52a8;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x8054;&#x5408;&#x5efa;&#x6a21;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"524,526"}}],"payload":{"tag":"li","lines":"520,526","fold":1}}],"payload":{"tag":"h4","lines":"518,519"}},{"content":"OViP: Online Vision-Language Preference Learning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;OViP&#x7684;&#x5728;&#x7ebf;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x504f;&#x597d;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x751f;&#x6210;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#x6765;&#x6784;&#x5efa;&#x5bf9;&#x6bd4;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x5176;&#x591a;&#x6a21;&#x6001;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"527,528"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x6216;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x9884;&#x5b9a;&#x4e49;&#x6216;&#x968f;&#x673a;&#x7f16;&#x8f91;&#x7684;&#x8d1f;&#x6837;&#x672c;&#xff0c;&#x65e0;&#x6cd5;&#x53cd;&#x6620;&#x6a21;&#x578b;&#x7684;&#x5b9e;&#x9645;&#x9519;&#x8bef;&#x6a21;&#x5f0f;&#xff0c;&#x5bfc;&#x81f4;&#x8bad;&#x7ec3;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;","children":[],"payload":{"tag":"li","lines":"529,530"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: OViP&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x6b65;&#x9aa4;&#xff1a;1) &#x5b9e;&#x65f6;&#x91c7;&#x6837;&#x6a21;&#x578b;&#x7684;&#x591a;&#x7ec4;&#x54cd;&#x5e94;&#xff0c;&#x5e76;&#x7528;&#x5916;&#x90e8;LLM&#x8bc4;&#x4f30;&#x5176;&#x4e0e;&#x771f;&#x5b9e;&#x7b54;&#x6848;&#x7684;&#x5339;&#x914d;&#x5ea6;&#xff1b;2) &#x6839;&#x636e;&#x5956;&#x52b1;&#x5206;&#x6570;&#x5dee;&#x5f02;&#x52a8;&#x6001;&#x7b5b;&#x9009;&#x9ad8;&#x8d28;&#x91cf;&#xff08;&#x6b63;&#xff09;&#x548c;&#x4f4e;&#x8d28;&#x91cf;&#xff08;&#x8d1f;&#xff09;&#x54cd;&#x5e94;&#x5bf9;&#xff1b;3) &#x5229;&#x7528;LLM&#x5206;&#x6790;&#x8bed;&#x4e49;&#x5dee;&#x5f02;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x6269;&#x6563;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x8d1f;&#x54cd;&#x5e94;&#x5bf9;&#x5e94;&#x7684;&#x8d1f;&#x56fe;&#x50cf;&#x3002;&#x6700;&#x540e;&#xff0c;&#x7ed3;&#x5408;&#x6587;&#x672c;&#x548c;&#x56fe;&#x50cf;&#x7ea7;&#x522b;&#x7684;&#x5bf9;&#x6bd4;&#x635f;&#x5931;&#xff08;DPO&#x635f;&#x5931;&#xff09;&#x8fdb;&#x884c;&#x8054;&#x5408;&#x4f18;&#x5316;&#xff0c;&#x5b9e;&#x73b0;&#x81ea;&#x9002;&#x5e94;&#x5728;&#x7ebf;&#x5b66;&#x4e60;&#x3002;","children":[],"payload":{"tag":"li","lines":"530,531"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;OViP&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x548c;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#xff08;&#x4f8b;&#x5982;&#x5728;&#x7279;&#x5b9a;&#x4efb;&#x52a1;&#x4e2d;&#x5c06;&#x9519;&#x8bef;&#x7387;&#x4ece;70.9%&#x964d;&#x81f3;7.1%&#xff09;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x7684;&#x6838;&#x5fc3;&#x80fd;&#x529b;&#x3002;&#x4e0e;&#x4f20;&#x7edf;&#x79bb;&#x7ebf;&#x65b9;&#x6cd5;&#x76f8;&#x6bd4;&#xff0c;&#x8bad;&#x7ec3;&#x6548;&#x7387;&#x66f4;&#x9ad8;&#xff0c;&#x4e14;&#x907f;&#x514d;&#x4e86;&#x8fc7;&#x62df;&#x5408;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"531,532"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: OViP&#x901a;&#x8fc7;&#x5b9e;&#x65f6;&#x6355;&#x6349;&#x6a21;&#x578b;&#x9519;&#x8bef;&#x5e76;&#x751f;&#x6210;&#x9488;&#x5bf9;&#x6027;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x66f4;&#x6709;&#x6548;&#x7684;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x5728;&#x7ebf;&#x5b66;&#x4e60;&#x548c;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6846;&#x67b6;&#x4e3a;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x9002;&#x7528;&#x6027;&#xff0c;&#x5e76;&#x4e3a;&#x6301;&#x7eed;&#x5b66;&#x4e60;&#x8303;&#x5f0f;&#x63d0;&#x4f9b;&#x53c2;&#x8003;&#x3002;","children":[],"payload":{"tag":"li","lines":"532,534"}}],"payload":{"tag":"li","lines":"528,534","fold":1}}],"payload":{"tag":"h4","lines":"526,527"}},{"content":"TL-DPO: Stop learning it all to mitigate visual hallucination, Focus on the hallucination target.","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;TL-DPO&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x805a;&#x7126;&#x4e8e;&#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x7684;&#x5177;&#x4f53;&#x76ee;&#x6807;&#x533a;&#x57df;&#xff08;&#x56fe;&#x50cf;&#x533a;&#x57df;&#x548c;&#x6587;&#x672c;&#x7247;&#x6bb5;&#xff09;&#xff0c;&#x800c;&#x975e;&#x5168;&#x5c40;&#x7279;&#x5f81;&#xff0c;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"535,536"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff0c;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x4fe1;&#x606f;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x9700;&#x8981;&#x7cbe;&#x786e;&#x5bf9;&#x8c61;&#x8bc6;&#x522b;&#x7684;&#x573a;&#x666f;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x4e8e;&#x504f;&#x597d;&#x7684;&#x5b66;&#x4e60;&#x65b9;&#x6cd5;&#xff08;&#x5982;RLHF&#xff09;&#x867d;&#x80fd;&#x63d0;&#x5347;&#x6574;&#x4f53;&#x8f93;&#x51fa;&#x8d28;&#x91cf;&#xff0c;&#x4f46;&#x5176;&#x5168;&#x5c40;&#x7279;&#x5f81;&#x5b66;&#x4e60;&#x65b9;&#x5f0f;&#x96be;&#x4ee5;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x5bf9;&#x8c61;&#x7ea7;&#x522b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4eec;&#x53ef;&#x80fd;&#x5b66;&#x4e60;&#x4e86;&#x4e0e;&#x5e7b;&#x89c9;&#x65e0;&#x5173;&#x7684;&#x4fe1;&#x53f7;&#xff0c;&#x5bfc;&#x81f4;&#x6821;&#x6b63;&#x6548;&#x679c;&#x4e0d;&#x4f73;&#x548c;&#x610f;&#x5916;&#x526f;&#x4f5c;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"537,538"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;TL-DPO&#xff08;Target-Learning Direct Preference Optimization&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6838;&#x5fc3;&#x662f;&#x201c;&#x76ee;&#x6807;&#x5b66;&#x4e60;&#x201d;&#xff08;Target Learning&#xff09;&#xff0c;&#x5176;&#x5173;&#x952e;&#x6b65;&#x9aa4;&#x5982;&#x4e0b;&#xff1a;1) &#x6784;&#x5efa;&#x4e00;&#x4e2a;&#x5305;&#x542b;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#x3001;&#x6b63;&#x786e;&#x54cd;&#x5e94;&#x548c;&#x76ee;&#x6807;&#x4fe1;&#x606f;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x4e2d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x53ca;&#x54cd;&#x5e94;&#x4e2d;&#x53d7;&#x5e7b;&#x89c9;&#x5f71;&#x54cd;&#x7684;&#x5177;&#x4f53;&#x7247;&#x6bb5;&#x4f4d;&#x7f6e;&#xff09;&#x7684;&#x6570;&#x636e;&#x96c6;&#xff1b;2) &#x5728;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x4ec5;&#x5c06;&#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x7684;&#x56fe;&#x50cf;&#x533a;&#x57df;&#x548c;&#x54cd;&#x5e94;&#x4e2d;&#x7684;&#x7279;&#x5b9a;&#x7247;&#x6bb5;&#x6807;&#x8bb0;&#x4e3a;&#x5b66;&#x4e60;&#x76ee;&#x6807;&#xff0c;&#x800c;&#x5c06;&#x5176;&#x4ed6;&#x90e8;&#x5206;&#x89c6;&#x4e3a;&#x65e0;&#x5173;&#x4fe1;&#x53f7;&#x5e76;&#x6392;&#x9664;&#xff1b;3) &#x8bbe;&#x8ba1;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x7684;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff0c;&#x8be5;&#x51fd;&#x6570;&#x57fa;&#x4e8e;DPO&#x6846;&#x67b6;&#xff0c;&#x4f46;&#x5c06;&#x6bd4;&#x8f83;&#x548c;&#x4f18;&#x5316;&#x9650;&#x5236;&#x5728;&#x76ee;&#x6807;&#x7247;&#x6bb5;&#x4e0a;&#xff0c;&#x901a;&#x8fc7;Bradley-Terry&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x504f;&#x597d;&#x5b66;&#x4e60;&#xff0c;&#x786e;&#x4fdd;&#x6a21;&#x578b;&#x53ea;&#x5173;&#x6ce8;&#x6821;&#x6b63;&#x5e7b;&#x89c9;&#x76f8;&#x5173;&#x7684;&#x4fe1;&#x53f7;&#x3002;","children":[],"payload":{"tag":"li","lines":"538,539"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;TL-DPO&#x5728;&#x591a;&#x4e2a;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x4efb;&#x52a1;&#x4e0a;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x4e0e;&#x4f20;&#x7edf;&#x7684;&#x5168;&#x5c40;&#x504f;&#x597d;&#x5b66;&#x4e60;&#x65b9;&#x6cd5;&#x76f8;&#x6bd4;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63d0;&#x9ad8;&#x6a21;&#x578b;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x5e76;&#x672a;&#x524a;&#x5f31;&#x5176;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5b9e;&#x73b0;&#x4e86;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x5b66;&#x4e60;&#xff0c;&#x56e0;&#x4e3a;&#x6a21;&#x578b;&#x53ea;&#x9700;&#x5173;&#x6ce8;&#x7279;&#x5b9a;&#x76ee;&#x6807;&#x800c;&#x975e;&#x6574;&#x4e2a;&#x54cd;&#x5e94;&#x6216;&#x56fe;&#x50cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"539,540"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x4e13;&#x6ce8;&#x4e8e;&#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x7684;&#x5177;&#x4f53;&#x76ee;&#x6807;&#xff08;&#x800c;&#x975e;&#x5168;&#x5c40;&#x8f93;&#x51fa;&#xff09;&#x8fdb;&#x884c;&#x5b66;&#x4e60;&#xff0c;&#x53ef;&#x4ee5;&#x66f4;&#x6709;&#x6548;&#x3001;&#x66f4;&#x9ad8;&#x6548;&#x5730;&#x51cf;&#x8f7b;MLLMs&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;TL-DPO&#x65b9;&#x6cd5;&#x4e3a;&#x6539;&#x8fdb;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65b0;&#x9014;&#x5f84;&#xff0c;&#x5176;&#x7406;&#x8bba;&#x6846;&#x67b6;&#xff08;&#x5982;Theorem 3.3&#xff09;&#x8bc1;&#x660e;&#x4e86;&#x76ee;&#x6807;&#x5b66;&#x4e60;&#x7684;&#x7b49;&#x4ef7;&#x6027;&#x548c;&#x4f18;&#x8d8a;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x3001;&#x66f4;&#x7cbe;&#x51c6;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x7684;&#x7406;&#x8bba;&#x548c;&#x5b9e;&#x8df5;&#x57fa;&#x7840;&#xff0c;&#x5c24;&#x5176;&#x9002;&#x7528;&#x4e8e;&#x5bf9;&#x51c6;&#x786e;&#x6027;&#x8981;&#x6c42;&#x9ad8;&#x7684;&#x5e94;&#x7528;&#x9886;&#x57df;&#x3002;","children":[],"payload":{"tag":"li","lines":"540,542"}}],"payload":{"tag":"li","lines":"536,542","fold":1}}],"payload":{"tag":"h4","lines":"534,535"}},{"content":"GRPO: Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x9996;&#x4e2a;&#x8bc4;&#x4f30;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x9000;&#x5316;&#x6587;&#x6863;OCR&#x4efb;&#x52a1;&#x4e2d;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x7684;&#x57fa;&#x51c6;KIE-HVQA&#xff0c;&#x5e76;&#x57fa;&#x4e8e;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x6846;&#x67b6;GRPO&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5728;&#x5173;&#x952e;&#x4fe1;&#x606f;&#x63d0;&#x53d6;&#x4efb;&#x52a1;&#x4e0a;&#x76f8;&#x6bd4;GPT-4o&#x63d0;&#x5347;28%&#x7684;&#x51c6;&#x786e;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"543,544"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x73b0;&#x6709;MLLMs&#x5728;&#x5904;&#x7406;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x4e2d;&#x9000;&#x5316;&#x6587;&#x6863;&#xff08;&#x5982;&#x6a21;&#x7cca;&#x3001;&#x906e;&#x6321;&#x3001;&#x4f4e;&#x5bf9;&#x6bd4;&#x5ea6;&#xff09;&#x65f6;&#xff0c;&#x7531;&#x4e8e;&#x7f3a;&#x4e4f;&#x5bf9;&#x89c6;&#x89c9;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x7684;&#x611f;&#x77e5;&#xff0c;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#xff0c;&#x5bfc;&#x81f4;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x56de;&#x7b54;&#x3002;&#x8fd9;&#x5728;&#x8eab;&#x4efd;&#x8bc1;&#x3001;&#x53d1;&#x7968;&#x3001;&#x5904;&#x65b9;&#x7b49;&#x5173;&#x952e;&#x4fe1;&#x606f;&#x63d0;&#x53d6;&#x573a;&#x666f;&#x4e2d;&#x53ef;&#x80fd;&#x5f15;&#x53d1;&#x4e25;&#x91cd;&#x9519;&#x8bef;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x89e3;&#x51b3;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x7684;&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"545,546"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x9636;&#x6bb5;&#x65b9;&#x6cd5;&#xff1a;1) &#x6784;&#x5efa;KIE-HVQA&#x57fa;&#x51c6;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5305;&#x542b;2000&#x4e2a;&#x8bad;&#x7ec3;&#x6837;&#x672c;&#x548c;400&#x4e2a;&#x6d4b;&#x8bd5;&#x6837;&#x672c;&#xff0c;&#x6a21;&#x62df;&#x771f;&#x5b9e;&#x9000;&#x5316;&#x573a;&#x666f;&#x5e76;&#x63d0;&#x4f9b;&#x50cf;&#x7d20;&#x7ea7;OCR&#x53ef;&#x9760;&#x6027;&#x6807;&#x6ce8;&#xff1b;2) &#x8bbe;&#x8ba1;&#x57fa;&#x4e8e;Group Relative Policy Optimization&#xff08;GRPO&#xff09;&#x7684;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5956;&#x52b1;&#x673a;&#x5236;&#x9f13;&#x52b1;&#x6a21;&#x578b;&#x5728;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x4e0d;&#x786e;&#x5b9a;&#x65f6;&#x62d2;&#x7edd;&#x56de;&#x7b54;&#x6216;&#x8c28;&#x614e;&#x63a8;&#x7406;&#xff0c;&#x51cf;&#x5c11;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x4f9d;&#x8d56;&#x3002;","children":[],"payload":{"tag":"li","lines":"546,547"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;KIE-HVQA&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x57fa;&#x4e8e;Qwen2.5-VL-7B&#x7684;&#x6a21;&#x578b;&#x76f8;&#x6bd4;GPT-4o&#x5728;&#x65e0;&#x5e7b;&#x89c9;&#x51c6;&#x786e;&#x7387;&#x4e0a;&#x7edd;&#x5bf9;&#x63d0;&#x5347;&#x7ea6;28%&#xff0c;&#x4e14;&#x5728;&#x6807;&#x51c6;OCR&#x4efb;&#x52a1;&#x4e0a;&#x672a;&#x51fa;&#x73b0;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x3002;&#x6a21;&#x578b;&#x80fd;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x9000;&#x5316;&#x533a;&#x57df;&#x5e76;&#x907f;&#x514d;&#x751f;&#x6210;&#x865a;&#x5047;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"547,548"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x5de5;&#x4f5c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x4e13;&#x7528;&#x57fa;&#x51c6;&#x548c;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;MLLMs&#x5728;&#x9000;&#x5316;&#x6587;&#x6863;&#x4e0b;&#x7684;&#x89c6;&#x89c9;&#x5fe0;&#x5b9e;&#x63a8;&#x7406;&#x80fd;&#x529b;&#xff0c;&#x4e3a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6587;&#x6863;&#x5206;&#x6790;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x57fa;&#x7840;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x66f4;&#x590d;&#x6742;&#x7684;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"548,550"}}],"payload":{"tag":"li","lines":"544,550","fold":1}}],"payload":{"tag":"h4","lines":"542,543"}},{"content":"TARS: MinMax Token-Adaptive Preference Strategy for MLLM Hallucination Reduction","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;TARS&#xff0c;&#x4e00;&#x79cd;&#x9488;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x4ee4;&#x724c;&#x81ea;&#x9002;&#x5e94;&#x504f;&#x597d;&#x4f18;&#x5316;&#x7b56;&#x7565;&#x3002;TARS&#x5c06;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x91cd;&#x65b0;&#x8868;&#x8ff0;&#x4e3a;&#x4e00;&#x4e2a;&#x6700;&#x5c0f;-&#x6700;&#x5927;&#x4f18;&#x5316;&#x95ee;&#x9898;&#xff0c;&#x901a;&#x8fc7;&#x6270;&#x52a8;&#x89c6;&#x89c9;&#x65e0;&#x5173;&#x4ee4;&#x724c;&#x6765;&#x6a21;&#x62df;&#x8bed;&#x4e49;&#x53d8;&#x5316;&#xff0c;&#x540c;&#x65f6;&#x6700;&#x5c0f;&#x5316;&#x504f;&#x597d;&#x635f;&#x5931;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x4f7f;&#x7528;4.8k&#x4e2a;&#x504f;&#x597d;&#x6837;&#x672c;&#xff0c;&#x65e0;&#x9700;&#x4e13;&#x5bb6;&#x53cd;&#x9988;&#xff0c;&#x5c31;&#x5c06;&#x5e7b;&#x89c9;&#x7387;&#x4ece;26.4%&#x964d;&#x81f3;13.2&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x5173;&#x952e;&#x6307;&#x6807;&#x4e0a;&#x5339;&#x914d;&#x4e86;GPT-4o&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"551,552"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x63a8;&#x7406;&#x4e2d;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x4e8b;&#x5b9e;&#x4e0a;&#x9519;&#x8bef;&#x6216;&#x7f3a;&#x4e4f;&#x89c6;&#x89c9;&#x4f9d;&#x636e;&#x7684;&#x8f93;&#x51fa;&#xff0c;&#x5373;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x3002;&#x73b0;&#x6709;&#x7684;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x65b9;&#x6cd5;&#x867d;&#x7136;&#x5e38;&#x7528;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e00;&#x4e2a;&#x6838;&#x5fc3;&#x5c40;&#x9650;&#xff1a;&#x5b83;&#x4eec;&#x5c06;&#x504f;&#x597d;&#x6570;&#x636e;&#x89c6;&#x4e3a;&#x9759;&#x6001;&#x76ee;&#x6807;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x62df;&#x5408;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e2d;&#x7684;&#x6d45;&#x5c42;&#x6587;&#x672c;&#x7ebf;&#x7d22;&#xff08;&#x5982;&#x9ad8;&#x9891;&#x77ed;&#x8bed;&#xff09;&#xff0c;&#x800c;&#x975e;&#x771f;&#x6b63;&#x5173;&#x6ce8;&#x56e0;&#x679c;&#x76f8;&#x5173;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x3002;&#x8fd9;&#x79cd;&#x201c;&#x5206;&#x5e03;&#x521a;&#x6027;&#x201d;&#x4f7f;&#x5f97;&#x6a21;&#x578b;&#x5728;&#x9047;&#x5230;&#x65b0;&#x7684;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x4e0a;&#x4e0b;&#x6587;&#x65f6;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x5dee;&#xff0c;&#x66f4;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"553,554"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;TARS&#xff08;Token-Adaptive Preference Strategy&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x4e00;&#x4e2a;&#x6700;&#x5c0f;-&#x6700;&#x5927;&#x4f18;&#x5316;&#x6846;&#x67b6;&#xff1a;","children":[{"content":"1. <strong>&#x6700;&#x5927;&#x5316;&#xff08;&#x5185;&#x5c42;&#xff09;</strong>&#xff1a;&#x5728;&#x660e;&#x786e;&#x7684;&#x8bed;&#x4e49;&#x7ea6;&#x675f;&#x4e0b;&#xff0c;&#x5bf9;&#x8f93;&#x5165;&#x6587;&#x672c;&#x4e2d;&#x7684;&#x201c;&#x89c6;&#x89c9;&#x65e0;&#x5173;&#x4ee4;&#x724c;&#x201d;&#xff08;Visual-agnostic Tokens&#xff0c;&#x5982;&#x4ecb;&#x8bcd;&#x3001;&#x8fde;&#x8bcd;&#x7b49;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x5173;&#x8054;&#x6027;&#x5f31;&#x7684;&#x8bcd;&#xff09;&#x65bd;&#x52a0;&#x6270;&#x52a8;&#xff08;Perturbation&#xff09;&#xff0c;&#x4ee5;&#x6a21;&#x62df;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x53d8;&#x5316;&#x548c;&#x8f93;&#x5165;&#x5206;&#x5e03;&#x7684;&#x504f;&#x79fb;&#xff0c;&#x800c;&#x4e0d;&#x6539;&#x53d8;&#x6574;&#x4f53;&#x8bed;&#x4e49;&#x3002;&#x8fd9;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x4e0d;&#x80fd;&#x4f9d;&#x8d56;&#x8fd9;&#x4e9b;&#x4e0d;&#x53ef;&#x9760;&#x7684;&#x6587;&#x672c;&#x7ebf;&#x7d22;&#x3002;","children":[],"payload":{"tag":"li","lines":"555,556","listIndex":1}},{"content":"2. <strong>&#x6700;&#x5c0f;&#x5316;&#xff08;&#x5916;&#x5c42;&#xff09;</strong>&#xff1a;&#x5728;&#x65bd;&#x52a0;&#x4e86;&#x4e0a;&#x8ff0;&#x6270;&#x52a8;&#x7684;&#x65b0;&#x5206;&#x5e03;&#x4e0b;&#xff0c;&#x4f7f;&#x7528;&#x6807;&#x51c6;&#x7684;DPO&#x76ee;&#x6807;&#x51fd;&#x6570;&#x6765;&#x6700;&#x5c0f;&#x5316;&#x504f;&#x597d;&#x635f;&#x5931;&#xff0c;&#x786e;&#x4fdd;&#x6a21;&#x578b;&#x7684;&#x8f93;&#x51fa;&#x4e0e;&#x4eba;&#x7c7b;&#x504f;&#x597d;&#xff08;&#x9009;&#x62e9;&#x975e;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff09;&#x5bf9;&#x9f50;&#x3002;<br>\n&#x6b64;&#x5916;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x8fd8;&#x5f15;&#x5165;&#x4e86;&#x4e00;&#x4e2a;&#x57fa;&#x4e8e;&#x9891;&#x7387;&#x7684;&#x6b63;&#x5219;&#x5316;&#x5668;&#xff08;Spectral Preference Alignment&#xff09;&#xff0c;&#x4ee5;&#x8fdb;&#x4e00;&#x6b65;&#x589e;&#x5f3a;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#x3002;&#x6574;&#x4e2a;&#x65b9;&#x6cd5;&#x8f7b;&#x91cf;&#x4e14;&#x9ad8;&#x6548;&#xff0c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x989d;&#x5916;&#x7684;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"556,558","listIndex":2}}],"payload":{"tag":"li","lines":"554,558"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x8bba;&#x6587;&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x8fdb;&#x884c;&#x4e86;&#x8bc4;&#x4f30;&#xff0c;&#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5982;&#x4e0b;&#xff1a;","children":[{"content":"<strong>&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;</strong>&#xff1a;&#x5728;LLaVA-v1.5&#x6a21;&#x578b;&#x4e0a;&#xff0c;TARS&#x5c06;&#x5e7b;&#x89c9;&#x7387;&#x4ece;26.4%&#x5927;&#x5e45;&#x964d;&#x4f4e;&#x81f3;13.2%&#x3002;","children":[],"payload":{"tag":"li","lines":"559,560"}},{"content":"<strong>&#x8d85;&#x8d8a;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;</strong>&#xff1a;TARS&#x7684;&#x8868;&#x73b0; consistently&#xff08;&#x6301;&#x7eed;&#x5730;&#xff09;&#x4f18;&#x4e8e;&#x6807;&#x51c6;&#x7684;DPO&#x4ee5;&#x53ca;&#x5176;&#x4ed6;&#x5148;&#x8fdb;&#x7684;&#x504f;&#x597d;&#x4f18;&#x5316;&#x65b9;&#x6cd5;&#xff08;&#x5982;RLHF, RLAIF, HALVA&#x7b49;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"560,561"}},{"content":"<strong>&#x5339;&#x914d;&#x9876;&#x5c16;&#x5546;&#x4e1a;&#x6a21;&#x578b;</strong>&#xff1a;&#x5728;AMBER&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x7684;&#x591a;&#x4e2a;&#x5173;&#x952e;&#x6307;&#x6807;&#x4e0a;&#xff0c;TARS&#x589e;&#x5f3a;&#x540e;&#x7684;LLaVA-v1.5&#x6a21;&#x578b;&#x6027;&#x80fd;&#x4e0e;OpenAI&#x7684;GPT-4o&#x76f8;&#x5f53;&#x3002;","children":[],"payload":{"tag":"li","lines":"561,562"}},{"content":"<strong>&#x9ad8;&#x6548;&#x7684;&#x6570;&#x636e;&#x5229;&#x7528;</strong>&#xff1a;&#x4ec5;&#x4f7f;&#x7528;&#x4e86;4.8k&#x4e2a;&#x504f;&#x597d;&#x6837;&#x672c;&#xff08;&#x4e14;&#x65e0;&#x9700;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x7c7b;&#x4e13;&#x5bb6;&#x53cd;&#x9988;&#xff09;&#x5c31;&#x8fbe;&#x5230;&#x4e86;&#x4e0a;&#x8ff0;&#x6548;&#x679c;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x9ad8;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"562,563"}},{"content":"<strong>&#x6ce8;&#x610f;&#x529b;&#x53ef;&#x89c6;&#x5316;&#x5206;&#x6790;</strong>&#xff1a;&#x5982;&#x56fe;2(d)(e)&#x6240;&#x793a;&#xff0c;TARS&#x8bad;&#x7ec3;&#x51fa;&#x7684;&#x6a21;&#x578b;&#x5176;&#x6ce8;&#x610f;&#x529b;&#x66f4;&#x96c6;&#x4e2d;&#x4e8e;&#x4e0e;&#x89c6;&#x89c9;&#x56e0;&#x679c;&#x76f8;&#x5173;&#x7684;&#x8bed;&#x4e49;&#x7ebf;&#x7d22;&#x4e0a;&#xff0c;&#x800c;&#x6807;&#x51c6;DPO&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5219;&#x5206;&#x6563;&#x4e8e;&#x4e00;&#x4e9b;&#x865a;&#x5047;&#x76f8;&#x5173;&#x7684;&#x4ee4;&#x724c;&#x4e0a;&#x3002;","children":[],"payload":{"tag":"li","lines":"563,564"}}],"payload":{"tag":"li","lines":"558,564"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;TARS&#x901a;&#x8fc7;&#x5176;&#x6700;&#x5c0f;-&#x6700;&#x5927;&#x4ee4;&#x724c;&#x81ea;&#x9002;&#x5e94;&#x7b56;&#x7565;&#xff0c;&#x6210;&#x529f;&#x5730;&#x89e3;&#x51b3;&#x4e86;&#x4f20;&#x7edf;DPO&#x65b9;&#x6cd5;&#x7684;&#x5206;&#x5e03;&#x521a;&#x6027;&#x548c;&#x8fc7;&#x62df;&#x5408;&#x95ee;&#x9898;&#x3002;&#x5b83;&#x4f7f;&#x6a21;&#x578b;&#x5b66;&#x4f1a;&#x4f9d;&#x8d56;&#x56e0;&#x679c;&#x76f8;&#x5173;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x800c;&#x975e;&#x865a;&#x5047;&#x7684;&#x6587;&#x672c;&#x5173;&#x8054;&#xff0c;&#x4ece;&#x800c;&#x663e;&#x8457;&#x4e14;&#x9c81;&#x68d2;&#x5730;&#x51cf;&#x5c11;&#x4e86;MLLM&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x8868;&#x660e;&#xff0c;&#x5728;&#x504f;&#x597d;&#x4f18;&#x5316;&#x4e2d;&#x5f15;&#x5165;&#x53d7;&#x63a7;&#x7684;&#x3001;&#x8bed;&#x4e49;&#x4fdd;&#x6301;&#x7684;&#x6270;&#x52a8;&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x9014;&#x5f84;&#xff0c;&#x80fd;&#x591f;&#x663e;&#x8457;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x5b89;&#x5168;&#x3001;&#x66f4;&#x5b9e;&#x7528;&#x7684;MLLM&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x7684;&#x6280;&#x672f;&#x65b9;&#x5411;&#x3002;&#x5176;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x540e;&#x7eed;&#x7684;&#x6a21;&#x578b;&#x5bf9;&#x9f50;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5373;&#x901a;&#x8fc7;&#x6a21;&#x62df;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x6765;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x7684;&#x6cdb;&#x5316;&#x548c; grounding &#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"564,566"}}],"payload":{"tag":"li","lines":"552,566","fold":1}}],"payload":{"tag":"h4","lines":"550,551"}},{"content":"CHAIR-DPO: Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;CHAIR-DPO&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;CHAIR&#x6307;&#x6807;&#x81ea;&#x52a8;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x901a;&#x8fc7;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x5fae;&#x8c03;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"567,568"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x56de;&#x7b54;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x8fd9;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x590d;&#x6742;&#x6d41;&#x6c34;&#x7ebf;&#x6216;&#x79c1;&#x6709;&#x6a21;&#x578b;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5dee;&#x3002;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"569,570"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6570;&#x636e;&#x6536;&#x96c6;&#xff1a;&#x4f7f;&#x7528;&#x5f00;&#x6e90;MLLM&#x4e3a;&#x540c;&#x4e00;&#x56fe;&#x6587;&#x8f93;&#x5165;&#x751f;&#x6210;&#x4e24;&#x4e2a;&#x56de;&#x7b54;&#xff0c;&#x901a;&#x8fc7;&#x9884;&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x5668;&#x63d0;&#x53d6;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x771f;&#x5b9e;&#x5bf9;&#x8c61;&#x96c6;&#x5408;&#xff1b;2. &#x504f;&#x597d;&#x6807;&#x6ce8;&#xff1a;&#x5229;&#x7528;CHAIR&#x6307;&#x6807;&#x8ba1;&#x7b97;&#x6bcf;&#x4e2a;&#x56de;&#x7b54;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff08; hallucinated objects&#x4e0e;mentioned objects&#x7684;&#x6bd4;&#x4f8b;&#xff09;&#xff0c;&#x9009;&#x62e9;&#x5e7b;&#x89c9;&#x7387;&#x8f83;&#x4f4e;&#x7684;&#x56de;&#x7b54;&#x4f5c;&#x4e3a;&#x4f18;&#x80dc;&#x6837;&#x672c;&#xff08;winner&#xff09;&#xff0c;&#x8f83;&#x9ad8;&#x8005;&#x4e3a;&#x52a3;&#x6c70;&#x6837;&#x672c;&#xff08;loser&#xff09;&#xff1b;3. &#x6a21;&#x578b;&#x4f18;&#x5316;&#xff1a;&#x57fa;&#x4e8e;&#x6784;&#x5efa;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x5bf9;&#xff0c;&#x91c7;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x7b97;&#x6cd5;&#x5fae;&#x8c03;MLLM&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x504f;&#x597d;&#x751f;&#x6210;&#x65e0;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#x3002;","children":[],"payload":{"tag":"li","lines":"570,571"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;AMBER&#x3001;CHAIR-MSCOCO&#x548c;Object HalBench&#x7b49;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;CHAIR-DPO&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x6027;&#x80fd;&#xff0c;&#x4e14;&#x672a;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x539f;&#x6709;&#x80fd;&#x529b;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x51c6;&#x786e;&#x6027;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"571,572"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CHAIR-DPO&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x9ad8;&#x6548;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6848;&#xff0c;&#x4ec5;&#x9700;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x548c;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x5668;&#x5373;&#x53ef;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x907f;&#x514d;&#x4e86;&#x5bf9;&#x590d;&#x6742;&#x6d41;&#x6c34;&#x7ebf;&#x6216;&#x79c1;&#x6709;&#x6a21;&#x578b;&#x7684;&#x4f9d;&#x8d56;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x5f00;&#x6e90;&#x793e;&#x533a;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"572,574"}}],"payload":{"tag":"li","lines":"568,574","fold":1}}],"payload":{"tag":"h4","lines":"566,567"}},{"content":"SCPO: Mitigating Visual Hallucinations via Semantic Curriculum Preference Optimization in MLLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;SCPO&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x8bed;&#x4e49;&#x8bfe;&#x7a0b;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x548c;&#x5bf9;&#x79f0;&#x53cc;&#x5411;&#x4f18;&#x5316;&#x76ee;&#x6807;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d; hallucination &#x7387;&#x964d;&#x4f4e;&#x9ad8;&#x8fbe;62.9%&#x3002;","children":[],"payload":{"tag":"li","lines":"575,576"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x56de;&#x7b54;&#x4e0e;&#x56fe;&#x50cf;&#x5b9e;&#x9645;&#x5185;&#x5bb9;&#x76f8;&#x77db;&#x76fe;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x8bca;&#x65ad;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x573a;&#x666f;&#x7684;&#x5e94;&#x7528;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"577,578"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x8bed;&#x4e49;&#x8bfe;&#x7a0b;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;SCPO&#xff09;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#xff1a;1&#xff09;&#x6784;&#x5efa;&#x8bed;&#x4e49;&#x8bfe;&#x7a0b;&#x504f;&#x597d;&#x5bf9;&#xff08;SCPP&#xff09;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x901a;&#x8fc7;&#x6574;&#x5408;&#x591a;&#x4e2a;&#x516c;&#x5171;&#x6570;&#x636e;&#x96c6;&#x5e76;&#x57fa;&#x4e8e;&#x8bed;&#x4e49;&#x96be;&#x5ea6;&#xff08;&#x6a21;&#x578b;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x3001;&#x8bed;&#x4e49;&#x90bb;&#x8fd1;&#x5ea6;&#x548c;&#x7ed3;&#x6784;&#x5dee;&#x5f02;&#xff09;&#x8fdb;&#x884c;&#x6613;&#x5230;&#x96be;&#x7684;&#x8bfe;&#x7a0b;&#x5212;&#x5206;&#xff1b;2&#xff09;&#x8bbe;&#x8ba1;&#x5bf9;&#x79f0;&#x53cc;&#x5411;&#x4f18;&#x5316;&#x76ee;&#x6807;&#xff0c;&#x540c;&#x65f6;&#x5b66;&#x4e60;&#x6587;&#x672c;&#x548c;&#x89c6;&#x89c9;&#x504f;&#x597d;&#xff0c;&#x907f;&#x514d;&#x6a21;&#x578b;&#x504f;&#x5411;&#x5355;&#x4e00;&#x6a21;&#x6001;&#xff1b;3&#xff09;&#x91c7;&#x7528;&#x52a8;&#x6001;&#x53c2;&#x8003;&#x6a21;&#x578b;&#x548c;&#x8fed;&#x4ee3;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#xff0c;&#x9010;&#x6b65;&#x4ece;&#x6613;&#x5230;&#x96be;&#x5b66;&#x4e60;&#xff0c;&#x63d0;&#x9ad8;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"578,579"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x89c4;&#x6a21;&#x548c;&#x7248;&#x672c;&#x7684;LLaVA&#x6a21;&#x578b;&#x4e0a;&#x8fdb;&#x884c;&#x5b9e;&#x9a8c;&#xff0c;SCPO&#x5728;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff0c;&#x6700;&#x9ad8;&#x5c06;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;&#x4e86;62.9%&#x3002;&#x540c;&#x65f6;&#xff0c;&#x5728;&#x901a;&#x7528;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;SCPO&#x5728;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x4e00;&#x822c;&#x80fd;&#x529b;&#x7684;&#x540c;&#x65f6;&#x63d0;&#x9ad8;&#x4e86;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x4e14;&#x6027;&#x80fd;&#x7a33;&#x5b9a;&#x3002;","children":[],"payload":{"tag":"li","lines":"579,580"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SCPO&#x662f;&#x9996;&#x4e2a;&#x7edf;&#x4e00;&#x8bed;&#x4e49;&#x3001;&#x5bf9;&#x79f0;&#x6027;&#x548c;&#x8bfe;&#x7a0b;&#x5b66;&#x4e60;&#x7684;MLLM&#x5bf9;&#x9f50;&#x6846;&#x67b6;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x7684;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;&#x8fd9;&#x4e00;&#x65b9;&#x6cd5;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x652f;&#x6301;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x7684;&#x9886;&#x57df;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"580,582"}}],"payload":{"tag":"li","lines":"576,582","fold":1}}],"payload":{"tag":"h4","lines":"574,575"}},{"content":"RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: RoVRM&#x662f;&#x4e00;&#x79cd;&#x9c81;&#x68d2;&#x7684;&#x89c6;&#x89c9;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;&#x4e09;&#x9636;&#x6bb5;&#x6e10;&#x8fdb;&#x5f0f;&#x8bad;&#x7ec3;&#x548c;&#x57fa;&#x4e8e;&#x6700;&#x4f18;&#x4f20;&#x8f93;&#x7684;&#x6587;&#x672c;&#x504f;&#x597d;&#x6570;&#x636e;&#x9009;&#x62e9;&#xff0c;&#x5229;&#x7528;&#x4e30;&#x5bcc;&#x7684;&#x6587;&#x672c;&#x504f;&#x597d;&#x6570;&#x636e;&#x5f25;&#x8865;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x6570;&#x636e;&#x7684;&#x7a00;&#x7f3a;&#x6027;&#xff0c;&#x4ece;&#x800c;&#x63d0;&#x5347;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e0e;&#x4eba;&#x7c7b;&#x504f;&#x597d;&#x7684;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"583,584"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5e38;&#x56e0;&#x4e0e;&#x4eba;&#x7c7b;&#x504f;&#x597d;&#x672a;&#x5bf9;&#x9f50;&#x800c;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x4e0d;&#x7b26;&#x7684;&#x8bef;&#x5bfc;&#x5185;&#x5bb9;&#xff09;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x4e8e;&#x4eba;&#x7c7b;&#x504f;&#x597d;&#x7684;&#x5bf9;&#x9f50;&#x6280;&#x672f;&#xff08;&#x5982;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#xff09;&#x4f9d;&#x8d56;&#x89c6;&#x89c9;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff08;VRM&#xff09;&#xff0c;&#x4f46;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x6570;&#x636e;&#x7a00;&#x7f3a;&#x4e14;&#x83b7;&#x53d6;&#x6210;&#x672c;&#x9ad8;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x6548;&#x679c;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x6570;&#x636e;&#x4e0d;&#x8db3;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x901a;&#x8fc7;&#x8de8;&#x6a21;&#x6001;&#x504f;&#x597d;&#x8fc1;&#x79fb;&#x63d0;&#x5347;&#x5bf9;&#x9f50;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"585,586"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;RoVRM&#x6a21;&#x578b;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x79cd;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#xff1a;1&#xff09;&#x4e09;&#x9636;&#x6bb5;&#x6e10;&#x8fdb;&#x8bad;&#x7ec3;&#xff1a;&#x7b2c;&#x4e00;&#x9636;&#x6bb5;&#x4f7f;&#x7528;&#x7eaf;&#x6587;&#x672c;&#x504f;&#x597d;&#x6570;&#x636e;&#x9884;&#x8bad;&#x7ec3;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff1b;&#x7b2c;&#x4e8c;&#x9636;&#x6bb5;&#x7528;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x66ff;&#x4ee3;&#x56fe;&#x50cf;&#x6784;&#x5efa;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x4ee5;&#x5f25;&#x5408;&#x4efb;&#x52a1;&#x5dee;&#x8ddd;&#xff1b;&#x7b2c;&#x4e09;&#x9636;&#x6bb5;&#x4f7f;&#x7528;&#x771f;&#x5b9e;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x6570;&#x636e;&#x5fae;&#x8c03;&#xff0c;&#x5f25;&#x5408;&#x6a21;&#x6001;&#x5dee;&#x8ddd;&#x3002;2&#xff09;&#x57fa;&#x4e8e;&#x6700;&#x4f18;&#x4f20;&#x8f93;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x9009;&#x62e9;&#xff1a;&#x4ece;&#x6587;&#x672c;&#x504f;&#x597d;&#x6570;&#x636e;&#x4e2d;&#x7b5b;&#x9009;&#x4e0e;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x504f;&#x597d;&#x4e00;&#x81f4;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x6837;&#x672c;&#xff0c;&#x63d0;&#x5347;&#x8bad;&#x7ec3;&#x6548;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"586,587"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x57fa;&#x4e8e;LLaVA-1.5-7B&#x548c;13B&#x6a21;&#x578b;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e0a;&#x9a8c;&#x8bc1;RoVRM&#x7684;&#x6709;&#x6548;&#x6027;&#xff1a;1&#xff09;&#x5728;LLaVA-Bench&#x57fa;&#x51c6;&#x4e0a;&#xff0c;RoVRM&#x6bd4;&#x4f20;&#x7edf;VRM&#x5728;best-of-n&#x91c7;&#x6837;&#x4e2d;&#x63d0;&#x5347;8.4&#x5206;&#xff1b;2&#xff09;&#x4e0e;DPO&#x7ed3;&#x5408;&#x65f6;&#xff0c;&#x5728;MM-Instruct&#x57fa;&#x51c6;&#x4e0a;&#x6bd4;&#x6807;&#x51c6;DPO&#x63d0;&#x5347;17.82&#x5206;&#xff1b;3&#xff09;&#x4e09;&#x9636;&#x6bb5;&#x8bad;&#x7ec3;&#x548c;&#x6570;&#x636e;&#x9009;&#x62e9;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x7f13;&#x89e3;&#x4e86;&#x6570;&#x636e;&#x7a00;&#x7f3a;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x80fd;&#x6cdb;&#x5316;&#x5230;&#x5176;&#x4ed6;&#x6392;&#x5e8f;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#xff08;&#x5982;SimPO&#x3001;ORPO&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"587,588"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: RoVRM&#x901a;&#x8fc7;&#x8de8;&#x6a21;&#x6001;&#x504f;&#x597d;&#x8fc1;&#x79fb;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x6570;&#x636e;&#x7a00;&#x7f3a;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x4e86;LVLMs&#x4e0e;&#x4eba;&#x7c7b;&#x504f;&#x597d;&#x7684;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;&#x591a;&#x79cd;&#x5bf9;&#x9f50;&#x6280;&#x672f;&#x4e2d;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5e76;&#x53ef;&#x80fd;&#x63a8;&#x52a8;&#x66f4;&#x9ad8;&#x6548;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x7684;&#x6a21;&#x578b;&#x4f18;&#x5316;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"588,590"}}],"payload":{"tag":"li","lines":"584,590","fold":1}}],"payload":{"tag":"h4","lines":"582,583"}},{"content":"TPO: A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x4e3b;&#x9898;&#x7ea7;&#x504f;&#x597d;&#x91cd;&#x5199;&#xff08;TPR&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x7cfb;&#x7edf;&#x4f18;&#x5316;&#x5956;&#x52b1;&#x5dee;&#x8ddd;&#x914d;&#x7f6e;&#x6765;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;TPR&#x901a;&#x8fc7;&#x5c06;&#x54cd;&#x5e94;&#x5206;&#x89e3;&#x4e3a;&#x8bed;&#x4e49;&#x4e3b;&#x9898;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x91cd;&#x91c7;&#x6837;&#x751f;&#x6210;&#x66ff;&#x4ee3;&#x5019;&#x9009;&#xff0c;&#x4ece;&#x800c;&#x7cbe;&#x7ec6;&#x63a7;&#x5236;&#x504f;&#x597d;&#x5bf9;&#x4e2d;&#x7684;&#x5956;&#x52b1;&#x5dee;&#x8ddd;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;TPR&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5e73;&#x5747;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x7ea6;20%&#xff0c;&#x5e76;&#x5728;ObjectHal-Bench&#x4e0a;&#x51cf;&#x5c11;&#x9ad8;&#x8fbe;93%&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x8868;&#x73b0;&#x51fa;&#x5353;&#x8d8a;&#x7684;&#x6570;&#x636e;&#x6548;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"591,592"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x63cf;&#x8ff0;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#xff09;&#xff0c;&#x8fd9;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x548c;&#x533b;&#x7597;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x573a;&#x666f;&#x4e2d;&#x5e26;&#x6765;&#x4e25;&#x91cd;&#x98ce;&#x9669;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x57fa;&#x4e8e;&#x6392;&#x540d;&#x6216;&#x91cd;&#x5199;&#x7684;&#x7b56;&#x7565;&#xff09;&#x5728;&#x6570;&#x636e;&#x6784;&#x5efa;&#x8fc7;&#x7a0b;&#x4e2d;&#x96be;&#x4ee5;&#x7cfb;&#x7edf;&#x4f18;&#x5316;&#x5956;&#x52b1;&#x5dee;&#x8ddd;&#x914d;&#x7f6e;&#xff0c;&#x5bfc;&#x81f4;&#x5b66;&#x4e60;&#x5230;&#x7684;&#x5956;&#x52b1;&#x51fd;&#x6570;&#x6548;&#x679c;&#x4e0d;&#x4f73;&#xff0c;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"593,594"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e3b;&#x9898;&#x7ea7;&#x504f;&#x597d;&#x91cd;&#x5199;&#xff08;TPR&#xff09;&#x6846;&#x67b6;&#xff1a;1. &#x5c06;VLM&#x54cd;&#x5e94;&#x5206;&#x89e3;&#x4e3a;&#x7ec6;&#x7c92;&#x5ea6;&#x8bed;&#x4e49;&#x5355;&#x5143;&#xff0c;&#x5e76;&#x6309;&#x4e3b;&#x9898;&#x805a;&#x7c7b;&#xff1b;2. &#x4f7f;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x5bf9;&#x6bcf;&#x4e2a;&#x4e3b;&#x9898;&#x8fdb;&#x884c;&#x5185;&#x90e8;&#x91cd;&#x91c7;&#x6837;&#xff0c;&#x751f;&#x6210;&#x591a;&#x6837;&#x5316;&#x7684;&#x66ff;&#x4ee3;&#x5019;&#x9009;&#xff1b;3. &#x901a;&#x8fc7;&#x9009;&#x62e9;&#x6027;&#x66ff;&#x6362;&#x539f;&#x59cb;&#x8bed;&#x4e49;&#x5355;&#x5143;&#x6784;&#x5efa;&#x504f;&#x597d;&#x5bf9;&#xff08;yw, yl&#xff09;&#xff0c;&#x652f;&#x6301;&#x8d2a;&#x5a6a;&#x7b56;&#x7565;&#xff08;&#x6700;&#x5927;&#x5316;&#x5956;&#x52b1;&#x5dee;&#x8ddd;&#xff09;&#x6216;&#x8bfe;&#x7a0b;&#x5b66;&#x4e60;&#x7b56;&#x7565;&#xff08;&#x9010;&#x6b65;&#x589e;&#x52a0;yl&#x7684;&#x96be;&#x5ea6;&#xff09;&#xff1b;4. &#x57fa;&#x4e8e;&#x6784;&#x5efa;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x4f7f;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x5bf9;&#x9f50;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"594,595"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: TPR&#x5728;&#x591a;&#x4e2a;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x6027;&#x80fd;&#xff1a;1. &#x5e73;&#x5747;&#x6027;&#x80fd;&#x4f18;&#x4e8e;&#x4e4b;&#x524d;&#x65b9;&#x6cd5;&#x7ea6;20%&#xff1b;2. &#x5728;ObjectHal-Bench&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;93%&#x7684;&#x5e7b;&#x89c9;&#xff1b;3. &#x5c55;&#x73b0;&#x51fa;&#x5353;&#x8d8a;&#x7684;&#x6570;&#x636e;&#x6548;&#x7387;&#xff08;&#x4ec5;&#x9700;&#x5c11;&#x91cf;&#x6570;&#x636e;&#x5373;&#x53ef;&#x8fbe;&#x5230;&#x5f3a;&#x57fa;&#x7ebf;&#x6548;&#x679c;&#xff09;&#xff0c;&#x4f18;&#x4e8e;&#x4f9d;&#x8d56;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"595,596"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: TPR&#x901a;&#x8fc7;&#x7cfb;&#x7edf;&#x4f18;&#x5316;&#x5956;&#x52b1;&#x5dee;&#x8ddd;&#x914d;&#x7f6e;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;VLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x8d21;&#x732e;&#x5728;&#x4e8e;&#x63d0;&#x4f9b;&#x4e86;&#x5bf9;&#x8bed;&#x4e49;&#x7ec6;&#x8282;&#x7684;&#x7cbe;&#x7ec6;&#x63a7;&#x5236;&#xff0c;&#x5e76;&#x652f;&#x6301;&#x9ad8;&#x7ea7;&#x6570;&#x636e;&#x6784;&#x5efa;&#x7b56;&#x7565;&#xff08;&#x5982;&#x8bfe;&#x7a0b;&#x5b66;&#x4e60;&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#x548c;&#x6570;&#x636e;&#x6548;&#x7387;&#xff0c;&#x8fd8;&#x4e3a;VLM&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x4e14;&#x6210;&#x672c;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"596,598"}}],"payload":{"tag":"li","lines":"592,598","fold":1}}],"payload":{"tag":"h4","lines":"590,591"}},{"content":"SENTINEL: Mitigating Object Hallucinations via Sentence-Level Early Intervention","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;SENTINEL&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x53e5;&#x5b50;&#x7ea7;&#x522b;&#x8fdb;&#x884c;&#x65e9;&#x671f;&#x5e72;&#x9884;&#xff0c;&#x5229;&#x7528;&#x81ea;&#x4e3e;&#x751f;&#x6210;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x57df;&#x5185;&#x504f;&#x597d;&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x964d;&#x4f4e;90%&#x4ee5;&#x4e0a;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"599,600"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x201c;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x201d;&#xff0c;&#x8fd9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x7684;&#x5b89;&#x5168;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#x6602;&#xff0c;&#x8981;&#x4e48;&#x56e0;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x6216;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x5bfc;&#x81f4;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e0e;&#x6a21;&#x578b;&#x539f;&#x59cb;&#x8f93;&#x51fa;&#x5b58;&#x5728;&#x5206;&#x5e03;&#x4e0d;&#x5339;&#x914d;&#x95ee;&#x9898;&#xff0c;&#x4ece;&#x800c;&#x5f71;&#x54cd;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"601,602"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;SENTINEL&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x6b65;&#x9aa4;&#xff1a;1) &#x57df;&#x5185;&#x5019;&#x9009;&#x81ea;&#x4e3e;&#xff1a;&#x901a;&#x8fc7;&#x591a;&#x6b21;&#x91c7;&#x6837;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#xff0c;&#x5e76;&#x4f7f;&#x7528;&#x4e24;&#x4e2a;&#x5f00;&#x653e;&#x8bcd;&#x6c47;&#x68c0;&#x6d4b;&#x5668;&#x8fdb;&#x884c;&#x4ea4;&#x53c9;&#x9a8c;&#x8bc1;&#xff0c;&#x5c06;&#x53e5;&#x5b50;&#x5206;&#x7c7b;&#x4e3a;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x6216;&#x201c;&#x975e;&#x5e7b;&#x89c9;&#x201d;&#xff1b;2) &#x6784;&#x5efa;&#x4e0a;&#x4e0b;&#x6587;&#x611f;&#x77e5;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#x5bf9;&#xff1a;&#x5229;&#x7528;&#x975e;&#x5e7b;&#x89c9;&#x53e5;&#x5b50;&#x4f5c;&#x4e3a;&#x6b63;&#x6837;&#x672c;&#xff0c;&#x5e7b;&#x89c9;&#x53e5;&#x5b50;&#x4f5c;&#x4e3a;&#x8d1f;&#x6837;&#x672c;&#xff0c;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x5f0f;&#x4e0a;&#x4e0b;&#x6587;&#x81ea;&#x4e3e;&#x6784;&#x5efa;&#x9ad8;&#x8d28;&#x91cf;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#xff1b;3) &#x4e0a;&#x4e0b;&#x6587;&#x611f;&#x77e5;&#x504f;&#x597d;&#x5b66;&#x4e60;&#xff1a;&#x4f7f;&#x7528;&#x6539;&#x8fdb;&#x7684;C-DPO&#x635f;&#x5931;&#x51fd;&#x6570;&#x8fdb;&#x884c;&#x8bad;&#x7ec3;&#xff0c;&#x5728;&#x53e5;&#x5b50;&#x7ea7;&#x522b;&#x6700;&#x5927;&#x5316;&#x6b63;&#x6837;&#x672c;&#x4f3c;&#x7136;&#x5e76;&#x6700;&#x5c0f;&#x5316;&#x8d1f;&#x6837;&#x672c;&#x4f3c;&#x7136;&#xff0c;&#x4ece;&#x6839;&#x6e90;&#x4e0a;&#x963b;&#x6b62;&#x5e7b;&#x89c9;&#x4f20;&#x64ad;&#x3002;","children":[],"payload":{"tag":"li","lines":"602,603"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;SENTINEL&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x53d6;&#x5f97;&#x663e;&#x8457;&#x6548;&#x679c;&#xff1a;&#x5728;Object Halbench&#x4e0a;&#x51cf;&#x5c11;&#x7ea6;92%&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x5728;AMBER&#x4e0a;&#x51cf;&#x5c11;65%&#xff0c;&#x540c;&#x65f6;&#x5728;HallusionBench&#x4e0a;&#x4e5f;&#x6709;&#x6301;&#x7eed;&#x6539;&#x8fdb;&#x3002;&#x66f4;&#x91cd;&#x8981;&#x7684;&#x662f;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x6ca1;&#x6709;&#x727a;&#x7272;&#x6a21;&#x578b;&#x7684;&#x901a;&#x7528;&#x80fd;&#x529b;&#xff0c;&#x5728;VQAv2&#x548c;TextVQA&#x4e0a;&#x4fdd;&#x6301;&#x6027;&#x80fd;&#xff0c;&#x5728;ScienceQA&#x548c;MM-Vet&#x4e0a;&#x751a;&#x81f3;&#x6709;&#x6240;&#x63d0;&#x5347;&#xff0c;&#x8d85;&#x8d8a;&#x4e86;&#x4e4b;&#x524d;&#x7684;&#x6700;&#x5148;&#x8fdb;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"603,604"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x5728;&#x5e7b;&#x89c9;&#x6700;&#x521d;&#x51fa;&#x73b0;&#x7684;&#x53e5;&#x5b50;&#x7ea7;&#x522b;&#x8fdb;&#x884c;&#x65e9;&#x671f;&#x5e72;&#x9884;&#x662f;&#x9632;&#x6b62;&#x5e7b;&#x89c9;&#x4f20;&#x64ad;&#x7684;&#x5173;&#x952e;&#x3002;SENTINEL&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x8d44;&#x6e90;&#x6216;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x4e0d;&#x4ec5;&#x80fd;&#x5927;&#x5e45;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x8fd8;&#x80fd;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x6280;&#x672f;&#x8def;&#x5f84;&#x3002;&#x5176;&#x6a21;&#x578b;&#x65e0;&#x5173;&#x7684;&#x8bbe;&#x8ba1;&#x4f7f;&#x5176;&#x53ef;&#x5e7f;&#x6cdb;&#x5e94;&#x7528;&#x4e8e;&#x5404;&#x79cd;MLLMs&#x3002;","children":[],"payload":{"tag":"li","lines":"604,606"}}],"payload":{"tag":"li","lines":"600,606","fold":1}}],"payload":{"tag":"h4","lines":"598,599"}},{"content":"MHR: Mitigating Multilingual Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x9996;&#x6b21;&#x9488;&#x5bf9;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x4e2d;&#x7684;&#x591a;&#x8bed;&#x8a00;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x4e24;&#x9636;&#x6bb5;&#x53bb;&#x5e7b;&#x89c9;&#x6846;&#x67b6;MHR&#x3002;&#x8be5;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x591a;&#x8bed;&#x8a00;&#x76d1;&#x7763;&#x5fae;&#x8c03;&#x63d0;&#x5347;&#x6307;&#x4ee4;&#x8ddf;&#x968f;&#x80fd;&#x529b;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x8de8;&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x81ea;&#x52a8;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x8fdb;&#x884c;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;(DPO)&#xff0c;&#x5728;13&#x79cd;&#x8bed;&#x8a00;&#x4e0a;&#x5e73;&#x5747;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x5347;19%&#x3002;","children":[],"payload":{"tag":"li","lines":"607,608"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x5728;&#x975e;&#x82f1;&#x8bed;&#x8bed;&#x8a00;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x5b9e;&#x9645;&#x9519;&#x8bef;&#x7684;&#x7b54;&#x6848;&#xff09;&#xff0c;&#x800c;&#x73b0;&#x6709;&#x53bb;&#x5e7b;&#x89c9;&#x65b9;&#x6cd5;&#x4ec5;&#x5173;&#x6ce8;&#x82f1;&#x8bed;&#x573a;&#x666f;&#x3002;&#x591a;&#x8bed;&#x8a00;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x9ad8;&#x8d44;&#x6e90;&#x548c;&#x4f4e;&#x8d44;&#x6e90;&#x8bed;&#x8a00;&#x7528;&#x6237;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4f53;&#x9a8c;&#xff0c;&#x4f46;&#x8be5;&#x95ee;&#x9898;&#x5c1a;&#x672a;&#x88ab;&#x7cfb;&#x7edf;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"609,610"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x9636;&#x6bb5;MHR&#x6846;&#x67b6;&#xff1a;1) &#x591a;&#x8bed;&#x8a00;&#x76d1;&#x7763;&#x5fae;&#x8c03;(SFT)&#x9636;&#x6bb5;&#xff1a;&#x4f7f;&#x7528;&#x591a;&#x8bed;&#x8a00;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x975e;&#x82f1;&#x8bed;&#x67e5;&#x8be2;&#x7684;&#x7406;&#x89e3;&#x80fd;&#x529b;&#xff0c;&#x907f;&#x514d;&#x56e0;&#x6307;&#x4ee4;&#x7406;&#x89e3;&#x504f;&#x5dee;&#x4ea7;&#x751f;&#x65e0;&#x5173;&#x56de;&#x7b54;&#xff1b;2) &#x5e7b;&#x89c9;&#x611f;&#x77e5;&#x504f;&#x597d;&#x4f18;&#x5316;&#x9636;&#x6bb5;&#xff1a;&#x63d0;&#x51fa;&#x8de8;&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x81ea;&#x52a8;&#x6784;&#x5efa;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x2014;&#x2014;&#x5229;&#x7528;LVLM&#x4e3a;&#x540c;&#x4e00;&#x56fe;&#x50cf;&#x67e5;&#x8be2;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x591a;&#x8bed;&#x8a00;&#x54cd;&#x5e94;&#xff0c;&#x901a;&#x8fc7;&#x4e0e;&#x82f1;&#x8bed;&#x6807;&#x51c6;&#x7b54;&#x6848;&#xff08;&#x6b63;&#x786e;/&#x5e7b;&#x89c9;&#x7b54;&#x6848;&#xff09;&#x7684;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x81ea;&#x52a8;&#x6807;&#x6ce8;&#x504f;&#x597d;&#x5bf9;&#xff08;&#x6b63;&#x4f8b;&#x4e3a;&#x4f4e;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff0c;&#x8d1f;&#x4f8b;&#x4e3a;&#x9ad8;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff09;&#xff0c;&#x6700;&#x540e;&#x4f7f;&#x7528;&#x8fd9;&#x4e9b;&#x6570;&#x636e;&#x5bf9;&#x8fdb;&#x884c;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;(DPO)&#x3002;","children":[],"payload":{"tag":"li","lines":"610,611"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x6269;&#x5c55;&#x7684;&#x591a;&#x8bed;&#x8a00;&#x8bc4;&#x6d4b;&#x57fa;&#x51c6;&#xff08;POPE MUL&#x3001;MME MUL&#x3001;AMBER MUL&#xff09;&#x4e0a;&#x9a8c;&#x8bc1;&#xff1a;1) &#x5728;13&#x79cd;&#x8bed;&#x8a00;&#x4e0a;&#xff0c;POPE&#x57fa;&#x51c6;&#x51c6;&#x786e;&#x7387;&#x5e73;&#x5747;&#x63d0;&#x5347;19.0%&#xff1b;2) &#x540c;&#x65f6;&#x63d0;&#x5347;&#x9ad8;&#x8d44;&#x6e90;&#x8bed;&#x8a00;&#xff08;&#x5982;&#x5fb7;&#x8bed;&#x3001;&#x6cd5;&#x8bed;&#xff09;&#x548c;&#x4f4e;&#x8d44;&#x6e90;&#x8bed;&#x8a00;&#xff08;&#x5982;&#x4e4c;&#x514b;&#x5170;&#x8bed;&#x3001;&#x4fdd;&#x52a0;&#x5229;&#x4e9a;&#x8bed;&#xff09;&#x7684;&#x6297;&#x5e7b;&#x89c9;&#x80fd;&#x529b;&#xff1b;3) &#x5728;LLaVA-1.5&#x548c;CogVLM&#x4e24;&#x4e2a;&#x4e3b;&#x6d41;&#x6a21;&#x578b;&#x4e0a;&#x5747;&#x6709;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"611,612"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x591a;&#x8bed;&#x8a00;&#x5e7b;&#x89c9;&#x662f;LVLMs&#x7684;&#x7cfb;&#x7edf;&#x6027;&#x95ee;&#x9898;&#xff0c;&#x6e90;&#x4e8e;&#x591a;&#x8bed;&#x8a00;&#x80fd;&#x529b;&#x4e0d;&#x8db3;&#x548c;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x7f3a;&#x9677;&#x3002;MHR&#x6846;&#x67b6;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x5176;&#x8de8;&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x907f;&#x514d;&#x4e86;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x5de5;&#x6570;&#x636e;&#x6807;&#x6ce8;&#xff0c;&#x4e3a;&#x591a;&#x8bed;&#x8a00;LVLMs&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x884c;&#x65b9;&#x6848;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x66f4;&#x591a;&#x8bed;&#x8a00;&#x548c;&#x6a21;&#x6001;&#x3002;","children":[],"payload":{"tag":"li","lines":"612,614"}}],"payload":{"tag":"li","lines":"608,614","fold":1}}],"payload":{"tag":"h4","lines":"606,607"}}],"payload":{"tag":"h3","lines":"448,449","fold":1}},{"content":"&#x81ea;&#x52a8;&#x5316;&#x504f;&#x597d;&#x6570;&#x636e;&#x751f;&#x6210;","children":[{"content":"SENA: Beyond Human Data: Aligning Multimodal Large Language Models by Iterative Self-Evolution","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;SENA&#x7684;&#x65b0;&#x578b;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x81ea;&#x8fdb;&#x5316;&#x6846;&#x67b6;&#xff0c;&#x8be5;&#x6846;&#x67b6;&#x4ec5;&#x9700;&#x672a;&#x6807;&#x6ce8;&#x56fe;&#x50cf;&#x5373;&#x53ef;&#x81ea;&#x4e3b;&#x751f;&#x6210;&#x9ad8;&#x8d28;&#x91cf;&#x95ee;&#x7b54;&#x5bf9;&#x8fdb;&#x884c;&#x504f;&#x597d;&#x5bf9;&#x9f50;&#xff0c;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x8f85;&#x52a9;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"617,618"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x7684;&#x4eba;&#x7c7b;&#x504f;&#x597d;&#x5bf9;&#x9f50;&#x4f9d;&#x8d56;&#x9ad8;&#x8d28;&#x91cf;&#x6807;&#x6ce8;&#x6570;&#x636e;&#xff0c;&#x4f46;&#x6570;&#x636e;&#x6536;&#x96c6;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x4f9d;&#x8d56;&#x4eba;&#x5de5;&#x6216;GPT-4v&#x7b49;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3002;&#x73b0;&#x6709;&#x81ea;&#x8fdb;&#x5316;&#x65b9;&#x6cd5;&#x4ecd;&#x9700;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x6216;&#x989d;&#x5916;&#x6a21;&#x578b;&#x652f;&#x6301;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x548c;&#x6548;&#x7387;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x7684;&#x6838;&#x5fc3;&#x5728;&#x4e8e;&#x5b9e;&#x73b0;&#x5b8c;&#x5168;&#x81ea;&#x4e3b;&#x7684;&#x6570;&#x636e;&#x751f;&#x6210;&#x4e0e;&#x4f18;&#x5316;&#xff0c;&#x63a8;&#x52a8;MLLMs&#x7684;&#x4f4e;&#x6210;&#x672c;&#x9ad8;&#x6548;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"619,620"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: SENA&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e09;&#x9636;&#x6bb5;&#xff1a;1) &#x95ee;&#x9898;&#x751f;&#x6210;&#x9636;&#x6bb5;&#xff1a;&#x901a;&#x8fc7;&#x56fe;&#x50cf;&#x9a71;&#x52a8;&#x7684;&#x81ea;&#x63d0;&#x95ee;&#x673a;&#x5236;&#xff08;SQ&#xff09;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x4e14;&#x53ef;&#x56de;&#x7b54;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x8865;&#x5145;&#x63cf;&#x8ff0;&#x6027;&#x95ee;&#x9898;&#x4ee5;&#x8986;&#x76d6;&#x591a;&#x6837;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#xff1b;2) &#x7b54;&#x6848;&#x751f;&#x6210;&#x9636;&#x6bb5;&#xff1a;&#x91c7;&#x7528;&#x7b54;&#x6848;&#x81ea;&#x589e;&#x5f3a;&#x6280;&#x672f;&#xff0c;&#x5229;&#x7528;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4f18;&#x5316;&#x9996;&#x9009;&#x7b54;&#x6848;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x6269;&#x6563;&#x566a;&#x58f0;&#x7834;&#x574f;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x88ab;&#x62d2;&#x7b54;&#x6848;&#xff0c;&#x5f62;&#x6210;&#x533a;&#x5206;&#x6027;&#x504f;&#x597d;&#x5bf9;&#xff1b;3) &#x4f18;&#x5316;&#x9636;&#x6bb5;&#xff1a;&#x7ed3;&#x5408;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x635f;&#x5931;&#x548c;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x5bf9;&#x9f50;&#x635f;&#x5931;&#xff0c;&#x901a;&#x8fc7;&#x6700;&#x5927;&#x5316;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4f3c;&#x7136;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x589e;&#x5f3a;&#x5bf9;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x7684;&#x5173;&#x6ce8;&#x3002;","children":[],"payload":{"tag":"li","lines":"620,621"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;SENA&#x5728;&#x591a;&#x9879;&#x751f;&#x6210;&#x5f0f;&#x548c;&#x5224;&#x522b;&#x5f0f;&#x4efb;&#x52a1;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5747;&#x8fbe;&#x5230;&#x4e0e;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x4fe1;&#x606f;&#x7684;&#x65b9;&#x6cd5;&#x76f8;&#x5f53;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x4ec5;&#x51ed;&#x672a;&#x6807;&#x6ce8;&#x56fe;&#x50cf;&#x5b9e;&#x73b0;&#x81ea;&#x8fdb;&#x5316;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x6a21;&#x578b;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3001;&#x63d0;&#x5347;&#x7b54;&#x6848;&#x51c6;&#x786e;&#x6027;&#x548c;&#x6307;&#x4ee4;&#x8ddf;&#x968f;&#x80fd;&#x529b;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x663e;&#x8457;&#x3002;","children":[],"payload":{"tag":"li","lines":"621,622"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SENA&#x9996;&#x6b21;&#x5b9e;&#x73b0;&#x4e86;&#x65e0;&#x9700;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x7684;&#x591a;&#x6a21;&#x6001;&#x81ea;&#x8fdb;&#x5316;&#xff0c;&#x4e3a;MLLMs&#x7684;&#x9ad8;&#x6548;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x8bad;&#x7ec3;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8def;&#x5f84;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x4ef7;&#x503c;&#x5728;&#x4e8e;&#x964d;&#x4f4e;&#x6570;&#x636e;&#x4f9d;&#x8d56;&#x6210;&#x672c;&#xff0c;&#x63a8;&#x52a8;&#x6a21;&#x578b;&#x81ea;&#x4e3b;&#x8fed;&#x4ee3;&#x4f18;&#x5316;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x8d44;&#x6e90;&#x53d7;&#x9650;&#x573a;&#x666f;&#x5e76;&#x4fc3;&#x8fdb;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x4ee3;&#x7406;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"622,624"}}],"payload":{"tag":"li","lines":"618,624","fold":1}}],"payload":{"tag":"h4","lines":"616,617"}},{"content":"Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;FiSAO&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x63d0;&#x4f9b;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;token&#x7ea7;&#x53cd;&#x9988;&#x6765;&#x4f18;&#x5316;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5927;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x9f50;&#xff0c;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x6570;&#x636e;&#x5373;&#x53ef;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x6027;&#x80fd;&#x4f18;&#x4e8e;&#x4f20;&#x7edf;&#x504f;&#x597d;&#x8c03;&#x4f18;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"625,626"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5927;&#x6a21;&#x578b;&#xff08;VLLMs&#xff09;&#x5728;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x4e0a;&#x5b58;&#x5728;&#x6311;&#x6218;&#xff0c;&#x5e38;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#xff08;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x7684;&#x5185;&#x5bb9;&#xff09;&#x548c;&#x4e0d;&#x5b89;&#x5168;&#x5185;&#x5bb9;&#x3002;&#x73b0;&#x6709;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x7c97;&#x7c92;&#x5ea6;&#x7684;&#x53e5;&#x5b50;&#x7ea7;&#x53cd;&#x9988;&#x548c;&#x5916;&#x90e8;&#x6570;&#x636e;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;VLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"627,628"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: FiSAO&#xff08;&#x7ec6;&#x7c92;&#x5ea6;&#x81ea;&#x5bf9;&#x9f50;&#x4f18;&#x5316;&#xff09;&#x662f;&#x4e00;&#x79cd;&#x81ea;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4f5c;&#x4e3a;&#x7ec6;&#x7c92;&#x5ea6;&#x9a8c;&#x8bc1;&#x5668;&#x3002;&#x5b83;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e2d;&#x6bcf;&#x4e2a;token&#x4e0e;&#x56fe;&#x50cf;&#x5d4c;&#x5165;&#x7684;&#x70b9;&#x79ef;&#xff0c;&#x5f97;&#x5230;token&#x7ea7;&#x5956;&#x52b1;&#x4fe1;&#x53f7;&#xff08;&#x800c;&#x975e;&#x53e5;&#x5b50;&#x7ea7;&#xff09;&#xff0c;&#x5e76;&#x57fa;&#x4e8e;&#x6b64;&#x8fdb;&#x884c;&#x7b56;&#x7565;&#x4f18;&#x5316;&#xff08;&#x5982;PPO&#xff09;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x3002;","children":[],"payload":{"tag":"li","lines":"628,629"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;token&#x7ea7;&#x5956;&#x52b1;&#x80fd;&#x66f4;&#x597d;&#x533a;&#x5206;&#x6b63;&#x786e;&#x4e0e;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#xff08;&#x5206;&#x5e03;&#x5206;&#x79bb;&#x660e;&#x663e;&#xff09;&#xff0c;&#x800c;&#x53e5;&#x5b50;&#x7ea7;&#x5956;&#x52b1;&#x91cd;&#x53e0;&#x4e25;&#x91cd;&#xff1b;2&#xff09;&#x53e5;&#x5b50;&#x7ea7;&#x5956;&#x52b1;&#x4e0e;BLEU/ROUGE&#x7b49;&#x6307;&#x6807;&#x76f8;&#x5173;&#x6027;&#x5f31;&#xff08;r=-0.01&#xff09;&#xff1b;3&#xff09;FiSAO&#x5728;&#x591a;&#x4e2a;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x3001;VQA&#xff09;&#x4e0a;&#x6027;&#x80fd;&#x8d85;&#x8d8a;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x6570;&#x636e;&#x7684;&#x4f20;&#x7edf;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"629,630"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: FiSAO&#x9996;&#x6b21;&#x5c06;token&#x7ea7;&#x5956;&#x52b1;&#x5f15;&#x5165;VLLM&#x5bf9;&#x9f50;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x76d1;&#x7763;&#x65b9;&#x5f0f;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x5916;&#x90e8;&#x6570;&#x636e;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x4e3a;VLLM&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x66f4;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x672a;&#x6765;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53d1;&#x5c55;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"630,632"}}],"payload":{"tag":"li","lines":"626,632","fold":1}}],"payload":{"tag":"h4","lines":"624,625"}},{"content":"CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;CLIP-DPO&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x9884;&#x8bad;&#x7ec3;&#x7684;CLIP&#x6a21;&#x578b;&#x4e3a;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x751f;&#x6210;&#x7ed3;&#x679c;&#x8fdb;&#x884c;&#x504f;&#x597d;&#x6392;&#x5e8f;&#xff0c;&#x4ee5;&#x6784;&#x5efa;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x5bf9;&#x8fdb;&#x884c;DPO&#x5fae;&#x8c03;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x3001;&#x4ed8;&#x8d39;API&#x6216;&#x5916;&#x90e8;LVLM&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"633,634"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x3001;&#x5c5e;&#x6027;&#x6216;&#x5173;&#x7cfb;&#x7b49;&#x7ec6;&#x8282;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;&#x57fa;&#x4e8e;GPT-4&#x7684;DPO&#x4f18;&#x5316;&#x4f9d;&#x8d56;&#x4ed8;&#x8d39;API&#x3001;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x53ef;&#x80fd;&#x52a0;&#x5267;&#x5e7b;&#x89c9;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x9ad8;&#x6548;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x4e14;&#x53ef;&#x9760;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x6765;&#x63d0;&#x5347;LVLM&#x7684;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"635,636"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;CLIP-DPO&#x65b9;&#x6cd5;&#xff1a;1. &#x4f7f;&#x7528;&#x76ee;&#x6807;LVLM&#x5728;&#x539f;&#x59cb;&#x76d1;&#x7763;&#x5fae;&#x8c03;&#xff08;SFT&#xff09;&#x6570;&#x636e;&#x4e0a;&#x751f;&#x6210;&#x591a;&#x6837;&#x5316;&#x7684;&#x9884;&#x6d4b;&#x7ed3;&#x679c;&#xff1b;2. &#x5229;&#x7528;&#x9884;&#x8bad;&#x7ec3;&#x7684;CLIP&#x6a21;&#x578b;&#x8ba1;&#x7b97;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x76f8;&#x4f3c;&#x5ea6;&#x5206;&#x6570;&#xff0c;&#x5bf9;&#x751f;&#x6210;&#x7ed3;&#x679c;&#x8fdb;&#x884c;&#x6392;&#x5e8f;&#xff1b;3. &#x901a;&#x8fc7;&#x57fa;&#x4e8e;&#x89c4;&#x5219;&#x7684;&#x8fc7;&#x6ee4;&#x65b9;&#x6cd5;&#xff0c;&#x7b5b;&#x9009;&#x51fa;&#x9ad8;&#x8d28;&#x91cf;&#x7684;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x5bf9;&#xff08;&#x6b63;&#x6837;&#x672c;&#x4e3a;CLIP&#x5206;&#x6570;&#x9ad8;&#x7684;&#x8f93;&#x51fa;&#xff0c;&#x8d1f;&#x6837;&#x672c;&#x4e3a;&#x5206;&#x6570;&#x4f4e;&#x7684;&#x8f93;&#x51fa;&#xff09;&#xff1b;4. &#x4f7f;&#x7528;&#x8fd9;&#x4e9b;&#x6837;&#x672c;&#x5bf9;&#x76f4;&#x63a5;&#x8fdb;&#x884c;DPO&#x4f18;&#x5316;&#xff0c;&#x5fae;&#x8c03;LVLM&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x6570;&#x636e;&#x6216;&#x6a21;&#x578b;&#xff0c;&#x4ec5;&#x4f9d;&#x8d56;CLIP&#x7684;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x80fd;&#x529b;&#x63d0;&#x4f9b;&#x504f;&#x597d;&#x4fe1;&#x53f7;&#x3002;","children":[],"payload":{"tag":"li","lines":"636,637"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x5728;MobileVLM-v2&#x548c;LLaVA-1.5&#x6a21;&#x578b;&#x4e0a;&#x5e94;&#x7528;CLIP-DPO&#x5fae;&#x8c03;&#x540e;&#xff0c;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x51cf;&#x5c11;&#xff0c;&#x5927;&#x5e45;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff1b;2. &#x5728;AMBER&#x57fa;&#x51c6;&#xff08;&#x5f53;&#x524d;&#x6700;&#x5168;&#x9762;&#x7684;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#xff09;&#x4e0a;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f18;&#x4e8e;&#x76f4;&#x63a5;&#x7ade;&#x4e89;&#x5bf9;&#x624b;HA-DPO&#x548c;&#x57fa;&#x4e8e;&#x5927;&#x89c4;&#x6a21;&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x7684;Qwen-VL&#xff1b;3. &#x96f6;&#x6837;&#x672c;&#x56fe;&#x50cf;&#x5206;&#x7c7b;&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff0c;&#x8868;&#x660e;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x8c61; grounding &#x80fd;&#x529b;&#x589e;&#x5f3a;&#xff1b;4. &#x539f;&#x59cb;LVLM&#x5728;&#x6807;&#x51c6;&#x57fa;&#x51c6;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x6574;&#x4f53;&#x4fdd;&#x6301;&#x672a;&#x9000;&#x5316;&#x3002;","children":[],"payload":{"tag":"li","lines":"637,638"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CLIP-DPO&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x7684;LVLM&#x4f18;&#x5316;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5229;&#x7528;CLIP&#x7684;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#x63d0;&#x4f9b;&#x53ef;&#x9760;&#x7684;&#x504f;&#x597d;&#x4fe1;&#x53f7;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x9c81;&#x68d2;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x8d44;&#x6e90;&#xff0c;&#x5177;&#x6709;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x4e3a;LVLM&#x7684;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"638,640"}}],"payload":{"tag":"li","lines":"634,640","fold":1}}],"payload":{"tag":"h4","lines":"632,633"}},{"content":"EMPO: Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;EMPO&#xff08;&#x5b9e;&#x4f53;&#x4e2d;&#x5fc3;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x52a8;&#x6784;&#x5efa;&#x56fe;&#x50cf;&#x3001;&#x6307;&#x4ee4;&#x548c;&#x54cd;&#x5e94;&#x4e09;&#x4e2a;&#x5c42;&#x9762;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x5e76;&#x5229;&#x7528;DPO&#x8fdb;&#x884c;&#x4f18;&#x5316;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x5927;&#x5e45;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"641,642"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#x95ee;&#x9898;&#x3002;&#x5e7b;&#x89c9;&#x8868;&#x73b0;&#x4e3a;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x6216;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff0c;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x4e24;&#x79cd;&#x539f;&#x56e0;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x4e4b;&#x95f4;&#x7684;&#x6a21;&#x6001;&#x672a;&#x5bf9;&#x9f50;&#xff08;Modality Misalignment&#xff09;&#xff0c;&#x5bfc;&#x81f4;&#x8bed;&#x4e49;&#x7406;&#x89e3;&#x9519;&#x8bef;&#xff1b;2&#xff09;LLM&#x672c;&#x8eab;&#x56fa;&#x6709;&#x7684;&#x77e5;&#x8bc6;&#x9519;&#x8bef;&#x6216;&#x5b9e;&#x4f53;&#x5171;&#x73b0;&#x504f;&#x89c1;&#xff08;LLM Inherent Hallucination&#xff09;&#xff0c;&#x4f7f;&#x5176;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x800c;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x4e0a;&#x4e0b;&#x6587;&#x8fdb;&#x884c;&#x9519;&#x8bef;&#x63a8;&#x65ad;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"643,644"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;EMPO&#x6846;&#x67b6;&#xff0c;&#x4ece;&#x6570;&#x636e;&#x6784;&#x5efa;&#x548c;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#x4e24;&#x4e2a;&#x5c42;&#x9762;&#x89e3;&#x51b3;&#x95ee;&#x9898;&#xff1a;","children":[{"content":"1. <strong>&#x6570;&#x636e;&#x6784;&#x5efa;</strong>&#xff1a;&#x4e3a;&#x4e86;&#x89e3;&#x51b3;&#x9ad8;&#x8d28;&#x91cf;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x6570;&#x636e;&#x7a00;&#x7f3a;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x4ed6;&#x4eec;&#x5229;&#x7528;&#x5f00;&#x6e90;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x52a8;&#x7f16;&#x8f91;&#x56fe;&#x50cf;-&#x6307;&#x4ee4;-&#x54cd;&#x5e94;&#x4e09;&#x5143;&#x7ec4;&#x4e2d;&#x7684;&#x5b9e;&#x4f53;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#xff0c;&#x6784;&#x5efa;&#x4e86;&#x6db5;&#x76d6;&#x4e09;&#x4e2a;&#x5c42;&#x9762;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x504f;&#x597d;&#x6570;&#x636e;&#xff1a;","children":[{"content":"<strong>&#x56fe;&#x50cf;&#x5c42;&#x9762;</strong>&#xff1a;&#x4f7f;&#x7528;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#x5b9a;&#x4f4d;&#x5173;&#x952e;&#x5b9e;&#x4f53;&#xff0c;&#x5e76;&#x5229;&#x7528;Stable Diffusion-2&#x5220;&#x9664;30%&#x7684;&#x5b9e;&#x4f53;&#x6216;&#x7528;&#x89c6;&#x89c9;&#x4e0a;&#x5408;&#x7406;&#x4f46;&#x9519;&#x8bef;&#x7684;&#x5b9e;&#x4f53;&#x8fdb;&#x884c;&#x66ff;&#x6362;&#xff0c;&#x751f;&#x6210;&#x88ab;&#x62d2;&#x7edd;&#x7684;&#x56fe;&#x50cf;&#x6837;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"646,647"}},{"content":"<strong>&#x6307;&#x4ee4;&#x5c42;&#x9762;</strong>&#xff1a;&#x4f7f;&#x7528;GPT-4o-mini&#x4fee;&#x6539;&#x539f;&#x59cb;&#x6307;&#x4ee4;&#x4e2d;&#x5df2;&#x8bc6;&#x522b;&#x5b9e;&#x4f53;&#x7684;&#x4f4d;&#x7f6e;&#x3001;&#x5c5e;&#x6027;&#x6216;&#x5173;&#x7cfb;&#xff0c;&#x751f;&#x6210;&#x9519;&#x8bef;&#x7684;&#x6307;&#x4ee4;&#x4f5c;&#x4e3a;&#x88ab;&#x62d2;&#x7edd;&#x6837;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"647,648"}},{"content":"<strong>&#x54cd;&#x5e94;&#x5c42;&#x9762;</strong>&#xff1a;&#x76f4;&#x63a5;&#x4fee;&#x6539;&#x539f;&#x59cb;&#x54cd;&#x5e94;&#x4e2d;&#x7684;&#x5b9e;&#x4f53;&#x4fe1;&#x606f;&#xff0c;&#x751f;&#x6210;&#x5305;&#x542b;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x7684;&#x88ab;&#x62d2;&#x7edd;&#x54cd;&#x5e94;&#x3002;","children":[],"payload":{"tag":"li","lines":"648,649"}}],"payload":{"tag":"li","lines":"645,649","listIndex":1}},{"content":"2. <strong>&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;</strong>&#xff1a;&#x91c7;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x7b97;&#x6cd5;&#xff0c;&#x4f46;&#x5c06;&#x5176;&#x6269;&#x5c55;&#x81f3;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#x3002;EMPO&#x540c;&#x65f6;&#x5728;&#x56fe;&#x50cf;&#x3001;&#x6307;&#x4ee4;&#x548c;&#x6a21;&#x578b;&#x54cd;&#x5e94;&#x8fd9;&#x4e09;&#x4e2a;&#x5c42;&#x9762;&#x8fdb;&#x884c;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;&#x800c;&#x4e0d;&#x4ec5;&#x4ec5;&#x662f;&#x54cd;&#x5e94;&#x5c42;&#x9762;&#xff09;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x5b66;&#x4e60;&#x5c06;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x5b9e;&#x4f53;&#x7279;&#x5f81;&#x4e0e;&#x7528;&#x6237;&#x6307;&#x4ee4;&#x53ca;&#x6a21;&#x578b;&#x54cd;&#x5e94;&#x4e2d;&#x7684;&#x6587;&#x672c;&#x8bed;&#x4e49;&#x7cbe;&#x786e;&#x5bf9;&#x9f50;&#xff0c;&#x4ece;&#x800c;&#x5b9e;&#x73b0;&#x4ee5;&#x5b9e;&#x4f53;&#x4e3a;&#x4e2d;&#x5fc3;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"649,650","listIndex":2}}],"payload":{"tag":"li","lines":"644,650"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;LLaVA-1.5&#x6846;&#x67b6;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x4f7f;&#x7528;&#x4e86;&#x4e24;&#x4e2a;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x96c6;&#x548c;&#x4e94;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x8fdb;&#x884c;&#x8bc4;&#x4f30;&#x3002;&#x5173;&#x952e;&#x7ed3;&#x679c;&#x5982;&#x4e0b;&#xff1a;","children":[{"content":"&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x5353;&#x8d8a;&#xff1a;&#x5728;Object-HalBench&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;&#x4e86;85.9%&#xff1b;&#x5728;MM-HalBench&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;&#x4e86;49.8%&#x3002;","children":[],"payload":{"tag":"li","lines":"651,652"}},{"content":"&#x6027;&#x80fd;&#x8d85;&#x8d8a;&#x57fa;&#x7ebf;&#xff1a;EMPO&#x5728;&#x4e94;&#x4e2a;&#x5e7f;&#x6cdb;&#x4f7f;&#x7528;&#x7684;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x8868;&#x73b0;&#x5747;&#x4f18;&#x4e8e;&#x8457;&#x540d;&#x7684;DPO&#x7b97;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"652,653"}},{"content":"&#x751a;&#x81f3;&#x4f18;&#x4e8e;&#x9876;&#x7ea7;&#x6a21;&#x578b;&#xff1a;&#x5176;&#x5e7b;&#x89c9;&#x7387;&#x5728;&#x4e09;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x4f4e;&#x4e8e;GPT-4V&#x3002;<br>\n&#x8fd9;&#x4e9b;&#x7ed3;&#x679c;&#x8bc1;&#x660e;&#x4e86;EMPO&#x5728;&#x589e;&#x5f3a;&#x591a;&#x6a21;&#x6001;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x548c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"653,655"}}],"payload":{"tag":"li","lines":"650,655"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;EMPO&#x901a;&#x8fc7;&#x5173;&#x6ce8;&#x5b9e;&#x4f53;&#x5c42;&#x9762;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x5bf9;&#x9f50;&#x548c;&#x6784;&#x5efa;&#x5168;&#x9762;&#x7684;&#x591a;&#x6a21;&#x6001;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e2d;&#x56fe;&#x50cf;&#x4e0e;&#x6587;&#x672c;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x4e0d;&#x8db3;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x8f7b;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#xff1a;","children":[{"content":"1. &#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x4f7f;&#x5176;&#x66f4;&#x9002;&#x7528;&#x4e8e;&#x5bf9;&#x51c6;&#x786e;&#x6027;&#x8981;&#x6c42;&#x9ad8;&#x7684;&#x5173;&#x952e;&#x9886;&#x57df;&#x3002;","children":[],"payload":{"tag":"li","lines":"656,657","listIndex":1}},{"content":"2. &#x4e3a;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x9896;&#x4e14;&#x6709;&#x6548;&#x7684;&#x6846;&#x67b6;&#xff08;&#x6570;&#x636e;&#x6784;&#x5efa;+&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"657,658","listIndex":2}},{"content":"3. &#x63d0;&#x51fa;&#x7684;&#x81ea;&#x52a8;&#x5316;&#x6570;&#x636e;&#x6784;&#x5efa;&#x65b9;&#x6cd5;&#x53ef;&#x4ee5;&#x5e94;&#x7528;&#x4e8e;&#x4efb;&#x4f55;&#x73b0;&#x6709;&#x7684;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x7684;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#xff0c;&#x6210;&#x672c;&#x4f4e;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5f3a;&#xff0c;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x4fbf;&#x5229;&#x3002;","children":[],"payload":{"tag":"li","lines":"658,660","listIndex":3}}],"payload":{"tag":"li","lines":"655,660"}}],"payload":{"tag":"li","lines":"642,660","fold":1}}],"payload":{"tag":"h4","lines":"640,641"}},{"content":"APASI: Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;APASI&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x76ee;&#x6807;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x81ea;&#x6211;&#x6ce8;&#x5165;&#x5e7b;&#x89c9;&#x6765;&#x6784;&#x5efa;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x6807;&#x6ce8;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"661,662"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x7684;&#x5185;&#x5bb9;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x4eba;&#x7c7b;&#x6807;&#x6ce8;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#x6536;&#x96c6;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x96be;&#x4ee5;&#x6301;&#x7eed;&#x6539;&#x8fdb;&#x3002;APASI&#x65e8;&#x5728;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x8d44;&#x6e90;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5b9e;&#x73b0;&#x81ea;&#x4e3b;&#x3001;&#x53ef;&#x6301;&#x7eed;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x3002;","children":[],"payload":{"tag":"li","lines":"663,664"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: APASI&#x57fa;&#x4e8e;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x76ee;&#x6807;&#x6a21;&#x578b;&#x81ea;&#x6211;&#x6ce8;&#x5165;&#x5e7b;&#x89c9;&#x6784;&#x5efa;&#x504f;&#x597d;&#x5bf9;&#xff1a;1) &#x4f7f;&#x7528;&#x521d;&#x59cb;&#x6a21;&#x578b;&#x751f;&#x6210;&#x9996;&#x9009;&#x54cd;&#x5e94;&#xff08;y+&#xff09;&#xff1b;2) &#x57fa;&#x4e8e;&#x5e7b;&#x89c9;&#x7684;&#x4e09;&#x4e2a;&#x5173;&#x952e;&#x89c2;&#x5bdf;&#xff08;&#x5bf9;&#x8c61;&#x5171;&#x73b0;&#x3001;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x3001;&#x4f4d;&#x7f6e;&#x56e0;&#x7d20;&#xff09;&#xff0c;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5173;&#x4e8e;&#x4e0d;&#x5b58;&#x5728;&#x4f46;&#x9ad8;&#x9891;&#x5171;&#x73b0;&#x5bf9;&#x8c61;&#x7684;&#x53e5;&#x5b50;&#xff0c;&#x5e76;&#x5c06;&#x5176;&#x6ce8;&#x5165;&#x5230;y+&#x7684;&#x540e;&#x534a;&#x90e8;&#x5206;&#xff0c;&#x5f62;&#x6210;&#x975e;&#x9996;&#x9009;&#x54cd;&#x5e94;&#xff08;y-&#xff09;&#xff1b;3) &#x7ed3;&#x5408;&#x8bfe;&#x7a0b;&#x5b66;&#x4e60;&#xff0c;&#x9010;&#x6b65;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x6ce8;&#x5165;&#x96be;&#x5ea6;&#xff0c;&#x8fed;&#x4ee3;&#x4f18;&#x5316;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"664,665"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x516d;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;APASI&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;LLaVA-v1.5&#x3001;LLaVA-v1.6&#x548c;Qwen2-VL&#x7b49;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x6027;&#x80fd;&#x8fbe;&#x5230;&#x6216;&#x8d85;&#x8fc7;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x8d44;&#x6e90;&#x7684;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"665,666"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: APASI&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x4f9d;&#x8d56;&#x7684;&#x901a;&#x7528;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x6ce8;&#x5165;&#x5e7b;&#x89c9;&#x548c;&#x8bfe;&#x7a0b;&#x5b66;&#x4e60;&#x5b9e;&#x73b0;&#x6301;&#x7eed;&#x6539;&#x8fdb;&#xff0c;&#x4e3a;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x4e14;&#x4f4e;&#x6210;&#x672c;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x672a;&#x6765;&#x81ea;&#x4e3b;&#x6a21;&#x578b;&#x4f18;&#x5316;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"666,668"}}],"payload":{"tag":"li","lines":"662,668","fold":1}}],"payload":{"tag":"h4","lines":"660,661"}},{"content":"Self-Consistency as a Free Lunch: Reducing Hallucinations in Vision-Language Models via Self-Reflection","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;Self-reflection Preference Optimization&#x7684;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x5229;&#x7528;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x81ea;&#x8eab;&#x5728;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x548c;&#x7b80;&#x77ed;&#x4e8c;&#x5143;&#x95ee;&#x9898;&#x56de;&#x7b54;&#x4e4b;&#x95f4;&#x7684;&#x4e0d;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x504f;&#x597d;&#x6570;&#x636e;&#x5e76;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x4ece;&#x800c;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x76d1;&#x7763;&#x5373;&#x53ef;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"669,670"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x865a;&#x6784;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x5176;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x4f9d;&#x8d56;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x5916;&#x90e8;&#x5f3a;&#x5927;&#x6a21;&#x578b;&#x7684;&#x76d1;&#x7763;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x96be;&#x4ee5;&#x6269;&#x5c55;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x5229;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x7684;&#x5185;&#x90e8;&#x4e00;&#x81f4;&#x6027;&#x4f5c;&#x4e3a;&#x514d;&#x8d39;&#x76d1;&#x7763;&#x4fe1;&#x53f7;&#xff0c;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"671,672"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x4e09;&#x9636;&#x6bb5;&#x7684;&#x81ea;&#x6211;&#x53cd;&#x601d;&#x504f;&#x597d;&#x4f18;&#x5316;&#x6846;&#x67b6;&#xff1a;1) &#x5e7b;&#x89c9;&#x66b4;&#x9732;&#xff1a;&#x8ba9;&#x6a21;&#x578b;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x7b80;&#x77ed;&#x7684;&#x4e8c;&#x5143;&#x95ee;&#x9898;&#xff08;&#x5982;&#x201c;&#x662f;/&#x5426;&#x201d;&#xff09;&#x67e5;&#x8be2;&#x5176;&#x4e2d;&#x7684;&#x539f;&#x5b50;&#x4e8b;&#x5b9e;&#x4ee5;&#x66b4;&#x9732;&#x6f5c;&#x5728;&#x5e7b;&#x89c9;&#xff1b;2) &#x81ea;&#x6211;&#x53cd;&#x601d;&#x504f;&#x597d;&#x6392;&#x5e8f;&#xff1a;&#x6bd4;&#x8f83;&#x8be6;&#x7ec6;&#x8f93;&#x51fa;&#x4e0e;&#x7b80;&#x77ed;&#x56de;&#x7b54;&#x7684;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x6839;&#x636e;&#x4e0d;&#x4e00;&#x81f4;&#x9891;&#x7387;&#x5bf9;&#x54cd;&#x5e94;&#x8fdb;&#x884c;&#x8bc4;&#x5206;&#x548c;&#x6392;&#x5e8f;&#xff1b;3) &#x81ea;&#x6211;&#x53cd;&#x601d;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff1a;&#x5229;&#x7528;&#x751f;&#x6210;&#x7684;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x5728;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x6846;&#x67b6;&#x4e0b;&#x5bf9;&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x4ee5;&#x504f;&#x597d;&#x4e8b;&#x5b9e;&#x4e00;&#x81f4;&#x7684;&#x54cd;&#x5e94;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5b8c;&#x5168;&#x57fa;&#x4e8e;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x4fe1;&#x53f7;&#xff0c;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x76d1;&#x7763;&#x3002;","children":[],"payload":{"tag":"li","lines":"672,673"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;AMBER, MultiObject-Hal, Object HalBench, MMHal-Bench&#xff09;&#x4e0a;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x6548;&#x679c;&#x3002;&#x5728;AMBER&#x5224;&#x522b;&#x4efb;&#x52a1;&#x4e0a;&#xff0c;&#x6574;&#x4f53;&#x5224;&#x522b;F1&#x5206;&#x6570;&#x4ece;74.7%&#x63d0;&#x5347;&#x81f3;85.2%&#xff0c;&#x5e7b;&#x89c9;&#x7387;&#x4ece;36.4%&#x5927;&#x5e45;&#x964d;&#x81f3;10.3%&#x3002;&#x540c;&#x65f6;&#xff0c;&#x5728;LLaVA-Bench&#x548c;MMBench&#x7b49;&#x901a;&#x7528;&#x57fa;&#x51c6;&#x4e0a;&#x4e5f;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x5f3a;&#x7684;&#x6307;&#x4ee4;&#x9075;&#x5faa;&#x80fd;&#x529b;&#x548c;&#x5e2e;&#x52a9;&#x6027;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x65b9;&#x6cd5;&#x7684;&#x6709;&#x6548;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"673,674"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x7814;&#x7a76;&#x8bc1;&#x660e;&#xff0c;&#x5229;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x7684;&#x5185;&#x90e8;&#x4e00;&#x81f4;&#x6027;&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x4e14;&#x65e0;&#x9700;&#x6210;&#x672c;&#x7684;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x81ea;&#x6211;&#x53cd;&#x601d;&#x673a;&#x5236;&#x53ef;&#x4ee5;&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x5f3a;&#x5927;&#x7684;&#x81ea;&#x6211;&#x76d1;&#x7763;&#x4fe1;&#x53f7;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;VLMs&#x7684;&#x4e8b;&#x5b9e;&#x53ef;&#x9760;&#x6027;&#x548c;&#x8f93;&#x51fa;&#x8d28;&#x91cf;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x7684;&#x591a;&#x6a21;&#x6001;&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x907f;&#x514d;&#x4e86;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x76d1;&#x7763;&#x7684;&#x590d;&#x6742;&#x6027;&#x548c;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"674,676"}}],"payload":{"tag":"li","lines":"670,676","fold":1}}],"payload":{"tag":"h4","lines":"668,669"}}],"payload":{"tag":"h3","lines":"614,615","fold":1}}],"payload":{"tag":"h2","lines":"404,405"}},{"content":"&#x6307;&#x4ee4;&#x5fae;&#x8c03;/&#x4ee5;&#x6570;&#x636e;&#x4e3a;&#x4e2d;&#x5fc3;&#x7684;&#x65b9;&#x6cd5;","children":[{"content":"&#x8d1f;&#x6837;&#x672c;/&#x5bf9;&#x6297;&#x6027;&#x6307;&#x4ee4;&#x6784;&#x5efa;","children":[{"content":"LRV-GAVIE: Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning","children":[{"content":"","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;LRV-Instruction&#x6570;&#x636e;&#x96c6;&#x548c;GAVIE&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5305;&#x542b;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x7684;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x6307;&#x4ee4;&#x9075;&#x5faa;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"681,682"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5728;&#x5904;&#x7406;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x6216;&#x4eba;&#x7c7b;&#x6307;&#x4ee4;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x8bef;&#x5bfc;&#x6027;&#x8f93;&#x51fa;&#xff0c;&#x5bf9;&#x7f3a;&#x4e4f;&#x4e13;&#x4e1a;&#x77e5;&#x8bc6;&#x7684;&#x7528;&#x6237;&#x4ea7;&#x751f;&#x5371;&#x5bb3;&#xff0c;&#x963b;&#x788d;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x7f3a;&#x4e4f;&#x591a;&#x6837;&#x5316;&#x7684;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x4e14;&#x5ffd;&#x7565;&#x8d1f;&#x6837;&#x672c;&#x8bad;&#x7ec3;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x65e0;&#x6cd5;&#x6b63;&#x786e;&#x5904;&#x7406;&#x5426;&#x5b9a;&#x6216;&#x5b58;&#x5728;&#x6b67;&#x4e49;&#x7684;&#x6307;&#x4ee4;&#x3002;","children":[],"payload":{"tag":"li","lines":"683,684"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6784;&#x5efa;LRV-Instruction&#x6570;&#x636e;&#x96c6;&#xff1a;&#x5305;&#x542b;40&#x4e07;&#x6761;&#x7531;GPT-4&#x751f;&#x6210;&#x7684;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#xff0c;&#x8986;&#x76d6;16&#x79cd;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#xff0c;&#x540c;&#x65f6;&#x5305;&#x542b;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x3002;&#x8d1f;&#x6837;&#x672c;&#x8bbe;&#x8ba1;&#x4e3a;&#x4e09;&#x4e2a;&#x8bed;&#x4e49;&#x5c42;&#x7ea7;&#xff1a;&#xff08;i&#xff09;&#x4e0d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x64cd;&#x63a7;&#xff08;ii&#xff09;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x64cd;&#x63a7;&#xff08;iii&#xff09;&#x77e5;&#x8bc6;&#x64cd;&#x63a7;&#xff0c;&#x5e76;&#x4ee5;&#x58f0;&#x660e;&#x5f0f;&#x548c;&#x7591;&#x95ee;&#x5f0f;&#x4e24;&#x79cd;&#x683c;&#x5f0f;&#x5448;&#x73b0;&#x3002;","children":[],"payload":{"tag":"li","lines":"684,685"}}],"payload":{"tag":"li","lines":"682,685","fold":1}}],"payload":{"tag":"ul","lines":"681,685"}},{"content":"","children":[{"content":"2. &#x63d0;&#x51fa;GAVIE&#x8bc4;&#x4f30;&#x6846;&#x67b6;&#xff1a;&#x57fa;&#x4e8e;GPT-4&#x81ea;&#x52a8;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x76f8;&#x5173;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x7b54;&#x6848;&#xff0c;&#x53ef;&#x9002;&#x914d;&#x591a;&#x79cd;&#x6307;&#x4ee4;&#x683c;&#x5f0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"685,686","listIndex":2}},{"content":"3. &#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff1a;&#x5728;MiniGPT4&#x548c;mPLUG-Owl&#x7b49;&#x6a21;&#x578b;&#x4e0a;&#x4f7f;&#x7528;LRV-Instruction&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x63a2;&#x7a76;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x6bd4;&#x4f8b;&#x5bf9;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x7684;&#x5f71;&#x54cd;&#x3002;","children":[{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x73b0;&#x6709;LMMs&#x5728;&#x8d1f;&#x6837;&#x672c;&#x6307;&#x4ee4;&#xff08;&#x5c24;&#x5176;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x64cd;&#x63a7;&#x548c;&#x77e5;&#x8bc6;&#x64cd;&#x63a7;&#xff09;&#x4e0a;&#x51fa;&#x73b0;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;","children":[],"payload":{"tag":"li","lines":"687,688"}}],"payload":{"tag":"li","lines":"686,688","listIndex":3}},{"content":"4. &#x4f7f;&#x7528;LRV-Instruction&#x5fae;&#x8c03;&#x7684;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x51cf;&#x5c11;&#xff0c;&#x5728;MME&#x3001;VQA&#x7b49;&#x516c;&#x5f00;&#x57fa;&#x51c6;&#x4e0a;&#x8fbe;&#x5230;SOTA&#x6027;&#x80fd;","children":[],"payload":{"tag":"li","lines":"688,689","listIndex":4}},{"content":"5. &#x5b58;&#x5728;&#x7269;&#x4f53;&#x64cd;&#x63a7;&#x548c;&#x77e5;&#x8bc6;&#x64cd;&#x63a7;&#x6307;&#x4ee4;&#x6bd4;&#x4e0d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x64cd;&#x63a7;&#x6307;&#x4ee4;&#x66f4;&#x5177;&#x6311;&#x6218;&#x6027;","children":[],"payload":{"tag":"li","lines":"689,690","listIndex":5}},{"content":"6. &#x6b63;&#x8d1f;&#x6837;&#x672c;&#x6bd4;&#x4f8b;&#x5e73;&#x8861;&#x65f6;&#xff08;&#x5982;1:1&#xff09;&#x6a21;&#x578b;&#x9c81;&#x68d2;&#x6027;&#x6700;&#x4f73;","children":[{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x901a;&#x8fc7;&#x5f15;&#x5165;&#x8d1f;&#x6837;&#x672c;&#x6307;&#x4ee4;&#x7684;&#x591a;&#x6837;&#x5316;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#xff0c;&#x53ef;&#x6709;&#x6548;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5bf9;&#x6307;&#x4ee4;&#x7684;&#x5fe0;&#x5b9e;&#x5ea6;&#x548c;&#x6297;&#x5e7b;&#x89c9;&#x80fd;&#x529b;&#x3002;LRV-Instruction&#x6570;&#x636e;&#x96c6;&#x548c;GAVIE&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x89c6;&#x9891;&#x3001;3D&#x7b49;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#xff0c;&#x5e76;&#x9700;&#x8fdb;&#x4e00;&#x6b65;&#x7814;&#x7a76;&#x5e7b;&#x89c9;&#x7684;&#x6839;&#x56e0;&#x548c;&#x7f13;&#x89e3;&#x673a;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"691,693"}}],"payload":{"tag":"li","lines":"690,693","listIndex":6}}],"payload":{"tag":"ol","lines":"685,693"}}],"payload":{"tag":"h4","lines":"680,681"}},{"content":"AIT: Mitigating Dialogue Hallucination for Large Multi-modal Models via Adversarial Instruction Tuning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5bf9;&#x8bdd;&#x4e2d;&#x5bb9;&#x6613;&#x56e0;&#x524d;&#x7f6e;&#x7684;&#x5bf9;&#x6297;&#x6027;&#x5bf9;&#x8bdd;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x9519;&#x8bef;&#x56de;&#x7b54;&#xff09;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;EvalDial&#x548c;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x5bf9;&#x6297;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#xff08;AIT&#xff09;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x6b64;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"694,695"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;LVLM&#x5728;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#x4e2d;&#x56e0;&#x524d;&#x7f6e;&#x5bf9;&#x8bdd;&#x5e72;&#x6270;&#x800c;&#x4ea7;&#x751f;&#x7684;&#x2018;&#x5bf9;&#x8bdd;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7528;&#x6237;&#x901a;&#x5e38;&#x901a;&#x8fc7;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#x4e0e;&#x7cfb;&#x7edf;&#x4ea4;&#x4e92;&#xff0c;&#x65e9;&#x671f;&#x5bf9;&#x8bdd;&#x4e2d;&#x7684;&#x65e0;&#x610f;&#x5e72;&#x6270;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x540e;&#x7eed;&#x56de;&#x7b54;&#x4e0d;&#x51c6;&#x786e;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;LVLM&#x4f5c;&#x4e3a;&#x901a;&#x7528;&#x52a9;&#x624b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"696,697"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;EvalDial&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x73b0;&#x6709;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5982;ScienceQA&#xff09;&#x4e0a;&#x6dfb;&#x52a0;&#x7531;&#x65b0;&#x578b;&#x5bf9;&#x6297;&#x95ee;&#x9898;&#x751f;&#x6210;&#x5668;&#xff08;AQG&#xff09;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x7684;&#x5e7b;&#x89c9;&#x5bf9;&#x8bdd;&#x6765;&#x91cf;&#x5316;&#x95ee;&#x9898;&#x3002;AQG&#x91c7;&#x7528;&#x9ed1;&#x76d2;&#x5bf9;&#x6297;&#x653b;&#x51fb;&#x6280;&#x672f;&#xff0c;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x4f46;&#x5177;&#x6709;&#x5bf9;&#x6297;&#x6027;&#x7684;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x5bf9;&#x8bdd;&#x3002;&#x4e3a;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x5bf9;&#x6297;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#xff08;AIT&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x63a9;&#x7801;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x6280;&#x672f;&#xff0c;&#x5728;&#x5305;&#x542b;&#x5e7b;&#x89c9;&#x5bf9;&#x8bdd;&#x7684;&#x589e;&#x5f3a;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5bf9;LVLM&#x8fdb;&#x884c;&#x9c81;&#x68d2;&#x5fae;&#x8c03;&#xff0c;&#x4ee5;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x524d;&#x7f6e;&#x5bf9;&#x8bdd;&#x7684;&#x9884;&#x6d4b;&#x504f;&#x5dee;&#x3002;","children":[],"payload":{"tag":"li","lines":"697,698"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;EvalDial&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x6700;&#x5148;&#x8fdb;&#x7684;LVLM&#xff08;&#x5982;LLaVA&#xff09;&#x7684;&#x96f6;&#x6837;&#x672c;&#x6027;&#x80fd;&#x663e;&#x8457;&#x4e0b;&#x964d;&#xff1a;VQA&#x4efb;&#x52a1;&#x51c6;&#x786e;&#x7387;&#x4e0b;&#x964d;&#x9ad8;&#x8fbe;37.7%&#xff0c;Captioning&#x4efb;&#x52a1;&#x7684;CIDEr&#x6307;&#x6807;&#x4e0b;&#x964d;59.6%&#x3002;&#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;&#x5e7b;&#x89c9;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x524d;&#x7f6e;&#x5bf9;&#x8bdd;&#x7684;&#x9884;&#x6d4b;&#x504f;&#x5dee;&#x800c;&#x975e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x3002;AIT&#x65b9;&#x6cd5;&#x6210;&#x529f;&#x5c06;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x63a7;&#x5236;&#x5728;1.4%&#x4ee5;&#x5185;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5bf9;&#x8bdd;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x539f;&#x59cb;&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"698,699"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x5bf9;&#x8bdd;&#x5e7b;&#x89c9;&#x662f;LVLM&#x7684;&#x4e00;&#x4e2a;&#x4e25;&#x91cd;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x53ef;&#x4ee5;&#x901a;&#x8fc7;AIT&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x662f;&#x63d0;&#x9ad8;&#x4e86;LVLM&#x5728;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x9c81;&#x68d2;&#x7684;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x52a9;&#x624b;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x65b9;&#x5411;&#x3002;EvalDial&#x57fa;&#x51c6;&#x548c;AIT&#x65b9;&#x6cd5;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x57fa;&#x7840;&#x548c;&#x5de5;&#x5177;&#x3002;","children":[],"payload":{"tag":"li","lines":"699,701"}}],"payload":{"tag":"li","lines":"695,701","fold":1}}],"payload":{"tag":"h4","lines":"693,694"}},{"content":"Prescribing the Right Remedy: Mitigating Hallucinations in Large Vision-Language Models via Targeted Instruction Tuning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;DFTG&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5148;&#x8bca;&#x65ad;&#x540e;&#x751f;&#x6210;&#x7684;&#x9488;&#x5bf9;&#x6027;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x65b9;&#x6cd5;&#xff0c;&#x89e3;&#x51b3;&#x4e0d;&#x540c;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x7279;&#x5f02;&#x6027;&#x95ee;&#x9898;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#x3002;","children":[],"payload":{"tag":"li","lines":"702,703"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x8de8;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x4e0e;&#x56fe;&#x50cf;&#x5b9e;&#x9645;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x3002;&#x5148;&#x524d;&#x7814;&#x7a76;&#x6307;&#x51fa;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x8d28;&#x91cf;&#x4f4e;&#xff08;&#x5c24;&#x5176;&#x662f;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x4e0d;&#x5e73;&#x8861;&#xff09;&#x662f;&#x4e3b;&#x8981;&#x539f;&#x56e0;&#x3002;&#x7136;&#x800c;&#xff0c;&#x4e0d;&#x540c;LVLM&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#x5177;&#x6709;&#x7279;&#x5f02;&#x6027;&#xff08;&#x5373;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x5bf9;&#x540c;&#x4e00;&#x56fe;&#x50cf;&#x4f1a;&#x4ea7;&#x751f;&#x4e0d;&#x540c;&#x7c7b;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x73b0;&#x6709;&#x901a;&#x7528;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5982;LRV-Instruction&#xff09;&#x672a;&#x8003;&#x8651;&#x8fd9;&#x79cd;&#x7279;&#x5f02;&#x6027;&#xff0c;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x6548;&#x679c;&#x53d7;&#x9650;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x76f4;&#x63a5;&#x5f71;&#x54cd;LVLM&#x5728;&#x73b0;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"704,705"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;DFTG&#xff08;Diagnose First, Then Generate&#xff09;&#x4e24;&#x9636;&#x6bb5;&#x6846;&#x67b6;&#xff1a;","children":[{"content":"1. &#x5e7b;&#x89c9;&#x8bca;&#x65ad;&#x9636;&#x6bb5;&#xff1a;","children":[{"content":"&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff08;Caption generation&#xff09;&#xff1a;&#x83b7;&#x53d6;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x7684;&#x63cf;&#x8ff0;&#x6587;&#x672c;","children":[],"payload":{"tag":"li","lines":"707,708"}},{"content":"&#x6587;&#x672c;&#x4fe1;&#x606f;&#x63d0;&#x53d6;&#xff08;Text information extraction&#xff09;&#xff1a;&#x8bc6;&#x522b;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x5bf9;&#x8c61;","children":[],"payload":{"tag":"li","lines":"708,709"}},{"content":"&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x63d0;&#x53d6;&#xff08;Image information extraction&#xff09;&#xff1a;&#x68c0;&#x6d4b;&#x56fe;&#x50cf;&#x4e2d;&#x5b9e;&#x9645;&#x5b58;&#x5728;&#x7684;&#x89c6;&#x89c9;&#x5bf9;&#x8c61;","children":[],"payload":{"tag":"li","lines":"709,710"}},{"content":"&#x5e7b;&#x89c9;&#x68c0;&#x67e5;&#xff08;Hallucination check&#xff09;&#xff1a;&#x5bf9;&#x6bd4;&#x6587;&#x672c;&#x548c;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#xff0c;&#x8bca;&#x65ad;&#x51fa;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;","children":[],"payload":{"tag":"li","lines":"710,711"}}],"payload":{"tag":"li","lines":"706,711","listIndex":1}},{"content":"2. &#x9488;&#x5bf9;&#x6027;&#x6570;&#x636e;&#x751f;&#x6210;&#x9636;&#x6bb5;&#xff1a;&#x57fa;&#x4e8e;&#x8bca;&#x65ad;&#x7ed3;&#x679c;&#x751f;&#x6210;&#x6b63;&#x8d1f;&#x6307;&#x4ee4;&#x5bf9;&#xff08;&#x4f8b;&#x5982;&#x9488;&#x5bf9;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#x751f;&#x6210;&#x5426;&#x5b9a;&#x5f0f;&#x95ee;&#x7b54;&#xff09;&#xff0c;&#x6784;&#x5efa;&#x9488;&#x5bf9;&#x6027;&#x5fae;&#x8c03;&#x6570;&#x636e;&#x96c6;","children":[],"payload":{"tag":"li","lines":"711,712","listIndex":2}}],"payload":{"tag":"li","lines":"705,712"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x3001;MME&#x3001;AMBER&#x548c;VHTest&#x56db;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;","children":[{"content":"DFTG&#x751f;&#x6210;&#x7684;&#x9488;&#x5bf9;&#x6027;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x6bd4;&#x5148;&#x524d;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5982;LRV-Instruction&#xff09;&#x66f4;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x6a21;&#x578b;&#x5e7b;&#x89c9;","children":[],"payload":{"tag":"li","lines":"713,714"}},{"content":"&#x9a8c;&#x8bc1;&#x4e86;&#x9488;&#x5bf9;&#x6a21;&#x578b;&#x7279;&#x5f02;&#x6027;&#x751f;&#x6210;&#x6307;&#x4ee4;&#x7684;&#x65b0;&#x8303;&#x5f0f;&#x4f18;&#x4e8e;&#x901a;&#x7528;&#x6307;&#x4ee4;&#x751f;&#x6210;&#x65b9;&#x6cd5;","children":[],"payload":{"tag":"li","lines":"714,715"}}],"payload":{"tag":"li","lines":"712,715"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#xff1a;1. &#x4e0d;&#x540c;LVLM&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x7279;&#x5f02;&#x6027;&#xff0c;&#x9700;&#x9488;&#x5bf9;&#x6027;&#x5904;&#x7406;2. DFTG&#x6846;&#x67b6;&#x80fd;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x9488;&#x5bf9;&#x7279;&#x5b9a;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#x7684;&#x6307;&#x4ee4;&#x6570;&#x636e;3. &#x9488;&#x5bf9;&#x6027;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x662f;&#x7f13;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x7684;&#x6709;&#x6548;&#x9014;&#x5f84; &#x6f5c;&#x5728;&#x5f71;&#x54cd;&#xff1a;&#x4e3a;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x66f4;&#x5b89;&#x5168;&#x5e94;&#x7528;","children":[],"payload":{"tag":"li","lines":"715,717"}}],"payload":{"tag":"li","lines":"703,717","fold":1}}],"payload":{"tag":"h4","lines":"701,702"}},{"content":"TextSquare: Scaling up Text-Centric Visual Instruction Tuning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x4ecb;&#x7ecd;&#x4e86;TextSquare&#x6a21;&#x578b;&#x53ca;&#x5176;&#x5927;&#x89c4;&#x6a21;&#x9ad8;&#x8d28;&#x91cf;&#x6587;&#x672c;&#x4e2d;&#x5fc3;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x6307;&#x4ee4;&#x8c03;&#x4f18;&#x6570;&#x636e;&#x96c6;Square-10M&#x3002;&#x8be5;&#x6570;&#x636e;&#x96c6;&#x901a;&#x8fc7;&#x95ed;&#x6e90;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x81ea;&#x6211;&#x63d0;&#x95ee;&#x3001;&#x56de;&#x7b54;&#x3001;&#x63a8;&#x7406;&#x548c;&#x8bc4;&#x4f30;&#x56db;&#x6b65;&#x751f;&#x6210;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x6587;&#x672c;&#x4e2d;&#x5fc3;VQA&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x751a;&#x81f3;&#x5728;&#x67d0;&#x4e9b;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8d85;&#x8d8a;&#x4e86;GPT4V&#x548c;Gemini&#x3002;","children":[],"payload":{"tag":"li","lines":"718,719"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f00;&#x6e90;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x6587;&#x672c;&#x4e2d;&#x5fc3;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x4ecd;&#x843d;&#x540e;&#x4e8e;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#xff08;&#x5982;GPT4V&#x548c;Gemini&#xff09;&#xff0c;&#x4e3b;&#x8981;&#x539f;&#x56e0;&#x662f;&#x7f3a;&#x4e4f;&#x5927;&#x89c4;&#x6a21;&#x9ad8;&#x8d28;&#x91cf;&#x7684;&#x6307;&#x4ee4;&#x8c03;&#x4f18;&#x6570;&#x636e;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63a8;&#x52a8;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x53d1;&#x5c55;&#x3001;&#x63d0;&#x5347;&#x6587;&#x672c;&#x7406;&#x89e3;&#x4e0e;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"720,721"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;Square&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x56db;&#x4e2a;&#x6b65;&#x9aa4;&#x751f;&#x6210;&#x9ad8;&#x8d28;&#x91cf;&#x6307;&#x4ee4;&#x6570;&#x636e;&#xff1a;1. &#x81ea;&#x6211;&#x63d0;&#x95ee;&#xff08;Self-Questioning&#xff09;&#xff1a;&#x5229;&#x7528;MLLM&#x5206;&#x6790;&#x56fe;&#x50cf;&#x6587;&#x672c;&#x5185;&#x5bb9;&#x751f;&#x6210;&#x95ee;&#x9898;&#xff1b;2. &#x56de;&#x7b54;&#xff08;Answering&#xff09;&#xff1a;&#x4f7f;&#x7528;&#x601d;&#x7ef4;&#x94fe;&#x548c;&#x5c11;&#x6837;&#x672c;&#x63d0;&#x793a;&#x7b49;&#x6280;&#x672f;&#x751f;&#x6210;&#x7b54;&#x6848;&#xff1b;3. &#x63a8;&#x7406;&#xff08;Reasoning&#xff09;&#xff1a;&#x83b7;&#x53d6;&#x7b54;&#x6848;&#x80cc;&#x540e;&#x7684;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#xff1b;4. &#x8bc4;&#x4f30;&#xff08;Evaluation&#xff09;&#xff1a;&#x7b5b;&#x9009;&#x9ad8;&#x8d28;&#x91cf;&#x95ee;&#x7b54;&#x5bf9;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5e94;&#x7528;&#x4e8e;&#x4ece;&#x591a;&#x79cd;&#x573a;&#x666f;&#x6536;&#x96c6;&#x7684;&#x6587;&#x672c;&#x4e30;&#x5bcc;&#x56fe;&#x50cf;&#xff0c;&#x6784;&#x5efa;&#x4e86;&#x5305;&#x542b;&#x5343;&#x4e07;&#x7ea7;&#x6837;&#x672c;&#x7684;Square-10M&#x6570;&#x636e;&#x96c6;&#x3002;","children":[],"payload":{"tag":"li","lines":"721,722"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. TextSquare&#x5728;OCRBench&#x4e0a;&#x8fbe;&#x5230;62.2%&#x7684;&#x51c6;&#x786e;&#x7387;&#xff0c;&#x5728;10&#x4e2a;&#x6587;&#x672c;&#x4e2d;&#x5fc3;&#x57fa;&#x51c6;&#x4e2d;6&#x4e2a;&#x8d85;&#x8d8a;GPT4V&#x548c;Gemini&#xff0c;&#x7efc;&#x5408;&#x6392;&#x540d;&#x7b2c;&#x4e00;&#xff08;2.2 vs GPT4V&#x7684;2.4&#xff09;&#x3002;2. &#x63a8;&#x7406;&#x6570;&#x636e;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x5e76;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5728;4&#x4e2a;&#x901a;&#x7528;VQA&#x548c;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5e73;&#x5747;&#x5f97;&#x5206;75.1%&#xff0c;&#x8d85;&#x8d8a;&#x4e4b;&#x524d;&#x6700;&#x4f18;&#x6a21;&#x578b;&#x3002;3. &#x6570;&#x636e;&#x91cf;&#x6307;&#x6570;&#x589e;&#x957f;&#x4e0e;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x6210;&#x6b63;&#x6bd4;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x5927;&#x89c4;&#x6a21;&#x9ad8;&#x8d28;&#x91cf;&#x6570;&#x636e;&#x7684;&#x5fc5;&#x8981;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"722,723"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Square-10M&#x6570;&#x636e;&#x96c6;&#x548c;TextSquare&#x6a21;&#x578b;&#x663e;&#x8457;&#x7f29;&#x5c0f;&#x4e86;&#x5f00;&#x6e90;&#x4e0e;&#x95ed;&#x6e90;MLLM&#x7684;&#x6027;&#x80fd;&#x5dee;&#x8ddd;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5927;&#x89c4;&#x6a21;&#x9ad8;&#x8d28;&#x91cf;&#x6307;&#x4ee4;&#x8c03;&#x4f18;&#x6570;&#x636e;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x63a8;&#x7406;&#x6570;&#x636e;&#x7684;&#x5f15;&#x5165;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x89e3;&#x91ca;&#x80fd;&#x529b;&#x548c;&#x6297;&#x5e7b;&#x89c9;&#x6027;&#x80fd;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x4e3a;&#x5f00;&#x6e90;&#x793e;&#x533a;&#x63d0;&#x4f9b;&#x4e86;&#x5b9d;&#x8d35;&#x8d44;&#x6e90;&#xff0c;&#x63a8;&#x52a8;&#x4e86;&#x6587;&#x672c;&#x4e2d;&#x5fc3;&#x591a;&#x6a21;&#x6001;&#x7814;&#x7a76;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"723,725"}}],"payload":{"tag":"li","lines":"719,725","fold":1}}],"payload":{"tag":"h4","lines":"717,718"}},{"content":"REVERIE: Reflective Instruction Tuning: Mitigating Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x2018;&#x53cd;&#x601d;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x2019;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x6b63;&#x8d1f;&#x4e24;&#x79cd;&#x7406;&#x7531;&#xff08;rationale&#xff09;&#x5b66;&#x4e60;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x53d1;&#x5e03;&#x4e86;&#x9996;&#x4e2a;&#x5927;&#x89c4;&#x6a21;&#x5e26;&#x7406;&#x7531;&#x6807;&#x6ce8;&#x7684;&#x6570;&#x636e;&#x96c6;REVERIE&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"726,727"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x591a;&#x79cd;&#x4efb;&#x52a1;&#x4e0a;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f46;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x6216;&#x6307;&#x4ee4;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x5176;&#x5b9e;&#x7528;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x5ffd;&#x7565;&#x4e86;&#x4e00;&#x4e2a;&#x5173;&#x952e;&#x95ee;&#x9898;&#xff1a;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x7f3a;&#x4e4f;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x63a8;&#x7406;&#x76d1;&#x7763;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x5efa;&#x7acb;&#x6307;&#x4ee4;&#x4e0e;&#x56de;&#x7b54;&#x4e4b;&#x95f4;&#x7684;&#x8868;&#x9762;&#x6377;&#x5f84;&#x800c;&#x975e;&#x771f;&#x6b63;&#x7406;&#x89e3;&#x63a8;&#x7406;&#x903b;&#x8f91;&#x3002;","children":[],"payload":{"tag":"li","lines":"728,729"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x53cd;&#x601d;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#xff08;Reflective Instruction Tuning&#xff09;&#xff0c;&#x5728;&#x4f20;&#x7edf;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x57fa;&#x7840;&#x4e0a;&#xff0c;&#x589e;&#x52a0;&#x8ba9;&#x6a21;&#x578b;&#x5b66;&#x4e60;&#x751f;&#x6210;&#x6b63;&#x8d1f;&#x7406;&#x7531;&#xff08;rationale&#xff09;&#x7684;&#x6b65;&#x9aa4;&#xff1a;&#x6b63;&#x7406;&#x7531;&#x89e3;&#x91ca;&#x6b63;&#x786e;&#x7b54;&#x6848;&#x7684;&#x4f9d;&#x636e;&#xff08;&#x5982;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#x548c;&#x9010;&#x6b65;&#x63a8;&#x7406;&#xff09;&#xff0c;&#x8d1f;&#x7406;&#x7531;&#x5206;&#x6790;&#x9519;&#x8bef;&#x7b54;&#x6848;&#x7684;&#x539f;&#x56e0;&#xff08;&#x5982;&#x9519;&#x8bef;&#x5206;&#x7c7b;&#x7684;&#x6838;&#x5fc3;&#x7279;&#x5f81;&#xff09;&#x3002;&#x4e3a;&#x6b64;&#xff0c;&#x6784;&#x5efa;&#x4e86;&#x5927;&#x89c4;&#x6a21;&#x6570;&#x636e;&#x96c6;REVERIE&#xff0c;&#x5305;&#x542b;11.5&#x4e07;&#x6761;&#x6307;&#x4ee4;&#xff0c;&#x6bcf;&#x6761;&#x914d;&#x6709;&#x6b63;&#x786e;&#x548c;&#x9519;&#x8bef;&#x56de;&#x7b54;&#x53ca;&#x8be6;&#x7ec6;&#x7406;&#x7531;&#xff0c;&#x603b;&#x8ba1;25.4&#x4e07;&#x4e2a;&#x8bad;&#x7ec3;&#x6837;&#x672c;&#xff08;&#x6307;&#x4ee4;-&#x56de;&#x7b54;-&#x7406;&#x7531;&#x4e09;&#x5143;&#x7ec4;&#xff09;&#x3002;&#x8bad;&#x7ec3;&#x65f6;&#x91c7;&#x7528;&#x5bf9;&#x8bdd;&#x683c;&#x5f0f;&#xff0c;&#x5c06;&#x7406;&#x7531;&#x751f;&#x6210;&#x4e0e;&#x56de;&#x7b54;&#x9884;&#x6d4b;&#x89e3;&#x8026;&#x3002;","children":[],"payload":{"tag":"li","lines":"729,730"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;LVLM&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#xff0c;&#x4f7f;&#x7528;REVERIE&#x8fdb;&#x884c;&#x53cd;&#x601d;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x7684;&#x6a21;&#x578b;&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x8868;&#x73b0;&#x51fa;&#x663e;&#x8457;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;&#x7279;&#x522b;&#x5730;&#xff0c;&#x4f7f;&#x7528;&#x8d1f;&#x7406;&#x7531;&#x6bd4;&#x4ec5;&#x4f7f;&#x7528;&#x6b63;&#x7406;&#x7531;&#x5e26;&#x6765;&#x4e86;&#x989d;&#x5916;&#x6539;&#x8fdb;&#xff0c;&#x8bc1;&#x660e;&#x6b63;&#x8d1f;&#x7406;&#x7531;&#x5171;&#x540c;&#x5b66;&#x4e60;&#x80fd;&#x66f4;&#x6709;&#x6548;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x8fa8;&#x522b;&#x6b63;&#x786e;&#x4e0e;&#x5e7b;&#x89c9;&#x56de;&#x7b54;&#x7684;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"730,731"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x53cd;&#x601d;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x7ec6;&#x7c92;&#x5ea6;&#x63a8;&#x7406;&#x76d1;&#x7763;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x3001;&#x6307;&#x4ee4;&#x7406;&#x89e3;&#x548c;&#x9519;&#x8bef;&#x610f;&#x8bc6;&#x3002;REVERIE&#x6570;&#x636e;&#x96c6;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x8d44;&#x6e90;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x6a21;&#x4eff;&#x4eba;&#x7c7b;&#x53cd;&#x601d;&#x8ba4;&#x77e5;&#x8fc7;&#x7a0b;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"731,733"}}],"payload":{"tag":"li","lines":"727,733","fold":1}}],"payload":{"tag":"h4","lines":"725,726"}},{"content":"Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Re-Critic&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5411;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x4e2d;&#x6dfb;&#x52a0;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff08;rationale&#xff09;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x91c7;&#x7528;&#x81ea;&#x6279;&#x5224;&#x673a;&#x5236;&#x8fdb;&#x884c;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"734,735"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x73b0;&#x6709;&#x7684;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4V&#x3001;LLaVA&#xff09;&#x5728;&#x89e3;&#x91ca;&#x56fe;&#x50cf;&#x65f6;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff08;&#x5982;&#x63cf;&#x8ff0;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x8bad;&#x7ec3;&#x4e2d;&#x7f3a;&#x4e4f;&#x5bf9;&#x903b;&#x8f91;&#x63a8;&#x7406;&#x548c;&#x65b9;&#x6cd5;&#x8bba;&#x7684;&#x5b66;&#x4e60;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x7edf;&#x8ba1;&#x6a21;&#x5f0f;&#x800c;&#x975e;&#x6df1;&#x5c42;&#x63a8;&#x7406;&#x3002;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"736,737"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;Re-Critic&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x94fe;&#x63d2;&#x5165;&#x6280;&#x672f;&#xff08;VCIT&#xff09;&#xff1a;&#x5229;&#x7528;&#x5927;&#x578b;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;&#x5982;GPT&#xff09;&#x4e3a;&#x539f;&#x59cb;&#x6307;&#x4ee4;-&#x7b54;&#x6848;&#x5bf9;&#x751f;&#x6210;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff08;&#x5305;&#x62ec;&#x57fa;&#x672c;&#x5224;&#x65ad;&#x57fa;&#x7840;&#x548c;&#x601d;&#x7ef4;&#x94fe;&#xff09;&#xff0c;&#x5e76;&#x5c06;&#x4f9d;&#x636e;&#x63d2;&#x5165;&#x95ee;&#x9898;&#x4e2d;&#xff0c;&#x5f62;&#x6210;&#x589e;&#x5f3a;&#x7684;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x7528;&#x4e8e;&#x5fae;&#x8c03;&#xff1b;2&#xff09;&#x81ea;&#x6279;&#x5224;&#x673a;&#x5236;&#xff1a;&#x8ba9;&#x6a21;&#x578b;&#x5bf9;&#x540c;&#x4e00;&#x95ee;&#x9898;&#x751f;&#x6210;&#x4e24;&#x4e2a;&#x54cd;&#x5e94;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x8eab;&#x6279;&#x5224;&#x9009;&#x62e9;&#x66f4;&#x51c6;&#x786e;&#x7684;&#x54cd;&#x5e94;&#xff0c;&#x6784;&#x5efa;&#x9ad8;&#x8d28;&#x91cf;&#x504f;&#x597d;&#x5bf9;&#xff08;&#x65e0;&#x9700;&#x5916;&#x90e8;API&#xff09;&#xff0c;&#x5e76;&#x91c7;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x8fdb;&#x884c;&#x5bf9;&#x9f50;&#x8bad;&#x7ec3;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x6a21;&#x62df;&#x4eba;&#x7c7b;&#x201c;&#x5148;&#x5b66;&#x4e60;&#x539f;&#x7406;&#x518d;&#x63a8;&#x7406;&#x201d;&#x7684;&#x8ba4;&#x77e5;&#x8fc7;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"737,738"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x9a8c;&#x8bc1;&#x6709;&#x6548;&#x6027;&#xff1a;&#x5728;&#x5e7b;&#x89c9;&#x4e13;&#x9879;&#x8bc4;&#x6d4b;&#xff08;MMHal&#x3001;HallusionBench&#x3001;POPE&#xff09;&#x4e2d;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x9519;&#x8bef;&#xff1b;&#x5728;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#xff08;MME&#x3001;MathVista&#xff09;&#x4e0a;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x660e;&#x663e;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;LLaVA-80K&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5fae;&#x8c03;&#x540e;&#xff0c;&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x6027;&#x80fd;&#x63d0;&#x5347;31.8%&#xff0c;&#x8bc1;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x901a;&#x8fc7;&#x65b9;&#x6cd5;&#x8bba;&#x5b66;&#x4e60;&#x5f25;&#x8865;&#x6570;&#x636e;&#x4e0d;&#x8db3;&#x3002;","children":[],"payload":{"tag":"li","lines":"738,739"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Re-Critic&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#x548c;&#x81ea;&#x6279;&#x5224;&#x673a;&#x5236;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x548c;&#x6297;&#x5e7b;&#x89c9;&#x6027;&#x80fd;&#xff0c;&#x4e14;&#x65b9;&#x6cd5;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5f3a;&#x3002;&#x7ed3;&#x8bba;&#x5f3a;&#x8c03;&#xff1a;&#x65b9;&#x6cd5;&#x8bba;&#x5b66;&#x4e60;&#xff08;&#x5148;&#x638c;&#x63e1;&#x539f;&#x7406;&#x518d;&#x63a8;&#x7406;&#xff09;&#x6bd4;&#x5355;&#x7eaf;&#x589e;&#x52a0;&#x6570;&#x636e;&#x66f4;&#x91cd;&#x8981;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5177;&#x6709;&#x4fc3;&#x8fdb;&#x6a21;&#x578b;&#x900f;&#x660e;&#x5316;&#x548c;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"739,741"}}],"payload":{"tag":"li","lines":"735,741","fold":1}}],"payload":{"tag":"h4","lines":"733,734"}}],"payload":{"tag":"h3","lines":"678,679","fold":1}},{"content":"&#x6570;&#x636e;&#x6e05;&#x6d17;&#x4e0e;&#x4fee;&#x6b63;","children":[{"content":"RAH-Bench: Mitigating Hallucination in Visual Language Models with Visual Supervision","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x76d1;&#x7763;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x65b9;&#x6cd5;&#x5305;&#x62ec;&#x6784;&#x5efa;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x96c6;RAI-30k&#xff0c;&#x5e76;&#x5f15;&#x5165;SAM&#x5206;&#x5272;&#x6a21;&#x578b;&#x548c;&#x63a9;&#x7801;&#x9884;&#x6d4b;&#x635f;&#x5931;&#x4f5c;&#x4e3a;&#x8f85;&#x52a9;&#x76d1;&#x7763;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x51fa;&#x65b0;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;RAH-Bench&#x3002;&#x5b9e;&#x9a8c;&#x663e;&#x793a;&#x8be5;&#x65b9;&#x6cd5;&#x5728;LLaVA&#x6a21;&#x578b;&#x4e0a;&#x63d0;&#x5347;8.4%&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"744,745"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x77db;&#x76fe;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x7f3a;&#x4e4f;&#x7ec6;&#x8282;&#x6ce8;&#x91ca;&#x548c;&#x635f;&#x5931;&#x51fd;&#x6570;&#x5bf9;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x4e0d;&#x8db3;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9700;&#x8981;&#x7cbe;&#x786e;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x7684;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#xff09;&#x4e2d;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"746,747"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x4e24;&#x65b9;&#x9762;&#x89e3;&#x51b3;&#xff1a;1&#xff09;&#x57fa;&#x4e8e;&#x5168;&#x666f;&#x573a;&#x666f;&#x56fe;&#xff08;PSG&#xff09;&#x6784;&#x5efa;RAI-30k&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5305;&#x542b;30,000&#x4e2a;&#x5f3a;&#x8c03;&#x7269;&#x4f53;&#x5173;&#x7cfb;&#xff08;&#x5982;&#x7a7a;&#x95f4;&#x3001;&#x5c5e;&#x6027;&#xff09;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#xff1b;2&#xff09;&#x5728;&#x8bad;&#x7ec3;&#x4e2d;&#x96c6;&#x6210;SAM&#xff08;Segment Anything Model&#xff09;&#x751f;&#x6210;&#x5b9e;&#x4f8b;&#x63a9;&#x7801;&#xff0c;&#x5e76;&#x6dfb;&#x52a0;&#x63a9;&#x7801;&#x9884;&#x6d4b;&#x635f;&#x5931;&#x4f5c;&#x4e3a;&#x8f85;&#x52a9;&#x76d1;&#x7763;&#xff0c;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x533a;&#x57df;&#x3002;&#x8bad;&#x7ec3;&#x65f6;&#x4ec5;&#x9700;&#x989d;&#x5916;&#x89c6;&#x89c9;&#x76d1;&#x7763;&#xff0c;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x3002;","children":[],"payload":{"tag":"li","lines":"747,748"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x63d0;&#x51fa;&#x7684;RAH-Bench&#x57fa;&#x51c6;&#x4e0a;&#xff08;&#x5305;&#x542b;3,000&#x4e2a;&#x95ee;&#x9898;&#xff0c;&#x5206;&#x7c7b;&#x578b;/&#x5c5e;&#x6027;/&#x5173;&#x7cfb;&#x4e09;&#x7c7b;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x6bd4;&#x539f;&#x59cb;LLaVA&#x6a21;&#x578b;&#x63d0;&#x5347;8.4%&#x7684;&#x51c6;&#x786e;&#x7387;&#xff0c;&#x4e14;&#x5728;&#x5404;&#x7c7b;&#x5e7b;&#x89c9;&#x5b50;&#x6307;&#x6807;&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x66f4;&#x597d;&#x3002;&#x793a;&#x4f8b;&#x663e;&#x793a;&#x6a21;&#x578b;&#x80fd;&#x6b63;&#x786e;&#x56de;&#x7b54;&#x7ec6;&#x8282;&#x95ee;&#x9898;&#xff08;&#x5982;&#x201c;&#x4eba;&#x662f;&#x5426;&#x63a8;&#x6469;&#x6258;&#x8f66;&#x201d;&#xff09;&#xff0c;&#x800c;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"748,749"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x89c6;&#x89c9;&#x76d1;&#x7763;&#x548c;&#x7ec6;&#x7c92;&#x5ea6;&#x6570;&#x636e;&#x96c6;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;LVLM&#x7684;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x80fd;&#x529b;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;RAH-Bench&#x4e3a;&#x5e7b;&#x89c9;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x66f4;&#x7cbe;&#x7ec6;&#x7684;&#x8bc4;&#x4f30;&#x5de5;&#x5177;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x89c6;&#x9891;&#x6216;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#xff0c;&#x63a8;&#x52a8;&#x53ef;&#x9760;AI&#x7cfb;&#x7edf;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"749,751"}}],"payload":{"tag":"li","lines":"745,751","fold":1}}],"payload":{"tag":"h4","lines":"743,744"}},{"content":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;HalluciDoctor&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x81ea;&#x52a8;&#x68c0;&#x6d4b;&#x548c;&#x6d88;&#x9664;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x7269;&#x4f53;&#x3001;&#x5173;&#x7cfb;&#x548c;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x53cd;&#x4e8b;&#x5b9e;&#x6570;&#x636e;&#x6269;&#x5c55;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6297;&#x5e7b;&#x89c9;&#x80fd;&#x529b;&#xff0c;&#x76f8;&#x5bf9;&#x51cf;&#x5c11;44.6%&#x7684;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"752,753"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x57fa;&#x4e8e;&#x673a;&#x5668;&#x751f;&#x6210;&#x7684;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#xff08;&#x5982;LLaVA-158K&#xff09;&#x5b58;&#x5728;&#x5927;&#x91cf;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7269;&#x4f53;&#x3001;&#x5173;&#x7cfb;&#x6216;&#x5c5e;&#x6027;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x5bfc;&#x81f4;MLLMs&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x8f93;&#x51fa;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5bf9;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x7684;&#x611f;&#x77e5;&#x51c6;&#x786e;&#x6027;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5c1a;&#x672a;&#x88ab;&#x7cfb;&#x7edf;&#x7814;&#x7a76;&#xff0c;&#x4e14;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4ec5;&#x9488;&#x5bf9;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x6216;&#x4f9d;&#x8d56;&#x4eba;&#x5de5;&#x4fee;&#x6b63;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;","children":[],"payload":{"tag":"li","lines":"754,755"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;HalluciDoctor&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x6b65;&#x9aa4;&#xff1a;1&#xff09;&#x901a;&#x8fc7;&#x6587;&#x672c;&#x573a;&#x666f;&#x56fe;&#x89e3;&#x6790;&#x63d0;&#x53d6;&#x7b54;&#x6848;&#x5757;&#xff08;&#x7269;&#x4f53;&#x3001;&#x5173;&#x7cfb;&#x3001;&#x5c5e;&#x6027;&#xff09;&#xff1b;2&#xff09;&#x4e3a;&#x6bcf;&#x4e2a;&#x7b54;&#x6848;&#x751f;&#x6210;&#x7ec6;&#x7c92;&#x5ea6;&#x95ee;&#x9898;&#xff1b;3&#xff09;&#x4f7f;&#x7528;&#x591a;&#x4e2a;MLLMs&#x751f;&#x6210;&#x56fe;&#x50cf;&#x5bfc;&#x5411;&#x7b54;&#x6848;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x4e00;&#x81f4;&#x6027;&#x4ea4;&#x53c9;&#x68c0;&#x67e5;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x5757;&#xff08;&#x4e00;&#x81f4;&#x6027;&#x5206;&#x6570;&#x4f4e;&#x4e8e;&#x9608;&#x503c;&#xff09;&#x3002;&#x968f;&#x540e;&#xff0c;&#x57fa;&#x4e8e;&#x53d1;&#x73b0;&#x7684;&#x957f;&#x5c3e;&#x5bf9;&#x8c61;&#x5171;&#x73b0;&#x504f;&#x5dee;&#xff0c;&#x91c7;&#x7528;&#x53cd;&#x4e8b;&#x5b9e;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6269;&#x5c55;&#x7b56;&#x7565;&#xff08;seesaw-based&#xff09;&#xff0c;&#x5e73;&#x8861;&#x6570;&#x636e;&#x5206;&#x5e03;&#x4ee5;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"755,756"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;LLaVA-158K&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;HalluciDoctor&#x5c06;&#x7269;&#x4f53;&#x3001;&#x5173;&#x7cfb;&#x548c;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#x5206;&#x522b;&#x4ece;28.1%&#x3001;36.0%&#x3001;33.7%&#x964d;&#x81f3;8.3%&#x3001;20.7%&#x3001;17.1%&#x3002;&#x6700;&#x7ec8;&#x6a21;&#x578b;&#xff08;LLaVA++&#xff09;&#x5728;MME&#x57fa;&#x51c6;&#x4e0a;&#x5f97;&#x5206;1275.99&#xff08;&#x4f18;&#x4e8e;&#x539f;LLaVA&#x7684;1148.93&#xff09;&#xff0c;&#x5e7b;&#x89c9;&#x7387;&#xff08;CHAIR&#xff09;&#x4ece;21.73%&#x964d;&#x81f3;12.03%&#xff0c;&#x76f8;&#x5bf9;&#x51cf;&#x5c11;44.6%&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x4fdd;&#x6301;&#x7ade;&#x4e89;&#x529b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"756,757"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x673a;&#x5668;&#x751f;&#x6210;&#x7684;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#x6bd2;&#x6027;&#xff0c;HalluciDoctor&#x80fd;&#x6709;&#x6548;&#x81ea;&#x52a8;&#x68c0;&#x6d4b;&#x548c;&#x6d88;&#x9664;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x53cd;&#x4e8b;&#x5b9e;&#x6570;&#x636e;&#x6269;&#x5c55;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6297;&#x5e7b;&#x89c9;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x6784;&#x5efa;&#x9ad8;&#x8d28;&#x91cf;&#x591a;&#x6a21;&#x6001;&#x6570;&#x636e;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#xff0c;&#x53ef;&#x4fc3;&#x8fdb;MLLMs&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"757,759"}}],"payload":{"tag":"li","lines":"753,759","fold":1}}],"payload":{"tag":"h4","lines":"751,752"}},{"content":"FGHE: Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ReCaption&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x4f7f;&#x7528;ChatGPT&#x91cd;&#x5199;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x5e76;&#x5fae;&#x8c03;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#xff0c;&#x4ee5;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x65f6;&#x4ea7;&#x751f;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7684;&#x5bf9;&#x8c61;&#x5c5e;&#x6027;&#x548c;&#x884c;&#x4e3a;&#xff09;&#x3002;&#x540c;&#x65f6;&#x63d0;&#x51fa;&#x4e86;&#x65b0;&#x7684;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;FGHE&#x6765;&#x91cf;&#x5316;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x3002;&#x5b9e;&#x9a8c;&#x8bc1;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"760,761"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x7684;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x6216;&#x884c;&#x4e3a;&#x3002;&#x73b0;&#x6709;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#xff08;&#x5982;CHAIR&#x3001;POPE&#xff09;&#x4ec5;&#x5173;&#x6ce8;&#x7c97;&#x7c92;&#x5ea6;&#x7684;&#x5bf9;&#x8c61;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#xff0c;&#x800c;&#x5ffd;&#x7565;&#x4e86;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x5c5e;&#x6027;&#xff08;&#x5982;&#x989c;&#x8272;&#x3001;&#x5f62;&#x72b6;&#xff09;&#x548c;&#x884c;&#x4e3a;&#xff08;&#x5982;&#x52a8;&#x4f5c;&#x72b6;&#x6001;&#xff09;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x7528;&#x6237;&#x4f53;&#x9a8c;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x9488;&#x5bf9;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x548c;&#x8bc4;&#x4f30;&#x4f53;&#x7cfb;&#x3002;","children":[],"payload":{"tag":"li","lines":"762,763"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;ReCaption&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x4f7f;&#x7528;ChatGPT&#x5bf9;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x8fdb;&#x884c;&#x91cd;&#x5199;&#xff1a;&#x901a;&#x8fc7;&#x4e24;&#x9636;&#x6bb5;&#x63d0;&#x793a;&#x7b56;&#x7565;&#xff0c;&#x9996;&#x5148;&#x63d0;&#x53d6;&#x539f;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x52a8;&#x8bcd;&#x3001;&#x540d;&#x8bcd;&#x548c;&#x5f62;&#x5bb9;&#x8bcd;&#x4f5c;&#x4e3a;&#x5173;&#x952e;&#x8bcd;&#xff0c;&#x7136;&#x540e;&#x8981;&#x6c42;ChatGPT&#x751f;&#x6210;&#x5305;&#x542b;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x8bcd;&#x7684;&#x591a;&#x6837;&#x5316;&#x65b0;&#x63cf;&#x8ff0;&#xff0c;&#x5f62;&#x6210;&#x9ad8;&#x8d28;&#x91cf;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x5bf9;&#xff1b;2&#xff09;&#x4f7f;&#x7528;&#x91cd;&#x5199;&#x540e;&#x7684;&#x63cf;&#x8ff0;&#x5bf9;&#x9884;&#x8bad;&#x7ec3;&#x7684;LVLM&#xff08;&#x5982;MiniGPT-4&#x3001;LLaVA&#x7b49;&#xff09;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x4ee5;&#x52a0;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x4e0e;&#x6587;&#x672c;&#x7ec6;&#x7c92;&#x5ea6;&#x5bf9;&#x9f50;&#x7684;&#x80fd;&#x529b;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;FGHE&#xff0c;&#x901a;&#x8fc7;&#x4e8c;&#x5143;&#x5206;&#x7c7b;&#x95ee;&#x9898;&#xff08;&#x4f8b;&#x5982;&#x201c;&#x56fe;&#x4e2d;&#x7537;&#x4eba;&#x7684;&#x8863;&#x670d;&#x662f;&#x84dd;&#x8272;&#x7684;&#x5417;&#xff1f;&#x201d;&#xff09;&#x540c;&#x65f6;&#x68c0;&#x6d4b;&#x5bf9;&#x8c61;&#x5b58;&#x5728;&#x6027;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x884c;&#x4e3a;&#x7684;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"763,764"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;ReCaption&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x591a;&#x79cd;LVLM&#xff08;&#x5305;&#x62ec;MiniGPT-4&#x3001;LLaVA&#x7b49;&#xff09;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x3002;&#x5728;FGHE&#x8bc4;&#x4f30;&#x4e2d;&#xff0c;&#x5fae;&#x8c03;&#x540e;&#x7684;&#x6a21;&#x578b;&#x5728;&#x5bf9;&#x8c61;&#x5c5e;&#x6027;&#xff08;&#x5982;&#x989c;&#x8272;&#x3001;&#x5f62;&#x72b6;&#xff09;&#x548c;&#x884c;&#x4e3a;&#xff08;&#x5982;&#x52a8;&#x4f5c;&#x72b6;&#x6001;&#xff09;&#x7684;&#x51c6;&#x786e;&#x7387;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff0c;&#x540c;&#x65f6;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x6574;&#x4f53;&#x8d28;&#x91cf;&#xff08;&#x5982;BLEU&#x3001;CIDEr&#x6307;&#x6807;&#xff09;&#x4e5f;&#x6709;&#x6240;&#x6539;&#x5584;&#x3002;&#x4f8b;&#x5982;&#xff0c;MiniGPT-4&#x5728;&#x4f7f;&#x7528;ReCaption&#x540e;&#x907f;&#x514d;&#x4e86;&#x5c06;&#x201c;&#x505c;&#x653e;&#x7684;&#x98de;&#x673a;&#x201d;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x4e3a;&#x201c;&#x6b63;&#x5728;&#x8d77;&#x98de;&#x201d;&#x3002;","children":[],"payload":{"tag":"li","lines":"764,765"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ReCaption&#x901a;&#x8fc7;&#x7b80;&#x5355;&#x7684;&#x63cf;&#x8ff0;&#x91cd;&#x5199;&#x548c;&#x5fae;&#x8c03;&#x7b56;&#x7565;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x5177;&#x6709;&#x6a21;&#x578b;&#x65e0;&#x5173;&#x6027;&#xff08;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;LVLM&#xff09;&#x3002;&#x63d0;&#x51fa;&#x7684;FGHE&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#x4e3a;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x91cf;&#x5316;&#x6807;&#x51c6;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4e5f;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x65b9;&#x5411;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x6539;&#x5584;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#x7b49;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x63cf;&#x8ff0;&#x7684;&#x5e94;&#x7528;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"765,769"}}],"payload":{"tag":"li","lines":"761,769","fold":1}}],"payload":{"tag":"h4","lines":"759,760"}}],"payload":{"tag":"h3","lines":"741,742","fold":1}},{"content":"&#x7279;&#x5b9a;&#x6280;&#x80fd;&#x6307;&#x4ee4;&#x751f;&#x6210;","children":[{"content":"IDK-Instructions: Visually Dehallucinative Instruction Generation: Know What You Don&#x2019;t Know","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65b0;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x6982;&#x5ff5;&#x2018;IK&#x5e7b;&#x89c9;&#x2019;&#xff0c;&#x9488;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5728;&#x65e0;&#x6cd5;&#x56de;&#x7b54;&#x95ee;&#x9898;&#x65f6;&#x4ecd;&#x751f;&#x6210;&#x9519;&#x8bef;&#x7b54;&#x6848;&#x7684;&#x73b0;&#x8c61;&#x3002;&#x4f5c;&#x8005;&#x521b;&#x5efa;&#x4e86;VQAv2-IDK&#x57fa;&#x51c6;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5e76;&#x5f00;&#x53d1;&#x4e86;IDK-Instructions&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x751f;&#x6210;&#x65b9;&#x6cd5;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x89c6;&#x89c9;&#x8bc6;&#x522b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"772,773"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4V&#x3001;Gemini&#xff09;&#x5728;&#x5904;&#x7406;&#x65e0;&#x6cd5;&#x56de;&#x7b54;&#x7684;&#x89c6;&#x89c9;&#x95ee;&#x9898;&#x65f6;&#x4f1a;&#x4ea7;&#x751f;&#x2018;IK&#x5e7b;&#x89c9;&#x2019;&#xff08;I Know Hallucination&#xff09;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x672c;&#x5e94;&#x56de;&#x7b54;&#x2018;&#x6211;&#x4e0d;&#x77e5;&#x9053;&#x2019;&#xff0c;&#x5374;&#x751f;&#x6210;&#x865a;&#x6784;&#x7b54;&#x6848;&#x3002;&#x8fd9;&#x79cd;&#x73b0;&#x8c61;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#xff0c;&#x4f46;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x4ec5;&#x5173;&#x6ce8;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x7b49;&#x4e8c;&#x5143;&#x5224;&#x65ad;&#xff0c;&#x7f3a;&#x4e4f;&#x5bf9;&#x8fd9;&#x7c7b;&#x590d;&#x6742;&#x5e7b;&#x89c9;&#x7684;&#x7cfb;&#x7edf;&#x5904;&#x7406;&#x3002;","children":[],"payload":{"tag":"li","lines":"774,775"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6784;&#x5efa;VQAv2-IDK&#x57fa;&#x51c6;&#xff1a;&#x4ece;VQAv2&#x6570;&#x636e;&#x96c6;&#x4e2d;&#x7b5b;&#x9009;&#x4eba;&#x7c7b;&#x6807;&#x6ce8;&#x5305;&#x542b;&#x2018;&#x6211;&#x4e0d;&#x77e5;&#x9053;&#x2019;&#x7c7b;&#x5173;&#x952e;&#x8bcd;&#xff08;&#x5982;unknown/not sure&#xff09;&#x7684;&#x56fe;&#x50cf;-&#x95ee;&#x9898;&#x5bf9;&#xff0c;&#x5f62;&#x6210;13,807&#x4e2a;&#x8bad;&#x7ec3;&#x6837;&#x672c;&#x548c;6,624&#x4e2a;&#x9a8c;&#x8bc1;&#x6837;&#x672c;&#x3002;2. &#x63d0;&#x51fa;&#x89c6;&#x89c9;&#x53bb;&#x5e7b;&#x89c9;&#x6307;&#x4ee4;&#x751f;&#x6210;&#x65b9;&#x6cd5;&#xff1a;&#x4f7f;&#x7528;&#x5c11;&#x6837;&#x672c;&#x63d0;&#x793a;&#xff08;few-shot prompt&#xff09;&#x9a71;&#x52a8;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff0c;&#x4e3a;&#x8fd9;&#x4e9b;&#x65e0;&#x6cd5;&#x56de;&#x7b54;&#x7684;&#x95ee;&#x9898;&#x751f;&#x6210;&#x5305;&#x542b;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x8868;&#x8fbe;&#xff08;&#x5982;&#x2018;&#x6211;&#x4e0d;&#x786e;&#x5b9a;&#x2019;&#xff09;&#x7684;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x6307;&#x4ee4;&#xff0c;&#x6700;&#x7ec8;&#x6784;&#x5efa;IDK-Instructions&#x6570;&#x636e;&#x96c6;&#x3002;","children":[],"payload":{"tag":"li","lines":"775,776"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x73b0;&#x6709;&#x6a21;&#x578b;&#x5728;VQAv2-IDK&#x57fa;&#x51c6;&#x4e0a;&#x666e;&#x904d;&#x5b58;&#x5728;IK&#x5e7b;&#x89c9;&#xff0c;&#x9519;&#x8bef;&#x751f;&#x6210;&#x786e;&#x5b9a;&#x6027;&#x7b54;&#x6848;&#x3002;2. &#x4f7f;&#x7528;IDK-Instructions&#x8bad;&#x7ec3;&#x7684;&#x6a21;&#x578b;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5728;&#x4fdd;&#x6301;&#x539f;&#x6709;&#x89c6;&#x89c9;&#x8bc6;&#x522b;&#x6027;&#x80fd;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x5bf9;&#x5404;&#x7c7b;&#x65e0;&#x6cd5;&#x56de;&#x7b54;&#x7684;&#x95ee;&#x9898;&#x6b63;&#x786e;&#x54cd;&#x5e94;&#x2018;&#x6211;&#x4e0d;&#x77e5;&#x9053;&#x2019;&#x3002;3. &#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x6846;&#x67b6;&#x65e0;&#x5173;&#x6027;&#xff0c;&#x53ef;&#x9002;&#x914d;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x548c;&#x6570;&#x636e;&#x96c6;&#x3002;","children":[],"payload":{"tag":"li","lines":"776,777"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: IK&#x5e7b;&#x89c9;&#x662f;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x7684;&#x5173;&#x952e;&#x6311;&#x6218;&#x3002;&#x672c;&#x6587;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x4e13;&#x7528;&#x57fa;&#x51c6;&#x548c;&#x6307;&#x4ee4;&#x751f;&#x6210;&#x65b9;&#x6cd5;&#xff0c;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x89e3;&#x51b3;&#x4e86;&#x2018;&#x5e94;&#x56de;&#x7b54;&#x4e0d;&#x77e5;&#x9053;&#x2019;&#x7684;&#x573a;&#x666f;&#x3002;&#x516c;&#x5f00;&#x7684;VQAv2-IDK&#x548c;IDK-Instructions&#x6570;&#x636e;&#x96c6;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x57fa;&#x7840;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"777,780"}}],"payload":{"tag":"li","lines":"773,780","fold":1}}],"payload":{"tag":"h4","lines":"771,772"}}],"payload":{"tag":"h3","lines":"769,770","fold":1}}],"payload":{"tag":"h2","lines":"676,677"}},{"content":"&#x540e;&#x5904;&#x7406;/&#x4fee;&#x6b63;","children":[{"content":"&#x77e5;&#x8bc6;&#x7f16;&#x8f91;","children":[{"content":"Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x81ea;&#x76d1;&#x7763;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#xff08;T2I&#xff09;&#x6a21;&#x578b;&#x5c06;&#x53ef;&#x80fd;&#x5305;&#x542b;&#x5e7b;&#x89c9;&#x7684;&#x63cf;&#x8ff0;&#x91cd;&#x5efa;&#x4e3a;&#x56fe;&#x50cf;&#xff0c;&#x4ee5;&#x653e;&#x5927;&#x548c;&#x66b4;&#x9732;&#x5e7b;&#x89c9;&#x4fe1;&#x53f7;&#xff0c;&#x5f62;&#x6210;&#x8d1f;&#x951a;&#x70b9;&#xff1b;&#x540c;&#x65f6;&#x5229;&#x7528;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x6b63;&#x951a;&#x70b9;&#x3002;&#x901a;&#x8fc7;&#x62c9;&#x8fd1;&#x4e0e;&#x6b63;&#x951a;&#x70b9;&#x7684;&#x8bed;&#x4e49;&#x8868;&#x793a;&#x5e76;&#x63a8;&#x79bb;&#x8d1f;&#x951a;&#x70b9;&#x7684;&#x5e7b;&#x89c9;&#x65b9;&#x5411;&#xff0c;&#x76f4;&#x63a5;&#x5728;&#x89e3;&#x7801;&#x5668;&#x5c42;&#x7f16;&#x8f91;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x4fdd;&#x6301;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x6027;&#x7684;&#x540c;&#x65f6;&#x6709;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x548c;&#x6a21;&#x578b;&#x67b6;&#x6784;&#x4e0a;&#x5747;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x7ea7;&#x522b;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x51e0;&#x4e4e;&#x4e0d;&#x4ea7;&#x751f;&#x526f;&#x4f5c;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"785,786"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x865a;&#x6784;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x6216;&#x5173;&#x7cfb;&#xff09;&#x3002;&#x5373;&#x4f7f;&#x66f4;&#x5927;&#x3001;&#x66f4;&#x5148;&#x8fdb;&#x7684;&#x6a21;&#x578b;&#x4e5f;&#x5b58;&#x5728;&#x6b64;&#x95ee;&#x9898;&#xff0c;&#x800c;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x989d;&#x5916;&#x5fae;&#x8c03;&#x3001;&#x624b;&#x5de5;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#xff0c;&#x6216;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x65f6;&#x727a;&#x7272;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x81ea;&#x76d1;&#x7763;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x800c;&#x4e0d;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x6709;&#x7528;&#x8f93;&#x51fa;&#x3002;","children":[],"payload":{"tag":"li","lines":"787,788"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x751f;&#x6210;&#x951a;&#x70b9;&#x7684;&#x8868;&#x793a;&#x7f16;&#x8f91;&#x65b9;&#x6cd5;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x4f7f;&#x7528;MLLM&#x751f;&#x6210;&#x521d;&#x59cb;&#x63cf;&#x8ff0;&#xff08;&#x53ef;&#x80fd;&#x542b;&#x5e7b;&#x89c9;&#xff09;&#xff1b;2) &#x901a;&#x8fc7;T2I&#x6a21;&#x578b;&#x5c06;&#x8be5;&#x63cf;&#x8ff0;&#x91cd;&#x5efa;&#x4e3a;&#x56fe;&#x50cf;&#xff0c;&#x4ee5;&#x653e;&#x5927;&#x5e7b;&#x89c9;&#x4fe1;&#x53f7;&#x4f5c;&#x4e3a;&#x8d1f;&#x951a;&#x70b9;&#xff1b;3) &#x5c06;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x7f16;&#x7801;&#x4e3a;&#x6b63;&#x951a;&#x70b9;&#xff1b;4) &#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x901a;&#x8fc7;&#x62c9;&#x8fd1;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x5d4c;&#x5165;&#x4e0e;&#x6b63;&#x951a;&#x70b9;&#x7684;&#x8ddd;&#x79bb;&#xff0c;&#x5e76;&#x63a8;&#x79bb;&#x8d1f;&#x951a;&#x70b9;&#x7684;&#x65b9;&#x5411;&#xff0c;&#x76f4;&#x63a5;&#x7f16;&#x8f91;&#x89e3;&#x7801;&#x5668;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3001;&#x4eba;&#x5de5;&#x5148;&#x9a8c;&#x6216;&#x5916;&#x90e8;&#x76d1;&#x7763;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x5b8c;&#x5168;&#x81ea;&#x76d1;&#x7763;&#x7684;&#x7aef;&#x5230;&#x7aef;&#x6821;&#x6b63;&#x3002;","children":[],"payload":{"tag":"li","lines":"788,789"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;CHAIR&#xff09;&#x548c;&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA-v1.5-7B&#x3001;LLaVA-NEXT-7B&#x3001;Cambrian-8B&#x548c;InstructBLIP-7B&#xff09;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) &#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x7ea7;&#x522b;&#x7684;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x51cf;&#x5c11;&#xff08;&#x4f8b;&#x5982;LLaVA-v1.5-7B&#x5728;CHAIR&#x4e0a;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x8d85;&#x8fc7;5%&#xff09;&#xff1b;2) &#x53ec;&#x56de;&#x7387;&#x548c;&#x63cf;&#x8ff0;&#x4e30;&#x5bcc;&#x6027;&#x5f97;&#x5230;&#x4fdd;&#x6301;&#xff1b;3) &#x8de8;&#x67b6;&#x6784;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x5f3a;&#xff1b;4) &#x5bf9;&#x65e0;&#x5e7b;&#x89c9;&#x7684;&#x63cf;&#x8ff0;&#x51e0;&#x4e4e;&#x4e0d;&#x4ea7;&#x751f;&#x526f;&#x4f5c;&#x7528;&#xff0c;&#x5177;&#x5907;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"789,790"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x66b4;&#x9732;&#x5e7b;&#x89c9;&#x6765;&#x6291;&#x5236;&#x5b83;&#x4eec;&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x4e14;&#x81ea;&#x76d1;&#x7763;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5728;&#x5fe0;&#x5b9e;&#x6027;&#x548c;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x6027;&#x4e4b;&#x95f4;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x4f18;&#x5e73;&#x8861;&#x3002;&#x5176;&#x8de8;&#x6a21;&#x578b;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x548c;&#x4f4e;&#x526f;&#x4f5c;&#x7528;&#x4f7f;&#x5176;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x7528;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x7814;&#x7a76;&#x5efa;&#x7acb;&#x4e86;&#x5f3a;&#x57fa;&#x7ebf;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x4e86;MLLM&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"790,792"}}],"payload":{"tag":"li","lines":"786,792","fold":1}}],"payload":{"tag":"h4","lines":"784,785"}},{"content":"TruthPrInt: Mitigating LVLM Object Hallucination Via Latent Truthful-Guided Pre-Intervention","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;TruthPrInt&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;LVLM&#x5185;&#x90e8;&#x72b6;&#x6001;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#xff0c;&#x5e76;&#x5728;&#x89e3;&#x7801;&#x65f6;&#x8fdb;&#x884c;&#x5e72;&#x9884;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"793,794"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x5b58;&#x5728;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#xff08;Object Hallucination, OH&#xff09;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x5c5e;&#x6027;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x63a2;&#x7d22;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x5185;&#x90e8;&#x72b6;&#x6001;&#x4e0e;&#x771f;&#x5b9e;&#x6027;&#x4e4b;&#x95f4;&#x7684;&#x5173;&#x7cfb;&#xff0c;&#x4f46;LVLM&#x5185;&#x90e8;&#x72b6;&#x6001;&#x662f;&#x5426;&#x80fd;&#x591f;&#x4f5c;&#x4e3a;&#x9010;&#x6807;&#x8bb0;&#xff08;per-token&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x6307;&#x793a;&#x5668;&#xff0c;&#x4ee5;&#x53ca;&#x5982;&#x4f55;&#x5229;&#x7528;&#x8fd9;&#x4e9b;&#x4fe1;&#x606f;&#x6765;&#x7f13;&#x89e3;OH&#xff0c;&#x4ecd;&#x672a;&#x88ab;&#x5145;&#x5206;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"795,796"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x901a;&#x8fc7;&#x6536;&#x96c6;LVLM&#x751f;&#x6210;&#x6807;&#x8bb0;&#x65f6;&#x7684;&#x5185;&#x90e8;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff08;hidden states&#xff09;&#xff0c;&#x5e76;&#x4e3a;&#x6bcf;&#x4e2a;&#x6807;&#x8bb0;&#x6807;&#x6ce8;&#x662f;&#x5426;&#x4e3a;&#x5e7b;&#x89c9;&#xff0c;&#x6784;&#x5efa;&#x6570;&#x636e;&#x96c6;&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;LVLM&#x5185;&#x90e8;&#x72b6;&#x6001;&#x662f;&#x9ad8;&#x7279;&#x5f02;&#x6027;&#x7684;&#x5e7b;&#x89c9;&#x6307;&#x793a;&#x5668;&#xff08;&#x5373;&#x8bef;&#x62a5;&#x7387;&#x6781;&#x4f4e;&#xff09;&#xff0c;&#x4e14;&#x4e0d;&#x540c;LVLM&#x5728;&#x6f5c;&#x5728;&#x5b50;&#x7a7a;&#x95f4;&#x4e2d;&#x5171;&#x4eab;&#x901a;&#x7528;&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;TruthPrInt&#x6846;&#x67b6;&#xff1a;1&#xff09;&#x5b66;&#x4e60;LVLM&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x201c;&#x771f;&#x5b9e;&#x65b9;&#x5411;&#x201d;&#xff08;truthful direction&#xff09;&#xff1b;2&#xff09;&#x5728;&#x89e3;&#x7801;&#x65f6;&#x8fdb;&#x884c;&#x57fa;&#x4e8e;&#x771f;&#x5b9e;&#x65b9;&#x5411;&#x7684;&#x5e72;&#x9884;&#xff0c;&#x62d2;&#x7edd;&#x5e7b;&#x89c9;&#x6807;&#x8bb0;&#x5e76;&#x56de;&#x6eaf;&#x5230;&#x201c;&#x65e9;&#x671f;&#x8d77;&#x70b9;&#x201d;&#x8fdb;&#x884c;&#x9884;&#x5e72;&#x9884;&#x3002;&#x540c;&#x65f6;&#xff0c;&#x63d0;&#x51fa;ComnHallu&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x548c;&#x5bf9;&#x9f50;&#x5e7b;&#x89c9;&#x6f5c;&#x5728;&#x5b50;&#x7a7a;&#x95f4;&#xff0c;&#x589e;&#x5f3a;&#x8de8;&#x6a21;&#x578b;&#x548c;&#x8de8;&#x6570;&#x636e;&#x7684;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x8fc1;&#x79fb;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"796,797"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;&#x4e3b;&#x6d41;LVLM&#xff08;&#x5982;MiniGPT-4&#x3001;LLaVA-1.5&#x3001;mPLUG-Owl2&#x7b49;&#xff09;&#x548c;OH&#x57fa;&#x51c6;&#xff08;&#x5982;CHAIR&#x3001;POPE&#x3001;LLaVA-Bench&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#x3002;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;TruthPrInt&#x5728;&#x57df;&#x5185;&#xff08;in-domain&#xff09;&#x548c;&#x57df;&#x5916;&#xff08;out-of-domain&#xff09;&#x573a;&#x666f;&#x4e0b;&#x5747;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x6700;&#x5148;&#x8fdb;&#x65b9;&#x6cd5;&#xff0c;&#x5185;&#x90e8;&#x72b6;&#x6001;&#x68c0;&#x6d4b;&#x7684;&#x9633;&#x6027;&#x4f3c;&#x7136;&#x6bd4;&#xff08;LR+&#xff09;&#x63a5;&#x8fd1;20&#xff0c;&#x8868;&#x660e;&#x5176;&#x5177;&#x6709;&#x6781;&#x4f4e;&#x7684;&#x8bef;&#x62a5;&#x7387;&#x548c;&#x9ad8;&#x7f6e;&#x4fe1;&#x5ea6;&#x68c0;&#x6d4b;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"797,798"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x8bc1;&#x5b9e;&#x4e86;LVLM&#x5185;&#x90e8;&#x72b6;&#x6001;&#x53ef;&#x4f5c;&#x4e3a;&#x9ad8;&#x7279;&#x5f02;&#x6027;&#x7684;&#x9010;&#x6807;&#x8bb0;&#x5e7b;&#x89c9;&#x6307;&#x793a;&#x5668;&#xff0c;&#x4e14;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x95f4;&#x5b58;&#x5728;&#x5171;&#x4eab;&#x7684;&#x201c;&#x771f;&#x5b9e;&#x65b9;&#x5411;&#x201d;&#x3002;TruthPrInt&#x901a;&#x8fc7;&#x6f5c;&#x5728;&#x5b50;&#x7a7a;&#x95f4;&#x5b66;&#x4e60;&#x548c;&#x89e3;&#x7801;&#x65f6;&#x5e72;&#x9884;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x548c;&#x5e72;&#x9884;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5177;&#x6709;&#x8de8;&#x6a21;&#x578b;&#x548c;&#x8de8;&#x6570;&#x636e;&#x7684;&#x6cdb;&#x5316;&#x6f5c;&#x529b;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;&#x53ef;&#x4fe1;&#x8d56;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53d1;&#x5c55;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"798,800"}}],"payload":{"tag":"li","lines":"794,800","fold":1}}],"payload":{"tag":"h4","lines":"792,793"}},{"content":"EFUF: Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: EFUF&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x7ec6;&#x7c92;&#x5ea6;&#x9057;&#x5fd8;&#x6846;&#x67b6;&#xff0c;&#x65e0;&#x9700;&#x914d;&#x5bf9;&#x6570;&#x636e;&#x5373;&#x53ef;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;(MLLM)&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x5c0f;&#x4e14;&#x6548;&#x679c;&#x597d;&#x3002;","children":[],"payload":{"tag":"li","lines":"801,802"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;(MLLM)&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff08;&#x5373;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x8fd9;&#x4f1a;&#x4f20;&#x64ad;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x5e76;&#x964d;&#x4f4e;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x9700;&#x8981;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x914d;&#x5bf9;&#x6570;&#x636e;&#xff08;&#x542b;&#x5e7b;&#x89c9;&#x548c;&#x4e0d;&#x542b;&#x5e7b;&#x89c9;&#x7684;&#x54cd;&#x5e94;&#xff09;&#x548c;&#x5927;&#x91cf;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5e94;&#x7528;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x9ad8;&#x6548;&#x3001;&#x6210;&#x672c;&#x66f4;&#x4f4e;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"803,804"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;EFUF&#x6846;&#x67b6;&#xff0c;&#x57fa;&#x4e8e;&#x2018;&#x9057;&#x5fd8;&#x5b66;&#x4e60;&#x2019;&#x539f;&#x7406;&#xff0c;&#x901a;&#x8fc7;&#x68af;&#x5ea6;&#x4e0a;&#x5347;&#x6280;&#x672f;&#x4f7f;&#x7528;&#x4e09;&#x79cd;&#x5b9a;&#x5236;&#x5316;&#x635f;&#x5931;&#x51fd;&#x6570;&#x6765;&#x6d88;&#x9664;&#x5e7b;&#x89c9;&#x3002;&#x5173;&#x952e;&#x521b;&#x65b0;&#x662f;&#x5229;&#x7528;CLIP&#x6a21;&#x578b;&#x81ea;&#x52a8;&#x8bc4;&#x4f30;&#x6587;&#x672c;-&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x6027;&#xff0c;&#x4ee5;&#x65e0;&#x4eba;&#x5de5;&#x65b9;&#x5f0f;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#xff08;&#x8d1f;&#x6837;&#x672c;&#xff09;&#x548c;&#x975e;&#x5e7b;&#x89c9;&#xff08;&#x6b63;&#x6837;&#x672c;&#xff09;&#x5bf9;&#x8c61;&#xff0c;&#x65e0;&#x9700;&#x914d;&#x5bf9;&#x6570;&#x636e;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x4f7f;&#x7528;CLIP&#x8ba1;&#x7b97;&#x5bf9;&#x8c61;&#x4e0e;&#x56fe;&#x50cf;&#x533a;&#x57df;&#x7684;&#x76f8;&#x4f3c;&#x6027;&#x5206;&#x6570;&#xff1b;2) &#x6839;&#x636e;&#x5206;&#x6570;&#x81ea;&#x52a8;&#x6784;&#x5efa;&#x6b63;&#x8d1f;&#x6837;&#x672c;&#x96c6;&#xff1b;3) &#x5bf9;&#x8d1f;&#x6837;&#x672c;&#xff08;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#xff09;&#x6267;&#x884c;&#x68af;&#x5ea6;&#x4e0a;&#x5347;&#x4f7f;&#x5176;&#x88ab;&#x2018;&#x9057;&#x5fd8;&#x2019;&#xff0c;&#x540c;&#x65f6;&#x5bf9;&#x6b63;&#x6837;&#x672c;&#x65bd;&#x52a0;&#x7ea6;&#x675f;&#x4ee5;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x6b63;&#x5e38;&#x751f;&#x6210;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"804,805"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) CLIP&#x5206;&#x6570;&#x80fd;&#x53ef;&#x9760;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x4e0e;&#x975e;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#xff08;t&#x68c0;&#x9a8c;p&#x503c;&#x6781;&#x663e;&#x8457;&#xff09;&#xff1b;2) EFUF&#x5728;&#x591a;&#x4e2a;MLLM&#xff08;&#x5982;MiniGPT4&#x3001;LLaVA&#xff09;&#x4e0a;&#x4e00;&#x81f4;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x751f;&#x6210;&#x8d28;&#x91cf;&#xff1b;3) &#x4e0e;&#x4f20;&#x7edf;&#x5fae;&#x8c03;&#x65b9;&#x6cd5;&#x76f8;&#x6bd4;&#xff0c;EFUF&#x5927;&#x5e45;&#x51cf;&#x5c11;&#x4e86;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x548c;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x9700;&#x6c42;&#x3002;","children":[],"payload":{"tag":"li","lines":"805,806"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: EFUF&#x9996;&#x6b21;&#x5c06;&#x9057;&#x5fd8;&#x5b66;&#x4e60;&#x5e94;&#x7528;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9; mitigation&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x6570;&#x636e;&#x6548;&#x7387;&#x548c;&#x8ba1;&#x7b97;&#x6548;&#x7387;&#x9ad8;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff0c;&#x53ef;&#x8f7b;&#x677e;&#x6269;&#x5c55;&#x5230;&#x73b0;&#x6709;MLLM&#xff0c;&#x4e3a;&#x63d0;&#x9ad8;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x51cf;&#x5c11;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;MLLM&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"806,808"}}],"payload":{"tag":"li","lines":"802,808","fold":1}}],"payload":{"tag":"h4","lines":"800,801"}},{"content":"Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection","children":[{"content":"","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;Nullu&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8bc6;&#x522b;&#x5e76;&#x6295;&#x5f71;&#x5230;&apos;&#x5e7b;&#x89c9;&#x5b50;&#x7a7a;&#x95f4;&apos;(HalluSpace)&#x7684;&#x96f6;&#x7a7a;&#x95f4;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;(OH)&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x63a8;&#x7406;&#x6210;&#x672c;&#x4e14;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x901a;&#x7528;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"809,810"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x5e7b;&#x89c9;(OH)&#xff0c;&#x5373;&#x63cf;&#x8ff0;&#x4e0e;&#x56fe;&#x50cf;&#x5b9e;&#x9645;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5bf9;&#x8c61;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4f1a;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x4f20;&#x64ad;&#x548c;&#x6709;&#x5bb3;&#x51b3;&#x7b56;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;AI&#x7cfb;&#x7edf;&#x7684;&#x5b89;&#x5168;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x5fae;&#x8c03;&#x6216;&#x540e;&#x5904;&#x7406;&#xff0c;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x672a;&#x6df1;&#x5165;&#x63a2;&#x7d22;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x6210;&#x56e0;&#x3002;","children":[],"payload":{"tag":"li","lines":"811,812"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6570;&#x636e;&#x6784;&#x5efa;&#xff1a;&#x4f7f;&#x7528;&#x540c;&#x4e00;&#x56fe;&#x50cf;&#x914d;&#x5bf9;&#x7684;&#x771f;&#x5b9e;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x548c;GPT-3.5&#x751f;&#x6210;&#x7684;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"812,813"}}],"payload":{"tag":"li","lines":"810,813","fold":1}}],"payload":{"tag":"ul","lines":"809,813"}},{"content":"","children":[{"content":"2. &#x8bc6;&#x522b;HalluSpace&#xff1a;&#x901a;&#x8fc7;PCA&#x548c;&#x5947;&#x5f02;&#x503c;&#x5206;&#x89e3;(SVD)&#x63d0;&#x53d6;&#x4e0d;&#x540c;&#x5c42;&#x4e2d;&#x771f;&#x5b9e;&#x4e0e;&#x5e7b;&#x89c9;&#x7279;&#x5f81;&#x5dee;&#x5f02;&#x77e9;&#x9635;&#x7684;&#x4e3b;&#x65b9;&#x5411;&#xff0c;&#x5f62;&#x6210;&#x4f4e;&#x79e9;&#x5e7b;&#x89c9;&#x5b50;&#x7a7a;&#x95f4;&#x3002;","children":[],"payload":{"tag":"li","lines":"813,814","listIndex":2}},{"content":"3. &#x6743;&#x91cd;&#x7f16;&#x8f91;&#xff1a;&#x5c06;&#x6a21;&#x578b;MLP&#x5c42;&#x7684;&#x6743;&#x91cd;&#x6295;&#x5f71;&#x5230;HalluSpace&#x7684;&#x96f6;&#x7a7a;&#x95f4;&#xff08;&#x901a;&#x8fc7;&#x6b63;&#x4ea4;&#x5316;&#x5b9e;&#x73b0;&#xff09;&#xff0c;&#x4ece;&#x800c;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x7279;&#x5f81;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x76f4;&#x63a5;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x6743;&#x91cd;&#x3002;","children":[{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x6709;&#x6548;&#x964d;&#x4f4e;&#x591a;&#x79cd;&#x5f00;&#x6e90;LVLM&#xff08;&#x5982;LLaVA&#xff09;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"815,816"}}],"payload":{"tag":"li","lines":"814,816","listIndex":3}},{"content":"4. &#x5728;&#x901a;&#x7528;LVLM&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4fdd;&#x6301;&#x5f3a;&#x52b2;&#x6027;&#x80fd;&#xff0c;&#x65e0;&#x663e;&#x8457;&#x6027;&#x80fd;&#x635f;&#x5931;&#x3002;","children":[],"payload":{"tag":"li","lines":"816,817","listIndex":4}},{"content":"5. &#x96f6;&#x989d;&#x5916;&#x63a8;&#x7406;&#x6210;&#x672c;&#xff0c;&#x8ba1;&#x7b97;&#x6548;&#x7387;&#x9ad8;&#x3002;","children":[],"payload":{"tag":"li","lines":"817,818","listIndex":5}},{"content":"6. &#x53d1;&#x73b0;HalluSpace&#x4e0e;LLM&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x5148;&#x9a8c;&#x662f;&#x5bfc;&#x81f4;OH&#x7684;&#x91cd;&#x8981;&#x56e0;&#x7d20;&#x3002;","children":[{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Nullu&#x901a;&#x8fc7;&#x7279;&#x5f81;&#x7a7a;&#x95f4;&#x5206;&#x6790;&#x548c;&#x6743;&#x91cd;&#x6295;&#x5f71;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x7684;OH&#x7f13;&#x89e3;&#x65b9;&#x6848;&#xff0c;&#x63ed;&#x793a;&#x4e86;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x5e7b;&#x89c9;&#x673a;&#x5236;&#x4e0e;LLM&#x5148;&#x9a8c;&#x7684;&#x5173;&#x8054;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;LVLM&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x4e0e;DPO&#x7b49;&#x65b9;&#x6cd5;&#x7684;&#x7406;&#x8bba;&#x8054;&#x7cfb;&#x62d3;&#x5bbd;&#x4e86;&#x5bf9;&#x6a21;&#x578b;&#x5b89;&#x5168;&#x884c;&#x4e3a;&#x7684;&#x7406;&#x89e3;&#x3002;","children":[],"payload":{"tag":"li","lines":"819,821"}}],"payload":{"tag":"li","lines":"818,821","listIndex":6}}],"payload":{"tag":"ol","lines":"813,821"}}],"payload":{"tag":"h4","lines":"808,809"}},{"content":"VTI: Reducing Hallucinations in Vision-Language Models via Latent Space Steering","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;VTI&#xff08;&#x89c6;&#x89c9;&#x4e0e;&#x6587;&#x672c;&#x5e72;&#x9884;&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x5f15;&#x5bfc;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x6f5c;&#x5728;&#x7a7a;&#x95f4;&#x8868;&#x793a;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#xff0c;&#x5373;&#x53ef;&#x6709;&#x6548;&#x63d0;&#x5347;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7684;&#x7a33;&#x5b9a;&#x6027;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x591a;&#x6307;&#x6807;&#x4e0a;&#x8d85;&#x8d8a;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"822,823"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x3001;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;&#x6587;&#x672c;&#x89e3;&#x7801;&#x5668;&#x9884;&#x8bad;&#x7ec3;&#x5206;&#x79bb;&#x5bfc;&#x81f4;&#x7684;&#x7279;&#x5f81;&#x5bf9;&#x9f50;&#x4e0d;&#x7a33;&#x5b9a;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"824,825"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;VTI&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x53cc;&#x91cd;&#x5e72;&#x9884;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x5e72;&#x9884;&#xff1a;&#x901a;&#x8fc7;&#x9884;&#x8ba1;&#x7b97;&#x56fe;&#x50cf;&#x8f7b;&#x5fae;&#x6270;&#x52a8;&#x540e;&#x7279;&#x5f81;&#x5e73;&#x5747;&#x5316;&#x7684;&#x65b9;&#x5411;&#x5411;&#x91cf;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x76f4;&#x63a5;&#x7f16;&#x8f91;&#x6f5c;&#x5728;&#x7a7a;&#x95f4;&#x7279;&#x5f81;&#xff0c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x7a33;&#x5b9a;&#x6027;&#xff1b;2&#xff09;&#x6587;&#x672c;&#x5e72;&#x9884;&#xff1a;&#x7c7b;&#x4f3c;&#x5730;&#x9884;&#x8ba1;&#x7b97;&#x6587;&#x672c;&#x7a7a;&#x95f4;&#x7684;&#x5e72;&#x9884;&#x65b9;&#x5411;&#x4ee5;&#x51cf;&#x5c11;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;50&#x4e2a;&#x6837;&#x672c;&#x9884;&#x8ba1;&#x7b97;&#x65b9;&#x5411;&#x5411;&#x91cf;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x591a;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x4efb;&#x4f55;&#x4efb;&#x52a1;&#x3002;","children":[],"payload":{"tag":"li","lines":"825,826"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7a33;&#x5b9a;&#x6027;&#x4e0e;&#x5e7b;&#x89c9;&#x7387;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff08;&#x4e0d;&#x7a33;&#x5b9a;&#x7279;&#x5f81;&#x7ea6;&#x5360;15%&#xff09;&#xff1b;2&#xff09;&#x5bf9;&#x56fe;&#x50cf;&#x6270;&#x52a8;&#x540e;&#x7279;&#x5f81;&#x5e73;&#x5747;&#x5316;&#x53ef;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#xff1b;3&#xff09;VTI&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MMHAL-Bench&#x3001;CHAIR&#xff09;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x4e14;&#x4f18;&#x4e8e;&#x6240;&#x6709;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"826,827"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: VTI&#x901a;&#x8fc7;&#x6f5c;&#x5728;&#x7a7a;&#x95f4;&#x5e72;&#x9884;&#x6709;&#x6548;&#x51cf;&#x5c11;LVLM&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x63ed;&#x793a;&#x4e86;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7a33;&#x5b9a;&#x6027;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x7684;&#x5173;&#x952e;&#x4f5c;&#x7528;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x8ba1;&#x7b97;&#x9ad8;&#x6548;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x63a8;&#x52a8;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"827,829"}}],"payload":{"tag":"li","lines":"823,829","fold":1}}],"payload":{"tag":"h4","lines":"821,822"}},{"content":"PROJECTAWAY: Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x901a;&#x8fc7;&#x89e3;&#x91ca;&#x548c;&#x7f16;&#x8f91;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5185;&#x90e8;&#x8868;&#x793a;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;&#x771f;&#x5b9e;&#x7269;&#x4f53;&#x5728;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x8868;&#x793a;&#x4e2d;&#x7684;&#x7f6e;&#x4fe1;&#x5ea6;&#x9ad8;&#x4e8e;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#xff0c;&#x5e76;&#x57fa;&#x4e8e;&#x6b64;&#x5f00;&#x53d1;&#x4e86;PROJECTAWAY&#x7b97;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x7ebf;&#x6027;&#x6b63;&#x4ea4;&#x5316;&#x7279;&#x5f81;&#x79fb;&#x9664;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;COCO2014&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x6700;&#x9ad8;&#x51cf;&#x5c11;25.7%&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x6027;&#x80fd;&#xff0c;&#x5e76;&#x5b9e;&#x73b0;&#x4e86;&#x96f6;&#x6837;&#x672c;&#x5206;&#x5272;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"830,831"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x3001;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5185;&#x5bb9;&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x73b0;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x5c3d;&#x7ba1;&#x6a21;&#x578b;&#x89c4;&#x6a21;&#x548c;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e0d;&#x65ad;&#x589e;&#x5927;&#xff0c;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4f9d;&#x7136;&#x5b58;&#x5728;&#xff0c;&#x4e14;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x4f7f;&#x7528;&#x5916;&#x90e8;&#x68c0;&#x6d4b;&#x5668;&#x6216;&#x5fae;&#x8c03;&#xff09;&#x96be;&#x4ee5;&#x533a;&#x5206;&#x7ec6;&#x5fae;&#x5e7b;&#x89c9;&#x4e14;&#x9700;&#x8981;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x53c2;&#x6570;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x76f4;&#x63a5;&#x7f16;&#x8f91;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x8868;&#x793a;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7cbe;&#x51c6;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"832,833"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x91c7;&#x7528;&#x4ee5;&#x4e0b;&#x65b9;&#x6cd5;&#xff1a;1. &#x4f7f;&#x7528;Logit Lens&#x6280;&#x672f;&#x5c06;VLM&#x5185;&#x90e8;&#x56fe;&#x50cf;&#x8868;&#x793a;&#x6295;&#x5f71;&#x5230;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x8bcd;&#x6c47;&#x8868;&#xff0c;&#x83b7;&#x5f97;&#x6bcf;&#x4e2a;&#x56fe;&#x50cf;&#x5757;&#x5bf9;&#x5e94;&#x7684;&#x8bcd;&#x6c47;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x4ece;&#x800c;&#x8ba1;&#x7b97;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x7684;&#x5185;&#x90e8;&#x7f6e;&#x4fe1;&#x5ea6;&#xff08;co&#xff09;&#x3002;2. &#x53d1;&#x73b0;&#x771f;&#x5b9e;&#x7269;&#x4f53;&#x7684;&#x5185;&#x90e8;&#x7f6e;&#x4fe1;&#x5ea6;&#x663e;&#x8457;&#x9ad8;&#x4e8e;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x8fd9;&#x4e00;&#x5dee;&#x5f02;&#x8fdb;&#x884c;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x3002;3. &#x63d0;&#x51fa;PROJECTAWAY&#x77e5;&#x8bc6;&#x64e6;&#x9664;&#x7b97;&#x6cd5;&#xff1a;&#x901a;&#x8fc7;&#x7ebf;&#x6027;&#x6b63;&#x4ea4;&#x5316;&#x64cd;&#x4f5c;&#xff0c;&#x5c06;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#x76f8;&#x5bf9;&#x4e8e;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x7684;&#x6587;&#x672c;&#x7279;&#x5f81;&#x8fdb;&#x884c;&#x7f16;&#x8f91;&#xff0c;&#x4ece;&#x800c;&#x4ece;&#x6f5c;&#x5728;&#x8868;&#x793a;&#x4e2d;&#x79fb;&#x9664;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x53c2;&#x6570;&#xff0c;&#x76f4;&#x63a5;&#x64cd;&#x4f5c;&#x4e8e;&#x5185;&#x90e8;&#x8868;&#x793a;&#x3002;","children":[],"payload":{"tag":"li","lines":"833,834"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1. &#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#xff1a;Logit Lens&#x5728;&#x4e24;&#x79cd;VLM&#xff08;InstructBLIP&#x548c;LLaVA&#xff09;&#x4e0a;&#x5c06;&#x5e73;&#x5747;&#x7cbe;&#x5ea6;&#xff08;mAP&#xff09;&#x63d0;&#x9ad8;&#x4e86;22.45%&#x548c;47.17%&#x3002;2. &#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#xff1a;PROJECTAWAY&#x7b97;&#x6cd5;&#x5728;COCO2014&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x6700;&#x9ad8;&#x51cf;&#x5c11;25.7%&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;3. &#x96f6;&#x6837;&#x672c;&#x5206;&#x5272;&#xff1a;&#x5229;&#x7528;Logit Lens&#x7684;&#x7a7a;&#x95f4;&#x5b9a;&#x4f4d;&#x80fd;&#x529b;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x4e0e;&#x6700;&#x5148;&#x8fdb;&#x96f6;&#x6837;&#x672c;&#x5206;&#x5272;&#x65b9;&#x6cd5;&#x76f8;&#x5f53;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"834,835"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x8868;&#x660e;&#xff1a;&#x901a;&#x8fc7;&#x89e3;&#x91ca;VLM&#x7684;&#x5185;&#x90e8;&#x8868;&#x793a;&#xff0c;&#x53ef;&#x4ee5;&#x7cbe;&#x51c6;&#x8bc6;&#x522b;&#x548c;&#x7f16;&#x8f91;&#x5e7b;&#x89c9;&#x7279;&#x5f81;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x3002;PROJECTAWAY&#x7b97;&#x6cd5;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x91cd;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x6709;&#x6548;&#x624b;&#x6bb5;&#xff0c;&#x4e14;&#x63ed;&#x793a;&#x4e86;&#x5185;&#x90e8;&#x8868;&#x793a;&#x7684;&#x53ef;&#x7f16;&#x8f91;&#x6027;&#x4e3a;&#x6a21;&#x578b;&#x4fee;&#x590d;&#x548c;&#x65b0;&#x80fd;&#x529b;&#xff08;&#x5982;&#x96f6;&#x6837;&#x672c;&#x5206;&#x5272;&#xff09;&#x5f00;&#x8f9f;&#x4e86;&#x9014;&#x5f84;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x65b0;&#x5de5;&#x5177;&#xff0c;&#x5e76;&#x4fc3;&#x8fdb;&#x5bf9;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x673a;&#x5236;&#x7684;&#x7406;&#x89e3;&#x3002;","children":[],"payload":{"tag":"li","lines":"835,837"}}],"payload":{"tag":"li","lines":"831,837","fold":1}}],"payload":{"tag":"h4","lines":"829,830"}},{"content":"VISTA: The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models via Visual Information Steering","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;VISTA&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;token&#x6392;&#x540d;&#x52a8;&#x6001;&#xff0c;&#x53d1;&#x73b0;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x9010;&#x6e10;&#x4e22;&#x5931;&#x3001;&#x65e9;&#x671f;&#x8bed;&#x4e49;&#x6fc0;&#x53d1;&#x548c;&#x9690;&#x85cf;&#x771f;&#x5b9e;&#x4fe1;&#x606f;&#x4e09;&#x79cd;&#x6a21;&#x5f0f;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x89c6;&#x89c9;&#x5f15;&#x5bfc;&#x5411;&#x91cf;&#x548c;&#x81ea;logits&#x589e;&#x5f3a;&#x6280;&#x672f;&#xff0c;&#x5e73;&#x5747;&#x51cf;&#x5c11;&#x7ea6;40%&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"838,839"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x8bed;&#x6cd5;&#x8fde;&#x8d2f;&#x4f46;&#x89c6;&#x89c9;&#x4e0a;&#x65e0;&#x4f9d;&#x636e;&#x7684;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x5176;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x4ea4;&#x4e92;&#x52a9;&#x624b;&#x7b49;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x4ece;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x6216;logits&#x5206;&#x5e03;&#x89d2;&#x5ea6;&#x63a2;&#x8ba8;&#x6b64;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x5e7b;&#x89c9;&#x5728;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x5982;&#x4f55;&#x4ea7;&#x751f;&#x548c;&#x4f20;&#x64ad;&#x4ecd;&#x4e0d;&#x660e;&#x786e;&#x3002;","children":[],"payload":{"tag":"li","lines":"840,841"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x901a;&#x8fc7;token&#x6392;&#x540d;&#x5206;&#x6790;&#x63ed;&#x793a;&#x4e86;LVLM&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x4e09;&#x79cd;&#x5173;&#x952e;&#x6a21;&#x5f0f;&#xff1a;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x9010;&#x6e10;&#x4e22;&#x5931;&#x3001;&#x8bed;&#x4e49;token&#x5728;&#x65e9;&#x671f;&#x5c42;&#x6fc0;&#x53d1;&#x3001;&#x4ee5;&#x53ca;&#x9690;&#x85cf;&#x7684;&#x771f;&#x5b9e;&#x4fe1;&#x606f;&#x59cb;&#x7ec8;&#x7ef4;&#x6301;&#x8f83;&#x9ad8;&#x6392;&#x540d;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e86;VISTA&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x5f15;&#x5bfc;&#x5411;&#x91cf;&#xff08;VSV&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x5305;&#x542b;&#x89c6;&#x89c9;token&#x7684;&#x6b63;&#x9762;&#x4e0a;&#x4e0b;&#x6587;&#x548c;&#x4e0d;&#x542b;&#x89c6;&#x89c9;token&#x7684;&#x8d1f;&#x9762;&#x4e0a;&#x4e0b;&#x6587;&#xff0c;&#x5728;&#x6fc0;&#x6d3b;&#x7a7a;&#x95f4;&#x4e2d;&#x63d0;&#x53d6;&#x65b9;&#x5411;&#x5411;&#x91cf;&#xff0c;&#x5e76;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x6ce8;&#x5165;&#x5230;&#x6b8b;&#x5dee;&#x6d41;&#x4e2d;&#x4ee5;&#x5f3a;&#x5316;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff1b;2&#xff09;&#x81ea;logits&#x589e;&#x5f3a;&#xff08;SLA&#xff09;&#xff1a;&#x5229;&#x7528;&#x65e9;&#x671f;&#x5c42;&#xff08;&#x5982;&#x5012;&#x6570;&#x7b2c;&#x4e8c;&#x5c42;&#xff09;&#x7684;&#x6fc0;&#x6d3b;&#x72b6;&#x6001;&#xff0c;&#x901a;&#x8fc7;logit lens&#x6280;&#x672f;&#x83b7;&#x53d6;&#x66f4;&#x8bed;&#x4e49;&#x4e30;&#x5bcc;&#x7684;token&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x4e0e;&#x6700;&#x7ec8;&#x5c42;&#x5206;&#x5e03;&#x52a0;&#x6743;&#x878d;&#x5408;&#xff0c;&#x4ee5;&#x4fc3;&#x8fdb;&#x6709;&#x610f;&#x4e49;&#x5185;&#x5bb9;&#x7684;&#x751f;&#x6210;&#x3002;","children":[],"payload":{"tag":"li","lines":"841,842"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x67b6;&#x6784;&#xff08;LLaVA&#x3001;Shikra&#x3001;MiniGPT-4&#x3001;InstructBLIP&#xff09;&#x548c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x4e0b;&#xff0c;VISTA&#x5728;&#x56db;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4e00;&#x81f4;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;&#x5728;&#x5f00;&#x653e;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e0a;&#xff0c;&#x5e73;&#x5747;&#x51cf;&#x5c11;&#x7ea6;40%&#x7684;&#x5e7b;&#x89c9;&#xff1b;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x4e2d;&#x4e5f;&#x6709;&#x663e;&#x8457;&#x63d0;&#x5347;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6a21;&#x578b;&#x4fee;&#x6539;&#xff0c;&#x4e14;&#x9002;&#x7528;&#x4e8e;&#x4e0d;&#x540c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"842,843"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: VISTA&#x901a;&#x8fc7;&#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x5347;&#x4e86;&#x771f;&#x5b9e;&#x4fe1;&#x606f;&#x7684;&#x751f;&#x6210;&#x3002;&#x5176;&#x5173;&#x952e;&#x4ef7;&#x503c;&#x5728;&#x4e8e;&#xff1a;1&#xff09;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x5206;&#x6790;&#x4e86;token&#x7ea7;&#x522b;&#x7684;&#x751f;&#x6210;&#x52a8;&#x6001;&#xff0c;&#x63ed;&#x793a;&#x4e86;&#x5e7b;&#x89c9;&#x4ea7;&#x751f;&#x673a;&#x5236;&#xff1b;2&#xff09;&#x63d0;&#x4f9b;&#x4e86;&#x8f7b;&#x91cf;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x5b9e;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff1b;3&#xff09;&#x4e3a;&#x7406;&#x89e3;&#x548c;&#x5b8c;&#x5584;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x673a;&#x5236;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x6846;&#x67b6;&#x3002;","children":[],"payload":{"tag":"li","lines":"843,845"}}],"payload":{"tag":"li","lines":"839,845","fold":1}}],"payload":{"tag":"h4","lines":"837,838"}},{"content":"SSL: Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;SSL&#x7684;&#x5373;&#x63d2;&#x5373;&#x7528;&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x7a00;&#x758f;&#x81ea;&#x7f16;&#x7801;&#x5668;&#xff08;SAE&#xff09;&#x8bc6;&#x522b;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5185;&#x90e8;&#x4e0e;&#x5e7b;&#x89c9;&#x548c;&#x5fe0;&#x5b9e;&#x8bed;&#x4e49;&#x76f8;&#x5173;&#x7684;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x5e72;&#x9884;&#x8fd9;&#x4e9b;&#x65b9;&#x5411;&#x6765;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x751f;&#x6210;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"846,847"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x98ce;&#x9669;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#x3001;&#x5bf9;&#x9f50;&#x8bad;&#x7ec3;&#x6216;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x901a;&#x5e38;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x8017;&#x65f6;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x7cbe;&#x786e;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"848,849"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x4f7f;&#x7528;&#x7a00;&#x758f;&#x81ea;&#x7f16;&#x7801;&#x5668;&#xff08;SAE&#xff09;&#x5206;&#x6790;LVLM&#x7684;&#x5185;&#x90e8;&#x8868;&#x793a;&#xff0c;&#x8bc6;&#x522b;&#x4e0e;&#x5e7b;&#x89c9;&#x548c;&#x5fe0;&#x5b9e;&#x8bed;&#x4e49;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#x7684;&#x6f5c;&#x5728;&#x65b9;&#x5411;&#xff08;&#x79f0;&#x4e3a;dhall&#x548c;dfaithful&#xff09;&#x3002;&#x5728;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x878d;&#x5408;&#x9636;&#x6bb5;&#x6ce8;&#x5165;&#x5fe0;&#x5b9e;&#x65b9;&#x5411;&#x4ee5;&#x589e;&#x5f3a;&#x63a5;&#x5730;&#x8bed;&#x4e49;&#xff0c;&#x5728;&#x8bed;&#x8a00;&#x751f;&#x6210;&#x9636;&#x6bb5;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x65b9;&#x5411;&#x4ee5;&#x51cf;&#x5c11;&#x9519;&#x8bef;&#x5185;&#x5bb9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x53ef;&#x5ffd;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"849,850"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;SSL&#x5728;&#x591a;&#x4e2a;LVLM&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x6548;&#x679c;&#x660e;&#x663e;&#x3002;&#x540c;&#x65f6;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x8de8;&#x67b6;&#x6784;&#x8fc1;&#x79fb;&#x6027;&#xff08;&#x5982;LLaVA1.5-7b&#x548c;InstructBLIP-7b&#xff09;&#xff0c;&#x4e14;&#x989d;&#x5916;&#x65f6;&#x95f4;&#x5f00;&#x9500;&#x53ef;&#x5ffd;&#x7565;&#x4e0d;&#x8ba1;&#x3002;","children":[],"payload":{"tag":"li","lines":"850,851"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SSL&#x901a;&#x8fc7;SAE&#x8bc6;&#x522b;&#x5e76;&#x5e72;&#x9884;&#x8bed;&#x4e49;&#x65b9;&#x5411;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x5177;&#x6709;&#x9ad8;&#x6548;&#x3001;&#x53ef;&#x8fc1;&#x79fb;&#x7684;&#x7279;&#x70b9;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x7406;&#x89e3;&#x548c;&#x7ba1;&#x7406;LVLM&#x7684;&#x5185;&#x90e8;&#x8868;&#x793a;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"851,853"}}],"payload":{"tag":"li","lines":"847,853","fold":1}}],"payload":{"tag":"h4","lines":"845,846"}},{"content":"SHE: Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;SHE&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x81ea;&#x9002;&#x5e94;&#x65f6;&#x95f4;&#x7a97;&#x53e3;&#x68c0;&#x6d4b;&#x548c;&#x6b63;&#x4ea4;&#x6295;&#x5f71;&#x6d88;&#x9664;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x5728;&#x5e8f;&#x5217;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x884c;&#x4e3a;&#x5e7b;&#x89c9;&#xff0c;BEACH&#x6307;&#x6807;&#x663e;&#x793a;&#x5e7b;&#x89c9;&#x964d;&#x4f4e;&#x8d85;10%&#x3002;","children":[],"payload":{"tag":"li","lines":"854,855"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x5e8f;&#x5217;&#x56fe;&#x50cf;&#x5904;&#x7406;&#x4e2d;&#x5b58;&#x5728;&#x884c;&#x4e3a;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x865a;&#x6784;&#x7269;&#x4f53;&#x884c;&#x4e3a;&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x6559;&#x80b2;&#x3001;&#x533b;&#x7597;&#x7b49;&#x654f;&#x611f;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x9759;&#x6001;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x800c;&#x884c;&#x4e3a;&#x5e7b;&#x89c9;&#x56e0;&#x65f6;&#x5e8f;&#x4f9d;&#x8d56;&#x548c;&#x8bef;&#x5dee;&#x4f20;&#x64ad;&#x66f4;&#x590d;&#x6742;&#x4e14;&#x7f3a;&#x4e4f;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"856,857"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: SHE&#x662f;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x4e24;&#x9636;&#x6bb5;&#x6846;&#x67b6;&#xff1a;1) &#x4f7f;&#x7528;&#x81ea;&#x9002;&#x5e94;&#x65f6;&#x95f4;&#x7a97;&#x53e3;&#x8fdb;&#x884c;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x5bf9;&#x9f50;&#x68c0;&#x67e5;&#x4ee5;&#x68c0;&#x6d4b;&#x5e7b;&#x89c9;&#xff1b;2) &#x5728;&#x8054;&#x5408;&#x5d4c;&#x5165;&#x7a7a;&#x95f4;&#x901a;&#x8fc7;&#x6b63;&#x4ea4;&#x6295;&#x5f71;&#x6d88;&#x9664;&#x5e7b;&#x89c9;&#x6210;&#x5206;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;&#x540c;&#x65f6;&#x63d0;&#x51fa;&#x65b0;&#x6307;&#x6807;BEACH&#x91cf;&#x5316;&#x884c;&#x4e3a;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x7a0b;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"857,858"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x6807;&#x51c6;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;SHE&#x5c06;&#x884c;&#x4e3a;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;&#x8d85;&#x8fc7;10%&#xff08;&#x57fa;&#x4e8e;BEACH&#x6307;&#x6807;&#xff09;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x63cf;&#x8ff0;&#x51c6;&#x786e;&#x6027;&#x3002;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6790;&#x663e;&#x793a;&#x6a21;&#x578b;87%&#x6ce8;&#x610f;&#x529b;&#x96c6;&#x4e2d;&#x4e8e;&#x6587;&#x672c;&#x4ee4;&#x724c;&#xff08;&#x800c;&#x975e;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff09;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x5148;&#x9a8c;&#x504f;&#x5dee;&#x7684;&#x5b58;&#x5728;&#x3002;","children":[],"payload":{"tag":"li","lines":"858,859"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SHE&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x89e3;&#x51b3;&#x5e8f;&#x5217;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x884c;&#x4e3a;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x901a;&#x8fc7;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x8f7b;&#x91cf;&#x7ea7;&#x65b9;&#x6cd5;&#x63d0;&#x5347;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x89c6;&#x9891;&#x7b49;&#x52a8;&#x6001;&#x573a;&#x666f;&#xff0c;&#x4e3a;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"859,861"}}],"payload":{"tag":"li","lines":"855,861","fold":1}}],"payload":{"tag":"h4","lines":"853,854"}}],"payload":{"tag":"h3","lines":"782,783","fold":1}},{"content":"&#x57fa;&#x4e8e;&#x5916;&#x90e8;&#x5de5;&#x5177;&#x7684;&#x9a8c;&#x8bc1;","children":[{"content":"Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: VisualFactChecker (VFC) &#x662f;&#x4e00;&#x4e2a;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5229;&#x7528;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x534f;&#x8c03;&#x5f00;&#x6e90;&#x89c6;&#x89c9;&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x4e8b;&#x5b9e;&#x6838;&#x67e5;&#x7684;&#x6d41;&#x7a0b;&#xff0c;&#x80fd;&#x751f;&#x6210;&#x9ad8;&#x4fdd;&#x771f;&#x3001;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x7684;2D&#x56fe;&#x50cf;&#x548c;3D&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x63cf;&#x8ff0;&#x8fc7;&#x4e8e;&#x7b80;&#x7565;&#x6216;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#x7684;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"864,865"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;&#x73b0;&#x6709;&#x81ea;&#x52a8;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff08;Captioning&#xff09;&#x65b9;&#x6cd5;&#x5b58;&#x5728;&#x7684;&#x4e09;&#x4e2a;&#x4e3b;&#x8981;&#x95ee;&#x9898;&#xff1a;1) &#x63cf;&#x8ff0;&#x7f3a;&#x4e4f;&#x7ec6;&#x8282;&#xff08;&#x5982;BLIP-2, OFA&#xff09;&#xff1b;2) &#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#xff08;&#x5982;InstructBLIP, LLaVA&#xff09;&#xff1b;3) &#x9075;&#x5faa;&#x590d;&#x6742;&#x6307;&#x4ee4;&#x7684;&#x80fd;&#x529b;&#x5dee;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x9ad8;&#x8d28;&#x91cf;&#x3001;&#x51c6;&#x786e;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x662f;&#x8ba1;&#x7b97;&#x673a;&#x89c6;&#x89c9;&#x4e0e;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x5904;&#x7406;&#x4ea4;&#x53c9;&#x9886;&#x57df;&#x7684;&#x6838;&#x5fc3;&#x6311;&#x6218;&#xff0c;&#x662f;&#x8bb8;&#x591a;&#x4e0b;&#x6e38;&#x5e94;&#x7528;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x3001;&#x5185;&#x5bb9;&#x751f;&#x6210;&#xff09;&#x7684;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"866,867"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x4e09;&#x9636;&#x6bb5;&#x7684;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x6d41;&#x7a0b;&#xff08;VFC&#xff09;&#xff1a;1.  <strong>&#x63d0;&#x8bae; (Proposal)</strong>&#xff1a;&#x4f7f;&#x7528;&#x591a;&#x4e2a;&#x73b0;&#x6210;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x6a21;&#x578b;&#xff08;&#x5982;&#x56fe;&#x4e2d;&#x7684;Captioner-1, Captioner-2&#xff09;&#x4e3a;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x6216;3D&#x7269;&#x4f53;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x521d;&#x6b65;&#x63cf;&#x8ff0;&#xff08;Initial Captions&#xff09;&#xff0c;&#x4ee5;&#x8986;&#x76d6;&#x5c3d;&#x53ef;&#x80fd;&#x591a;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x3002;2.  <strong>&#x9a8c;&#x8bc1; (Verification)</strong>&#xff1a;&#x4f7f;&#x7528;&#x4e00;&#x4e2a;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x4f5c;&#x4e3a;&#x201c;&#x5927;&#x8111;&#x201d;&#xff0c;&#x8c03;&#x7528;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff08;&#x5982;&#x7269;&#x4f53;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#x3001;&#x89c6;&#x89c9;&#x95ee;&#x7b54;VQA&#x6a21;&#x578b;&#xff09;&#x5bf9;&#x521d;&#x6b65;&#x63cf;&#x8ff0;&#x8fdb;&#x884c;&#x4e8b;&#x5b9e;&#x6838;&#x67e5;&#xff08;Fact-Check&#xff09;&#x3002;&#x4f8b;&#x5982;&#xff0c;LLM&#x4f1a;&#x89e3;&#x6790;&#x63cf;&#x8ff0;&#xff0c;&#x5217;&#x51fa;&#x6240;&#x6709;&#x53ef;&#x88ab;&#x68c0;&#x6d4b;&#x7684;&#x7269;&#x4f53;&#xff0c;&#x7136;&#x540e;&#x8c03;&#x7528;&#x68c0;&#x6d4b;&#x5668;&#x786e;&#x8ba4;&#x5b83;&#x4eec;&#x662f;&#x5426;&#x5b58;&#x5728;&#xff1b;&#x6216;&#x63d0;&#x51fa;&#x5173;&#x952e;&#x95ee;&#x9898;&#xff0c;&#x8c03;&#x7528;VQA&#x6a21;&#x578b;&#x6765;&#x9a8c;&#x8bc1;&#x63cf;&#x8ff0;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;3.  <strong>&#x63cf;&#x8ff0;&#x751f;&#x6210; (Captioning)</strong>&#xff1a;LLM&#x7efc;&#x5408;&#x521d;&#x6b65;&#x63cf;&#x8ff0;&#x548c;&#x4e8b;&#x5b9e;&#x6838;&#x67e5;&#x7684;&#x7ed3;&#x679c;&#xff0c;&#x9075;&#x5faa;&#x7528;&#x6237;&#x7684;&#x6307;&#x4ee4;&#xff08;&#x5982;&#x201c;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x201d;&#xff09;&#xff0c;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7684;&#x9ad8;&#x4fdd;&#x771f;&#x3001;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;LLM&#x7684;&#x534f;&#x8c03;&#x80fd;&#x529b;&#xff0c;&#x5c06;&#x591a;&#x4e2a;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x4e32;&#x8054;&#x6210;&#x4e00;&#x4e2a;&#x7edf;&#x4e00;&#x6d41;&#x7a0b;&#xff0c;&#x540c;&#x65f6;&#x9002;&#x7528;&#x4e8e;2D&#x548c;3D&#x5185;&#x5bb9;&#x3002;","children":[],"payload":{"tag":"li","lines":"867,868"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x548c;&#x53d1;&#x73b0;&#x5305;&#x62ec;&#xff1a;1.  <strong>&#x5b9a;&#x91cf;&#x8bc4;&#x4f30;</strong>&#xff1a;&#x5728;COCO&#xff08;2D&#xff09;&#x548c;Objaverse&#xff08;3D&#xff09;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;VFC&#x5728;CLIP-Score&#xff08;&#x56fe;&#x6587;&#x76f8;&#x4f3c;&#x5ea6;&#xff09;&#x548c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x7684;&#x65b0;&#x6307;&#x6807;CLIP-Image-Score&#xff08;&#x539f;&#x56fe;&#x4e0e;&#x6839;&#x636e;&#x63cf;&#x8ff0;&#x91cd;&#x5efa;&#x7684;&#x56fe;&#x50cf;&#x4e4b;&#x95f4;&#x7684;&#x76f8;&#x4f3c;&#x5ea6;&#xff09;&#x4e0a;&#xff0c;&#x5747;&#x4f18;&#x4e8e;&#x5176;&#x4ed6;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x3002;2.  <strong>&#x4eba;&#x5de5;&#x8bc4;&#x4f30;</strong>&#xff1a;&#x5728;Amazon Mechanical Turk&#x4e0a;&#x8fdb;&#x884c;&#x7684;&#x4eba;&#x5de5;&#x8bc4;&#x4f30;&#x8868;&#x660e;&#xff0c;VFC&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x8d28;&#x91cf;&#x66f4;&#x9ad8;&#x3002;3.  <strong>GPT-4V&#x8bc4;&#x4f30;</strong>&#xff1a;&#x4f7f;&#x7528;GPT-4V&#x8fdb;&#x884c;&#x7ec6;&#x7c92;&#x5ea6;&#x8bc4;&#x4f30;&#x548c;&#x63a8;&#x7406;&#xff0c;&#x7ed3;&#x679c;&#x4e5f;&#x652f;&#x6301;VFC&#x7684;&#x4f18;&#x8d8a;&#x6027;&#x3002;4.  <strong>&#x5b9a;&#x6027;&#x5bf9;&#x6bd4;</strong>&#xff1a;&#x5982;&#x56fe;1&#x548c;&#x56fe;2&#x6240;&#x793a;&#xff0c;VFC&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x5728;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x5ea6;&#x548c;&#x51c6;&#x786e;&#x6027;&#x4e0a;&#x63a5;&#x8fd1;GPT-4V&#xff08;&#x4e13;&#x6709;&#x6a21;&#x578b;&#xff09;&#x7684;&#x8d28;&#x91cf;&#xff0c;&#x5e76;&#x4e14;&#x5728;3D&#x63cf;&#x8ff0;&#x4e0a;&#x663e;&#x8457;&#x4f18;&#x4e8e;Cap3D&#x3002;&#x901a;&#x8fc7;&#x63cf;&#x8ff0;&#x91cd;&#x5efa;&#x7684;&#x56fe;&#x50cf;/3D&#x6a21;&#x578b;&#x4e0e;&#x539f;&#x56fe;/&#x539f;&#x7269;&#x4f53;&#x66f4;&#x76f8;&#x4f3c;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x63cf;&#x8ff0;&#x7684;&#x9ad8;&#x4fdd;&#x771f;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"868,869"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;VisualFactChecker (VFC) &#x6210;&#x529f;&#x8bc1;&#x660e;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b0;&#x8303;&#x5f0f;&#xff1a;&#x901a;&#x8fc7;&#x5de7;&#x5999;&#x5730;&#x5c06;&#x591a;&#x4e2a;&#x73b0;&#x6709;&#x7684;&#x3001;&#x8f83;&#x5c0f;&#x7684;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#xff08;LLM&#x3001;&#x68c0;&#x6d4b;&#x5668;&#x3001;VQA&#x3001;&#x63cf;&#x8ff0;&#x6a21;&#x578b;&#xff09;&#x7ec4;&#x5408;&#x6210;&#x4e00;&#x4e2a;&#x7531;LLM&#x534f;&#x8c03;&#x7684;&#x6d41;&#x7a0b;&#xff0c;&#x53ef;&#x4ee5;&#x751f;&#x6210;&#x5728;&#x8d28;&#x91cf;&#x548c;&#x7ec6;&#x8282;&#x4e0a;&#x5ab2;&#x7f8e;GPT-4V&#x7b49;&#x5927;&#x578b;&#x4e13;&#x6709;&#x6a21;&#x578b;&#x7684;&#x9ad8;&#x4fdd;&#x771f;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff0c;&#x540c;&#x65f6;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#xff1a;1.  &#x4e3a;&#x793e;&#x533a;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x9ad8;&#x6027;&#x80fd;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x4e14;&#x65e0;&#x9700;&#x6602;&#x8d35;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;2.  &#x5c55;&#x793a;&#x4e86;LLM&#x4f5c;&#x4e3a;&#x201c;&#x63a7;&#x5236;&#x5668;&#x201d;&#x6216;&#x201c;&#x5927;&#x8111;&#x201d;&#x6765;&#x534f;&#x8c03;&#x548c;&#x5229;&#x7528;&#x73b0;&#x6709;&#x4e13;&#x5bb6;&#x6a21;&#x578b;&#x7684;&#x5de8;&#x5927;&#x6f5c;&#x529b;&#x3002;3.  &#x63d0;&#x51fa;&#x7684;CLIP-Image-Score&#x4e3a;&#x63cf;&#x8ff0;&#x8d28;&#x91cf;&#x8bc4;&#x4f30;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x7684;&#x3001;&#x6709;&#x4ef7;&#x503c;&#x7684;&#x7ef4;&#x5ea6;&#x3002;4.  &#x8be5;&#x6846;&#x67b6;&#x53ef;&#x7075;&#x6d3b;&#x6269;&#x5c55;&#x81f3;&#x5176;&#x4ed6;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#xff0c;&#x5e76;&#x652f;&#x6301;&#x9075;&#x5faa;&#x590d;&#x6742;&#x6307;&#x4ee4;&#x751f;&#x6210;&#x4e0d;&#x540c;&#x98ce;&#x683c;&#x7684;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"869,871"}}],"payload":{"tag":"li","lines":"865,871","fold":1}}],"payload":{"tag":"h4","lines":"863,864"}},{"content":"LURE: Analyzing and Mitigating Object Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;LURE&#x7684;&#x540e;&#x5904;&#x7406;&#x7b97;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5e7b;&#x89c9;&#x7684;&#x4e09;&#x4e2a;&#x5173;&#x952e;&#x56e0;&#x7d20;&#xff08;&#x5171;&#x73b0;&#x6027;&#x3001;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x548c;&#x4f4d;&#x7f6e;&#xff09;&#xff0c;&#x5e76;&#x5229;&#x7528;GPT-3.5&#x751f;&#x6210;&#x4fee;&#x6b63;&#x6570;&#x636e;&#x96c6;&#x6765;&#x8bad;&#x7ec3;&#x4fee;&#x6b63;&#x5668;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x591a;&#x4e2a;LVLM&#x7684;&#x751f;&#x6210;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"872,873"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x8fd9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x89c6;&#x89c9;&#x6458;&#x8981;&#x3001;&#x63a8;&#x7406;&#x7b49;&#x4e0b;&#x6e38;&#x4efb;&#x52a1;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x963b;&#x788d;&#x5176;&#x5728;&#x673a;&#x5668;&#x4eba;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x548c;&#x4eba;&#x673a;&#x4ea4;&#x4e92;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x5927;&#x91cf;&#x9ad8;&#x8d28;&#x91cf;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x5fae;&#x8c03;&#x6a21;&#x578b;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x4e0d;&#x7075;&#x6d3b;&#x3002;","children":[],"payload":{"tag":"li","lines":"874,875"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x901a;&#x8fc7;&#x7edf;&#x8ba1;&#x5206;&#x6790;&#x786e;&#x5b9a;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x56e0;&#x7d20;&#xff1a;&#x5171;&#x73b0;&#x6027;&#xff08;&#x7269;&#x4f53;&#x95f4;&#x7684;&#x865a;&#x5047;&#x5173;&#x8054;&#xff09;&#x3001;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#xff08;&#x89e3;&#x7801;&#x65f6;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x7684;&#x7269;&#x4f53;&#xff09;&#x548c;&#x4f4d;&#x7f6e;&#xff08;&#x5e7b;&#x89c9;&#x591a;&#x51fa;&#x73b0;&#x5728;&#x6587;&#x672c;&#x540e;&#x534a;&#x90e8;&#x5206;&#xff09;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x4ed6;&#x4eec;&#x63d0;&#x51fa;LURE&#x7b97;&#x6cd5;&#xff1a;1&#xff09;&#x4f7f;&#x7528;GPT-3.5&#x4fee;&#x6539;&#x6b63;&#x786e;&#x6807;&#x6ce8;&#xff0c;&#x63d2;&#x5165;&#x9ad8;&#x9891;&#x5171;&#x73b0;&#x7269;&#x4f53;&#x6216;&#x66ff;&#x6362;&#x4e0d;&#x786e;&#x5b9a;/&#x672b;&#x5c3e;&#x7269;&#x4f53;&#x4e3a;&#x5360;&#x4f4d;&#x7b26;&#xff0c;&#x6784;&#x5efa;&#x5e7b;&#x89c9;&#x6570;&#x636e;&#x96c6;&#xff1b;2&#xff09;&#x7528;&#x8be5;&#x6570;&#x636e;&#x96c6;&#x8bad;&#x7ec3;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x4fee;&#x6b63;&#x5668;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;&#x4efb;&#x4f55;LVLM&#x4e2d;&#xff0c;&#x5bf9;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x8fdb;&#x884c;&#x540e;&#x5904;&#x7406;&#x4fee;&#x6b63;&#x3002;","children":[],"payload":{"tag":"li","lines":"875,876"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x516d;&#x4e2a;&#x5f00;&#x6e90;LVLM&#x4e0a;&#x8fdb;&#x884c;&#xff0c;LURE&#x5728;&#x591a;&#x9879;&#x8bc4;&#x4f30;&#x4e2d;&#x5747;&#x4f18;&#x4e8e;&#x4e4b;&#x524d;&#x7684;&#x6700;&#x4f73;&#x65b9;&#x6cd5;&#xff1a;1&#xff09;&#x901a;&#x7528;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR&#xff09;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff1b;2&#xff09;GPT&#x8bc4;&#x4f30;&#x548c;&#x4eba;&#x5de5;&#x8bc4;&#x4f30;&#x4e00;&#x81f4;&#x786e;&#x8ba4;&#x5176;&#x6709;&#x6548;&#x6027;&#xff1b;3&#xff09;&#x7edf;&#x8ba1;&#x5206;&#x6790;&#x9a8c;&#x8bc1;&#x4e86;&#x5171;&#x73b0;&#x6027;&#x3001;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x548c;&#x4f4d;&#x7f6e;&#x4e0e;&#x5e7b;&#x89c9;&#x7684;&#x5f3a;&#x76f8;&#x5173;&#x6027;&#xff08;&#x5982;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#x5171;&#x73b0;&#x5206;&#x6570;&#x66f4;&#x9ad8;&#x3001;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x591a;&#x4f4d;&#x4e8e;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x533a;&#x95f4;&#x548c;&#x6587;&#x672c;&#x540e;&#x534a;&#x90e8;&#x5206;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"876,877"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: LURE&#x662f;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x53ef;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x540e;&#x5904;&#x7406;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x4e0d;&#x4ec5;&#x63d0;&#x4f9b;&#x4e86;&#x5b9e;&#x7528;&#x7684;&#x5de5;&#x5177;&#xff0c;&#x8fd8;&#x901a;&#x8fc7;&#x7406;&#x8bba;&#x548c;&#x5b9e;&#x8bc1;&#x5206;&#x6790;&#x63ed;&#x793a;&#x4e86;&#x5e7b;&#x89c9;&#x7684;&#x6210;&#x56e0;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x6a21;&#x578b;&#x6539;&#x8fdb;&#x63d0;&#x4f9b;&#x4e86;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x63d0;&#x5347;LVLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"877,879"}}],"payload":{"tag":"li","lines":"873,879","fold":1}}],"payload":{"tag":"h4","lines":"871,872"}},{"content":"Woodpecker: Hallucination Correction for Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: Woodpecker&#x662f;&#x4e00;&#x4e2a;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x540e;&#x5904;&#x7406;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x68c0;&#x6d4b;&#x548c;&#x4fee;&#x6b63;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x63cf;&#x8ff0;&#xff09;&#x3002;&#x5b83;&#x901a;&#x8fc7;&#x4e94;&#x4e2a;&#x6b65;&#x9aa4;&#xff1a;&#x5173;&#x952e;&#x6982;&#x5ff5;&#x63d0;&#x53d6;&#x3001;&#x95ee;&#x9898;&#x751f;&#x6210;&#x3001;&#x89c6;&#x89c9;&#x77e5;&#x8bc6;&#x9a8c;&#x8bc1;&#x3001;&#x89c6;&#x89c9;&#x58f0;&#x660e;&#x751f;&#x6210;&#x548c;&#x5e7b;&#x89c9;&#x4fee;&#x6b63;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x5e76;&#x5728;POPE&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x5927;&#x5e45;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"880,881"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x73b0;&#x8c61;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x6587;&#x672c;&#xff08;&#x4f8b;&#x5982;&#x865a;&#x6784;&#x7269;&#x4f53;&#x6216;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#xff09;&#x3002;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x901a;&#x8fc7;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x6765;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x9700;&#x8981;&#x5927;&#x91cf;&#x6570;&#x636e;&#x548c;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x8fdb;&#x884c;&#x6a21;&#x578b;&#x91cd;&#x8bad;&#x7ec3;&#xff0c;&#x4e14;&#x5f80;&#x5f80;&#x4ee5;&#x727a;&#x7272;&#x751f;&#x6210;&#x4e3a;&#x7ec6;&#x8282;&#x4e3a;&#x4ee3;&#x4ef7;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x4e14;&#x80fd;&#x76f4;&#x63a5;&#x4fee;&#x6b63;&#x9519;&#x8bef;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"882,883"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x4e94;&#x9636;&#x6bb5;&#x7684;&#x6d41;&#x6c34;&#x7ebf;&#x6846;&#x67b6;Woodpecker&#xff1a;1. &#x5173;&#x952e;&#x6982;&#x5ff5;&#x63d0;&#x53d6;&#xff1a;&#x4f7f;&#x7528;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x4ece;MLLM&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x4e2d;&#x63d0;&#x53d6;&#x4e3b;&#x8981;&#x7269;&#x4f53;&#xff08;&#x5982;&#x201c;&#x4eba;&#x201d;&#x3001;&#x201c;&#x81ea;&#x884c;&#x8f66;&#x201d;&#xff09;&#x3002;2. &#x95ee;&#x9898;&#x751f;&#x6210;&#xff1a;&#x9488;&#x5bf9;&#x6bcf;&#x4e2a;&#x63d0;&#x53d6;&#x7684;&#x6982;&#x5ff5;&#xff0c;&#x751f;&#x6210;&#x5173;&#x4e8e;&#x5176;&#x5b58;&#x5728;&#x6027;&#x3001;&#x6570;&#x91cf;&#x3001;&#x5c5e;&#x6027;&#x3001;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#x7b49;&#x7684;&#x95ee;&#x9898;&#xff08;&#x5982;&#x201c;&#x56fe;&#x50cf;&#x4e2d;&#x6709;&#x51e0;&#x8f86;&#x81ea;&#x884c;&#x8f66;&#xff1f;&#x201d;&#xff09;&#x3002;3. &#x89c6;&#x89c9;&#x77e5;&#x8bc6;&#x9a8c;&#x8bc1;&#xff1a;&#x4f7f;&#x7528;&#x73b0;&#x6709;&#x7684;&#x89c6;&#x89c9;&#x4e13;&#x5bb6;&#x6a21;&#x578b;&#xff08;&#x5982;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x3001;&#x5c5e;&#x6027;&#x8bc6;&#x522b;&#x6a21;&#x578b;&#xff09;&#x56de;&#x7b54;&#x751f;&#x6210;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x83b7;&#x53d6;&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x7684;&#x771f;&#x5b9e;&#x4fe1;&#x606f;&#x3002;4. &#x89c6;&#x89c9;&#x58f0;&#x660e;&#x751f;&#x6210;&#xff1a;&#x5c06;&#x95ee;&#x7b54;&#x5bf9;&#x8f6c;&#x6362;&#x4e3a;&#x7ed3;&#x6784;&#x5316;&#x7684;&#x89c6;&#x89c9;&#x77e5;&#x8bc6;&#x5e93;&#xff0c;&#x5305;&#x542b;&#x7269;&#x4f53;&#x7ea7;&#x548c;&#x5c5e;&#x6027;&#x7ea7;&#x7684;&#x58f0;&#x660e;&#xff08;&#x5982;&#x201c;&#x6709;1&#x8f86;&#x81ea;&#x884c;&#x8f66;&#x201d;&#xff09;&#x3002;5. &#x5e7b;&#x89c9;&#x4fee;&#x6b63;&#xff1a;&#x4f9d;&#x636e;&#x89c6;&#x89c9;&#x77e5;&#x8bc6;&#x5e93;&#xff0c;&#x4fee;&#x6b63;&#x539f;&#x59cb;&#x6587;&#x672c;&#x4e2d;&#x7684;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#xff08;&#x5982;&#x5220;&#x9664;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x5e76;&#x6dfb;&#x52a0;&#x8fb9;&#x754c;&#x6846;&#x5750;&#x6807;&#x4f5c;&#x4e3a;&#x8bc1;&#x636e;&#x4ee5;&#x589e;&#x5f3a;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x3002;&#x6574;&#x4e2a;&#x6d41;&#x7a0b;&#x65e0;&#x9700;&#x8bad;&#x7ec3;MLLM&#x672c;&#x8eab;&#xff0c;&#x53ef;&#x4f5c;&#x4e3a;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x540e;&#x5904;&#x7406;&#x6a21;&#x5757;&#x670d;&#x52a1;&#x4e8e;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"883,884"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x3001;MME&#x548c;LLaVA-QA90&#x7b49;&#x57fa;&#x51c6;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;","children":[],"payload":{"tag":"li","lines":"884,885"}}],"payload":{"tag":"li","lines":"881,885","fold":1}},{"content":"&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#xff0c;Woodpecker&#x5c06;MiniGPT-4&#x7684;&#x51c6;&#x786e;&#x7387;&#x4ece;54.67%&#x63d0;&#x5347;&#x81f3;85.33%&#xff08;&#x63d0;&#x5347;30.66%&#xff09;&#xff0c;&#x5c06;mPLUG-Owl&#x4ece;62%&#x63d0;&#x5347;&#x81f3;86.33%&#xff08;&#x63d0;&#x5347;24.33%&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"885,886"}},{"content":"&#x5b9a;&#x6027;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x4fee;&#x6b63;&#x7269;&#x4f53;&#x7ea7;&#xff08;&#x5982;&#x79fb;&#x9664;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x201c;&#x5176;&#x4ed6;&#x4eba;&#x201d;&#xff09;&#x548c;&#x5c5e;&#x6027;&#x7ea7;&#xff08;&#x5982;&#x4fee;&#x6b63;&#x989c;&#x8272;&#x3001;&#x4f4d;&#x7f6e;&#xff09;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x4f9b;&#x53ef;&#x89c6;&#x5316;&#x7684;&#x8fb9;&#x754c;&#x6846;&#x8bc1;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"886,887"}},{"content":"&#x6846;&#x67b6;&#x5177;&#x6709;&#x826f;&#x597d;&#x7684;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#xff0c;&#x4e2d;&#x95f4;&#x5404;&#x9636;&#x6bb5;&#x7684;&#x8f93;&#x51fa;&#xff08;&#x5982;&#x63d0;&#x53d6;&#x7684;&#x6982;&#x5ff5;&#x3001;&#x751f;&#x6210;&#x7684;&#x95ee;&#x9898;&#x3001;&#x9a8c;&#x8bc1;&#x7ed3;&#x679c;&#xff09;&#x6e05;&#x6670;&#x53ef;&#x89c1;&#x3002;","children":[{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Woodpecker&#x9996;&#x6b21;&#x4ee5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x4fee;&#x6b63;&#x65b9;&#x5f0f;&#x5904;&#x7406;MLLM&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e3a;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#x3002;&#x5176;&#x5173;&#x952e;&#x4f18;&#x52bf;&#x5728;&#x4e8e;&#xff1a;1. &#x65e0;&#x9700;&#x91cd;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x8282;&#x7701;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x4e14;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#x5230;&#x4e0d;&#x540c;MLLM&#x3002;2. &#x901a;&#x8fc7;&#x7ed3;&#x6784;&#x5316;&#x6d41;&#x6c34;&#x7ebf;&#x63d0;&#x4f9b;&#x9ad8;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#xff0c;&#x7528;&#x6237;&#x53ef;&#x8ffd;&#x6eaf;&#x4fee;&#x6b63;&#x8fc7;&#x7a0b;&#x3002;3. &#x80fd;&#x540c;&#x65f6;&#x5904;&#x7406;&#x7269;&#x4f53;&#x7ea7;&#x548c;&#x5c5e;&#x6027;&#x7ea7;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x4f9b;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x8868;&#x660e;&#xff0c;&#x540e;&#x5904;&#x7406;&#x4fee;&#x6b63;&#x662f;&#x4e00;&#x6761;&#x6709;&#x6548;&#x4e14;&#x5b9e;&#x7528;&#x7684;&#x9014;&#x5f84;&#xff0c;&#x6709;&#x671b;&#x63a8;&#x52a8;MLLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"888,890"}}],"payload":{"tag":"li","lines":"887,890"}}],"payload":{"tag":"h4","lines":"879,880"}},{"content":"FACT: Teaching MLLMs with Faithful, Concise and Transferable Rationales","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Fact&#x8303;&#x5f0f;&#xff0c;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x5fe0;&#x5b9e;&#x3001;&#x7b80;&#x6d01;&#x4e14;&#x53ef;&#x8fc1;&#x79fb;&#x7684;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff08;Rationale&#xff09;&#x6765;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x63a8;&#x7406;&#x80fd;&#x529b;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5229;&#x7528;&#x53ef;&#x9a8c;&#x8bc1;&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7a0b;&#x751f;&#x6210;&#x4ee3;&#x7801;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x526a;&#x679d;&#x3001;&#x5408;&#x5e76;&#x548c;&#x6865;&#x63a5;&#x64cd;&#x4f5c;&#x4f18;&#x5316;&#x63a8;&#x7406;&#x94fe;&#xff0c;&#x6700;&#x7ec8;&#x901a;&#x8fc7;&#x84b8;&#x998f;&#x6280;&#x672f;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"891,892"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5176;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e0d;&#x900f;&#x660e;&#xff0c;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x7ec4;&#x5408;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x6709;&#x9650;&#xff08;&#x5982;&#x8ba1;&#x6570;&#x3001;&#x7a7a;&#x95f4;&#x63a8;&#x7406;&#x7b49;&#xff09;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65b9;&#x6cd5;&#x80fd;&#x591f;&#x751f;&#x6210;&#x5fe0;&#x5b9e;&#xff08;&#x4e0e;&#x7ed3;&#x8bba;&#x4e00;&#x81f4;&#xff09;&#x3001;&#x7b80;&#x6d01;&#xff08;&#x65e0;&#x5197;&#x4f59;&#x4fe1;&#x606f;&#xff09;&#x4e14;&#x53ef;&#x8fc1;&#x79fb;&#xff08;&#x9002;&#x7528;&#x4e8e;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x548c;&#x4efb;&#x52a1;&#xff09;&#x7684;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff0c;&#x4ee5;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x89e3;&#x91ca;&#x6027;&#x548c;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"893,894"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;Fact&#x8303;&#x5f0f;&#xff0c;&#x5305;&#x542b;&#x56db;&#x4e2a;&#x6b65;&#x9aa4;&#xff1a;1) &#x5fe0;&#x5b9e;&#x7a0b;&#x5e8f;&#x751f;&#x6210;&#xff1a;&#x4f7f;&#x7528;&#x4ee3;&#x7801;&#x751f;&#x6210;&#x6a21;&#x578b;&#xff08;&#x5982;Codex&#xff09;&#x751f;&#x6210;&#x53ef;&#x6267;&#x884c;&#x4ee3;&#x7801;&#xff0c;&#x786e;&#x4fdd;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x7684;&#x6b63;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9a8c;&#x8bc1;&#x6027;&#xff1b;2) &#x7b80;&#x6d01;&#x63a8;&#x7406;&#x94fe;&#x8f6c;&#x6362;&#xff1a;&#x901a;&#x8fc7;&#x526a;&#x679d;&#xff08;&#x79fb;&#x9664;&#x672a;&#x6267;&#x884c;&#x7684;&#x4ee3;&#x7801;&#x5206;&#x652f;&#xff09;&#x3001;&#x5408;&#x5e76;&#xff08;&#x7b80;&#x5316;&#x5faa;&#x73af;&#x4e2d;&#x7684;&#x91cd;&#x590d;&#x64cd;&#x4f5c;&#xff09;&#x548c;&#x6865;&#x63a5;&#xff08;&#x586b;&#x5145;&#x903b;&#x8f91;&#x95f4;&#x9699;&#xff09;&#x5c06;&#x4ee3;&#x7801;&#x8f6c;&#x6362;&#x4e3a;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x63a8;&#x7406;&#x94fe;&#xff1b;3) &#x53ef;&#x8fc1;&#x79fb;&#x6027;&#x9a8c;&#x8bc1;&#xff1a;&#x7b5b;&#x9009;&#x80fd;&#x6210;&#x529f;&#x8fc1;&#x79fb;&#x5230;&#x7aef;&#x5230;&#x7aef;&#x6a21;&#x578b;&#x7684;&#x63a8;&#x7406;&#x94fe;&#xff1b;4) &#x9010;&#x6b65;&#x84b8;&#x998f;&#xff1a;&#x5c06;&#x4f18;&#x5316;&#x540e;&#x7684;&#x63a8;&#x7406;&#x94fe;&#x7528;&#x4e8e;&#x8bad;&#x7ec3;MLLM&#xff0c;&#x901a;&#x8fc7;&#x8054;&#x5408;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff08;&#x63a8;&#x7406;&#x94fe;&#x635f;&#x5931;&#x548c;&#x6807;&#x7b7e;&#x635f;&#x5931;&#xff09;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"894,895"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;GQA&#x3001;OKVQA&#x3001;TallyQA&#x548c;COCO&#x7b49;&#x89c6;&#x89c9;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1) Fact&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;MLLM&#x7684;&#x7ec4;&#x5408;&#x63a8;&#x7406;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x8ba1;&#x6570;&#x548c;&#x7a7a;&#x95f4;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e0a;&#x8868;&#x73b0;&#x7a81;&#x51fa;&#xff1b;2) &#x751f;&#x6210;&#x7684;&#x63a8;&#x7406;&#x94fe;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x56e0;&#x6587;&#x672c;&#x4e0e;&#x56fe;&#x50cf;&#x95f4;&#x9ad8;&#x5ea6;&#x5173;&#x8054;&#xff1b;3) &#x6d88;&#x878d;&#x5b9e;&#x9a8c;&#x9a8c;&#x8bc1;&#x4e86;&#x5fe0;&#x5b9e;&#x6027;&#x3001;&#x7b80;&#x6d01;&#x6027;&#x548c;&#x53ef;&#x8fc1;&#x79fb;&#x6027;&#x4e09;&#x4e2a;&#x5c5e;&#x6027;&#x7684;&#x5fc5;&#x8981;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"895,896"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Fact&#x8303;&#x5f0f;&#x901a;&#x8fc7;&#x7a0b;&#x5e8f;&#x5316;&#x65b9;&#x6cd5;&#x751f;&#x6210;&#x9ad8;&#x8d28;&#x91cf;&#x7684;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff0c;&#x6210;&#x529f;&#x63d0;&#x5347;&#x4e86;MLLM&#x7684;&#x63a8;&#x7406;&#x900f;&#x660e;&#x6027;&#x548c;&#x6027;&#x80fd;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x8d21;&#x732e;&#x5728;&#x4e8e;&#x5c06;&#x4ee3;&#x7801;&#x7684;&#x903b;&#x8f91;&#x4e25;&#x8c28;&#x6027;&#x4e0e;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x7684;&#x7075;&#x6d3b;&#x6027;&#x7ed3;&#x5408;&#xff0c;&#x4e3a;&#x6a21;&#x578b;&#x84b8;&#x998f;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63a8;&#x52a8;&#x53ef;&#x89e3;&#x91ca;AI&#x7684;&#x53d1;&#x5c55;&#xff0c;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x90e8;&#x7f72;&#x6210;&#x672c;&#xff0c;&#x5e76;&#x4fc3;&#x8fdb;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x5728;&#x590d;&#x6742;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"896,898"}}],"payload":{"tag":"li","lines":"892,898","fold":1}}],"payload":{"tag":"h4","lines":"890,891"}}],"payload":{"tag":"h3","lines":"861,862","fold":1}},{"content":"&#x57fa;&#x4e8e;&#x903b;&#x8f91;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x68c0;&#x67e5;","children":[{"content":"LogicCheckGPT: Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;LogicCheckGPT&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x903b;&#x8f91;&#x95ed;&#x73af;&#x68c0;&#x6d4b;&#x548c;&#x7f13;&#x89e3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x57fa;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x771f;&#x5b9e;&#x7269;&#x4f53;&#x54cd;&#x5e94;&#x903b;&#x8f91;&#x4e00;&#x81f4;&#x3001;&#x5bf9;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x76f4;&#x89c9;&#xff0c;&#x901a;&#x8fc7;&#x5c5e;&#x6027;&#x4e0e;&#x7269;&#x4f53;&#x7684;&#x76f8;&#x4e92;&#x63d0;&#x95ee;&#x5f62;&#x6210;&#x903b;&#x8f91;&#x95ed;&#x73af;&#x8fdb;&#x884c;&#x5224;&#x65ad;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"901,902"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;LVLM&#x58f0;&#x79f0;&#x56fe;&#x50cf;&#x4e2d;&#x5b58;&#x5728;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#x4e25;&#x91cd;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x573a;&#x666f;&#x4e2d;&#x540e;&#x679c;&#x4e25;&#x91cd;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x9700;&#x8981;&#x5927;&#x91cf;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x8fdb;&#x884c;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#xff0c;&#x8981;&#x4e48;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#xff0c;&#x672a;&#x80fd;&#x5145;&#x5206;&#x5229;&#x7528;LVLM&#x81ea;&#x8eab;&#x80fd;&#x529b;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x63a2;&#x7d22;LVLM&#x7684;&#x5185;&#x5728;&#x903b;&#x8f91;&#x4e00;&#x81f4;&#x6027;&#x6765;&#x8bc6;&#x522b;&#x548c;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x4e2a;&#x7814;&#x7a76;&#x4e0d;&#x8db3;&#x4f46;&#x6f5c;&#x529b;&#x5de8;&#x5927;&#x7684;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"903,904"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;LogicCheckGPT&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x201c;&#x903b;&#x8f91;&#x95ed;&#x73af;&#x201d;&#x601d;&#x60f3;&#x3002;&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e94;&#x4e2a;&#x6b65;&#x9aa4;&#xff1a;1) &#x4ece;LVLM&#x521d;&#x59cb;&#x54cd;&#x5e94;&#x4e2d;&#x63d0;&#x53d6;&#x63d0;&#x53ca;&#x7684;&#x7269;&#x4f53;&#xff1b;2) &#x5bf9;&#x6bcf;&#x4e2a;&#x7269;&#x4f53;&#x8fdb;&#x884c;&#x201c;&#x7269;&#x4f53;&#x5230;&#x5c5e;&#x6027;&#x201d;&#x8be2;&#x95ee;&#xff08;&#x5982;&#x201c;&#x63cf;&#x8ff0;&#x8fd9;&#x4e2a;&#x82f9;&#x679c;&#x201d;&#xff09;&#xff1b;3) &#x57fa;&#x4e8e;&#x5f97;&#x5230;&#x7684;&#x5c5e;&#x6027;&#x8fdb;&#x884c;&#x201c;&#x5c5e;&#x6027;&#x5230;&#x7269;&#x4f53;&#x201d;&#x7684;&#x53cd;&#x5411;&#x8be2;&#x95ee;&#xff08;&#x5982;&#x201c;&#x56fe;&#x50cf;&#x4e2d;&#x4ec0;&#x4e48;&#x662f;&#x7ea2;&#x8272;&#x7684;&#xff1f;&#x201d;&#xff09;&#xff1b;4) &#x68c0;&#x67e5;&#x4e24;&#x6b21;&#x8be2;&#x95ee;&#x7684;&#x7b54;&#x6848;&#x80fd;&#x5426;&#x5f62;&#x6210;&#x903b;&#x8f91;&#x95ed;&#x73af;&#xff08;&#x5373;&#x53cd;&#x5411;&#x8be2;&#x95ee;&#x80fd;&#x5426;&#x6307;&#x56de;&#x539f;&#x7269;&#x4f53;&#xff09;&#xff1b;5) &#x6839;&#x636e;&#x95ed;&#x73af;&#x6bd4;&#x4f8b;&#x5224;&#x65ad;&#x7269;&#x4f53;&#x662f;&#x5426;&#x4e3a;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x8fdb;&#x884c;&#x4fee;&#x6b63;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x901a;&#x8fc7;&#x8bed;&#x8a00;&#x4ea4;&#x4e92;&#x5b9e;&#x73b0;&#xff0c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"904,905"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5982;POPE&#xff09;&#x4e0a;&#x5bf9;&#x56db;&#x79cd;&#x4e3b;&#x6d41;LVLM&#xff08;&#x5982;mPLUG-Owl&#x3001;MiniGPT-4&#xff09;&#x8fdb;&#x884c;&#x4e86;&#x7efc;&#x5408;&#x8bc4;&#x4f30;&#x3002;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x4f8b;&#x5982;&#x5728;POPE&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x4f7f;mPLUG-Owl&#x548c;MiniGPT-4&#x7684;&#x6027;&#x80fd;&#x5206;&#x522b;&#x63d0;&#x5347;&#x4e86;31.33%&#x548c;10.00%&#x3002;GPT-4V&#x8f85;&#x52a9;&#x8bc4;&#x4f30;&#x4e5f;&#x9a8c;&#x8bc1;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x4f18;&#x8d8a;&#x6027;&#x548c;&#x666e;&#x9002;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"905,906"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x5229;&#x7528;LVLM&#x81ea;&#x8eab;&#x7684;&#x903b;&#x8f91;&#x4e00;&#x81f4;&#x6027;&#x6765;&#x68c0;&#x6d4b;&#x5e7b;&#x89c9;&#x662f;&#x53ef;&#x884c;&#x4e14;&#x6709;&#x6548;&#x7684;&#x3002;LogicCheckGPT&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x3001;&#x4ec5;&#x9700;&#x8bed;&#x8a00;&#x4ea4;&#x4e92;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x4e3a;&#x7f13;&#x89e3;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x9896;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x6210;&#x529f;&#x8868;&#x660e;&#xff0c;&#x6316;&#x6398;&#x6a21;&#x578b;&#x5185;&#x5728;&#x7279;&#x6027;&#x800c;&#x975e;&#x4ec5;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x8d44;&#x6e90;&#xff0c;&#x662f;&#x89e3;&#x51b3;LVLM&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x4e00;&#x4e2a;&#x91cd;&#x8981;&#x65b9;&#x5411;&#xff0c;&#x5177;&#x6709;&#x63a8;&#x52a8;&#x5176;&#x66f4;&#x5b89;&#x5168;&#x3001;&#x66f4;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x7684;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"906,908"}}],"payload":{"tag":"li","lines":"902,908","fold":1}}],"payload":{"tag":"h4","lines":"900,901"}}],"payload":{"tag":"h3","lines":"898,899","fold":1}},{"content":"&#x5176;&#x4ed6;","children":[{"content":"INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;INTER&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x89e3;&#x7801;&#x7b97;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5bfc;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x663e;&#x5f0f;&#x5229;&#x7528;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x4fe1;&#x606f;&#xff0c;&#x4ee5;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5e73;&#x5747;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x8fbe;3.4%&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"911,912"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x5e7b;&#x89c9;&#x4f1a;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x4e0d;&#x53ef;&#x9760;&#xff0c;&#x800c;&#x4eba;&#x7c7b;&#x5374;&#x80fd;&#x901a;&#x8fc7;&#x6709;&#x6548;&#x5229;&#x7528;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x4fe1;&#x606f;&#x907f;&#x514d;&#x6b64;&#x7c7b;&#x9519;&#x8bef;&#x3002;&#x8bba;&#x6587;&#x65e8;&#x5728;&#x63a2;&#x7d22;LVLM&#x7684;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x673a;&#x5236;&#xff0c;&#x5e76;&#x5f00;&#x53d1;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"913,914"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x57fa;&#x4e8e;&#x535a;&#x5f08;&#x8bba;&#x4e2d;&#x7684;Harsanyi&#x80a1;&#x606f;&#x91cf;&#x5316;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x4f5c;&#x7528;&#xff0c;&#x5e76;&#x63d0;&#x51fa;INTER&#x7b97;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;1&#xff09;&#x4ea4;&#x4e92;&#x5f15;&#x5bfc;&#x5b9a;&#x4f4d;&#x5668;&#xff08;Interactive Guided Locator&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x65b9;&#x5dee;&#x8fc7;&#x6ee4;&#x81ea;&#x52a8;&#x8bc6;&#x522b;&#x5bf9;&#x54cd;&#x5e94;&#x51c6;&#x786e;&#x6027;&#x5173;&#x952e;&#x7684;&#x8868;&#x5f81;&#xff1b;2&#xff09;&#x4ea4;&#x4e92;&#x6982;&#x7387;&#x4fee;&#x6b63;&#x5668;&#xff08;Interaction Probability Modifier&#xff09;&#xff0c;&#x5728;&#x91c7;&#x6837;&#x5173;&#x952e;&#x8868;&#x5f81;&#x65f6;&#x589e;&#x5f3a;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x7684;&#x4f9d;&#x8d56;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6570;&#x636e;&#xff0c;&#x4ec5;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x5b9e;&#x73b0;&#x3002;","children":[],"payload":{"tag":"li","lines":"914,915"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x5305;&#x62ec;VQA&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x5728;&#x5185;&#x7684;&#x516d;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MME&#x3001;POPE&#x3001;CHAIR&#x7b49;&#xff09;&#x4e0a;&#xff0c;INTER&#x5728;&#x4e94;&#x6b3e;&#x4e3b;&#x6d41;LVLM&#xff08;&#x5982;InternVL2.5&#x3001;LLaVA-v1.5&#x7b49;&#xff09;&#x4e0a;&#x5e73;&#x5747;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x6700;&#x9ad8;&#x8fbe;3.4%&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x5728;CHAIR&#x548c;MME&#x57fa;&#x51c6;&#x4e0a;&#x5206;&#x522b;&#x6bd4;&#x73b0;&#x6709;&#x6700;&#x4f18;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x5e73;&#x5747;&#x63d0;&#x5347;4.1%&#x548c;2.6%&#x3002;&#x6848;&#x4f8b;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;INTER&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x7684;&#x5bf9;&#x8c61;&#x63cf;&#x8ff0;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"915,916"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x8868;&#x660e;LVLM&#x9690;&#x542b;&#x5177;&#x5907;&#x7c7b;&#x4eba;&#x7684;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x7406;&#x89e3;&#x80fd;&#x529b;&#xff0c;&#x4f46;&#x5229;&#x7528;&#x4e0d;&#x5145;&#x5206;&#x3002;INTER&#x901a;&#x8fc7;&#x663e;&#x5f0f;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x91cd;&#x7528;&#x8fd9;&#x79cd;&#x7406;&#x89e3;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x5bf9;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#xff0c;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x535a;&#x5f08;&#x8bba;&#x5206;&#x6790;&#x89c6;&#x89d2;&#x3002;","children":[],"payload":{"tag":"li","lines":"916,918"}}],"payload":{"tag":"li","lines":"912,918","fold":1}}],"payload":{"tag":"h4","lines":"910,911"}}],"payload":{"tag":"h3","lines":"908,909","fold":1}}],"payload":{"tag":"h2","lines":"780,781"}},{"content":"&#x5fae;&#x8c03;","children":[{"content":"FROM PIXELS TO TOKENS: Revisiting Object Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;PATCH&#x7684;&#x53c2;&#x6570;&#x9ad8;&#x6548;&#x5fae;&#x8c03;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x53ef;&#x8bad;&#x7ec3;&#x7684;&#x865a;&#x62df;&#x4ee4;&#x724c;&#x6765;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x4e0e;&#x6587;&#x672c;&#x7684;&#x5bf9;&#x9f50;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"923,924"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x4f7f;&#x7528;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x5c06;&#x5e7b;&#x89c9;&#x5f52;&#x56e0;&#x4e8e;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x4e0d;&#x8db3;&#xff0c;&#x4f46;&#x5ffd;&#x7565;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x7279;&#x5f81;&#x63d0;&#x53d6;&#x548c;&#x89e3;&#x8026;&#x65b9;&#x9762;&#x7684;&#x6839;&#x672c;&#x6027;&#x7f3a;&#x9677;&#x3002;","children":[],"payload":{"tag":"li","lines":"925,926"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x4ece;&#x67b6;&#x6784;&#x89d2;&#x5ea6;&#x5206;&#x6790;&#x4e86;&#x5e7b;&#x89c9;&#x7684;&#x6765;&#x6e90;&#xff0c;&#x53d1;&#x73b0;&#x5176;&#x4e3b;&#x8981;&#x539f;&#x56e0;&#x5728;&#x4e8e;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6a21;&#x5757;&#x7684;&#x7279;&#x5f81;&#x89e3;&#x8026;&#x4e0d;&#x8db3;&#xff0c;&#x800c;&#x975e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x7279;&#x5f81;&#x63d0;&#x53d6;&#x80fd;&#x529b;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x4ed6;&#x4eec;&#x63d0;&#x51fa;&#x4e86;PATCH&#xff08;Pluggable virtuAl Tokens for objeCt Hallucinations&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x53c2;&#x6570;&#x9ad8;&#x6548;&#x5fae;&#x8c03;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#x548c;&#x589e;&#x5f3a;&#x63d0;&#x793a;&#x6587;&#x672c;&#x4e4b;&#x95f4;&#x5f15;&#x5165;&#x53ef;&#x8bad;&#x7ec3;&#x7684;&#x865a;&#x62df;&#x4ee4;&#x724c;&#xff0c;&#x4ee5;&#x6700;&#x5c0f;&#x7684;&#x53c2;&#x6570;&#x8c03;&#x6574;&#x6765;&#x6865;&#x63a5;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x548c;&#x6587;&#x672c;&#x8bed;&#x4e49;&#x4e4b;&#x95f4;&#x7684;&#x5dee;&#x8ddd;&#x3002;&#x8fd9;&#x4e9b;&#x865a;&#x62df;&#x4ee4;&#x724c;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x88ab;&#x6dfb;&#x52a0;&#x5230;&#x539f;&#x59cb;&#x8bcd;&#x6c47;&#x8868;&#x4e2d;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x7269;&#x4f53;&#x76f8;&#x5173;&#x4fe1;&#x606f;&#x7684;&#x5229;&#x7528;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"926,927"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x4e09;&#x4e2a;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5305;&#x62ec;POPE&#xff09;&#x548c;&#x4e09;&#x4e2a;LVLM&#xff08;LLaVA-v1.5&#x3001;MiniGPT-4&#x3001;MiniGPT-v2&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#x3002;PATCH&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6240;&#x6709;&#x6a21;&#x578b;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x5176;&#x4e2d;&#x5728;POPE&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;LLaVA-v1.5&#x7684;&#x51c6;&#x786e;&#x7387;&#x4ece;85.17%&#x63d0;&#x5347;&#x81f3;90.20%&#xff08;&#x63d0;&#x5347;5.03%&#xff09;&#xff0c;MiniGPT-4&#x4ece;57.67%&#x63d0;&#x5347;&#x81f3;88.13%&#xff08;&#x63d0;&#x5347;30.46%&#xff09;&#xff0c;MiniGPT-v2&#x4ece;83.33%&#x63d0;&#x5347;&#x81f3;90.03%&#xff08;&#x63d0;&#x5347;6.70%&#xff09;&#x3002;&#x7ed3;&#x679c;&#x8868;&#x660e;PATCH&#x5728;&#x7f13;&#x89e3;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6c34;&#x5e73;&#xff0c;&#x5e76;&#x5728;&#x5904;&#x7406;&#x5f3a;&#x8bef;&#x5bfc;&#x6027;&#x95ee;&#x9898;&#x65f6;&#x8868;&#x73b0;&#x51fa;&#x5de8;&#x5927;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"927,928"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLM&#x4e2d;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x4e3b;&#x8981;&#x6839;&#x6e90;&#x662f;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6a21;&#x5757;&#x5728;&#x7279;&#x5f81;&#x89e3;&#x8026;&#x65b9;&#x9762;&#x7684;&#x4e0d;&#x8db3;&#x3002;PATCH&#x901a;&#x8fc7;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x901a;&#x7528;&#x7684;&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#xff0c;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6297;&#x5e7b;&#x89c9;&#x7684;&#x80fd;&#x529b;&#xff0c;&#x8fd8;&#x589e;&#x5f3a;&#x4e86;&#x4e00;&#x822c;&#x6027;&#x80fd;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x7406;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x7684;&#x6839;&#x672c;&#x539f;&#x56e0;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x89c6;&#x89d2;&#xff0c;&#x5e76;&#x4e3a;&#x672a;&#x6765;&#x5728;&#x8be5;&#x9886;&#x57df;&#x7684;&#x8fdb;&#x4e00;&#x6b65;&#x7814;&#x7a76;&#x548c;&#x521b;&#x65b0;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#x3002;&#x5176;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x96c6;&#x6210;&#x5230;&#x5404;&#x79cd;LVLM&#x4e2d;&#xff0c;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5e94;&#x7528;&#x524d;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"928,930"}}],"payload":{"tag":"li","lines":"924,930","fold":1}}],"payload":{"tag":"h4","lines":"922,923"}}],"payload":{"tag":"h2","lines":"918,919","fold":1}},{"content":"&#x6a21;&#x578b;&#x67b6;&#x6784;/&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#x4fee;&#x6539;","children":[{"content":"&#x65b0;&#x67b6;&#x6784;","children":[{"content":"ADAVIB: Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ADAVIB&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x53d8;&#x5206;&#x4fe1;&#x606f;&#x74f6;&#x9888;&#x548c;&#x57fa;&#x4e8e;&#x71b5;&#x7684;&#x81ea;&#x9002;&#x5e94;&#x566a;&#x58f0;&#x63a7;&#x5236;&#xff0c;&#x7f13;&#x89e3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"935,936"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#xff0c;&#x5373;&#x63cf;&#x8ff0;&#x4e2d;&#x5305;&#x542b;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x5c24;&#x5176;&#x662f;&#x5728;&#x9700;&#x8981;&#x7cbe;&#x786e;&#x5224;&#x65ad;&#x7684;&#x573a;&#x666f;&#x4e2d;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x5173;&#x6ce8;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x6216;&#x6570;&#x636e;&#x504f;&#x5dee;&#xff0c;&#x4f46;&#x8f83;&#x5c11;&#x89e3;&#x51b3;&#x89c6;&#x89c9;&#x6a21;&#x5f0f;&#x4e0e;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x4e4b;&#x95f4;&#x7684;&#x6a21;&#x6001;&#x9e3f;&#x6c9f;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"937,938"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;ADAVIB&#x65b9;&#x6cd5;&#xff0c;&#x57fa;&#x4e8e;&#x53d8;&#x5206;&#x4fe1;&#x606f;&#x74f6;&#x9888;&#xff08;VIB&#xff09;&#x539f;&#x7406;&#xff0c;&#x5728;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6295;&#x5f71;&#x5668;&#xff08;vision-language projector&#xff09;&#x7684;&#x8bad;&#x7ec3;&#x4e2d;&#x5f15;&#x5165;&#x538b;&#x7f29;&#x9879;&#xff0c;&#x901a;&#x8fc7;&#x6dfb;&#x52a0;&#x968f;&#x673a;&#x566a;&#x58f0;&#x6765;&#x7ea6;&#x675f;&#x65e0;&#x5173;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7684;&#x4fe1;&#x606f;&#x6d41;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x8bbe;&#x8ba1;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x71b5;&#x7684;&#x566a;&#x58f0;&#x63a7;&#x5236;&#x7b56;&#x7565;&#xff0c;&#x6839;&#x636e;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x4e0e;LLM&#x8bcd;&#x5d4c;&#x5165;&#x4e4b;&#x95f4;&#x76f8;&#x4f3c;&#x6027;&#x5206;&#x5e03;&#x7684;&#x5e73;&#x6ed1;&#x5ea6;&#xff0c;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x8c03;&#x6574;&#x566a;&#x58f0;&#x5f3a;&#x5ea6;&#xff0c;&#x4ee5;&#x52a8;&#x6001;&#x5e94;&#x5bf9;&#x4e0d;&#x540c;&#x6837;&#x672c;&#x7684;&#x5dee;&#x5f02;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;&#x5fae;&#x8c03;&#x6295;&#x5f71;&#x5668;&#x53c2;&#x6570;&#xff0c;&#x65e0;&#x9700;&#x6539;&#x52a8;&#x5176;&#x4ed6;&#x6a21;&#x5757;&#x3002;","children":[],"payload":{"tag":"li","lines":"938,939"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;MiniGPT4&#x548c;LLaVA-1.5&#x7b49;&#x4e0d;&#x540c;&#x67b6;&#x6784;&#x7684;LVLM&#x4e0a;&#x8fdb;&#x884c;&#xff0c;ADAVIB&#x6709;&#x6548;&#x5e73;&#x6ed1;&#x4e86;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x4e0e;&#x8bcd;&#x5d4c;&#x5165;&#x7684;&#x76f8;&#x4f3c;&#x6027;&#x5206;&#x5e03;&#xff0c;&#x964d;&#x4f4e;&#x4e86;&#x5bf9;&#x65e0;&#x5173;&#x7279;&#x5f81;&#x7684;&#x8fc7;&#x5ea6;&#x81ea;&#x4fe1;&#x3002;&#x5728;&#x4e24;&#x4e2a;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5747;&#x53d6;&#x5f97;&#x4e00;&#x81f4;&#x6027;&#x80fd;&#x63d0;&#x5347;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x5176;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x548c;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"939,940"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ADAVIB&#x901a;&#x8fc7;&#x4fe1;&#x606f;&#x74f6;&#x9888;&#x548c;&#x81ea;&#x9002;&#x5e94;&#x566a;&#x58f0;&#x63a7;&#x5236;&#xff0c;&#x663e;&#x8457;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x8f7b;&#x91cf;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x5176;&#x5728;&#x5176;&#x4ed6;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"940,942"}}],"payload":{"tag":"li","lines":"936,942","fold":1}}],"payload":{"tag":"h4","lines":"934,935"}},{"content":"HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;HalluRNN&#xff0c;&#x4e00;&#x79cd;&#x901a;&#x8fc7;&#x8de8;&#x5c42;&#x5faa;&#x73af;&#x63a8;&#x7406;&#x7f13;&#x89e3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x7684;&#x65b0;&#x67b6;&#x6784;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x53cc;&#x95e8;&#x6df1;&#x5ea6;&#x4f20;&#x64ad;&#x5355;&#x5143;&#xff08;DG-DPU&#xff09;&#xff0c;&#x8be5;&#x6a21;&#x5757;&#x5728;&#x6240;&#x6709;Transformer&#x5c42;&#x95f4;&#x5171;&#x4eab;&#xff0c;&#x80fd;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x7cbe;&#x70bc;&#x548c;&#x4f20;&#x64ad;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff0c;&#x589e;&#x5f3a;&#x8868;&#x793a;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x4ec5;&#x9700;&#x5fae;&#x8c03;DG-DPU&#x6a21;&#x5757;&#xff0c;&#x5373;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x5f3a;&#x52b2;&#x4e14;&#x9c81;&#x68d2;&#x7684;&#x6297;&#x5e7b;&#x89c9;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"943,944"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5c3d;&#x7ba1;&#x6027;&#x80fd;&#x5353;&#x8d8a;&#xff0c;&#x4f46;&#x4ecd;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e0a;&#x5408;&#x7406;&#x4f46;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x8f93;&#x51fa;&#xff09;&#x3002;&#x8fd9;&#x7834;&#x574f;&#x4e86;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#xff0c;&#x5e76;&#x5bf9;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x6784;&#x6210;&#x91cd;&#x5927;&#x6311;&#x6218;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x57fa;&#x4e8e;&#x5fae;&#x8c03;&#x6216;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7684;&#x7b56;&#x7565;&#xff09;&#x901a;&#x5e38;&#x9700;&#x8981;&#x5927;&#x91cf;&#x8d44;&#x6e90;&#x6216;&#x4efb;&#x52a1;&#x7279;&#x5b9a;&#x914d;&#x7f6e;&#xff0c;&#x4e14;&#x5ffd;&#x7565;&#x4e86;&#x5e7b;&#x89c9;&#x5728;&#x591a;&#x6b65;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x9010;&#x6b65;&#x4ea7;&#x751f;&#x7684;&#x95ee;&#x9898;&#x3002;&#x5c42;&#x95f4;&#x5206;&#x6790;&#x65b9;&#x6cd5;&#xff08;&#x5982;DAMO&#x548c;DeCO&#xff09;&#x5b58;&#x5728;&#x8de8;&#x5c42;&#x8fde;&#x63a5;&#x7c97;&#x7cd9;&#x3001;&#x8d85;&#x53c2;&#x6570;&#x654f;&#x611f;&#x6027;&#x5f3a;&#x7b49;&#x5c40;&#x9650;&#x6027;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x81ea;&#x9002;&#x5e94;&#x3001;&#x9c81;&#x68d2;&#x7684;&#x67b6;&#x6784;&#x7ea7;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"945,946"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;HalluRNN&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x65b0;&#x9896;&#x7684;&#x53cc;&#x95e8;&#x6df1;&#x5ea6;&#x4f20;&#x64ad;&#x5355;&#x5143;&#xff08;DG-DPU&#xff09;&#x3002;DG-DPU&#x662f;&#x4e00;&#x4e2a;&#x5728;&#x6240;&#x6709;Transformer&#x5c42;&#x4e4b;&#x95f4;&#x5171;&#x4eab;&#x7684;&#x5faa;&#x73af;&#x795e;&#x7ecf;&#x7f51;&#x7edc;&#xff08;RNN&#xff09;&#x6a21;&#x5757;&#x3002;&#x5b83;&#x5c06;&#x6bcf;&#x4e00;&#x5c42;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x89c6;&#x4e3a;&#x5e8f;&#x5217;&#x4e2d;&#x7684;&#x4e00;&#x6b65;&#xff0c;&#x901a;&#x8fc7;&#x5faa;&#x73af;&#x673a;&#x5236;&#x52a8;&#x6001;&#x5730;&#x7cbe;&#x70bc;&#xff08;refine&#xff09;&#x548c;&#x4f20;&#x64ad;&#x4fe1;&#x606f;&#x3002;&#x8fd9;&#x79cd;&#x201c;&#x5faa;&#x73af;&#x8de8;&#x5c42;&#x63a8;&#x7406;&#x201d;&#x673a;&#x5236;&#x80fd;&#x591f;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x6574;&#x5408;&#x5404;&#x5c42;&#x4fe1;&#x606f;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x7531;&#x63a8;&#x7406;&#x4e0d;&#x4e00;&#x81f4;&#x6216;&#x8868;&#x793a;&#x6f02;&#x79fb;&#xff08;representational drift&#xff09;&#x5f15;&#x8d77;&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;&#x4f7f;&#x7528;&#x5c11;&#x91cf;&#x6570;&#x636e;&#x5bf9;&#x8f7b;&#x91cf;&#x7ea7;&#x7684;DG-DPU&#x6a21;&#x5757;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x800c;&#x975e;&#x6574;&#x4e2a;&#x6a21;&#x578b;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x9ad8;&#x6548;&#x548c;&#x6709;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"946,947"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x8bba;&#x6587;&#x901a;&#x8fc7;&#x5728;&#x591a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MME&#x548c;POPE&#xff09;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x5c55;&#x793a;&#x4e86;HalluRNN&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;&#x4e0e;&#x73b0;&#x6709;&#x5c42;&#x95f4;&#x65b9;&#x6cd5;&#xff08;&#x5982;DAMO&#x548c;DeCO&#xff09;&#x76f8;&#x6bd4;&#xff0c;HalluRNN&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x5f3a;&#x5065;&#x548c;&#x7a33;&#x5b9a;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x5bf9;&#x8d85;&#x53c2;&#x6570;&#x53d8;&#x5316;&#x7684;&#x654f;&#x611f;&#x6027;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x3002;&#x4f8b;&#x5982;&#xff0c;DAMO&#x548c;DeCO&#x7684;&#x6027;&#x80fd;&#x4f1a;&#x56e0;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x6216;&#x9000;&#x51fa;&#x5c42;&#x6570;&#x7b49;&#x8d85;&#x53c2;&#x6570;&#x8bbe;&#x7f6e;&#x7684;&#x4e0d;&#x540c;&#x800c;&#x5267;&#x70c8;&#x6ce2;&#x52a8;&#xff0c;&#x800c;HalluRNN&#x5219;&#x80fd;&#x4fdd;&#x6301;&#x4e00;&#x81f4;&#x7684;&#x9ad8;&#x6027;&#x80fd;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x66f4;&#x597d;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x548c;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"947,948"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;RNN&#x5f0f;&#x7684;&#x5faa;&#x73af;&#x8de8;&#x5c42;&#x63a8;&#x7406;&#x673a;&#x5236;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x5730;&#x7f13;&#x89e3;LVLMs&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;HalluRNN&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x4efb;&#x52a1;&#x7279;&#x5b9a;&#x8c03;&#x4f18;&#x7684;&#x67b6;&#x6784;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x663e;&#x8457;&#x589e;&#x5f3a;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x7a33;&#x5b9a;&#x6027;&#x548c;&#x8f93;&#x51fa;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x8fd9;&#x662f;&#x9996;&#x4e2a;&#x5c06;&#x57fa;&#x4e8e;RNN&#x7684;&#x8de8;&#x5c42;&#x63a8;&#x7406;&#x7528;&#x4e8e;LVLM&#x6297;&#x5e7b;&#x89c9;&#x7684;&#x5de5;&#x4f5c;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x65b9;&#x5411;&#xff0c;&#x5177;&#x6709;&#x91cd;&#x8981;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"948,950"}}],"payload":{"tag":"li","lines":"944,950","fold":1}}],"payload":{"tag":"h4","lines":"942,943"}},{"content":"ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;ReCo&#xff08;Reminder Composition&#xff09;&#xff0c;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x53ef;&#x8bad;&#x7ec3;&#x6a21;&#x5757;&#xff0c;&#x7528;&#x4e8e;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff09;&#x3002;ReCo&#x57fa;&#x4e8e;&#x51e0;&#x4f55;&#x4ee3;&#x6570;&#x548c;&#x5173;&#x7cfb;&#x7ec4;&#x5408;&#x601d;&#x60f3;&#xff0c;&#x65e0;&#x9700;&#x4fee;&#x6539;VLM&#x6838;&#x5fc3;&#x67b6;&#x6784;&#xff0c;&#x4ec5;&#x9700;&#x6dfb;&#x52a0;&#x4e00;&#x4e2a;&#x5c0f;&#x578b;&#x6a21;&#x5757;&#x5373;&#x53ef;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x591a;&#x79cd;&#x4e3b;&#x6d41;VLM&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"951,952"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x672a;&#x57fa;&#x4e8e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x751a;&#x81f3;&#x4e0e;&#x4e4b;&#x77db;&#x76fe;&#x7684;&#x6587;&#x672c;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x4ee5;&#x53ca;&#x5728;&#x6587;&#x672c;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x201c;&#x8bb0;&#x5fc6;&#x6d88;&#x9000;&#x6548;&#x5e94;&#x201d;&#xff08;fading memory effect&#xff09;&#x2014;&#x2014;&#x968f;&#x7740;&#x751f;&#x6210;&#x8fdb;&#x884c;&#xff0c;&#x6a21;&#x578b;&#x9010;&#x6e10;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;VLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x9ad8;&#x6548;&#x4e14;&#x8f7b;&#x91cf;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"953,954"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;ReCo&#x6a21;&#x5757;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x901a;&#x8fc7;&#x51e0;&#x4f55;&#x4ee3;&#x6570;&#xff08;Geometric Algebra&#xff09;&#x548c;&#x5411;&#x91cf;&#x7b26;&#x53f7;&#x67b6;&#x6784;&#xff08;VSA&#xff09;&#x4e2d;&#x7684;&#x7ec4;&#x5408;&#x64cd;&#x4f5c;&#xff0c;&#x663e;&#x5f0f;&#x5730;&#x5c06;&#x6587;&#x672c;&#x5d4c;&#x5165;&#xff08;&#x1d447;&#x1d461;&#xff09;&#x548c;&#x56fe;&#x50cf;&#x5d4c;&#x5165;&#xff08;&#x1d43c;&#xff09;&#x7ec4;&#x5408;&#x6210;&#x4e00;&#x4e2a;&#x591a;&#x5411;&#x91cf;&#xff08;&#x1d435;&#x1d461;&#xff09;&#xff0c;&#x4ee5;&#x786e;&#x4fdd;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5728;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x4e0d;&#x88ab;&#x5ffd;&#x7565;&#x3002;&#x5177;&#x4f53;&#x5b9e;&#x73b0;&#x4e3a;&#xff1a;&#x5728;&#x9884;&#x8bad;&#x7ec3;&#x7684;VLM&#xff08;&#x5982;InstructBLIP&#x3001;LLaVA&#x3001;MiniGPT4&#xff09;&#x9876;&#x90e8;&#x6dfb;&#x52a0;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x53ef;&#x8bad;&#x7ec3;&#x5c42;&#xff0c;&#x8be5;&#x5c42;&#x901a;&#x8fc7;&#x77e9;&#x9635;&#x7ed1;&#x5b9a;&#x64cd;&#x4f5c;&#xff08;Matrix Binder&#xff09;&#x8ba1;&#x7b97; &#x1d435;&#x1d461; = &#x1d44a;&#x1d447;&#x1d447;&#x1d461; + &#x1d44a;&#x1d43c;(&#x2295; &#x1d43c;&#x1d457;)&#xff0c;&#x5176;&#x4e2d;&#x1d44a;&#x1d447;&#x548c;&#x1d44a;&#x1d43c;&#x4e3a;&#x53ef;&#x5b66;&#x4e60;&#x53c2;&#x6570;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;&#x5c11;&#x91cf;&#x4ee3;&#x7801;&#x4fee;&#x6539;&#xff0c;&#x4e0d;&#x6539;&#x53d8;VLM&#x5185;&#x90e8;&#x7ed3;&#x6784;&#xff0c;&#x4e14;&#x53ef;&#x4e0e;&#x73b0;&#x6709;&#x6297;&#x5e7b;&#x89c9;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x4f7f;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"954,955"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: ReCo&#x5728;&#x4e09;&#x4e2a;&#x4e3b;&#x6d41;VLM&#xff08;InstructBLIP&#x3001;LLaVA&#x3001;MiniGPT4&#xff09;&#x548c;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;POPE&#x3001;CHAIR&#x3001;AMBER&#x3001;HallusionBench&#x3001;MME&#xff09;&#x4e0a;&#x5747;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;InstructBLIP&#x4e2d;&#xff0c;ReCo&#x6210;&#x529f;&#x7ea0;&#x6b63;&#x4e86;&#x6a21;&#x578b;&#x5c06;&#x201c;&#x6551;&#x62a4;&#x8f66;&#x201d;&#x8bef;&#x8bc6;&#x522b;&#x4e3a;&#x201c;&#x591a;&#x8f86;&#x6c7d;&#x8f66;&#x548c;&#x5361;&#x8f66;&#x201d;&#x7684;&#x9519;&#x8bef;&#x3002;&#x6b64;&#x5916;&#xff0c;ReCo&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x6781;&#x5c0f;&#xff0c;&#x4e0d;&#x5f71;&#x54cd;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#xff0c;&#x4e14;&#x4e0e;&#x5176;&#x5b83;&#x89c4;&#x5219;&#x5f0f;&#x6297;&#x5e7b;&#x89c9;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x65f6;&#x80fd;&#x8fdb;&#x4e00;&#x6b65;&#x6539;&#x5584;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"955,956"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ReCo&#x901a;&#x8fc7;&#x51e0;&#x4f55;&#x4ee3;&#x6570;&#x542f;&#x53d1;&#x7684;&#x7ec4;&#x5408;&#x673a;&#x5236;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;VLMs&#x7684;&#x5e7b;&#x89c9;&#x548c;&#x8bb0;&#x5fc6;&#x6d88;&#x9000;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x5177;&#x5907;&#x8f7b;&#x91cf;&#x3001;&#x901a;&#x7528;&#x3001;&#x6613;&#x90e8;&#x7f72;&#x7684;&#x4f18;&#x70b9;&#x3002;&#x5176;&#x7ed3;&#x8bba;&#x8868;&#x660e;&#xff1a;&#x663e;&#x5f0f;&#x7ec4;&#x5408;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x662f;&#x63a7;&#x5236;VLM&#x751f;&#x6210;&#x53ef;&#x9760;&#x6027;&#x7684;&#x5173;&#x952e;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x4e3a;&#x6539;&#x8fdb;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65b0;&#x8303;&#x5f0f;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;VLMs&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x53ef;&#x884c;&#x6027;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x7ec4;&#x5408;&#x6027;&#x7406;&#x8bba;&#x4e0e;&#x6df1;&#x5ea6;&#x5b66;&#x4e60;&#x6a21;&#x578b;&#x7684;&#x8fdb;&#x4e00;&#x6b65;&#x7ed3;&#x5408;&#x3002;","children":[],"payload":{"tag":"li","lines":"956,958"}}],"payload":{"tag":"li","lines":"952,958","fold":1}}],"payload":{"tag":"h4","lines":"950,951"}}],"payload":{"tag":"h3","lines":"932,933","fold":1}},{"content":"&#x65b0;&#x8bad;&#x7ec3;&#x76ee;&#x6807;","children":[{"content":"ObjMLM: Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x7cfb;&#x7edf;&#x7814;&#x7a76;&#x4e86;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x9884;&#x8bad;&#x7ec3;&#xff08;VLP&#xff09;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x53d1;&#x73b0;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x666e;&#x904d;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x6807;&#x51c6;&#x6307;&#x6807;&#xff08;&#x5982;CIDEr&#xff09;&#x7684;&#x63d0;&#x5347;&#x53ef;&#x80fd;&#x4f34;&#x968f;&#x66f4;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x4e0d;&#x540c;&#x56fe;&#x50cf;&#x7f16;&#x7801;&#x65b9;&#x5f0f;&#x548c;&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#xff0c;&#x53d1;&#x73b0;&#x57fa;&#x4e8e;&#x8865;&#x4e01;&#x7684;&#x7279;&#x5f81;&#x548c;&#x66f4;&#x5c0f;&#x7684;&#x8865;&#x4e01;&#x5206;&#x8fa8;&#x7387;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x65b0;&#x635f;&#x5931;&#x51fd;&#x6570;ObjMLM&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5c06;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x964d;&#x4f4e;&#x9ad8;&#x8fbe;17.4%&#x3002;","children":[],"payload":{"tag":"li","lines":"961,962"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x9884;&#x8bad;&#x7ec3;&#xff08;VLP&#xff09;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x4ea7;&#x751f;&#x975e;&#x771f;&#x5b9e;&#x89c6;&#x89c9;&#x7269;&#x4f53;&#xff08;&#x5373;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff09;&#x7684;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x5e76;&#x5f15;&#x53d1;&#x5b89;&#x5168;&#x98ce;&#x9669;&#xff0c;&#x4f8b;&#x5982;&#x5728;&#x751f;&#x7269;&#x533b;&#x5b66;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4e2d;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x8bef;&#x8bca;&#x7b49;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x5c3d;&#x7ba1;VLP&#x6a21;&#x578b;&#x5728;&#x591a;&#x9879;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f46;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x5c1a;&#x672a;&#x88ab;&#x7cfb;&#x7edf;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"963,964"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x4ece;&#x4e09;&#x4e2a;&#x65b9;&#x9762;&#x7cfb;&#x7edf;&#x7814;&#x7a76;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff1a;1&#xff09;&#x8bc4;&#x4f30;&#x5f53;&#x524d;&#x6700;&#x5148;&#x8fdb;VLP&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x7a0b;&#x5ea6;&#xff0c;&#x4f7f;&#x7528;&#x6269;&#x5c55;&#x7684;CHAIR&#x6307;&#x6807;&#x8fdb;&#x884c;&#x91cf;&#x5316;&#xff1b;2&#xff09;&#x6bd4;&#x8f83;&#x4e0d;&#x540c;&#x56fe;&#x50cf;&#x7f16;&#x7801;&#x65b9;&#x5f0f;&#xff08;&#x57fa;&#x4e8e;&#x533a;&#x57df;&#x3001;&#x7f51;&#x683c;&#x548c;&#x8865;&#x4e01;&#xff09;&#x5bf9;&#x5e7b;&#x89c9;&#x7684;&#x5f71;&#x54cd;&#xff1b;3&#xff09;&#x89e3;&#x8026;&#x5e76;&#x5206;&#x6790;&#x5e38;&#x89c1;VLP&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#xff08;&#x5982;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x5bf9;&#x6bd4;&#x635f;&#x5931;ITC&#x3001;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x5339;&#x914d;&#x635f;&#x5931;ITM&#x548c;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#x8bed;&#x8a00;&#x5efa;&#x6a21;&#x635f;&#x5931;ICLM&#xff09;&#x7684;&#x4f5c;&#x7528;&#x3002;&#x57fa;&#x4e8e;&#x53d1;&#x73b0;&#xff0c;&#x63d0;&#x51fa;&#x4e00;&#x79cd;&#x65b0;&#x9884;&#x8bad;&#x7ec3;&#x635f;&#x5931;ObjMLM&#xff0c;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x6587;&#x672c;&#x6807;&#x8bb0;&#x4e0e;&#x89c6;&#x89c9;&#x5bf9;&#x8c61;&#x4e4b;&#x95f4;&#x7684;&#x5bf9;&#x9f50;&#x548c;&#x7ea6;&#x675f;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"964,965"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x6240;&#x6709;&#x6d4b;&#x8bd5;&#x7684;VLP&#x6a21;&#x578b;&#x5747;&#x5b58;&#x5728;&#x663e;&#x8457;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x7ea6;10%&#x7684;&#x751f;&#x6210;&#x53e5;&#x5b50;&#x81f3;&#x5c11;&#x542b;&#x4e00;&#x4e2a;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x4e14;&#x57df;&#x5916;&#x6570;&#x636e;&#xff08;NoCaps&#xff09;&#x4e0a;&#x66f4;&#x4e25;&#x91cd;&#xff1b;2&#xff09;&#x4f7f;&#x7528;SCST&#x4f18;&#x5316;&#x867d;&#x63d0;&#x5347;CIDEr&#x7b49;&#x6307;&#x6807;&#xff0c;&#x4f46;&#x52a0;&#x5267;&#x5e7b;&#x89c9;&#xff1b;3&#xff09;&#x8865;&#x4e01;&#x7f16;&#x7801;&#x4f18;&#x4e8e;&#x533a;&#x57df;&#x548c;&#x7f51;&#x683c;&#x7f16;&#x7801;&#xff0c;&#x66f4;&#x5c0f;&#x7684;&#x8865;&#x4e01;&#x5206;&#x8fa8;&#x7387;&#x80fd;&#x8fdb;&#x4e00;&#x6b65;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff1b;4&#xff09;&#x751f;&#x6210;&#x5f0f;&#x635f;&#x5931;ICLM&#x662f;&#x5e7b;&#x89c9;&#x4e3b;&#x56e0;&#xff0c;&#x800c;ITC/ITM&#x635f;&#x5931;&#x5bf9;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x4f5c;&#x7528;&#x6709;&#x9650;&#xff1b;5&#xff09;&#x63d0;&#x51fa;&#x7684;ObjMLM&#x635f;&#x5931;&#x5728;COCO Caption&#xff08;&#x57df;&#x5185;&#xff09;&#x548c;NoCaps&#xff08;&#x57df;&#x5916;&#xff09;&#x4e0a;&#x5206;&#x522b;&#x5c06;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x964d;&#x4f4e;&#x6700;&#x591a;17.4%&#x3002;","children":[],"payload":{"tag":"li","lines":"965,966"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x5728;VLP&#x6a21;&#x578b;&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#x4e14;&#x672a;&#x88ab;&#x73b0;&#x6709;&#x6280;&#x672f;&#x89e3;&#x51b3;&#xff0c;&#x751a;&#x81f3;&#x6807;&#x51c6;&#x6307;&#x6807;&#x4f18;&#x5316;&#x53ef;&#x80fd;&#x6076;&#x5316;&#x5e7b;&#x89c9;&#x3002;&#x8865;&#x4e01;&#x7f16;&#x7801;&#x548c;&#x53ef;&#x63a7;&#x751f;&#x6210;&#x662f;&#x5173;&#x952e;&#x51cf;&#x5c11;&#x56e0;&#x7d20;&#x3002;&#x63d0;&#x51fa;&#x7684;ObjMLM&#x635f;&#x5931;&#x7b80;&#x5355;&#x6709;&#x6548;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x5373;&#x53ef;&#x663e;&#x8457;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x4e9b;&#x53d1;&#x73b0;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;VLP&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x6307;&#x5bfc;&#xff0c;&#x5bf9;&#x533b;&#x7597;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"966,968"}}],"payload":{"tag":"li","lines":"962,968","fold":1}}],"payload":{"tag":"h4","lines":"960,961"}},{"content":"C-PMI: Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CMI-VLD&#x7684;&#x65b0;&#x9896;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x6761;&#x4ef6;&#x9010;&#x70b9;&#x4e92;&#x4fe1;&#x606f;&#xff08;C-PMI&#xff09;&#x6700;&#x5927;&#x5316;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x52a8;&#x6001;&#x5730;&#x4f18;&#x5316;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;token&#x7684;&#x76f8;&#x4e92;&#x4f9d;&#x8d56;&#x6027;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x89e3;&#x7801;&#x6548;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"969,970"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x5185;&#x5bb9;&#x8bed;&#x4e49;&#x4e0a;&#x5408;&#x7406;&#x4f46;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x6216;&#x5b58;&#x5728;&#x4e8b;&#x5b9e;&#x9519;&#x8bef;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x91d1;&#x878d;&#x7cfb;&#x7edf;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x89e3;&#x51b3;&#x3002;","children":[],"payload":{"tag":"li","lines":"971,972"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x4ece;&#x4fe1;&#x606f;&#x8bba;&#x7684;&#x89d2;&#x5ea6;&#x5c06;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x91cd;&#x65b0;&#x8868;&#x8ff0;&#x4e3a;&#x4e00;&#x4e2a;&#x6761;&#x4ef6;&#x4e92;&#x4fe1;&#x606f;&#x6700;&#x5927;&#x5316;&#x95ee;&#x9898;&#x3002;&#x4ed6;&#x4eec;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x57fa;&#x4e8e;&#x6761;&#x4ef6;&#x9010;&#x70b9;&#x4e92;&#x4fe1;&#x606f;&#xff08;C-PMI&#xff09;&#x7684;&#x53cc;&#x5c42;&#x4f18;&#x5316;&#x6846;&#x67b6;&#xff08;CMI-VLD&#xff09;&#x3002;&#x5728;&#x5185;&#x5c42;&#x5b50;&#x95ee;&#x9898;&#x4e2d;&#xff0c;&#x901a;&#x8fc7;&#x63a8;&#x5bfc;&#x51fa;&#x7684;&#x516c;&#x5f0f;&#x6821;&#x51c6;token&#x5206;&#x5e03;&#xff0c;&#x4f18;&#x5148;&#x9009;&#x62e9;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#x7684;&#x6587;&#x672c;token&#x3002;&#x5728;&#x5916;&#x5c42;&#x5b50;&#x95ee;&#x9898;&#x4e2d;&#xff0c;&#x8bbe;&#x8ba1;&#x4e86;&#x4e00;&#x4e2a;&#x53ef;&#x5b66;&#x4e60;&#x7684;&#x89c6;&#x89c9;token&#x51c0;&#x5316;&#x5668;&#xff0c;&#x52a8;&#x6001;&#x5730;&#x63d0;&#x70bc;&#x4e0e;&#x5f53;&#x524d;&#x6587;&#x672c;&#x4e0a;&#x4e0b;&#x6587;&#x6700;&#x76f8;&#x5173;&#x7684;&#x56fe;&#x50cf;token&#xff0c;&#x8fc7;&#x6ee4;&#x6389;&#x90a3;&#x4e9b;&#x4f1a;&#x635f;&#x5bb3;&#x4e92;&#x4fe1;&#x606f;&#x7684;&#x5197;&#x4f59;&#x89c6;&#x89c9;token&#x3002;&#x8be5;&#x7b56;&#x7565;&#x901a;&#x8fc7;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x8c03;&#x6574;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#xff0c;&#x6700;&#x5927;&#x5316;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e4b;&#x95f4;&#x7684;&#x76f8;&#x4e92;&#x4f9d;&#x8d56;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"972,973"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;LVLM&#xff08;&#x5982;LLaVA-1.5&#xff09;&#x548c;&#x4e94;&#x4e2a;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;&#x4e0a;&#x8fdb;&#x884c;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;CMI-VLD&#x65b9;&#x6cd5;&#x5728;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x5353;&#x8d8a;&#x7684;&#x6709;&#x6548;&#x6027;&#xff0c;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x5176;&#x4ed6;&#x7ade;&#x4e89;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x8f83;&#x9ad8;&#x7684;&#x89e3;&#x7801;&#x6548;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"973,974"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x7814;&#x7a76;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x4ece;&#x4fe1;&#x606f;&#x8bba;&#x89d2;&#x5ea6;&#x51fa;&#x53d1;&#xff0c;&#x901a;&#x8fc7;&#x6700;&#x5927;&#x5316;&#x6761;&#x4ef6;&#x4e92;&#x4fe1;&#x606f;&#x6765;&#x52a0;&#x5f3a;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0e;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e4b;&#x95f4;&#x7684;&#x53cc;&#x5411;&#x4f9d;&#x8d56;&#xff0c;&#x662f;&#x7f13;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x7684;&#x6709;&#x6548;&#x9014;&#x5f84;&#x3002;CMI-VLD&#x7b56;&#x7565;&#x4e3a;&#x6b64;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x7406;&#x8bba;&#x624e;&#x5b9e;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x80fd;&#x591f;&#x5927;&#x5e45;&#x63d0;&#x5347;LVLM&#x5728;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"974,976"}}],"payload":{"tag":"li","lines":"970,976","fold":1}}],"payload":{"tag":"h4","lines":"968,969"}},{"content":"HALVA: Mitigating Object Hallucination via Data Augmented Contrastive Tuning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x6570;&#x636e;&#x589e;&#x5f3a;&#x77ed;&#x8bed;&#x7ea7;&#x5bf9;&#x9f50;&#xff08;DPA&#xff09;&#x7684;&#x65b0;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff0c;&#x7528;&#x4e8e;&#x5fae;&#x8c03;&#x73b0;&#x6210;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#xff0c;&#x4ee5;&#x51cf;&#x8f7b;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x5176;&#x901a;&#x7528;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x80fd;&#x529b;&#x3002;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x5f0f;&#x6570;&#x636e;&#x589e;&#x5f3a;&#x6784;&#x5efa;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x548c;&#x2018;&#x6b63;&#x786e;&#x2019;&#x54cd;&#x5e94;&#x5bf9;&#xff0c;&#x5e76;&#x5728;&#x77ed;&#x8bed;&#x7ea7;&#x522b;&#x8fdb;&#x884c;&#x5bf9;&#x9f50;&#x8bad;&#x7ec3;&#xff0c;&#x5b9e;&#x9a8c;&#x663e;&#x793a;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"977,978"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0d;&#x7b26;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff0c;&#x5373;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x5728;&#x5e8f;&#x5217;&#x7ea7;&#x522b;&#x8fdb;&#x884c;&#x64cd;&#x4f5c;&#xff0c;&#x5bfc;&#x81f4;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x548c;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x80fd;&#x529b;&#x4e4b;&#x95f4;&#x5b58;&#x5728;&#x660e;&#x663e;&#x6743;&#x8861;&#xff0c;&#x4e14;&#x53ef;&#x80fd;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x6210;&#x672c;&#x6216;&#x9700;&#x8981;&#x5927;&#x91cf;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7cbe;&#x51c6;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x800c;&#x4e0d;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"979,980"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x6570;&#x636e;&#x589e;&#x5f3a;&#x77ed;&#x8bed;&#x7ea7;&#x5bf9;&#x9f50;&#xff08;DPA&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x9996;&#x5148;&#xff0c;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x5f0f;&#x6570;&#x636e;&#x589e;&#x5f3a;&#x6784;&#x5efa;&#x2018;&#x6b63;&#x786e;&#x2019;&#x548c;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x54cd;&#x5e94;&#x5bf9;&#xff0c;&#x5373;&#x9009;&#x62e9;&#x6027;&#x66ff;&#x6362;&#x6b63;&#x786e;&#x54cd;&#x5e94;&#x4e2d;&#x7684;&#x771f;&#x5b9e;&#x6982;&#x5ff5;&#x4e3a;&#x5e7b;&#x89c9;&#x6982;&#x5ff5;&#x3002;&#x7136;&#x540e;&#xff0c;&#x8bbe;&#x8ba1;DPA&#x635f;&#x5931;&#x51fd;&#x6570;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x90e8;&#x5206;&#xff1a;&#x4e00;&#x662f;&#x8ba1;&#x7b97;&#x5e7b;&#x89c9;&#x8bcd;&#x5143;&#x4e0e;&#x6b63;&#x786e;&#x8bcd;&#x5143;&#x7684;&#x76f8;&#x5bf9;&#x5bf9;&#x6570;&#x6982;&#x7387;&#xff0c;&#x4ee5;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x8bcd;&#x5143;&#x7684;&#x53ef;&#x80fd;&#x6027;&#xff1b;&#x4e8c;&#x662f;&#x4f7f;&#x7528;&#x51bb;&#x7ed3;&#x53c2;&#x8003;&#x6a21;&#x578b;&#x8ba1;&#x7b97;&#x8bcd;&#x5143;&#x7ea7;KL&#x6563;&#x5ea6;&#x4f5c;&#x4e3a;&#x6b63;&#x5219;&#x9879;&#xff0c;&#x4ee5;&#x6700;&#x5c0f;&#x5316;&#x6a21;&#x578b;&#x504f;&#x79bb;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x77ed;&#x8bed;&#x7ea7;&#x522b;&#x64cd;&#x4f5c;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x7ec6;&#x7c92;&#x5ea6;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"980,981"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;&#x4f7f;&#x7528;DPA&#x5fae;&#x8c03;&#x7684;&#x6a21;&#x578b;&#xff08;HALVA&#xff09;&#x5728;&#x5e7b;&#x89c9;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x4e0a;F1&#x5206;&#x6570;&#x63d0;&#x5347;&#x9ad8;&#x8fbe;13.4%&#xff0c;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x5e7b;&#x89c9;&#x7387;&#x964d;&#x4f4e;&#x9ad8;&#x8fbe;4.2%&#x3002;&#x540c;&#x65f6;&#xff0c;&#x8be5;&#x6a21;&#x578b;&#x5728;&#x901a;&#x7528;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x5f97;&#x5230;&#x4fdd;&#x6301;&#x6216;&#x7565;&#x6709;&#x63d0;&#x5347;&#xff0c;&#x907f;&#x514d;&#x4e86;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5bfc;&#x81f4;&#x7684;&#x6027;&#x80fd;&#x9000;&#x5316;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"981,982"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: DPA&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x7684;&#x77ed;&#x8bed;&#x7ea7;&#x5fae;&#x8c03;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x591f;&#x663e;&#x8457;&#x51cf;&#x5c11;MLLMs&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x5176;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x65f6;&#x95f4;&#x6216;&#x5927;&#x91cf;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x73b0;&#x6210;&#x6a21;&#x578b;&#xff0c;&#x5177;&#x6709;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;&#x4ee3;&#x7801;&#x548c;&#x6570;&#x636e;&#x96c6;&#x5df2;&#x5f00;&#x6e90;&#xff0c;&#x4fc3;&#x8fdb;&#x8fdb;&#x4e00;&#x6b65;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"982,984"}}],"payload":{"tag":"li","lines":"978,984","fold":1}}],"payload":{"tag":"h4","lines":"976,977"}},{"content":"EVRB: Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x6846;&#x67b6;EVRB&#xff0c;&#x4ece;&#x8d1d;&#x53f6;&#x65af;&#x89c6;&#x89d2;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x901a;&#x8fc7;&#x526a;&#x679d;&#x5197;&#x4f59;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x3001;&#x7ea0;&#x6b63;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x504f;&#x5dee;&#x548c;&#x63d0;&#x524d;&#x505c;&#x6b62;&#x751f;&#x6210;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6587;&#x672c;&#x751f;&#x6210;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"985,986"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x4f46;&#x4e0a;&#x4e0b;&#x6587;&#x8fde;&#x8d2f;&#x7684;&#x5185;&#x5bb9;&#xff0c;&#x5373;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x548c;&#x53ef;&#x9760;&#x6027;&#x573a;&#x666f;&#xff08;&#x5982;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x7279;&#x5b9a;&#x6a21;&#x6001;&#x7279;&#x5f81;&#x6765;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x672a;&#x80fd;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x63d0;&#x5347;&#x6587;&#x672c;&#x751f;&#x6210;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x56e0;&#x6b64;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;","children":[],"payload":{"tag":"li","lines":"987,988"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x4ece;&#x8d1d;&#x53f6;&#x65af;&#x89c6;&#x89d2;&#x63d0;&#x51fa;&#x4e09;&#x65b9;&#x9762;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff1a;1. &#x5197;&#x4f59;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x526a;&#x679d;&#xff1a;&#x8ba1;&#x7b97;&#x6bcf;&#x4e2a;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x7684;&#x4e0b;&#x4e00;&#x8bcd;&#x9884;&#x6d4b;&#x71b5;&#xff0c;&#x79fb;&#x9664;&#x9ad8;&#x71b5;&#xff08;&#x6a21;&#x7cca;&#x65e0;&#x7528;&#xff09;&#x7684;&#x6807;&#x8bb0;&#x4ee5;&#x51cf;&#x5c11;&#x5e72;&#x6270;&#xff1b;2. &#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7ea0;&#x504f;&#xff1a;&#x5c06;&#x540e;&#x9a8c;&#x6982;&#x7387;&#x76f4;&#x63a5;&#x9664;&#x4ee5;&#x524d;&#x9a8c;&#x6982;&#x7387;&#xff0c;&#x6d88;&#x9664;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x56fa;&#x6709;&#x7684;&#x8bcd;&#x6c47;&#x504f;&#x597d;&#x504f;&#x5dee;&#xff1b;3. &#x540e;&#x9a8c;&#x5d29;&#x584c;&#x9884;&#x9632;&#xff1a;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x540e;&#x9a8c;&#x4e0e;&#x524d;&#x9a8c;&#x5206;&#x5e03;&#x7684;JS&#x6563;&#x5ea6;&#xff0c;&#x76d1;&#x6d4b;&#x89c6;&#x89c9;&#x4f9d;&#x8d56;&#x7a0b;&#x5ea6;&#xff0c;&#x5e76;&#x5728;&#x6563;&#x5ea6;&#x4f4e;&#x4e8e;&#x9608;&#x503c;&#x65f6;&#x63d0;&#x524d;&#x7ec8;&#x6b62;&#x751f;&#x6210;&#x4ee5;&#x907f;&#x514d;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"988,989"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x3001;CHAIR&#x548c;MME&#x4e09;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#xff0c;EVRB&#x5747;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x6027;&#x80fd;&#x4f18;&#x4e8e;&#x6b64;&#x524d;&#x6700;&#x5148;&#x8fdb;&#x65b9;&#x6cd5;&#x3002;&#x4f8b;&#x5982;&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x5347;&#x663e;&#x8457;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x4e8e;&#x73b0;&#x6709;LVLM&#x3002;","children":[],"payload":{"tag":"li","lines":"989,990"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x7814;&#x7a76;&#x4ece;&#x8d1d;&#x53f6;&#x65af;&#x7406;&#x8bba;&#x5c42;&#x9762;&#x63ed;&#x793a;&#x4e86;LVLM&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x4e09;&#x5927;&#x6839;&#x6e90;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x901a;&#x7528;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x65b9;&#x6848;&#x3002;EVRB&#x6846;&#x67b6;&#x80fd;&#x6709;&#x6548;&#x589e;&#x5f3a;&#x6587;&#x672c;&#x751f;&#x6210;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5bf9;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x7684;&#x5b89;&#x5168;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"990,992"}}],"payload":{"tag":"li","lines":"986,992","fold":1}}],"payload":{"tag":"h4","lines":"984,985"}},{"content":"BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;BIMA&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x5229;&#x7528;&#x6807;&#x51c6;&#x5316;&#x6d41;&#x7406;&#x8bba;&#xff0c;&#x901a;&#x8fc7;&#x53cc;&#x5c04;&#x6700;&#x5927;&#x4f3c;&#x7136;&#x5b66;&#x4e60;&#x6765;&#x9884;&#x6d4b;&#x548c;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;POPE&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;85.06%&#x7684;&#x5e73;&#x5747;F1&#x5206;&#x6570;&#xff0c;&#x5e76;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;CHAIR&#x6307;&#x6807;&#x3002;","children":[],"payload":{"tag":"li","lines":"993,994"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#xff0c;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#xff0c;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x51cf;&#x5c11;&#x8fd9;&#x79cd;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x9ad8;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"995,996"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;BIMA&#xff08;Bijective Maximum Likelihood Learning&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x57fa;&#x4e8e;&#x6807;&#x51c6;&#x5316;&#x6d41;&#x7406;&#x8bba;&#x5efa;&#x7acb;&#x53cc;&#x5c04;&#x6620;&#x5c04;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6d41;&#x751f;&#x6210;&#x6a21;&#x578b;&#x4f30;&#x8ba1;&#x771f;&#x5b9e;&#x54cd;&#x5e94;&#x5206;&#x5e03;&#xff0c;&#x6784;&#x5efa;&#x53c2;&#x8003;&#x5206;&#x5e03;&#xff0c;&#x5e76;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x5229;&#x7528;&#x53cc;&#x5c04;&#x635f;&#x5931;&#x5c06;&#x6a21;&#x578b;&#x54cd;&#x5e94;&#x6620;&#x5c04;&#x5230;&#x53c2;&#x8003;&#x5206;&#x5e03;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x9700;&#x8981;&#x5728;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x9636;&#x6bb5;&#x96c6;&#x6210;&#x5230;LVLMs&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"996,997"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: BIMA&#x5728;POPE&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5e73;&#x5747;F1&#x5206;&#x6570;&#x8fbe;&#x5230;85.06%&#xff0c;&#x540c;&#x65f6;&#x5c06;CHAIR-S&#x548c;CHAIR-I&#x6307;&#x6807;&#x5206;&#x522b;&#x964d;&#x4f4e;&#x4e86;7.6%&#x548c;2.6%&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"997,998"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: BIMA&#x662f;&#x9996;&#x4e2a;&#x5229;&#x7528;&#x53cc;&#x5c04;&#x6620;&#x5c04;&#x6846;&#x67b6;&#x51cf;&#x5c11;LVLMs&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5efa;&#x7acb;&#x53c2;&#x8003;&#x5206;&#x5e03;&#x548c;&#x53cc;&#x5c04;&#x5ea6;&#x91cf;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x9ad8;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x8f93;&#x51fa;&#x51c6;&#x786e;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x5177;&#x6709;&#x6f5c;&#x5728;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"998,1000"}}],"payload":{"tag":"li","lines":"994,1000","fold":1}}],"payload":{"tag":"h4","lines":"992,993"}},{"content":"Symmetrical Visual Contrastive Optimization (S-VCO): Aligning Vision-Language Models with Minimal Contrastive Images","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;S-VCO&#xff08;&#x5bf9;&#x79f0;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x4f18;&#x5316;&#xff09;&#x65b9;&#x6cd5;&#x548c;MVC&#xff08;&#x6700;&#x5c0f;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#xff09;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x4ee5;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x800c;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x7ec6;&#x8282;&#x7684;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5bf9;&#x79f0;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x589e;&#x5f3a;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x5bf9;&#x9f50;&#xff0c;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x548c;&#x63d0;&#x5347;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#x6027;&#x80fd;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x663e;&#x8457;&#x3002;","children":[],"payload":{"tag":"li","lines":"1001,1002"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5b58;&#x5728;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5148;&#x9a8c;&#x800c;&#x5ffd;&#x89c6;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5bfc;&#x81f4;&#x5728;&#x89c6;&#x89c9;&#x57fa;&#x7840;&#x4efb;&#x52a1;&#x4e2d;&#x51fa;&#x73b0;&#x9519;&#x8bef;&#x548c;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5728;&#x9700;&#x8981;&#x7cbe;&#x7ec6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x611f;&#x77e5;&#x3001;&#x63a8;&#x7406;&#x548c;OCR&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;VLM&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1003,1004"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x6838;&#x5fc3;&#x521b;&#x65b0;&#xff1a;1) S-VCO&#xff08;&#x5bf9;&#x79f0;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x4f18;&#x5316;&#xff09;&#xff1a;&#x4e00;&#x79cd;&#x5fae;&#x8c03;&#x76ee;&#x6807;&#x51fd;&#x6570;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x79f0;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x7ec6;&#x8282;&#x4e0e;&#x6587;&#x672c;&#x6807;&#x8bb0;&#x7684;&#x7cbe;&#x786e;&#x5bf9;&#x5e94;&#x3002;&#x5b83;&#x5305;&#x62ec;&#x4e24;&#x4e2a;&#x7ec4;&#x4ef6;&#xff1a;L_Attend&#xff08;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x5339;&#x914d;&#x56fe;&#x50cf;&#xff09;&#x548c;L_Reject&#xff08;&#x60e9;&#x7f5a;&#x6a21;&#x578b;&#x5728;&#x77db;&#x76fe;&#x56fe;&#x50cf;&#x4e0b;&#x751f;&#x6210;&#x9519;&#x8bef;&#x6587;&#x672c;&#xff09;&#x3002;2) MVC&#x6570;&#x636e;&#x96c6;&#xff1a;&#x901a;&#x8fc7;&#x81ea;&#x52a8;&#x7b5b;&#x9009;&#x548c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x53cd;&#x4e8b;&#x5b9e;&#x6570;&#x636e;&#x6784;&#x5efa;&#x7684;&#x6210;&#x5bf9;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5305;&#x542b;&#x6700;&#x5c0f;&#x89c6;&#x89c9;&#x5dee;&#x5f02;&#x7684;&#x5bf9;&#x6bd4;&#x6837;&#x672c;&#xff08;&#x4f8b;&#x5982;&#x4ec5;&#x6539;&#x53d8;&#x56fe;&#x50cf;&#x4e2d;&#x4e00;&#x4e2a;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x4ee5;&#x63d0;&#x4f9b;&#x66f4;&#x5177;&#x6311;&#x6218;&#x6027;&#x7684;&#x8bad;&#x7ec3;&#x6837;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1004,1005"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4e00;&#x81f4;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff0c;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x9ad8;&#x8fbe;22%&#xff1b;2) &#x5728;&#x89c6;&#x89c9;&#x4e2d;&#x5fc3;&#x4efb;&#x52a1;&#xff08;&#x5982;OCR&#x3001;&#x573a;&#x666f;&#x7406;&#x89e3;&#xff09;&#x548c;&#x901a;&#x7528;&#x4efb;&#x52a1;&#x4e2d;&#x5747;&#x53d6;&#x5f97;&#x663e;&#x8457;&#x589e;&#x76ca;&#xff1b;3) &#x6027;&#x80fd;&#x63d0;&#x5347;&#x5728;&#x89c6;&#x89c9;&#x4f9d;&#x8d56;&#x6027;&#x9ad8;&#x7684;&#x4efb;&#x52a1;&#x4e2d;&#x5c24;&#x4e3a;&#x660e;&#x663e;&#xff1b;4) &#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x504f;&#x597d;&#x4f18;&#x5316;&#x65b9;&#x6cd5;&#xff08;&#x5982;DPO&#x548c;mDPO&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1005,1006"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: S-VCO&#x901a;&#x8fc7;&#x5bf9;&#x79f0;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;VLM&#x7684;&#x89c6;&#x89c9;&#x5ffd;&#x89c6;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x4e3a;VLM&#x8bad;&#x7ec3;&#x63d0;&#x4f9b;&#x65b0;&#x8303;&#x5f0f;&#xff0c;&#x63a8;&#x52a8;&#x5176;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1006,1008"}}],"payload":{"tag":"li","lines":"1002,1008","fold":1}}],"payload":{"tag":"h4","lines":"1000,1001"}},{"content":"F-CLipScore: Vision-Encoders (Already) Know What They See: Mitigating Object Hallucination via Simple Fine-Grained CLIPScore","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x7814;&#x7a76;&#x6311;&#x6218;&#x4e86;&#x5148;&#x524d;&#x5173;&#x4e8e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x80fd;&#x529b;&#x6709;&#x9650;&#x662f;&#x5bfc;&#x81f4;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x4e3b;&#x8981;&#x539f;&#x56e0;&#x7684;&#x89c2;&#x70b9;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b0;&#x8bc4;&#x4f30;&#x6307;&#x6807;F-CLIPScore&#x3002;&#x8be5;&#x6307;&#x6807;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x540d;&#x8bcd;&#x7ea7;&#x522b;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x6587;&#x672c;&#x5d4c;&#x5165;&#xff0c;&#x5728;OHD-Caps&#x57fa;&#x51c6;&#x4e0a;&#x51c6;&#x786e;&#x7387;&#x663e;&#x8457;&#x63d0;&#x5347;39.6%&#xff0c;&#x5e76;&#x80fd;&#x5728;LVLM&#x9884;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x7b5b;&#x9009;&#x4e2d;&#x51cf;&#x5c11;4.9%&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1009,1010"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x5305;&#x542b;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4f1a;&#x4e25;&#x91cd;&#x524a;&#x5f31;LVLM&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;&#x5148;&#x524d;&#x7814;&#x7a76;&#x8ba4;&#x4e3a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x8868;&#x5f81;&#x80fd;&#x529b;&#x6709;&#x9650;&#x662f;&#x4e3b;&#x8981;&#x539f;&#x56e0;&#xff0c;&#x4f46;&#x672c;&#x6587;&#x901a;&#x8fc7;&#x5b9e;&#x8bc1;&#x5206;&#x6790;&#x5bf9;&#x6b64;&#x63d0;&#x51fa;&#x4e86;&#x8d28;&#x7591;&#x3002;","children":[],"payload":{"tag":"li","lines":"1011,1012"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x7ec6;&#x7c92;&#x5ea6;CLIPScore&#xff08;F-CLIPScore&#xff09;&#x8fd9;&#x4e00;&#x65b0;&#x9896;&#x7684;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x4f7f;&#x7528;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff1a;1&#xff09;&#x4f7f;&#x7528;spaCy&#x89e3;&#x6790;&#x5668;&#x4ece;&#x53e5;&#x5b50;&#x4e2d;&#x63d0;&#x53d6;&#x540d;&#x8bcd;&#xff1b;2&#xff09;&#x8ba1;&#x7b97;&#x6574;&#x4e2a;&#x53e5;&#x5b50;&#x7684;CLIPScore&#x4e0e;&#x6bcf;&#x4e2a;&#x540d;&#x8bcd;&#x7684;CLIPScore&#x7684;&#x5e73;&#x5747;&#x503c;&#x3002;&#x6570;&#x5b66;&#x8868;&#x8fbe;&#x5f0f;&#x4e3a;&#xff1a;F-CLIPScore(S) = [CLIPScore(S) + &#x3a3;CLIPScore(n_i)] / (N + 1)&#xff0c;&#x5176;&#x4e2d;S&#x4e3a;&#x5b8c;&#x6574;&#x53e5;&#x5b50;&#xff0c;n_i&#x4e3a;&#x540d;&#x8bcd;&#xff0c;N&#x4e3a;&#x540d;&#x8bcd;&#x603b;&#x6570;&#x3002;&#x8fd9;&#x79cd;&#x65b9;&#x6cd5;&#x589e;&#x5f3a;&#x4e86;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x5904;&#x7406;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1012,1013"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x5728;OHD-Caps&#x6d4b;&#x8bd5;&#x96c6;&#x4e0a;&#xff0c;F-CLIPScore&#x65e0;&#x9700;&#x4efb;&#x4f55;&#x8bad;&#x7ec3;&#x5373;&#x8fbe;&#x5230;62.2%&#x7684;&#x51c6;&#x786e;&#x7387;&#xff0c;&#x6bd4;&#x539f;&#x59cb;CLIPScore&#xff08;22.6%&#xff09;&#x63d0;&#x5347;39.6&#x4e2a;&#x767e;&#x5206;&#x70b9;&#xff1b;2&#xff09;&#x5c06;F-CLIPScore&#x4f5c;&#x4e3a;&#x635f;&#x5931;&#x51fd;&#x6570;&#x7684;&#x4e00;&#x90e8;&#x5206;&#x8fdb;&#x884c;&#x8bad;&#x7ec3;&#xff0c;&#x5728;COCO&#x548c;NoCaps&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x6027;&#x80fd;&#x8fdb;&#x4e00;&#x6b65;&#x5206;&#x522b;&#x63d0;&#x5347;0.7&#x548c;0.1&#x4e2a;&#x767e;&#x5206;&#x70b9;&#xff1b;3&#xff09;&#x5728;LLaVA&#x9884;&#x8bad;&#x7ec3;&#x4e2d;&#x4f7f;&#x7528;F-CLIPScore&#x8fc7;&#x6ee4;&#x4f4e;&#x8d28;&#x91cf;&#x6570;&#x636e;&#xff08;&#x4fdd;&#x7559;&#x524d;70%&#xff09;&#xff0c;&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#x7684;&#x51c6;&#x786e;&#x7387;&#x6bd4;&#x4f7f;&#x7528;&#x5168;&#x6570;&#x636e;&#x96c6;&#x8bad;&#x7ec3;&#x63d0;&#x5347;4.9%&#xff0c;&#x800c;&#x968f;&#x673a;&#x8fc7;&#x6ee4;&#x6216;&#x4f7f;&#x7528;OHD-Caps&#x8bad;&#x7ec3;&#x7684;CLIP&#x8fc7;&#x6ee4;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;","children":[],"payload":{"tag":"li","lines":"1013,1014"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x6700;&#x7ec8;&#x7ed3;&#x8bba;&#x662f;&#xff1a;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x4e3b;&#x8981;&#x6839;&#x6e90;&#x5e76;&#x975e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x80fd;&#x529b;&#x9650;&#x5236;&#xff0c;&#x800c;&#x662f;&#x9700;&#x8981;&#x66f4;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x6587;&#x672c;-&#x56fe;&#x50cf;&#x5bf9;&#x9f50;&#x65b9;&#x6cd5;&#x3002;F-CLIPScore&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x6709;&#x6548;&#x7684;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff0c;&#x4e0d;&#x4ec5;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x6027;&#x80fd;&#xff0c;&#x8fd8;&#x80fd;&#x901a;&#x8fc7;&#x6570;&#x636e;&#x7b5b;&#x9009;&#x6709;&#x6548;&#x51cf;&#x5c11;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8fd9;&#x4e00;&#x53d1;&#x73b0;&#x5bf9;LVLM&#x7684;&#x5f00;&#x53d1;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#xff0c;&#x8868;&#x660e;&#x672a;&#x6765;&#x7814;&#x7a76;&#x5e94;&#x66f4;&#x591a;&#x5173;&#x6ce8;&#x8d85;&#x8d8a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x80fd;&#x529b;&#x7684;&#x5176;&#x4ed6;&#x56e0;&#x7d20;&#xff0c;&#x5982;&#x6587;&#x672c;&#x8868;&#x5f81;&#x7684;&#x4f18;&#x5316;&#x548c;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x7684;&#x8d28;&#x91cf;&#x63a7;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"1014,1016"}}],"payload":{"tag":"li","lines":"1010,1016","fold":1}}],"payload":{"tag":"h4","lines":"1008,1009"}},{"content":"PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;PerturboLLaVA&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x5bf9;&#x6297;&#x6027;&#x6270;&#x52a8;&#x6587;&#x672c;&#x8bad;&#x7ec3;&#xff0c;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x51fa;&#x65b0;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x8bc4;&#x4f30;&#x6307;&#x6807;HalFscore&#x3002;","children":[],"payload":{"tag":"li","lines":"1017,1018"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5728;&#x5bc6;&#x96c6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x5c24;&#x4e3a;&#x7a81;&#x51fa;&#xff0c;&#x56e0;&#x4e3a;&#x9700;&#x8981;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x4e14;&#x5168;&#x9762;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x9700;&#x8981;&#x7cbe;&#x786e;&#x89c6;&#x89c9;&#x63cf;&#x8ff0;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x800c;&#x73b0;&#x6709;&#x7684;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x65e0;&#x6cd5;&#x7ec6;&#x7c92;&#x5ea6;&#x5730;&#x8861;&#x91cf;&#x63cf;&#x8ff0;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1019,1020"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x63d0;&#x51fa;&#x4e86;HalFscore&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff0c;&#x57fa;&#x4e8e;&#x8bed;&#x8a00;&#x56fe;&#xff08;language graph&#xff09;&#x6765;&#x7ec6;&#x7c92;&#x5ea6;&#x8bc4;&#x4f30;&#x63cf;&#x8ff0;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x5b8c;&#x6574;&#x6027;&#x3002;&#x9488;&#x5bf9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4f5c;&#x8005;&#x53d1;&#x73b0;&#x5176;&#x6839;&#x672c;&#x539f;&#x56e0;&#x662f;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x9884;&#x8bad;&#x7ec3;&#x7684;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e86;PerturboLLaVA&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x5728;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x5f15;&#x5165;&#x5bf9;&#x6297;&#x6027;&#x6270;&#x52a8;&#x6587;&#x672c;&#xff08;&#x5982;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x51b2;&#x7a81;&#x4f46;&#x7b26;&#x5408;&#x5e38;&#x8bc6;&#x7684;&#x6587;&#x672c;&#x63d0;&#x793a;&#xff09;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x66f4;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x800c;&#x975e;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;&#x8bad;&#x7ec3;&#x6d41;&#x7a0b;&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"1020,1021"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: PerturboLLaVA&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#xff0c;&#x5728;&#x5bc6;&#x96c6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x66f4;&#x51c6;&#x786e;&#x3001;&#x66f4;&#x5fe0;&#x5b9e;&#x4e8e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x3002;&#x4e0e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x3001;RLAIF-V&#x3001;OPERA&#xff09;&#x76f8;&#x6bd4;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x8fd8;&#x80fd;&#x5728;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff0c;&#x4e14;&#x4e0d;&#x589e;&#x52a0;&#x8bad;&#x7ec3;&#x6216;&#x63a8;&#x7406;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1021,1022"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x7814;&#x7a76;&#x901a;&#x8fc7;HalFscore&#x6307;&#x6807;&#x548c;PerturboLLaVA&#x65b9;&#x6cd5;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x6709;&#x6548;&#x7684;&#x8bc4;&#x4f30;&#x548c;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x589e;&#x5f3a;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x80fd;&#x529b;&#xff0c;&#x5177;&#x6709;&#x9ad8;&#x6548;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x548c;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#x7684;&#x4f18;&#x70b9;&#xff0c;&#x5bf9;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1022,1024"}}],"payload":{"tag":"li","lines":"1018,1024","fold":1}}],"payload":{"tag":"h4","lines":"1016,1017"}},{"content":"HACL: Hallucination Augmented Contrastive Learning for Multimodal Large Language Model","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;HACL&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x5229;&#x7528;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x4f5c;&#x4e3a;&#x56f0;&#x96be;&#x8d1f;&#x6837;&#x672c;&#x6765;&#x6539;&#x5584;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x5e76;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1025,1026"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x6574;&#x5408;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x9519;&#x8bef;&#x6216;&#x865a;&#x6784;&#x4fe1;&#x606f;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;&#x8bba;&#x6587;&#x4ece;&#x8868;&#x793a;&#x5b66;&#x4e60;&#x7684;&#x89d2;&#x5ea6;&#x51fa;&#x53d1;&#xff0c;&#x53d1;&#x73b0;&#x89c6;&#x89c9;&#x4e0e;&#x6587;&#x672c;&#x8868;&#x793a;&#x4e4b;&#x95f4;&#x5b58;&#x5728;&#x663e;&#x8457;&#x7684;&#x6a21;&#x6001;&#x9e3f;&#x6c9f;&#xff0c;&#x4e14;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x4e0e;&#x975e;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x7684;&#x8868;&#x793a;&#x76f8;&#x4e92;&#x7ea0;&#x7f20;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x96be;&#x4ee5;&#x533a;&#x5206;&#x6b63;&#x786e;&#x4e0e;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1027,1028"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x5e7b;&#x89c9;&#x589e;&#x5f3a;&#x8de8;&#x6a21;&#x6001;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#xff08;HACL&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x4f7f;&#x7528;GPT-4&#x751f;&#x6210;&#x5305;&#x542b;&#x5c5e;&#x6027;&#x9519;&#x8bef;&#x6216;&#x865a;&#x6784;&#x4fe1;&#x606f;&#x7684;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x4f5c;&#x4e3a;&#x56f0;&#x96be;&#x8d1f;&#x6837;&#x672c;&#xff1b;2) &#x5728;MLLM&#x8bad;&#x7ec3;&#x4e2d;&#x5f15;&#x5165;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#xff0c;&#x4ee5;&#x56fe;&#x50cf;&#x4e3a;&#x951a;&#x70b9;&#xff0c;&#x62c9;&#x8fd1;&#x975e;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x4e0e;&#x89c6;&#x89c9;&#x8868;&#x793a;&#x7684;&#x8ddd;&#x79bb;&#xff0c;&#x540c;&#x65f6;&#x63a8;&#x8fdc;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x7684;&#x8868;&#x793a;&#xff1b;3) &#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x3001;&#x53ef;&#x5b66;&#x4e60;&#x63a5;&#x53e3;&#x548c;LLM&#x6a21;&#x5757;&#x63d0;&#x53d6;&#x5168;&#x5c40;&#x8868;&#x793a;&#xff08;&#x5982;<eos>&#x6807;&#x8bb0;&#x7684;&#x6700;&#x7ec8;&#x5c42;&#x8f93;&#x51fa;&#xff09;&#xff0c;&#x6784;&#x5efa;&#x8de8;&#x6a21;&#x6001;&#x5bf9;&#x6bd4;&#x635f;&#x5931;&#x51fd;&#x6570;&#x4ee5;&#x4f18;&#x5316;&#x8868;&#x793a;&#x7a7a;&#x95f4;&#x3002;</eos>","children":[],"payload":{"tag":"li","lines":"1028,1029"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;HACL&#x663e;&#x8457;&#x6539;&#x5584;&#x4e86;&#x8868;&#x793a;&#x7a7a;&#x95f4;&#xff1a;&#x89c6;&#x89c9;&#x4e0e;&#x6587;&#x672c;&#x8868;&#x793a;&#x5bf9;&#x9f50;&#x66f4;&#x7d27;&#x5bc6;&#xff0c;&#x5e7b;&#x89c9;&#x4e0e;&#x975e;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x7684;&#x533a;&#x5206;&#x5ea6;&#x63d0;&#x9ad8;&#x3002;&#x5728;MMhal-Bench&#x57fa;&#x51c6;&#x4e0a;&#xff0c;LLaVA&#x6a21;&#x578b;&#x6027;&#x80fd;&#x63d0;&#x5347;29.5%&#xff0c;MiniGPT-4&#x63d0;&#x5347;34.66%&#xff1b;&#x5728;MME&#x7efc;&#x5408;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;&#x4e0a;&#x6027;&#x80fd;&#x63d0;&#x5347;11%&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x663e;&#x793a;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5e7b;&#x89c9;&#x7684;&#x73b0;&#x8c61;&#x660e;&#x663e;&#x51cf;&#x5c11;&#x3002;","children":[],"payload":{"tag":"li","lines":"1029,1030"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: HACL&#x901a;&#x8fc7;&#x8868;&#x793a;&#x5b66;&#x4e60;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;MLLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x5347;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6570;&#x636e;&#xff0c;&#x5177;&#x6709;&#x7b80;&#x5355;&#x9ad8;&#x6548;&#x7684;&#x4f18;&#x70b9;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x89e3;&#x51b3;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x548c;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1030,1032"}}],"payload":{"tag":"li","lines":"1026,1032","fold":1}}],"payload":{"tag":"h4","lines":"1024,1025"}}],"payload":{"tag":"h3","lines":"958,959","fold":1}},{"content":"&#x5c42;&#x5f52;&#x4e00;&#x5316;","children":[{"content":"TTA Framework: Mitigating Image Captioning Hallucinations in Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x7684;&#x6d4b;&#x8bd5;&#x65f6;&#x81ea;&#x9002;&#x5e94;&#x6846;&#x67b6;&#xff0c;&#x4ec5;&#x66f4;&#x65b0;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x5c42;&#x5f52;&#x4e00;&#x5316;&#x7684;&#x53ef;&#x5b66;&#x4e60;&#x53c2;&#x6570;&#xff08;&#x7ea6;0.003%&#xff09;&#xff0c;&#x901a;&#x8fc7;CLIP&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x53cc;&#x91cd;&#x5956;&#x52b1;&#xff08;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x5206;&#x6570;&#x548c;&#x975e;&#x5e7b;&#x89c9;&#x6982;&#x7387;&#xff09;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x52a8;&#x6001;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x5b9e;&#x9a8c;&#x663e;&#x793a;&#xff0c;LLaVA&#x548c;InstructBLIP&#x7684;&#x5e7b;&#x89c9;&#x7387;&#x5206;&#x522b;&#x964d;&#x4f4e;15.4%&#x548c;17.3%&#xff0c;&#x6548;&#x679c;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1035,1036"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x5e38;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x8f93;&#x51fa;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#xff09;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9884;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e0e;&#x6d4b;&#x8bd5;&#x6837;&#x672c;&#x5b58;&#x5728;&#x5206;&#x5e03;&#x504f;&#x79fb;&#x65f6;&#x3002;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;VLMs&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x56fe;&#x50cf;&#x5206;&#x6790;&#x7b49;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x9886;&#x57df;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x91cd;&#x8bad;&#x7ec3;&#x3001;&#x5fae;&#x8c03;&#x6216;&#x96c6;&#x6210;&#x6a21;&#x578b;&#xff09;&#x5b58;&#x5728;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#x3001;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#x7b49;&#x95ee;&#x9898;&#xff0c;&#x7f3a;&#x4e4f;&#x9ad8;&#x6548;&#x4e14;&#x8f7b;&#x91cf;&#x7ea7;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1037,1038"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x6d4b;&#x8bd5;&#x65f6;&#x81ea;&#x9002;&#x5e94;&#xff08;TTA&#xff09;&#x6846;&#x67b6;&#xff0c;&#x5c06;VLM&#x4f5c;&#x4e3a;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x4e2d;&#x7684;&#x7b56;&#x7565;&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;&#x4ee5;&#x4e0b;&#x6b65;&#x9aa4;&#x52a8;&#x6001;&#x4f18;&#x5316;&#xff1a;1) &#x4f7f;&#x7528;&#x675f;&#x641c;&#x7d22;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x5019;&#x9009;&#x63cf;&#x8ff0;&#xff1b;2) &#x57fa;&#x4e8e;CLIP&#x7684;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x8ba1;&#x7b97;&#x53cc;&#x91cd;&#x5956;&#x52b1;&#xff1a;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x5206;&#x6570;&#xff08;SAS&#xff09;&#x8861;&#x91cf;&#x56fe;&#x6587;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x975e;&#x5e7b;&#x89c9;&#x6982;&#x7387;&#xff08;NHP&#xff09;&#x8bc4;&#x4f30;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#xff1b;3) &#x4ec5;&#x66f4;&#x65b0;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x5c42;&#x5f52;&#x4e00;&#x5316;&#x7684;gamma&#x53c2;&#x6570;&#xff08;&#x5360;&#x603b;&#x6570;0.003%&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x7b56;&#x7565;&#x68af;&#x5ea6;&#x635f;&#x5931;&#x4f18;&#x5316;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x91cd;&#x8bad;&#x7ec3;&#x6216;&#x989d;&#x5916;VLM&#xff0c;&#x4ec5;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x8fed;&#x4ee3;&#x4f18;&#x5316;&#x3002;","children":[],"payload":{"tag":"li","lines":"1038,1039"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1) &#x5728;LLaVA&#x548c;InstructBLIP&#x4e0a;&#xff0c;&#x5e7b;&#x89c9;&#x7387;&#x5206;&#x522b;&#x964d;&#x4f4e;15.4%&#x548c;17.3%&#xff1b;2) &#x76f8;&#x6bd4;&#x6700;&#x5148;&#x8fdb;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;VCD&#xff0c;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x6548;&#x679c;&#x63d0;&#x5347;68.3%&#xff1b;3) &#x6d88;&#x878d;&#x7814;&#x7a76;&#x9a8c;&#x8bc1;&#x4e86;&#x53cc;&#x91cd;&#x5956;&#x52b1;&#x673a;&#x5236;&#x548c;&#x53c2;&#x6570;&#x9ad8;&#x6548;&#x66f4;&#x65b0;&#x7b56;&#x7565;&#x7684;&#x6709;&#x6548;&#x6027;&#xff1b;4) &#x65b9;&#x6cd5;&#x4ec5;&#x9700;&#x66f4;&#x65b0;&#x6781;&#x5c11;&#x91cf;&#x53c2;&#x6570;&#xff0c;&#x8ba1;&#x7b97;&#x6548;&#x7387;&#x663e;&#x8457;&#x9ad8;&#x4e8e;&#x5fae;&#x8c03;&#x6574;&#x4e2a;&#x6a21;&#x578b;&#x6216;&#x8de8;&#x6a21;&#x6001;&#x6295;&#x5f71;&#x5c42;&#xff08;&#x53c2;&#x6570;&#x91cf;&#x51cf;&#x5c11;&#x7ea6;100&#x500d;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1039,1040"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x5de5;&#x4f5c;&#x8bc1;&#x660e;&#x4e86;&#x901a;&#x8fc7;&#x8f7b;&#x91cf;&#x7ea7;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#x5728;&#x6d4b;&#x8bd5;&#x65f6;&#x52a8;&#x6001;&#x7f13;&#x89e3;VLMs&#x5e7b;&#x89c9;&#x7684;&#x53ef;&#x884c;&#x6027;&#xff0c;&#x4ec5;&#x66f4;&#x65b0;&#x6781;&#x5c11;&#x91cf;&#x53c2;&#x6570;&#x5373;&#x53ef;&#x663e;&#x8457;&#x63d0;&#x5347;&#x8f93;&#x51fa;&#x53ef;&#x9760;&#x6027;&#x3002;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x9ad8;&#x6548;&#x6027;&#x3001;&#x901a;&#x7528;&#x6027;&#x548c;&#x4f4e;&#x8d44;&#x6e90;&#x9700;&#x6c42;&#xff0c;&#x4e3a;VLMs&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#x548c;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x7684;&#x7f13;&#x89e3;&#xff0c;&#x5e76;&#x63a2;&#x7d22;&#x66f4;&#x590d;&#x6742;&#x7684;&#x5956;&#x52b1;&#x673a;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"1040,1042"}}],"payload":{"tag":"li","lines":"1036,1042","fold":1}}],"payload":{"tag":"h4","lines":"1034,1035"}}],"payload":{"tag":"h3","lines":"1032,1033","fold":1}},{"content":"&#x56e0;&#x679c;&#x5206;&#x6790;","children":[{"content":"CAUSALMM: Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CAUSALMM&#x7684;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#x8bf1;&#x5bfc;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x7ed3;&#x6784;&#x56e0;&#x679c;&#x5efa;&#x6a21;&#x548c;&#x540e;&#x95e8;&#x8c03;&#x6574;&#xff0c;&#x5728;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x6ce8;&#x610f;&#x529b;&#x5c42;&#x9762;&#x8fdb;&#x884c;&#x53cd;&#x4e8b;&#x5b9e;&#x63a8;&#x7406;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x6743;&#x91cd;&#x5373;&#x53ef;&#x5373;&#x63d2;&#x5373;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1045,1046"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x7f16;&#x7801;&#x5668;&#x4e2d;&#x5b58;&#x5728;&#x56fa;&#x6709;&#x7684;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#x504f;&#x5dee;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x5148;&#x9a8c;&#x548c;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff09;&#xff0c;&#x8fd9;&#x4e9b;&#x504f;&#x5dee;&#x901a;&#x8fc7;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5f71;&#x54cd;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x7684;&#x5bf9;&#x9f50;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x8f93;&#x51fa;&#x4e0e;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#xff09;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x4f9d;&#x8d56;&#x7edf;&#x8ba1;&#x76f8;&#x5173;&#x6027;&#xff0c;&#x5ffd;&#x7565;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x4e0e;&#x8f93;&#x51fa;&#x4e4b;&#x95f4;&#x7684;&#x56e0;&#x679c;&#x5173;&#x7cfb;&#xff0c;&#x56e0;&#x6b64;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1047,1048"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x6784;&#x5efa;&#x4e86;&#x4e00;&#x4e2a;&#x7ed3;&#x6784;&#x56e0;&#x679c;&#x6a21;&#x578b;&#xff08;SCM&#xff09;&#xff0c;&#x5c06;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#xff08;&#x89c6;&#x89c9;&#x5148;&#x9a8c;Pv&#x548c;&#x8bed;&#x8a00;&#x5148;&#x9a8c;Pl&#xff09;&#x89c6;&#x4e3a;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff08;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;Ai&#x548c;&#x8bed;&#x8a00;&#x6ce8;&#x610f;&#x529b;At&#xff09;&#x4e0e;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#xff08;O&#xff09;&#x4e4b;&#x95f4;&#x7684;&#x6df7;&#x6dc6;&#x56e0;&#x5b50;&#x3002;&#x901a;&#x8fc7;&#x540e;&#x95e8;&#x8c03;&#x6574;&#x548c;&#x53cd;&#x4e8b;&#x5b9e;&#x63a8;&#x7406;&#xff0c;&#x5bf9;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x6ce8;&#x610f;&#x529b;&#x8fdb;&#x884c;&#x5e72;&#x9884;&#xff08;&#x5982;&#x968f;&#x673a;&#x6ce8;&#x610f;&#x529b;&#x3001;&#x5747;&#x5300;&#x6ce8;&#x610f;&#x529b;&#x3001;&#x53cd;&#x8f6c;&#x6ce8;&#x610f;&#x529b;&#x548c;&#x6253;&#x4e71;&#x6ce8;&#x610f;&#x529b;&#xff09;&#xff0c;&#x751f;&#x6210;&#x53cd;&#x4e8b;&#x5b9e;&#x6ce8;&#x610f;&#x529b;&#x72b6;&#x6001;&#xff08;A*_i&#x548c;A*_t&#xff09;&#xff0c;&#x4ee5;&#x91cf;&#x5316;&#x6ce8;&#x610f;&#x529b;&#x5bf9;&#x8f93;&#x51fa;&#x7684;&#x56e0;&#x679c;&#x6548;&#x5e94;&#x3002;&#x5177;&#x4f53;&#x5305;&#x62ec;&#x4e09;&#x79cd;&#x63a8;&#x7406;&#x6a21;&#x5f0f;&#xff1a;&#x4ec5;&#x89c6;&#x89c9;&#x5e72;&#x9884;&#x3001;&#x4ec5;&#x8bed;&#x8a00;&#x5e72;&#x9884;&#x4ee5;&#x53ca;&#x591a;&#x6a21;&#x6001;&#x534f;&#x540c;&#x5e72;&#x9884;&#x3002;","children":[],"payload":{"tag":"li","lines":"1048,1049"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;CAUSALMM&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff1a;&#x5728;VLind-Bench&#x7684;6&#x4e2a;&#x6307;&#x6807;&#x4e0a;&#x6700;&#x9ad8;&#x63d0;&#x5347;65.3%&#xff08;&#x539f;&#x6587;&#x4ea6;&#x63d0;&#x53ca;143.7&#x5206;&#x6539;&#x8fdb;&#xff09;&#xff0c;&#x5728;MME Benchmark&#x4e0a;&#x63d0;&#x5347;164&#x5206;&#xff0c;&#x5728;POPE&#x7684;&#x4e09;&#x4e2a;&#x57fa;&#x51c6;&#x4e0a;&#x5e73;&#x5747;&#x63d0;&#x5347;5.37%&#x3002;&#x8fd9;&#x4e9b;&#x7ed3;&#x679c;&#x9a8c;&#x8bc1;&#x4e86;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#x504f;&#x5dee;&#xff0c;&#x589e;&#x5f3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5373;&#x63d2;&#x5373;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1049,1050"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;CAUSALMM&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x63ed;&#x793a;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x4e0e;&#x8f93;&#x51fa;&#x7684;&#x56e0;&#x679c;&#x5173;&#x7cfb;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x6a21;&#x6001;&#x5148;&#x9a8c;&#x5f15;&#x8d77;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#xff1a;&#x4e3a;MLLMs&#x7684;&#x504f;&#x5dee;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x53ef;&#x89e3;&#x91ca;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff1b;&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x65b9;&#x6cd5;&#xff0c;&#x53ef;&#x4e0e;&#x73b0;&#x6709;&#x6280;&#x672f;&#x7ed3;&#x5408;&#x4f7f;&#x7528;&#xff1b;&#x63a8;&#x52a8;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x5728;&#x591a;&#x6a21;&#x6001;&#x5b66;&#x4e60;&#x4e2d;&#x7684;&#x66f4;&#x5e7f;&#x6cdb;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1050,1052"}}],"payload":{"tag":"li","lines":"1046,1052","fold":1}}],"payload":{"tag":"h4","lines":"1044,1045"}},{"content":"WhoBrings the Frisbee: Probing Hidden Hallucination Factors in Large Vision-Language Model via Causality Analysis","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x56e0;&#x679c;&#x5206;&#x6790;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x63a2;&#x6d4b;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#xff08;&#x751f;&#x6210;&#x4e0d;&#x5b58;&#x5728;&#x89c6;&#x89c9;&#x5143;&#x7d20;&#xff09;&#x7684;&#x9690;&#x85cf;&#x56e0;&#x7d20;&#xff0c;&#x5982;&#x7269;&#x4f53;&#x3001;&#x4e0a;&#x4e0b;&#x6587;&#x548c;&#x8bed;&#x4e49;&#x7ed3;&#x6784;&#x3002;&#x901a;&#x8fc7;&#x5e72;&#x9884;&#x56fe;&#x50cf;&#x3001;&#x6587;&#x672c;&#x548c;&#x5d4c;&#x5165;&#x8868;&#x793a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1053,1054"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x6216;&#x5b8c;&#x5168;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5143;&#x7d20;&#xff08;&#x5982;&#x865a;&#x6784;&#x7684;&#x98de;&#x76d8;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#xff0c;&#x4f46;&#x5f53;&#x524d;&#x7814;&#x7a76;&#x5bf9;&#x5176;&#x80cc;&#x540e;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x673a;&#x5236;&#x7406;&#x89e3;&#x4e0d;&#x8db3;&#xff0c;&#x5c24;&#x5176;&#x662f;&#x9690;&#x85cf;&#x7684;&#x4e0a;&#x4e0b;&#x6587;&#x56e0;&#x7d20;&#xff08;&#x5982;&#x8349;&#x5730;&#x3001;&#x5929;&#x7a7a;&#x7b49;&#xff09;&#x5982;&#x4f55;&#x8bf1;&#x53d1;&#x5e7b;&#x89c9;&#x5c1a;&#x672a;&#x88ab;&#x7cfb;&#x7edf;&#x63a2;&#x7d22;&#x3002;","children":[],"payload":{"tag":"li","lines":"1055,1056"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x56e0;&#x679c;&#x5173;&#x7cfb;&#x7684;&#x5e7b;&#x89c9;&#x63a2;&#x6d4b;&#x7cfb;&#x7edf;&#xff08;hallucination probing system&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6784;&#x5efa;&#x7ed3;&#x6784;&#x56e0;&#x679c;&#x6a21;&#x578b;&#xff08;SCM&#xff09;&#x5206;&#x6790;&#x56fe;&#x50cf;&#x3001;&#x6587;&#x672c;&#x63d0;&#x793a;&#x548c;&#x7f51;&#x7edc;&#x663e;&#x8457;&#x6027;&#x4e4b;&#x95f4;&#x7684;&#x56e0;&#x679c;&#x5173;&#x7cfb;&#xff0c;&#x5e76;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x5b9e;&#x65bd;&#x5e72;&#x9884;&#xff08;intervention&#xff09;&#x6765;&#x963b;&#x65ad;&#x9690;&#x85cf;&#x56e0;&#x7d20;&#x3002;&#x5177;&#x4f53;&#x5305;&#x62ec;&#x4e09;&#x79cd;&#x5e72;&#x9884;&#x65b9;&#x5f0f;&#xff1a;1&#xff09;&#x56fe;&#x50cf;&#x5e72;&#x9884;&#xff08;&#x4fee;&#x6539;&#x56fe;&#x50cf;&#x80cc;&#x666f;&#x6216;&#x4e0a;&#x4e0b;&#x6587;&#xff09;&#xff0c;2&#xff09;&#x6587;&#x672c;&#x5e72;&#x9884;&#xff08;&#x8c03;&#x6574;&#x8f93;&#x5165;&#x63d0;&#x793a;&#xff09;&#xff0c;3&#xff09;&#x5d4c;&#x5165;&#x5e72;&#x9884;&#xff08;&#x4fee;&#x6539;&#x7f51;&#x7edc;&#x5185;&#x90e8;&#x7684;&#x6f5c;&#x5728;&#x5d4c;&#x5165;&#x8868;&#x793a;&#xff09;&#x3002;&#x5b9e;&#x9a8c;&#x4f7f;&#x7528;AMBER&#x548c;COCO&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5e76;&#x5728;InstructBLIP&#x548c;mPLUG-Owl2&#x7b49;&#x4e3b;&#x6d41;LVLM&#x4e0a;&#x9a8c;&#x8bc1;&#x3002;","children":[],"payload":{"tag":"li","lines":"1056,1057"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x53d1;&#x73b0;&#x4e86;&#x5e7b;&#x89c9;&#x8bcd;&#x4e0e;&#x8bf1;&#x53d1;&#x8bcd;&#xff08;&#x5982;&#x201c;&#x8349;&#x5730;&#x201d;&#x8bf1;&#x53d1;&#x201c;&#x98de;&#x76d8;&#x201d;&#xff09;&#x4e4b;&#x95f4;&#x7684;&#x5f3a;&#x5173;&#x8054;&#x6027;&#xff1b;2&#xff09;&#x9891;&#x7e41;&#x5171;&#x73b0;&#x7684;&#x5e7b;&#x89c9;&#x8bcd;&#x5bf9;&#xff08;&#x5982;&#x201c;&#x4eba;&#x548c;&#x676f;&#x5b50;&#x201d;&#xff09;&#x5177;&#x6709;&#x8bed;&#x6cd5;&#x5408;&#x7406;&#x6027;&#xff0c;&#x6613;&#x88ab;&#x6a21;&#x578b;&#x540c;&#x65f6;&#x751f;&#x6210;&#xff1b;3&#xff09;&#x901a;&#x8fc7;&#x7b80;&#x5355;&#x7684;&#x56e0;&#x679c;&#x5e72;&#x9884;&#xff08;&#x5982;&#x4fee;&#x6539;&#x56fe;&#x50cf;&#x80cc;&#x666f;&#x6216;&#x8c03;&#x6574;&#x6587;&#x672c;&#x63d0;&#x793a;&#xff09;&#xff0c;&#x53ef;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;&#xff08;&#x5982;CHAIR&#x3001;HAL&#x7b49;&#x6307;&#x6807;&#x6539;&#x5584;&#xff09;&#xff1b;4&#xff09;&#x7f51;&#x7edc;&#x5185;&#x90e8;&#x7684;&#x5d4c;&#x5165;&#x8868;&#x793a;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x9ad8;&#x5e7b;&#x89c9;&#x6837;&#x672c;&#x4e0e;&#x6b63;&#x5e38;&#x6837;&#x672c;&#x5728;&#x6f5c;&#x5728;&#x7a7a;&#x95f4;&#x4e2d;&#x5b58;&#x5728;&#x53ef;&#x533a;&#x5206;&#x7684;&#x663e;&#x8457;&#x6027;&#x6a21;&#x5f0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1057,1058"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff1a;LVLM&#x7684;&#x5e7b;&#x89c9;&#x7531;&#x9690;&#x85cf;&#x7684;&#x4e0a;&#x4e0b;&#x6587;&#x548c;&#x8bed;&#x4e49;&#x56e0;&#x7d20;&#x56e0;&#x679c;&#x9a71;&#x52a8;&#xff0c;&#x800c;&#x975e;&#x968f;&#x673a;&#x9519;&#x8bef;&#x3002;&#x901a;&#x8fc7;&#x56e0;&#x679c;&#x5e72;&#x9884;&#x53ef;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6216;&#x6a21;&#x578b;&#x5fae;&#x8c03;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#xff1a;&#x4e3a;&#x7406;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x884c;&#x4e3a;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x4e3a;&#x540e;&#x7eed;&#x7684;&#x6a21;&#x578b;&#x6821;&#x51c6;&#x3001;&#x5d4c;&#x5165;&#x7f16;&#x8f91;&#x548c;&#x8f7b;&#x91cf;&#x7ea7;&#x9664;&#x5e7b;&#x6280;&#x672f;&#x5f00;&#x8f9f;&#x4e86;&#x9053;&#x8def;&#xff0c;&#x6709;&#x671b;&#x63d0;&#x5347;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1058,1060"}}],"payload":{"tag":"li","lines":"1054,1060","fold":1}}],"payload":{"tag":"h4","lines":"1052,1053"}},{"content":"See or Guess: Counterfactually Regularized Image Captioning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x53cd;&#x4e8b;&#x5b9e;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x751f;&#x6210;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x603b;&#x6548;&#x5e94;&#xff08;TE&#xff09;&#x548c;&#x81ea;&#x7136;&#x76f4;&#x63a5;&#x6548;&#x5e94;&#xff08;NDE&#xff09;&#x4e24;&#x79cd;&#x53d8;&#x4f53;&#xff0c;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x5bf9;&#x56fe;&#x50cf;&#x7684;&#x771f;&#x5b9e;&#x6027;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x4e0d;&#x540c;&#x89c4;&#x6a21;&#x7684;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1061,1062"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x751f;&#x6210;&#x6a21;&#x578b;&#x5728;&#x6b63;&#x5e38;&#x56fe;&#x50cf;&#x4e0a;&#x8868;&#x73b0;&#x826f;&#x597d;&#xff0c;&#x4f46;&#x5728;&#x56fe;&#x50cf;&#x90e8;&#x5206;&#x88ab;&#x906e;&#x6321;&#x6216;&#x7f16;&#x8f91;&#x7684;&#x53cd;&#x4e8b;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x4e0d;&#x5b58;&#x5728;&#x5bf9;&#x8c61;&#xff09;&#xff0c;&#x4f9d;&#x8d56;&#x6570;&#x636e;&#x4e2d;&#x7684;&#x5173;&#x8054;&#x6a21;&#x5f0f;&#x800c;&#x975e;&#x771f;&#x5b9e;&#x7406;&#x89e3;&#x3002;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x548c;&#x5728;&#x5206;&#x5e03;&#x5916;&#x573a;&#x666f;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#xff0c;&#x800c;&#x4eba;&#x7c7b;&#x5374;&#x80fd;&#x8f7b;&#x677e;&#x5904;&#x7406;&#x6b64;&#x7c7b;&#x60c5;&#x51b5;&#x3002;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1063,1064"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x5c06;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x5efa;&#x6a21;&#x4e3a;&#x5e8f;&#x5217;&#x56e0;&#x679c;&#x56fe;&#xff08;&#x56fe;&#x50cf;&#x2192;&#x524d;&#x9762;&#x8bcd;&#x2192;&#x5f53;&#x524d;&#x8bcd;&#xff09;&#xff0c;&#x5229;&#x7528;&#x53cd;&#x4e8b;&#x5b9e;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x533a;&#x5206;&#x8bcd;&#x751f;&#x6210;&#x7684;&#x76f4;&#x63a5;&#x548c;&#x95f4;&#x63a5;&#x8def;&#x5f84;&#x3002;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#xff1a;1&#xff09;&#x603b;&#x6548;&#x5e94;&#xff08;TE&#xff09;&#xff1a;&#x76f4;&#x63a5;&#x5bf9;&#x6bd4;&#x4e8b;&#x5b9e;&#x56fe;&#x50cf;&#x548c;&#x53cd;&#x4e8b;&#x5b9e;&#x56fe;&#x50cf;&#xff08;&#x5982;&#x906e;&#x6321;&#x90e8;&#x5206;&#x533a;&#x57df;&#xff09;&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x5dee;&#x5f02;&#xff1b;2&#xff09;&#x81ea;&#x7136;&#x76f4;&#x63a5;&#x6548;&#x5e94;&#xff08;NDE&#xff09;&#xff1a;&#x56fa;&#x5b9a;&#x524d;&#x9762;&#x8bcd;&#x4e3a;&#x53cd;&#x4e8b;&#x5b9e;&#x72b6;&#x6001;&#xff0c;&#x4ec5;&#x6539;&#x53d8;&#x56fe;&#x50cf;&#x4ee5;&#x8861;&#x91cf;&#x56fe;&#x50cf;&#x5bf9;&#x8bcd;&#x7684;&#x76f4;&#x63a5;&#x56e0;&#x679c;&#x5f71;&#x54cd;&#x3002;&#x901a;&#x8fc7;&#x6b63;&#x5219;&#x5316;&#x8bad;&#x7ec3;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x4e0e;&#x6587;&#x672c;&#x7684;&#x76f4;&#x63a5;&#x5bf9;&#x5e94;&#xff0c;&#x51cf;&#x5c11;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x4f9d;&#x8d56;&#x3002;","children":[],"payload":{"tag":"li","lines":"1064,1065"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5982;COCO&#xff09;&#x548c;&#x6a21;&#x578b;&#xff08;BLIP&#x3001;BLIP2&#x3001;ClipCap&#xff09;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#xff08;&#x5982;CHAIR&#x6307;&#x6807;&#x63d0;&#x5347;&#xff09;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x63cf;&#x8ff0;&#x5bf9;&#x56fe;&#x50cf;&#x7684;&#x5fe0;&#x5b9e;&#x5ea6;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x6539;&#x52a8;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x5373;&#x53ef;&#x5b9e;&#x73b0;&#x9ad8;&#x53ef;&#x79fb;&#x690d;&#x6027;&#x3002;&#x53cd;&#x4e8b;&#x5b9e;&#x5e72;&#x9884;&#x4f7f;&#x6a21;&#x578b;&#x5728;&#x906e;&#x6321;&#x56fe;&#x50cf;&#x4e0a;&#x751f;&#x6210;&#x66f4;&#x51c6;&#x786e;&#x7684;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1065,1066"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x4f7f;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x6a21;&#x578b;&#x66f4;&#x63a5;&#x8fd1;&#x4eba;&#x7c7b;&#x63a8;&#x7406;&#x65b9;&#x5f0f;&#xff0c;&#x63d0;&#x5347;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x548c;&#x9c81;&#x68d2;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x53cd;&#x4e8b;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x672a;&#x6765;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#xff0c;&#x4fc3;&#x8fdb;&#x53ef;&#x9760;AI&#x7cfb;&#x7edf;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1066,1068"}}],"payload":{"tag":"li","lines":"1062,1068","fold":1}}],"payload":{"tag":"h4","lines":"1060,1061"}},{"content":"Treble Counterfactual VLMs: A Causal Approach to Hallucination","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x7684;&#x65b0;&#x65b9;&#x6cd5;Treble Counterfactual VLMs&#xff0c;&#x901a;&#x8fc7;&#x53cd;&#x4e8b;&#x5b9e;&#x5206;&#x6790;&#x548c;&#x81ea;&#x7136;&#x76f4;&#x63a5;&#x6548;&#x5e94;&#xff08;NDE&#xff09;&#x4f30;&#x8ba1;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5bf9;&#x5355;&#x6a21;&#x6001;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1069,1070"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x3001;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x6216;&#x63d0;&#x793a;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x7b49;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x4ece;&#x7edf;&#x8ba1;&#x504f;&#x5dee;&#x3001;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7b49;&#x89d2;&#x5ea6;&#x5206;&#x6790;&#xff0c;&#x4f46;&#x7f3a;&#x4e4f;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x7ed3;&#x6784;&#x4e2d;&#x56e0;&#x679c;&#x5173;&#x7cfb;&#x7684;&#x7cfb;&#x7edf;&#x6027;&#x7406;&#x89e3;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x7ed3;&#x6784;&#x5316;&#x56e0;&#x679c;&#x65b9;&#x6cd5;&#x4ece;&#x6839;&#x672c;&#x4e0a;&#x8bca;&#x65ad;&#x548c;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1071,1072"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e09;&#x6b65;&#x9aa4;&#x65b9;&#x6cd5;&#xff1a;1) &#x6784;&#x5efa;&#x7ed3;&#x6784;&#x56e0;&#x679c;&#x56fe;&#xff08;SCG&#xff09;&#xff0c;&#x533a;&#x5206;&#x6b63;&#x786e;&#x7684;&#x591a;&#x6a21;&#x6001;&#x878d;&#x5408;&#x8def;&#x5f84;&#x4e0e;&#x865a;&#x5047;&#x7684;&#x5355;&#x6a21;&#x6001;&#x6377;&#x5f84;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x6216;&#x6587;&#x672c;&#x76f4;&#x63a5;&#x5f71;&#x54cd;&#x8f93;&#x51fa;&#xff09;&#xff1b;2) &#x4f7f;&#x7528;&#x53cd;&#x4e8b;&#x5b9e;&#x5206;&#x6790;&#x4f30;&#x8ba1;&#x89c6;&#x89c9;&#x3001;&#x6587;&#x672c;&#x53ca;&#x8de8;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x7684;&#x81ea;&#x7136;&#x76f4;&#x63a5;&#x6548;&#x5e94;&#xff08;NDE&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x6270;&#x52a8;&#x56fe;&#x50cf;&#x8868;&#x793a;&#xff08;&#x968f;&#x673a;&#x63a9;&#x7801;&#xff09;&#x3001;&#x751f;&#x6210;&#x5e7b;&#x89c9;&#x6587;&#x672c;&#x5d4c;&#x5165;&#xff08;&#x7528;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#xff09;&#x4ee5;&#x53ca;&#x964d;&#x89e3;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#xff08;&#x5982;&#x7a7a;&#x4fe1;&#x53f7;&#xff09;&#x6765;&#x91cf;&#x5316;&#x5404;&#x6a21;&#x6001;&#x7684;&#x72ec;&#x7acb;&#x5f71;&#x54cd;&#xff1b;3) &#x8bbe;&#x8ba1;&#x52a8;&#x6001;&#x6d4b;&#x8bd5;&#x65f6;&#x5e72;&#x9884;&#x6a21;&#x5757;&#xff0c;&#x901a;&#x8fc7;PCA&#x63d0;&#x53d6;&#x4e3b;&#x8981;&#x6a21;&#x6001;&#x5f71;&#x54cd;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x5b9e;&#x65f6;&#x8c03;&#x6574;&#x6a21;&#x578b;&#x5bf9;&#x5404;&#x6a21;&#x6001;&#x7684;&#x4f9d;&#x8d56;&#x6743;&#x91cd;&#xff0c;&#x786e;&#x4fdd;&#x8f93;&#x51fa;&#x7531;&#x591a;&#x6a21;&#x6001;&#x878d;&#x5408;&#x4e3b;&#x5bfc;&#x3002;","children":[],"payload":{"tag":"li","lines":"1072,1073"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;VQA&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#xff09;&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff1a;&#x4f8b;&#x5982;&#xff0c;&#x5728;&#x6848;&#x4f8b;&#x5bf9;&#x6bd4;&#x4e2d;&#xff0c;&#x539f;&#x59cb;LLaVA 1.5&#x6a21;&#x578b;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x201c;&#x53a8;&#x623f;&#x4e2d;&#x6709;&#x4e24;&#x4eba;&#x5403;&#x996d;&#x201d;&#xff08;&#x5b9e;&#x9645;&#x65e0;&#x4eba;&#xff09;&#xff0c;&#x800c;&#x4f7f;&#x7528;&#x8be5;&#x65b9;&#x6cd5;&#x540e;&#x8f93;&#x51fa;&#x6b63;&#x786e;&#x63cf;&#x8ff0;&#x201c;&#x65e0;&#x4eba;&#x5403;&#x996d;&#x201d;&#xff1b;&#x53e6;&#x4e00;&#x6848;&#x4f8b;&#x4e2d;&#xff0c;&#x6a21;&#x578b;&#x6b63;&#x786e;&#x8bc6;&#x522b;&#x201c;&#x4e2d;&#x95f4;&#x732b;&#x5f20;&#x5634;&#x201d;&#x800c;&#x975e;&#x9519;&#x8bef;&#x7b54;&#x6848;&#x201c;&#x53f3;&#x4fa7;&#x732b;&#x5f20;&#x5634;&#x201d;&#x3002;&#x7ed3;&#x679c;&#x9a8c;&#x8bc1;&#x4e86;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x4fdd;&#x6301;&#x4efb;&#x52a1;&#x6027;&#x80fd;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x63d0;&#x5347;&#x8f93;&#x51fa;&#x4e0e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1073,1074"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff0c;&#x5e7b;&#x89c9;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x6216;&#x6587;&#x672c;&#x6a21;&#x6001;&#x7684;&#x610f;&#x5916;&#x76f4;&#x63a5;&#x5f71;&#x54cd;&#xff0c;&#x800c;&#x975e;&#x591a;&#x6a21;&#x6001;&#x878d;&#x5408;&#x672c;&#x8eab;&#xff1b;&#x901a;&#x8fc7;&#x56e0;&#x679c;&#x63a8;&#x7406;&#x548c;&#x53cd;&#x4e8b;&#x5b9e;&#x5e72;&#x9884;&#xff0c;&#x53ef;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;VLM&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x89e3;&#x91ca;&#x3001;&#x9c81;&#x68d2;&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x6846;&#x67b6;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x56e0;&#x679c;&#x5206;&#x6790;&#x8303;&#x5f0f;&#xff0c;&#x5e76;&#x4fc3;&#x8fdb;&#x5176;&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1074,1076"}}],"payload":{"tag":"li","lines":"1070,1076","fold":1}}],"payload":{"tag":"h4","lines":"1068,1069"}}],"payload":{"tag":"h3","lines":"1042,1043","fold":1}},{"content":"&#x6ce8;&#x610f;&#x529b;&#x67b6;&#x6784;","children":[{"content":"CCA: Mitigating Object Hallucination via Concentric Causal Attention","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e0e;&#x65cb;&#x8f6c;&#x4f4d;&#x7f6e;&#x7f16;&#x7801;&#xff08;RoPE&#xff09;&#x7684;&#x957f;&#x7a0b;&#x8870;&#x51cf;&#x7279;&#x6027;&#x5bc6;&#x5207;&#x76f8;&#x5173;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x540c;&#x5fc3;&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;&#xff08;CCA&#xff09;&#x7684;&#x4f4d;&#x7f6e;&#x5bf9;&#x9f50;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x91cd;&#x65b0;&#x7ec4;&#x7ec7;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x987a;&#x5e8f;&#x5e76;&#x4fee;&#x6b63;&#x56e0;&#x679c;&#x63a9;&#x7801;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x89c6;&#x89c9;&#x4e0e;&#x6307;&#x4ee4;&#x4ee4;&#x724c;&#x95f4;&#x7684;&#x76f8;&#x5bf9;&#x8ddd;&#x79bb;&#xff0c;&#x4ece;&#x800c;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x8d85;&#x8d8a;&#x4e86;&#x73b0;&#x6709;&#x6280;&#x672f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1079,1080"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x4e0e;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x963b;&#x788d;&#x4e86;&#x5176;&#x90e8;&#x7f72;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x6216;&#x4f9d;&#x8d56;&#x6602;&#x8d35;&#x7684;&#x6570;&#x636e;&#x6807;&#x6ce8;&#xff0c;&#x6216;&#x5bfc;&#x81f4;&#x63a8;&#x7406;&#x6548;&#x7387;&#x4e0b;&#x964d;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x6839;&#x672c;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1081,1082"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x540c;&#x5fc3;&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;&#xff08;CCA&#xff09;&#x6846;&#x67b6;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;1&#xff09;&#x4f4d;&#x7f6e;&#x91cd;&#x7ec4;&#x6a21;&#x5757;&#xff1a;&#x5c06;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x5e8f;&#x5217;&#x987a;&#x5e8f;&#x4ece;&#x4f20;&#x7edf;&#x7684;&#x6805;&#x683c;&#x626b;&#x63cf;&#x987a;&#x5e8f;&#xff08;&#x5de6;&#x4e0a;&#x5230;&#x53f3;&#x4e0b;&#xff09;&#x6539;&#x4e3a;&#x4ece;&#x56fe;&#x50cf;&#x5916;&#x56f4;&#x5230;&#x4e2d;&#x5fc3;&#x7684;&#x540c;&#x5fc3;&#x5706;&#x987a;&#x5e8f;&#xff0c;&#x4ece;&#x800c;&#x663e;&#x8457;&#x7f29;&#x77ed;&#x4e86;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x4e0e;&#x540e;&#x7eed;&#x6307;&#x4ee4;&#x4ee4;&#x724c;&#x4e4b;&#x95f4;&#x7684;&#x5e73;&#x5747;&#x76f8;&#x5bf9;&#x8ddd;&#x79bb;&#x3002;2&#xff09;&#x56e0;&#x679c;&#x63a9;&#x7801;&#x4fee;&#x6b63;&#x6a21;&#x5757;&#xff1a;&#x4e3a;&#x4e86;&#x9002;&#x5e94;2D&#x56fe;&#x50cf;&#x7684;&#x7a7a;&#x95f4;&#x5c40;&#x90e8;&#x6027;&#xff0c;&#x5bf9;&#x4f20;&#x7edf;&#x7684;1D&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;&#x63a9;&#x7801;&#x8fdb;&#x884c;&#x4e86;&#x4fee;&#x6b63;&#xff0c;&#x4ee5;&#x66f4;&#x597d;&#x5730;&#x5efa;&#x6a21;2D&#x8fde;&#x7eed;&#x4f4d;&#x7f6e;&#x4f9d;&#x8d56;&#x5173;&#x7cfb;&#x3002;&#x8fd9;&#x79cd;&#x65b9;&#x6cd5;&#x81ea;&#x7136;&#x5730;&#x7f13;&#x89e3;&#x4e86;RoPE&#x957f;&#x7a0b;&#x8870;&#x51cf;&#x5bf9;&#x89c6;&#x89c9;-&#x6307;&#x4ee4;&#x4ea4;&#x4e92;&#x7684;&#x8d1f;&#x9762;&#x5f71;&#x54cd;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x7684;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x6216;&#x590d;&#x6742;&#x7684;&#x540e;&#x5904;&#x7406;&#x3002;","children":[],"payload":{"tag":"li","lines":"1082,1083"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5982;&#x4e0b;&#xff1a;1&#xff09;&#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;POPE&#x4e0a;&#xff0c;CCA&#x65b9;&#x6cd5;&#x5728;&#x51c6;&#x786e;&#x7387;&#xff08;Accuracy&#xff09;&#x4e0a;&#x6bd4;&#x4e4b;&#x524d;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x65b9;&#x6cd5;&#x63d0;&#x5347;&#x4e86;4.24%&#xff0c;&#x5728;F1&#x5206;&#x6570;&#x4e0a;&#x63d0;&#x5347;&#x4e86;2.73%&#x3002;2&#xff09;&#x521d;&#x6b65;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x53e6;&#x5916;6&#x4e2a;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;VQA&#x7b49;&#xff09;&#x4e0a;&#x4e5f;&#x4e00;&#x81f4;&#x6027;&#x5730;&#x8d85;&#x8d8a;&#x4e86;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x4e0d;&#x4ec5;&#x80fd;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x80fd;&#x666e;&#x904d;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x3002;3&#xff09;&#x5206;&#x6790;&#x5b9e;&#x9a8c;&#x8bc1;&#x5b9e;&#x4e86;RoPE&#x7684;&#x957f;&#x7a0b;&#x8870;&#x51cf;&#x786e;&#x5b9e;&#x662f;&#x5bfc;&#x81f4;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x56e0;&#x7d20;&#xff0c;&#x800c;CCA&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1083,1084"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x65cb;&#x8f6c;&#x4f4d;&#x7f6e;&#x7f16;&#x7801;&#xff08;RoPE&#xff09;&#x7684;&#x957f;&#x7a0b;&#x8870;&#x51cf;&#x7279;&#x6027;&#x662f;&#x5bfc;&#x81f4;LVLM&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x4e00;&#x4e2a;&#x6839;&#x672c;&#x539f;&#x56e0;&#x3002;&#x6240;&#x63d0;&#x51fa;&#x7684;&#x540c;&#x5fc3;&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;&#xff08;CCA&#xff09;&#x901a;&#x8fc7;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x800c;&#x6709;&#x6548;&#x7684;&#x4f4d;&#x7f6e;&#x5bf9;&#x9f50;&#x7b56;&#x7565;&#xff0c;&#x4ece;&#x6839;&#x672c;&#x4e0a;&#x7f13;&#x89e3;&#x4e86;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#xff1a;1&#xff09;&#x4e3a;&#x7406;&#x89e3;&#x548c;&#x89e3;&#x51b3;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x7684;&#x89c6;&#x89d2;&#xff08;&#x4ece;&#x4f4d;&#x7f6e;&#x7f16;&#x7801;&#x5165;&#x624b;&#xff09;&#x3002;2&#xff09;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6807;&#x6ce8;&#x6216;&#x727a;&#x7272;&#x63a8;&#x7406;&#x6548;&#x7387;&#x7684;&#x9ad8;&#x6548;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;3&#xff09;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x51cf;&#x8f7b;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x666e;&#x904d;&#x589e;&#x5f3a;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x591a;&#x6a21;&#x6001;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x5b9e;&#x7528;&#x7684;LVLM&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"1084,1086"}}],"payload":{"tag":"li","lines":"1080,1086","fold":1}}],"payload":{"tag":"h4","lines":"1078,1079"}},{"content":"MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;MCA-LLaVA&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;&#x66fc;&#x54c8;&#x987f;&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x6539;&#x8fdb;&#x65cb;&#x8f6c;&#x4f4d;&#x7f6e;&#x7f16;&#x7801;(RoPE)&#x7684;&#x957f;&#x671f;&#x8870;&#x51cf;&#x95ee;&#x9898;&#xff0c;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1087,1088"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x591a;&#x6a21;&#x6001;&#x7279;&#x5f81;&#x9519;&#x4f4d;&#x662f;&#x4e3b;&#x8981;&#x539f;&#x56e0;&#xff0c;&#x800c;RoPE&#x4f4d;&#x7f6e;&#x7f16;&#x7801;&#x7684;&#x957f;&#x671f;&#x8870;&#x51cf;&#x7279;&#x6027;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x4e0d;&#x540c;&#x4f4d;&#x7f6e;token&#x7684;&#x611f;&#x77e5;&#x4e0d;&#x5747;&#xff08;&#x5c24;&#x5176;&#x504f;&#x5411;&#x53f3;&#x4e0b;&#x533a;&#x57df;&#xff09;&#xff0c;&#x9020;&#x6210;&#x56fe;&#x50cf;-&#x6307;&#x4ee4;&#x4ea4;&#x4e92;&#x4e0d;&#x8db3;&#x548c;&#x9519;&#x4f4d;&#xff0c;&#x8fd9;&#x79cd;&apos;&#x56fe;&#x50cf;&#x5bf9;&#x9f50;&#x504f;&#x5dee;&apos;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1089,1090"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x66fc;&#x54c8;&#x987f;&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;(MCA)&#x673a;&#x5236;&#xff1a;1. &#x7528;&#x66fc;&#x54c8;&#x987f;&#x8ddd;&#x79bb;&#x8ba1;&#x7b97;token&#x95f4;&#x4e8c;&#x7ef4;&#x7a7a;&#x95f4;&#x76f8;&#x5bf9;&#x8ddd;&#x79bb;&#xff0c;&#x5c06;RoPE&#x7684;&#x4e00;&#x7ef4;&#x8870;&#x51cf;&#x6269;&#x5c55;&#x4e3a;&#x4e8c;&#x7ef4;&#x591a;&#x65b9;&#x5411;&#x7a7a;&#x95f4;&#x8870;&#x51cf;&#xff1b;2. &#x91cd;&#x65b0;&#x5206;&#x914d;&#x56fe;&#x50cf;token&#x7684;&#x4e8c;&#x7ef4;&#x5750;&#x6807;&#x4ee5;&#x9002;&#x914d;&#x66fc;&#x54c8;&#x987f;&#x8ddd;&#x79bb;&#x8ba1;&#x7b97;&#xff0c;&#x66ff;&#x6362;&#x539f;&#x6805;&#x683c;&#x626b;&#x63cf;&#x4f4d;&#x7f6e;&#x7d22;&#x5f15;&#xff1b;3. &#x901a;&#x8fc7;&#x66fc;&#x54c8;&#x987f;&#x56e0;&#x679c;&#x63a9;&#x7801;&#x6a21;&#x5757;&#x4fdd;&#x7559;&#x4e8c;&#x7ef4;&#x7a7a;&#x95f4;&#x5b9a;&#x4f4d;&#x7279;&#x6027;&#xff0c;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x5404;&#x4f4d;&#x7f6e;token&#x7684;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1090,1091"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;&#x5728;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;POPE&#x4e0a;F1&#x5206;&#x6570;&#x63d0;&#x5347;6.7%&#xff0c;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x5347;6.7%&#xff1b;&#x5728;CHAIR&#x4e0a;&#x53e5;&#x5b50;&#x7ea7;&#x548c;&#x5b9e;&#x4f8b;&#x7ea7;&#x5e7b;&#x89c9;&#x5206;&#x522b;&#x51cf;&#x5c11;9%&#x548c;2.9%&#xff1b;&#x5728;MME&#x3001;SQA&#x7b49;&#x901a;&#x7528;&#x4efb;&#x52a1;&#x4e2d;&#x4e5f;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4e14;&#x5728;&#x4e0d;&#x540c;LVLMs&#x4e0a;&#x5747;&#x5e26;&#x6765;&#x4e00;&#x81f4;&#x6539;&#x8fdb;&#x3002;","children":[],"payload":{"tag":"li","lines":"1091,1092"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: RoPE&#x7684;&#x957f;&#x671f;&#x8870;&#x51cf;&#x4f1a;&#x5bfc;&#x81f4;&#x591a;&#x6a21;&#x6001;&#x9519;&#x4f4d;&#x548c;&#x5e7b;&#x89c9;&#xff0c;MCA&#x673a;&#x5236;&#x901a;&#x8fc7;&#x4e8c;&#x7ef4;&#x7a7a;&#x95f4;&#x8870;&#x51cf;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x56fe;&#x50cf;&#x5bf9;&#x9f50;&#x504f;&#x5dee;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;LVLMs&#x4e2d;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x4f4d;&#x7f6e;&#x5efa;&#x6a21;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1092,1094"}}],"payload":{"tag":"li","lines":"1088,1094","fold":1}}],"payload":{"tag":"h4","lines":"1086,1087"}},{"content":"Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;LACING&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x591a;&#x6a21;&#x6001;&#x53cc;&#x91cd;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff08;MDA&#xff09;&#x548c;&#x8f6f;&#x56fe;&#x50cf;&#x5f15;&#x5bfc;&#xff08;SIG&#xff09;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x8bed;&#x8a00;&#x504f;&#x89c1;&#x95ee;&#x9898;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x80fd;&#x529b;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"1095,1096"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5b58;&#x5728;&#x8bed;&#x8a00;&#x504f;&#x89c1;&#x95ee;&#x9898;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x8f93;&#x5165;&#x800c;&#x5ffd;&#x7565;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x7684;&#x5e7b;&#x89c9;&#x56de;&#x7b54;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#x5b83;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;LVLM&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8f85;&#x52a9;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#x504f;&#x89c1;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x4e24;&#x4e2a;&#x539f;&#x56e0;&#xff1a;1&#xff09;LLM&#x9884;&#x8bad;&#x7ec3;&#x4e0e;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x9636;&#x6bb5;&#x7684;&#x6570;&#x636e;&#x89c4;&#x6a21;&#x5dee;&#x5f02;&#xff1b;2&#xff09;&#x6587;&#x672c;&#x6570;&#x636e;&#x7684;&#x77ed;&#x671f;&#x4f9d;&#x8d56;&#x7279;&#x6027;&#x5bfc;&#x81f4;&#x63a8;&#x7406;&#x504f;&#x5dee;&#x3002;","children":[],"payload":{"tag":"li","lines":"1097,1098"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x7cfb;&#x7edf;&#x6027;&#x6846;&#x67b6;LACING&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1. &#x591a;&#x6a21;&#x6001;&#x53cc;&#x91cd;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff08;MDA&#xff09;&#xff1a;&#x5728;&#x8bad;&#x7ec3;&#x548c;&#x63a8;&#x7406;&#x4e2d;&#x91c7;&#x7528;&#x5e76;&#x884c;&#x53cc;&#x6ce8;&#x610f;&#x529b;&#x7ed3;&#x6784;&#xff0c;&#x5206;&#x522b;&#x5904;&#x7406;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x8f93;&#x5165;&#xff0c;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x5728;&#x6240;&#x6709;&#x5c42;&#x4e2d;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x6587;&#x672c;&#x7684;&#x56e0;&#x679c;&#x6ce8;&#x610f;&#x529b;&#x3002;2. &#x8f6f;&#x56fe;&#x50cf;&#x5f15;&#x5bfc;&#xff08;SIG&#xff09;&#xff1a;&#x5f15;&#x5165;&#x53ef;&#x5b66;&#x4e60;&#x7684;&#x8f6f;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff08;soft visual prompt&#xff09;&#x5728;&#x8bad;&#x7ec3;&#x65f6;&#x968f;&#x673a;&#x66ff;&#x6362;&#x90e8;&#x5206;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#xff0c;&#x63a8;&#x7406;&#x65f6;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x8f93;&#x5165;&#x548c;&#x4ec5;&#x542b;&#x8f6f;&#x63d0;&#x793a;&#x7684;&#x201c;&#x591a;&#x6a21;&#x6001;&#x7a7a;&#x8f93;&#x5165;&#x201d;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x51cf;&#x5c11;&#x5bf9;&#x6587;&#x672c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x3002;","children":[],"payload":{"tag":"li","lines":"1098,1099"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x8bed;&#x8a00;&#x504f;&#x89c1;&#xff1a;&#x5728;LLaVA-Bench&#x4e0a;&#x63d0;&#x5347;11.8&#x4e2a;&#x767e;&#x5206;&#x70b9;&#xff0c;&#x5728;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#xff08;Object Hallucinations Benchmark&#xff09;&#x4e0a;&#x6539;&#x5584;40%&#x3002;&#x6a21;&#x578b;&#x5728;&#x6240;&#x6709;&#x5c42;&#x5747;&#x4fdd;&#x6301;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x6709;&#x6548;&#x5173;&#x6ce8;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x8bad;&#x7ec3;&#x8d44;&#x6e90;&#x5373;&#x53ef;&#x5b9e;&#x73b0;&#x4e00;&#x81f4;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"1099,1100"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: LACING&#x901a;&#x8fc7;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x4ece;&#x8bad;&#x7ec3;&#x548c;&#x63a8;&#x7406;&#x89d2;&#x5ea6;&#x89e3;&#x51b3;&#x8bed;&#x8a00;&#x504f;&#x89c1;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x5e76;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x7684;&#x901a;&#x7528;&#x6027;&#x548c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8d44;&#x6e90;&#x7684;&#x7279;&#x6027;&#xff0c;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;LVLM&#x4e2d;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"1100,1102"}}],"payload":{"tag":"li","lines":"1096,1102","fold":1}}],"payload":{"tag":"h4","lines":"1094,1095"}},{"content":"Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;FarSight&#xff0c;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x4f18;&#x5316;&#x56e0;&#x679c;&#x63a9;&#x7801;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x56fe;&#x50cf;&#x548c;&#x89c6;&#x9891;&#x4efb;&#x52a1;&#x4e0a;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1103,1104"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x5353;&#x8d8a;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x865a;&#x5047;&#x63cf;&#x8ff0;&#x3002;&#x5e7b;&#x89c9;&#x5206;&#x4e3a;&#x4e24;&#x7c7b;&#xff1a;&#x521d;&#x59cb;&#x5e7b;&#x89c9;&#xff08;&#x56e0;&#x4fe1;&#x606f;&#x4e0d;&#x8db3;&#x800c;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#xff09;&#x548c;&#x96ea;&#x7403;&#x5e7b;&#x89c9;&#xff08;&#x4e3a;&#x7ef4;&#x6301;&#x4e00;&#x81f4;&#x6027;&#x800c;&#x5ef6;&#x7eed;&#x9519;&#x8bef;&#xff09;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x963b;&#x788d;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x68c0;&#x7d22;&#x6216;&#x5fae;&#x8c03;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x672a;&#x6df1;&#x5165;&#x5206;&#x6790;token&#x4ea4;&#x4e92;&#x8fc7;&#x7a0b;&#x7684;&#x6839;&#x672c;&#x539f;&#x56e0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1105,1106"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;FarSight&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x662f;&#x901a;&#x8fc7;&#x4f18;&#x5316;&#x56e0;&#x679c;&#x63a9;&#x7801;&#xff08;causal mask&#xff09;&#x6765;&#x5e72;&#x9884;token&#x95f4;&#x7684;&#x4fe1;&#x606f;&#x4f20;&#x64ad;&#xff1a;1. <strong>&#x6ce8;&#x610f;&#x529b;&#x5bc4;&#x5b58;&#x5668;&#xff08;Attention Register&#xff09;</strong>&#xff1a;&#x5728;&#x56e0;&#x679c;&#x63a9;&#x7801;&#x7684;&#x4e0a;&#x4e09;&#x89d2;&#x77e9;&#x9635;&#x4e2d;&#x5f15;&#x5165;&#x5bc4;&#x5b58;&#x5668;&#x7ed3;&#x6784;&#xff0c;&#x52a8;&#x6001;&#x6355;&#x83b7;&#x5e76;&#x91cd;&#x65b0;&#x5206;&#x914d;&#x88ab;&#x5f02;&#x5e38;token&#xff08;&#x5982;&#x80cc;&#x666f;&#x3001;&#x6807;&#x70b9;&#xff09;&#x5206;&#x6563;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x786e;&#x4fdd;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;token&#x83b7;&#x5f97;&#x8db3;&#x591f;&#x5173;&#x6ce8;&#x3002;2. <strong>&#x4f4d;&#x7f6e;&#x611f;&#x77e5;&#x7f16;&#x7801;&#xff08;Positional Awareness Encoding&#xff09;</strong>&#xff1a;&#x9488;&#x5bf9;RoPE&#x7f16;&#x7801;&#x5728;&#x957f;&#x5e8f;&#x5217;&#x4e2d;&#x4f4d;&#x7f6e;&#x4fe1;&#x606f;&#x8870;&#x51cf;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x8bbe;&#x8ba1;&#x9012;&#x51cf;&#x63a9;&#x7801;&#x7387;&#x673a;&#x5236;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x80fd;&#x5173;&#x6ce8;&#x66f4;&#x8fdc;&#x8ddd;&#x79bb;&#x7684;&#x5148;&#x524d;token&#xff08;&#x5c24;&#x5176;&#x5bf9;&#x89c6;&#x9891;&#x5e8f;&#x5217;&#x91cd;&#x8981;&#xff09;&#xff0c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;-&#x6587;&#x672c;token&#x7684;&#x4ea4;&#x4e92;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x6216;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x4f5c;&#x4e3a;&#x89e3;&#x7801;&#x63d2;&#x4ef6;&#x4f7f;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1106,1107"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x591a;&#x4e2a;&#x56fe;&#x50cf;&#xff08;&#x5982;LLaVA-1.5-7B&#xff09;&#x548c;&#x89c6;&#x9891;&#xff08;&#x5982;Video-LLaVA-7B&#xff09;&#x6a21;&#x578b;&#x53ca;&#x57fa;&#x51c6;&#xff08;CHAIR&#x3001;MMBench&#x3001;PoPE&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#xff1a;1. FarSight&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x521d;&#x59cb;&#x5e7b;&#x89c9;&#x548c;&#x96ea;&#x7403;&#x5e7b;&#x89c9;&#x7684;&#x6bd4;&#x4f8b;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x89c6;&#x9891;&#x4efb;&#x52a1;&#x4e2d;&#x96ea;&#x7403;&#x5e7b;&#x89c9;&#x6539;&#x5584;&#x660e;&#x663e;&#xff08;&#x89c1;&#x56fe;2&#xff09;&#x3002;2. &#x6ce8;&#x610f;&#x529b;&#x53ef;&#x89c6;&#x5316;&#xff08;&#x56fe;3&#xff09;&#x663e;&#x793a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x51cf;&#x5c11;&#x4e86;&#x5f02;&#x5e38;token&#x7684;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#xff0c;&#x7f13;&#x89e3;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x584c;&#x9677;&#x548c;&#x4f4d;&#x7f6e;&#x4fe1;&#x606f;&#x8870;&#x51cf;&#x95ee;&#x9898;&#x3002;3. &#x76f8;&#x6bd4;&#x5176;&#x4ed6;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff08;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff09;&#xff0c;FarSight&#x5728;&#x4fdd;&#x6301;&#x751f;&#x6210;&#x6548;&#x7387;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x54cd;&#x5e94;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1107,1108"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff1a;1. MLLMs&#x7684;&#x5e7b;&#x89c9;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;token&#x4ea4;&#x4e92;&#x4e2d;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x584c;&#x9677;&#x548c;&#x4f4d;&#x7f6e;&#x4fe1;&#x606f;&#x8870;&#x51cf;&#x3002;2. FarSight&#x901a;&#x8fc7;&#x4f18;&#x5316;&#x56e0;&#x679c;&#x63a9;&#x7801;&#xff0c;&#x6709;&#x6548;&#x589e;&#x5f3a;&#x4e86;&#x591a;&#x6a21;&#x6001;token&#x95f4;&#x7684;&#x4fe1;&#x606f;&#x4f20;&#x64ad;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;3. &#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x5b83;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#x3001;&#x9ad8;&#x6548;&#x6027;&#x548c;&#x6613;&#x90e8;&#x7f72;&#x4f18;&#x52bf;&#xff0c;&#x4e3a;&#x672a;&#x6765;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63a8;&#x52a8;&#x6a21;&#x578b;&#x5728;&#x5b89;&#x5168;&#x654f;&#x611f;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1108,1110"}}],"payload":{"tag":"li","lines":"1104,1110","fold":1}}],"payload":{"tag":"h4","lines":"1102,1103"}}],"payload":{"tag":"h3","lines":"1076,1077","fold":1}},{"content":"&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;","children":[{"content":"Through the Magnifying Glass: Adaptive Perception Magnification for Hallucination-Free VLM Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x611f;&#x77e5;&#x653e;&#x5927;&#x5668;&#xff08;PM&#xff09;&#x7684;&#x65b0;&#x9896;&#x89c6;&#x89c9;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x5730;&#x57fa;&#x4e8e;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5b9a;&#x4f4d;&#x5e76;&#x653e;&#x5927;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x533a;&#x57df;&#xff0c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5bf9;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x4e0d;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x7684;&#x524d;&#x63d0;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x5e76;&#x751f;&#x6210;&#x66f4;&#x51c6;&#x786e;&#x7684;&#x54cd;&#x5e94;&#x3002;","children":[],"payload":{"tag":"li","lines":"1113,1114"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x73b0;&#x6709;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff0c;&#x5373;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x4e0d;&#x51c6;&#x786e;&#x5185;&#x5bb9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6216;&#x89c6;&#x89c9;&#x8868;&#x793a;&#x80fd;&#x529b;&#x6709;&#x9650;&#xff0c;&#x5bfc;&#x81f4;&#x65e0;&#x6cd5;&#x5145;&#x5206;&#x6355;&#x6349;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#x3002;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;VLM&#x8f93;&#x51fa;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x771f;&#x5b9e;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5173;&#x952e;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x4f7f;&#x7528;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x5fae;&#x8c03;&#x5373;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1115,1116"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x611f;&#x77e5;&#x653e;&#x5927;&#x5668;&#xff08;PM&#xff09;&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x68af;&#x5ea6;&#x7684;&#x89e3;&#x7801;&#x7aef;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x6d41;&#x7a0b;&#x5305;&#x62ec;&#xff1a;1) <strong>&#x611f;&#x77e5;&#x56fe;&#x6784;&#x5efa;</strong>&#xff1a;&#x5728;&#x6bcf;&#x4e00;&#x6b65;&#x89e3;&#x7801;&#x65f6;&#xff0c;&#x805a;&#x5408;VLM&#x4e2d;&#x95f4;&#x5c42;&#x81f3;&#x6df1;&#x5c42;&#xff08;l&#x2265;L&#xff09;&#x7684;&#x591a;&#x4e2a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x901a;&#x8fc7;&#x6700;&#x5927;&#x6c60;&#x5316;&#x751f;&#x6210;&#x4e00;&#x4e2a;token&#x7ea7;&#x522b;&#x7684;&#x70ed;&#x529b;&#x56fe;&#xff08;H&#xff09;&#xff0c;&#x4ee5;&#x7a81;&#x51fa;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x7684;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x533a;&#x57df;&#x3002;2) <strong>&#x8fed;&#x4ee3;&#x7cbe;&#x5316;</strong>&#xff1a;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x5730;&#x5c4f;&#x853d;&#x9ad8;&#x6ce8;&#x610f;&#x529b;token&#x5e76;&#x91cd;&#x65b0;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff0c;&#x6536;&#x96c6;&#x591a;&#x6b21;&#x7684;&#x70ed;&#x529b;&#x56fe;&#x5e76;&#x5408;&#x5e76;&#xff08;H&#x2217;&#xff09;&#xff0c;&#x4ee5;&#x63d0;&#x5347;&#x611f;&#x77e5;&#x56fe;&#x7684;&#x8986;&#x76d6;&#x5ea6;&#x548c;&#x5e73;&#x6ed1;&#x6027;&#x3002;3) <strong>&#x540e;&#x5904;&#x7406;&#x4e0e;&#x4e0a;&#x91c7;&#x6837;</strong>&#xff1a;&#x5bf9;&#x5408;&#x5e76;&#x540e;&#x7684;&#x70ed;&#x529b;&#x56fe;&#x8fdb;&#x884c;&#x540e;&#x5904;&#x7406;&#xff08;&#x5982;&#x5747;&#x503c;&#x6ee4;&#x6ce2;&#xff09;&#xff0c;&#x5e76;&#x4e0a;&#x91c7;&#x6837;&#x81f3;&#x50cf;&#x7d20;&#x7ea7;&#x522b;&#xff0c;&#x5f62;&#x6210;&#x6700;&#x7ec8;&#x7684;&#x611f;&#x77e5;&#x56fe;&#xff08;P&#xff09;&#x3002;4) <strong>&#x81ea;&#x9002;&#x5e94;&#x653e;&#x5927;</strong>&#xff1a;&#x6839;&#x636e;&#x611f;&#x77e5;&#x56fe;P&#xff0c;&#x5bf9;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x8fdb;&#x884c;&#x91cd;&#x91c7;&#x6837;&#xff08;&#x4f7f;&#x7528;&#x7c7b;&#x4f3c;&#x7a7a;&#x95f4;&#x53d8;&#x6362;&#x5668;&#x7f51;&#x7edc;&#x7684;&#x6280;&#x672f;&#xff09;&#xff0c;&#x653e;&#x5927;&#x5173;&#x952e;&#x533a;&#x57df;&#x7684;&#x540c;&#x65f6;&#x538b;&#x7f29;&#x6b21;&#x8981;&#x533a;&#x57df;&#xff0c;&#x4fdd;&#x7559;&#x6574;&#x4f53;&#x7a7a;&#x95f4;&#x7ed3;&#x6784;&#x548c;&#x4e0a;&#x4e0b;&#x6587;&#x4fe1;&#x606f;&#xff0c;&#x751f;&#x6210;&#x7ec6;&#x5316;&#x540e;&#x7684;&#x56fe;&#x50cf;&#x4f5c;&#x4e3a;&#x4e0b;&#x4e00;&#x6b65;&#x89e3;&#x7801;&#x7684;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x3002;","children":[],"payload":{"tag":"li","lines":"1116,1117"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1) <strong>&#x5e7b;&#x89c9;&#x7f13;&#x89e3;</strong>&#xff1a;&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;PM&#x5728;&#x95ee;&#x7b54;&#x548c;&#x5f00;&#x653e;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x4f18;&#x5f02;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x6027;&#x80fd;&#xff0c;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x7684;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b49;&#x65b9;&#x6cd5;&#x3002;2) <strong>&#x751f;&#x6210;&#x8d28;&#x91cf;&#x63d0;&#x5347;</strong>&#xff1a;PM&#x4e0d;&#x4ec5;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x589e;&#x5f3a;&#x4e86;&#x8bed;&#x8a00;&#x751f;&#x6210;&#x7684;&#x6574;&#x4f53;&#x8d28;&#x91cf;&#xff0c;&#x751f;&#x6210;&#x7684;&#x53cd;&#x5e94;&#x66f4;&#x51c6;&#x786e;&#x3001;&#x66f4;&#x5fe0;&#x5b9e;&#x4e8e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x3002;3) <strong>&#x63a8;&#x7406;&#x80fd;&#x529b;&#x4fdd;&#x7559;</strong>&#xff1a;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63d0;&#x5347;&#x89c6;&#x89c9;&#x5fe0;&#x5b9e;&#x5ea6;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x5b8c;&#x5168;&#x4fdd;&#x7559;&#x4e86;VLM&#x539f;&#x6709;&#x7684;&#x5f3a;&#x5927;&#x63a8;&#x7406;&#x80fd;&#x529b;&#xff0c;&#x672a;&#x5bf9;&#x6027;&#x80fd;&#x4ea7;&#x751f;&#x8d1f;&#x9762;&#x5f71;&#x54cd;&#x3002;4) <strong>&#x6d88;&#x878d;&#x5b9e;&#x9a8c;</strong>&#xff1a;&#x6d88;&#x878d;&#x7814;&#x7a76;&#x8bc1;&#x5b9e;&#x4e86;&#x8fed;&#x4ee3;&#x7cbe;&#x5316;&#x3001;&#x4e2d;&#x95f4;&#x5c42;&#x6ce8;&#x610f;&#x529b;&#x805a;&#x5408;&#x7b49;&#x5173;&#x952e;&#x7ec4;&#x4ef6;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1117,1118"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;PM&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x901a;&#x7528;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x80fd;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x589e;&#x5f3a;VLM&#x5bf9;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#x7684;&#x611f;&#x77e5;&#xff0c;&#x4ece;&#x6839;&#x672c;&#x4e0a;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x5728;&#x4e8e;&#xff1a;1) <strong>&#x65e0;&#x9700;&#x5fae;&#x8c03;</strong>&#xff1a;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x5de5;&#x4f5c;&#xff0c;&#x65e0;&#x9700;&#x6602;&#x8d35;&#x7684;&#x6a21;&#x578b;&#x5fae;&#x8c03;&#x6216;&#x989d;&#x5916;&#x6570;&#x636e;&#x3002;2) <strong>&#x4fdd;&#x6301;&#x5b8c;&#x6574;&#x6027;</strong>&#xff1a;&#x901a;&#x8fc7;&#x538b;&#x7f29;&#x800c;&#x975e;&#x4e22;&#x5f03;&#x6b21;&#x8981;&#x533a;&#x57df;&#xff0c;&#x4fdd;&#x6301;&#x4e86;&#x56fe;&#x50cf;&#x7684;&#x7a7a;&#x95f4;&#x7ed3;&#x6784;&#x548c;&#x6a21;&#x578b;&#x7684;&#x6574;&#x4f53;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;3) <strong>&#x5e7f;&#x6cdb;&#x9002;&#x7528;&#x6027;</strong>&#xff1a;&#x9002;&#x7528;&#x4e8e;&#x4e3b;&#x6d41;&#x91c7;&#x7528;&#x7edf;&#x4e00;&#x89c6;&#x89c9;&#x91c7;&#x6837;&#x7684;&#x4ea4;&#x7ec7;&#x578b;VLM&#x3002;4) <strong>&#x6f5c;&#x5728;&#x5f71;&#x54cd;</strong>&#xff1a;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x53ef;&#x4fe1;&#x7684;VLM&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x6709;&#x671b;&#x63a8;&#x52a8;&#x5176;&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1118,1120"}}],"payload":{"tag":"li","lines":"1114,1120","fold":1}}],"payload":{"tag":"h4","lines":"1112,1113"}},{"content":"Attention Reallocation: Towards Zero-cost and Controllable Hallucination Mitigation of MLLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;AttnReal&#x7684;&#x96f6;&#x6210;&#x672c;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x91cd;&#x65b0;&#x5206;&#x914d;&#x6ce8;&#x610f;&#x529b;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6291;&#x5236;&#x5386;&#x53f2;&#x8f93;&#x51fa;&#x4ee4;&#x724c;&#x7684;&#x8fc7;&#x5ea6;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x5e76;&#x5c06;&#x5176;&#x91cd;&#x65b0;&#x5206;&#x914d;&#x7ed9;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x4e0d;&#x589e;&#x52a0;&#x989d;&#x5916;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x5b9e;&#x73b0;&#x53ef;&#x63a7;&#x7684;&#x6027;&#x80fd;&#x6743;&#x8861;&#x3002;","children":[],"payload":{"tag":"li","lines":"1121,1122"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x8f93;&#x51fa;&#x4e0e;&#x8f93;&#x5165;&#x4fe1;&#x606f;&#x4e0d;&#x7b26;&#xff09;&#xff0c;&#x8fd9;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8f85;&#x52a9;&#x7b49;&#x5b89;&#x5168;&#x654f;&#x611f;&#x573a;&#x666f;&#x4e2d;&#x5177;&#x6709;&#x4e25;&#x91cd;&#x98ce;&#x9669;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x6216;&#x5f15;&#x5165;&#x9ad8;&#x5f00;&#x9500;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x96f6;&#x6210;&#x672c;&#x3001;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x4e14;&#x4e0d;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x5ef6;&#x8fdf;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1123,1124"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x6ce8;&#x610f;&#x529b;&#x91cd;&#x5206;&#x914d;&#xff08;AttnReal&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x57fa;&#x4e8e;&#x4e24;&#x4e2a;&#x5173;&#x952e;&#x89c2;&#x5bdf;&#xff1a;1&#xff09;MLLM&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x914d;&#x4e0d;&#x5408;&#x7406;&#xff0c;&#x5386;&#x53f2;&#x8f93;&#x51fa;&#x4ee4;&#x724c;&#x8fc7;&#x5ea6;&#x4e3b;&#x5bfc;&#x7279;&#x5f81;&#xff1b;2&#xff09;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x4e0e;&#x6587;&#x672c;&#x4ee4;&#x724c;&#x7684;&#x7279;&#x5f81;&#x5206;&#x5e03;&#x5b58;&#x5728;&#x663e;&#x8457;&#x5dee;&#x8ddd;&#x3002;AttnReal&#x901a;&#x8fc7;&#x68c0;&#x6d4b;&#x5e76;&#x6291;&#x5236;&#x5386;&#x53f2;&#x8f93;&#x51fa;&#x4ee4;&#x724c;&#x4e2d;&#x7684;&#x201c;&#x6ce8;&#x610f;&#x529b;&#x6c89;&#x6ca1;&#x201d;&#xff08;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x7684;&#x4ee4;&#x724c;&#xff09;&#xff0c;&#x5e76;&#x5c06;&#x8fd9;&#x4e9b;&#x6ce8;&#x610f;&#x529b;&#x5747;&#x5300;&#x91cd;&#x65b0;&#x5206;&#x914d;&#x7ed9;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x5355;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x4e2d;&#x5b9e;&#x73b0;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x89e3;&#x7801;&#x6b65;&#x9aa4;&#x6216;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1124,1125"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x516d;&#x4e2a;&#x5f00;&#x6e90;MLLM&#x548c;&#x4e09;&#x79cd;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x4e0a;&#x9a8c;&#x8bc1;&#x4e86;AttnReal&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;&#x7ed3;&#x679c;&#x663e;&#x793a;&#xff1a;1&#xff09;&#x5728;CHAIR&#x7b49;&#x57fa;&#x51c6;&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff08;&#x66f4;&#x4f4e;CHAIR&#x5206;&#x6570;&#xff09;&#xff1b;2&#xff09;&#x5728;&#x4fdd;&#x6301;&#x9ad8;F1&#x5206;&#x6570;&#xff08;&#x6574;&#x4f53;&#x6027;&#x80fd;&#xff09;&#x7684;&#x540c;&#x65f6;&#x5b9e;&#x73b0;&#x66f4;&#x597d;&#x7684;&#x5fe0;&#x5b9e;&#x5ea6;&#xff1b;3&#xff09;&#x4ec5;&#x589e;&#x52a0;&#x7ea6;2%&#x7684;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#xff08;104 vs. 102 GFLOPs/token&#xff09;&#xff0c;&#x8fdc;&#x4f4e;&#x4e8e;&#x5bf9;&#x6bd4;&#x65b9;&#x6cd5;&#xff08;&#x5982;OPERA&#x7684;1036 GFLOPs/token&#xff09;&#xff1b;4&#xff09;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x91cd;&#x5206;&#x914d;&#x5f3a;&#x5ea6;&#xff0c;&#x53ef;&#x5b9e;&#x73b0;&#x5fe0;&#x5b9e;&#x5ea6;&#x4e0e;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x4e4b;&#x95f4;&#x7684;&#x53ef;&#x63a7;&#x6743;&#x8861;&#x3002;","children":[],"payload":{"tag":"li","lines":"1125,1126"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: AttnReal&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x96f6;&#x6210;&#x672c;&#x7684;&#x63d2;&#x4ef6;&#x5f0f;&#x65b9;&#x6cd5;&#xff0c;&#x53ef;&#x5e7f;&#x6cdb;&#x9002;&#x7528;&#x4e8e;&#x4e0d;&#x540c;MLLM&#x548c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x8d21;&#x732e;&#x5728;&#x4e8e;&#x63ed;&#x793a;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x914d;&#x4e0e;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x8054;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x91cd;&#x5206;&#x914d;&#x673a;&#x5236;&#x5b9e;&#x73b0;&#x8fd1;&#x4e4e;&#x514d;&#x8d39;&#x7684;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x3002;&#x672a;&#x6765;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x7684;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#xff0c;&#x63a8;&#x52a8;MLLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x843d;&#x5730;&#x3002;","children":[],"payload":{"tag":"li","lines":"1126,1128"}}],"payload":{"tag":"li","lines":"1122,1128","fold":1}}],"payload":{"tag":"h4","lines":"1120,1121"}},{"content":"EAZY: Eliminating Hallucinations in LVLMs by Zeroing out Hallucinatory Image Tokens","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;EAZY&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8bc6;&#x522b;&#x5e76;&#x5f52;&#x96f6;&#x56fe;&#x50cf;&#x4e2d;&#x4ec5;&#x5360;1.5%&#x7684;&#x5c11;&#x6570;&#x5173;&#x952e;&#x5e7b;&#x89c9;&#x4ee4;&#x724c;&#xff08;HITs&#xff09;&#xff0c;&#x6709;&#x6548;&#x68c0;&#x6d4b;&#x548c;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x6a21;&#x578b;&#x548c;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x9a8c;&#x8bc1;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1129,1130"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;Object Hallucination&#xff09;&#xff0c;&#x5373;&#x8f93;&#x51fa;&#x4e2d;&#x9519;&#x8bef;&#x5305;&#x542b;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x4ece;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x504f;&#x5dee;&#x89d2;&#x5ea6;&#x5206;&#x6790;&#xff0c;&#x4f46;&#x672c;&#x6587;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x4ece;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x89d2;&#x5ea6;&#x63a2;&#x7a76;&#x5e7b;&#x89c9;&#x4ea7;&#x751f;&#x7684;&#x6839;&#x6e90;&#xff0c;&#x65e8;&#x5728;&#x63ed;&#x793a;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x5982;&#x4f55;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x5bfb;&#x6c42;&#x66f4;&#x76f4;&#x63a5;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1131,1132"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x5206;&#x6790;LVLM&#xff08;&#x5982;LLaVA&#xff09;&#x4e2d;&#x95f4;&#x5c42;&#xff08;&#x5982;&#x7b2c;15&#x5c42;&#xff09;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x53d1;&#x73b0;&#x751f;&#x6210;&#x7269;&#x4f53;&#x4ee4;&#x724c;&#x65f6;&#x6ce8;&#x610f;&#x529b;&#x9ad8;&#x5ea6;&#x96c6;&#x4e2d;&#x4e8e;&#x56fe;&#x50cf;&#x4e2d;&#x5bf9;&#x5e94;&#x533a;&#x57df;&#xff08;&#x79f0;&#x4e3a;&#x56fe;&#x50cf;&#x951a;&#x70b9;&#xff09;&#x3002;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x56e0;&#x65e0;&#x771f;&#x5b9e;&#x951a;&#x70b9;&#xff0c;&#x5176;&#x6ce8;&#x610f;&#x529b;&#x4f1a;&#x96c6;&#x4e2d;&#x5728;&#x89c6;&#x89c9;&#x76f8;&#x5173;&#x533a;&#x57df;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;EAZY&#x65b9;&#x6cd5;&#xff1a;1&#xff09;&#x5728;&#x6a21;&#x578b;&#x4e2d;&#x95f4;&#x5c42;&#x8ba1;&#x7b97;&#x751f;&#x6210;&#x7269;&#x4f53;&#x4ee4;&#x724c;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6570;&#xff1b;2&#xff09;&#x8bc6;&#x522b;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6570;&#x6700;&#x9ad8;&#x7684;Top-K&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x4f5c;&#x4e3a;&#x5019;&#x9009;HITs&#xff1b;3&#xff09;&#x901a;&#x8fc7;&#x5f52;&#x96f6;&#xff08;zero-out&#xff09;&#x8fd9;&#x4e9b;HITs&#x6765;&#x6d88;&#x9664;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#xff1b;4&#xff09;&#x5229;&#x7528;&#x5f52;&#x96f6;&#x64cd;&#x4f5c;&#x524d;&#x540e;&#x751f;&#x6210;&#x6587;&#x672c;&#x7684;&#x53d8;&#x5316;&#xff0c;&#x65e0;&#x76d1;&#x7763;&#x68c0;&#x6d4b;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x7269;&#x4f53;&#x662f;&#x5426;&#x6d88;&#x5931;&#xff09;&#xff0c;&#x5e76;&#x8fdb;&#x4e00;&#x6b65;&#x805a;&#x5408;&#x6240;&#x6709;&#x68c0;&#x6d4b;&#x5230;&#x7684;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x7684;HITs&#x8fdb;&#x884c;&#x6574;&#x4f53;&#x7f13;&#x89e3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1132,1133"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x53d1;&#x73b0;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x4ec5;&#x9700;&#x5f52;&#x96f6;1.5%&#x7684;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff08;HITs&#xff09;&#x5373;&#x53ef;&#x6d88;&#x9664;&#x5927;&#x591a;&#x6570;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff1b;2&#xff09;&#x5f52;&#x96f6;&#x64cd;&#x4f5c;&#x5bf9;&#x771f;&#x5b9e;&#x7269;&#x4f53;&#x5f71;&#x54cd;&#x6781;&#x5c0f;&#xff1b;3&#xff09;EAZY&#x5728;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x4efb;&#x52a1;&#x4e0a;&#x8fbe;&#x5230;80%&#x7684;&#x51c6;&#x786e;&#x7387;&#x548c;&#x7cbe;&#x786e;&#x7387;&#xff0c;&#x76f8;&#x6bd4;&#x4e4b;&#x524d;&#x65b9;&#x6cd5;&#x63d0;&#x5347;15%&#xff1b;4&#xff09;&#x5728;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x4efb;&#x52a1;&#x4e0a;&#xff0c;EAZY&#x5728;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x5b9e;&#x7528;&#x6027;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x9002;&#x914d;&#x4e0d;&#x540c;LVLM&#x67b6;&#x6784;&#xff08;&#x5982;LLaVA-1.5&#x3001;LLaVA-Next&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1133,1134"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff0c;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x4e3b;&#x8981;&#x7531;&#x5c11;&#x6570;&#x9ad8;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x9a71;&#x52a8;&#xff0c;&#x5f52;&#x96f6;&#x8fd9;&#x4e9b;&#x4ee4;&#x724c;&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x6709;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;EAZY&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x8bad;&#x7ec3;&#x3001;&#x8f7b;&#x91cf;&#x7ea7;&#x65b9;&#x6cd5;&#xff0c;&#x4e3a;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5176;&#x901a;&#x7528;&#x6027;&#x548c;&#x9ad8;&#x6548;&#x6027;&#x6709;&#x671b;&#x4fc3;&#x8fdb;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x5e94;&#x7528;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x63a2;&#x7d22;HITs&#x7684;&#x66f4;&#x6df1;&#x5c42;&#x673a;&#x5236;&#x53ca;&#x4e0e;&#x5176;&#x4ed6;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x7684;&#x5173;&#x8054;&#x3002;","children":[],"payload":{"tag":"li","lines":"1134,1139"}}],"payload":{"tag":"li","lines":"1130,1139","fold":1}}],"payload":{"tag":"h4","lines":"1128,1129"}},{"content":"VHD: Cracking the Code of Hallucination in LVLMs with Vision-aware Head Divergence","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;VHR&#xff0c;&#x901a;&#x8fc7;&#x8bc6;&#x522b;&#x5e76;&#x5f3a;&#x5316;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x654f;&#x611f;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff08;Vision-aware Heads&#xff09;&#xff0c;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x57fa;&#x4e8e;&#x65b0;&#x6307;&#x6807;VHD&#x91cf;&#x5316;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x5bf9;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x654f;&#x611f;&#x6027;&#xff0c;&#x5b9e;&#x9a8c;&#x8bc1;&#x660e;VHR&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x51e0;&#x4e4e;&#x4e0d;&#x589e;&#x52a0;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"1140,1141"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x5728;&#x751f;&#x6210;&#x540e;&#x9636;&#x6bb5;&#x8fdb;&#x884c;&#x4fee;&#x6b63;&#xff08;&#x5982;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x8c03;&#x6574;&#x6216;&#x540e;&#x5904;&#x7406;&#xff09;&#xff0c;&#x800c;&#x672a;&#x6df1;&#x5165;&#x63a2;&#x7a76;&#x5176;&#x5185;&#x90e8;&#x673a;&#x5236;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x63ed;&#x793a;&#x5e7b;&#x89c9;&#x4ea7;&#x751f;&#x7684;&#x6839;&#x672c;&#x539f;&#x56e0;&#x2014;&#x2014;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x5148;&#x9a8c;&#x8bed;&#x8a00;&#x6a21;&#x5f0f;&#x800c;&#x975e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#xff0c;&#x5e76;&#x636e;&#x6b64;&#x63d0;&#x51fa;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1142,1143"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x5934;&#x5206;&#x6b67;&#xff08;VHD&#xff09;&#x6307;&#x6807;&#xff0c;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x79fb;&#x9664;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x524d;&#x540e;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x8f93;&#x51fa;&#x7684;&#x6b27;&#x6c0f;&#x8ddd;&#x79bb;&#xff0c;&#x6765;&#x91cf;&#x5316;&#x6bcf;&#x4e2a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x654f;&#x611f;&#x6027;&#x3002;&#x57fa;&#x4e8e;VHD&#xff0c;&#x8fdb;&#x4e00;&#x6b65;&#x63d0;&#x51fa;&#x4e86;Token-VHD&#xff08;T-VHD&#xff09;&#x6307;&#x6807;&#xff0c;&#x7528;&#x4e8e;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x6bcf;&#x4e2a;token&#x65f6;&#x5bf9;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x7684;&#x4f9d;&#x8d56;&#x7a0b;&#x5ea6;&#x3002;&#x968f;&#x540e;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x5934;&#x5f3a;&#x5316;&#xff08;VHR&#xff09;&#x65b9;&#x6cd5;&#xff1a;&#x5728;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x52a8;&#x6001;&#x8bc6;&#x522b;&#x6bcf;&#x4e2a;&#x5c42;&#x4e2d;&#x5bf9;&#x89c6;&#x89c9;&#x654f;&#x611f;&#x7684;&#x524d;&#x4e00;&#x534a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff08;&#x57fa;&#x4e8e;VHD&#x5206;&#x6570;&#xff09;&#xff0c;&#x5e76;&#x5c06;&#x5176;&#x8f93;&#x51fa;&#x6309;&#x56e0;&#x5b50;&#x3b1;&#x653e;&#x5927;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x5bf9;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x6291;&#x5236;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4e3b;&#x5bfc;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x4e14;&#x8ba1;&#x7b97;&#x9ad8;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"1143,1144"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1. VHD&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;&#x53ea;&#x6709;&#x5c11;&#x6570;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x9ad8;&#x5ea6;&#x654f;&#x611f;&#xff0c;&#x800c;&#x5927;&#x591a;&#x6570;&#x5934;&#x5219;&#x51e0;&#x4e4e;&#x65e0;&#x53cd;&#x5e94;&#x3002;2. T-VHD&#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x7684;&#x8bcd;&#x8bed;&#x548c;&#x53e5;&#x5b50;&#x901a;&#x5e38;&#x5bf9;&#x5e94;&#x8f83;&#x4f4e;&#x7684;T-VHD&#x503c;&#xff0c;&#x8bc1;&#x5b9e;&#x4e86;&#x8bed;&#x8a00;&#x504f;&#x5dee;&#x4e0e;&#x5e7b;&#x89c9;&#x7684;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#x6027;&#x3002;3. &#x5728;CHAIR&#x7b49;&#x6807;&#x51c6;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;VHR&#x65b9;&#x6cd5;&#x5728;&#x51cf;&#x8f7b;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x7684;&#x6700;&#x5148;&#x8fdb;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x3001;&#x675f;&#x641c;&#x7d22;&#x7b49;&#xff09;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6781;&#x9ad8;&#x7684;&#x6548;&#x7387;&#xff0c;&#x989d;&#x5916;&#x65f6;&#x95f4;&#x5f00;&#x9500;&#x53ef;&#x5ffd;&#x7565;&#x4e0d;&#x8ba1;&#x3002;","children":[],"payload":{"tag":"li","lines":"1144,1145"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLM&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x800c;&#x975e;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x3002;&#x901a;&#x8fc7;&#x63d0;&#x51fa;&#x7684;VHD&#x6307;&#x6807;&#x548c;VHR&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x591f;&#x6709;&#x6548;&#x5730;&#x8bc6;&#x522b;&#x5e76;&#x5f3a;&#x5316;&#x5bf9;&#x89c6;&#x89c9;&#x654f;&#x611f;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x6839;&#x6e90;&#x4e0a;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x9ad8;&#x6548;&#x3001;&#x5b9e;&#x7528;&#x7684;&#x7279;&#x70b9;&#xff0c;&#x4e3a;&#x7406;&#x89e3;&#x548c;&#x6539;&#x8fdb;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x65b9;&#x5411;&#xff0c;&#x5e76;&#x6709;&#x671b;&#x63a8;&#x52a8;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1145,1148"}}],"payload":{"tag":"li","lines":"1141,1148","fold":1}}],"payload":{"tag":"h4","lines":"1139,1140"}},{"content":"llava-fix-hallucination: Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: PAINT&#x662f;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x89c6;&#x89c9;Transformer&#x4e2d;&#x7684;&#x81ea;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x8bc6;&#x522b;&#x51fa;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4f20;&#x9012;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x7684;&#x5c40;&#x90e8;&#x4ee4;&#x724c;&#x548c;&#x6458;&#x8981;&#x4ee4;&#x724c;&#xff0c;&#x5e76;&#x5206;&#x522b;&#x7528;&#x4e0d;&#x540c;&#x7684;&#x589e;&#x5f3a;&#x5e45;&#x5ea6;&#xff08;&#x3b1;&#x548c;&#x3b2;&#xff09;&#x9009;&#x62e9;&#x6027;&#x63d0;&#x5347;&#x5b83;&#x4eec;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x4ece;&#x800c;&#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x8fbe;62.3%&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1149,1150"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x7406;&#x89e3;&#x548c;&#x63cf;&#x8ff0;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x7ecf;&#x5e38;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x7ec6;&#x8282;&#xff0c;&#x5373;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x73b0;&#x8c61;&#x3002;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x9519;&#x8bef;&#xff0c;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5e94;&#x7528;&#x5b89;&#x5168;&#x6027;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x6df1;&#x5165;&#x7406;&#x89e3;&#x5e7b;&#x89c9;&#x4ea7;&#x751f;&#x7684;&#x539f;&#x56e0;&#x5e76;&#x5f00;&#x53d1;&#x6709;&#x6548;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1151,1152"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;PAINT&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x8bc6;&#x522b;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4f20;&#x9012;&#x5230;LLM&#x7684;&#x4e24;&#x7c7b;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff1a;&#x5c40;&#x90e8;&#x4ee4;&#x724c;&#xff08;&#x7f16;&#x7801;&#x56fe;&#x50cf;&#x4e2d;&#x5bf9;&#x8c61;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x4fe1;&#x606f;&#xff09;&#x548c;&#x6458;&#x8981;&#x4ee4;&#x724c;&#xff08;&#x7f16;&#x7801;&#x56fe;&#x50cf;&#x7684;&#x5168;&#x5c40;&#x805a;&#x5408;&#x4fe1;&#x606f;&#xff09;&#x3002;&#x901a;&#x8fc7;&#x5206;&#x6790;ViT&#x4e2d;CLS&#x4ee4;&#x724c;&#x5230;&#x56fe;&#x50cf;&#x5757;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#xff0c;&#x5206;&#x522b;&#x9009;&#x53d6;&#x9876;&#x5c42;&#x548c;&#x5e95;&#x5c42;&#x4e2d;&#x6ce8;&#x610f;&#x529b;&#x6700;&#x9ad8;&#x7684;&#x524d;N%&#x4ee4;&#x724c;&#x4f5c;&#x4e3a;&#x8fd9;&#x4e24;&#x7c7b;&#x4ee4;&#x724c;&#x3002;&#x968f;&#x540e;&#xff0c;&#x5728;LLM&#x7684;&#x81ea;&#x6ce8;&#x610f;&#x529b;&#x5c42;&#x8fdb;&#x884c;&#x5e72;&#x9884;&#xff0c;&#x4f7f;&#x7528;&#x901a;&#x8fc7;&#x5b9e;&#x9a8c;&#x5b66;&#x4e60;&#x5230;&#x7684;&#x5e45;&#x5ea6;&#x3b1;&#x548c;&#x3b2;&#xff0c;&#x5206;&#x522b;&#x9009;&#x62e9;&#x6027;&#x589e;&#x5f3a;&#x5c40;&#x90e8;&#x4ee4;&#x724c;&#x548c;&#x6458;&#x8981;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x800c;&#x4e0d;&#x9700;&#x8981;&#x989d;&#x5916;&#x7684;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1152,1153"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x8bc4;&#x4f30;&#x663e;&#x793a;&#xff0c;PAINT&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff1a;&#x4e0e;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x76f8;&#x6bd4;&#xff0c;&#x53e5;&#x5b50;&#x7ea7;&#x5e7b;&#x89c9;&#x6307;&#x6807;&#xff08;CHAIRS&#xff09;&#x4ece;46.2%&#x964d;&#x81f3;17.6%&#xff08;&#x964d;&#x4f4e;61.9%&#xff09;&#xff0c;&#x5b9e;&#x4f8b;&#x7ea7;&#x5e7b;&#x89c9;&#x6307;&#x6807;&#xff08;CHAIRI&#xff09;&#x4ece;13.8%&#x964d;&#x81f3;4.0%&#xff08;&#x964d;&#x4f4e;71.0%&#xff09;&#xff0c;&#x5927;&#x5e45;&#x4f18;&#x4e8e;&#x5148;&#x524d;&#x65b9;&#x6cd5;PAI&#x3002;&#x540c;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x4fdd;&#x6301;&#x4e86;&#x5408;&#x7406;&#x7684;F1&#x5206;&#x6570;&#xff08;71.8%&#xff09;&#xff0c;&#x8868;&#x660e;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#x6ca1;&#x6709;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x3002;&#x6d88;&#x878d;&#x7814;&#x7a76;&#x786e;&#x5b9a;&#x4e86;&#x3b1;&#x548c;&#x3b2;&#x7684;&#x6700;&#x4f73;&#x53d6;&#x503c;&#xff08;&#x7ea6;0.7&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1153,1154"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLM&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e0e;&#x6df1;&#x5c42;&#x7f51;&#x7edc;&#x4e2d;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x7684;&#x9010;&#x6e10;&#x51cf;&#x5f31;&#x5bc6;&#x5207;&#x76f8;&#x5173;&#xff0c;&#x800c;&#x975e;&#x5747;&#x5300;&#x5730;&#x63d0;&#x5347;&#x6240;&#x6709;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x3002;&#x901a;&#x8fc7;&#x533a;&#x5206;&#x5e76;&#x9009;&#x62e9;&#x6027;&#x589e;&#x5f3a;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4f20;&#x9012;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x7684;&#x5c40;&#x90e8;&#x4ee4;&#x724c;&#x548c;&#x6458;&#x8981;&#x4ee4;&#x724c;&#xff0c;&#x53ef;&#x4ee5;&#x66f4;&#x6709;&#x6548;&#x5730;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;PAINT&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x5373;&#x63d2;&#x5373;&#x7528;&#x65b9;&#x6cd5;&#xff0c;&#x4e3a;&#x6539;&#x8fdb;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#xff0c;&#x7279;&#x522b;&#x662f;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x51c6;&#x786e;&#x6027;&#x7684;&#x9886;&#x57df;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x548c;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"1154,1157"}}],"payload":{"tag":"li","lines":"1150,1157","fold":1}}],"payload":{"tag":"h4","lines":"1148,1149"}},{"content":"AvisC: Don&#x2019;t Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;AVISC&#x7684;&#x6d4b;&#x8bd5;&#x65f6;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x8bc6;&#x522b;&#x5e76;&#x52a8;&#x6001;&#x6821;&#x51c6;&#x6a21;&#x578b;&#x5bf9;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x5757;&#xff08;&#x76f2;&#x4ee4;&#x724c;&#xff09;&#x7684;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#xff0c;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x6216;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#xff0c;&#x5373;&#x53ef;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1158,1159"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x548c;&#x63cf;&#x8ff0;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;hallucinations&#xff09;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x53ef;&#x9760;&#x6027;&#x548c;&#x7cbe;&#x786e;&#x6027;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x4f5c;&#x8005;&#x53d1;&#x73b0;&#xff0c;LVLM&#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x4f1a;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x4e2d;&#x4e00;&#x4e9b;&#x65e0;&#x5173;&#x7684;&#x201c;&#x76f2;&#x4ee4;&#x724c;&#x201d;&#xff08;&#x5982;&#x80cc;&#x666f;&#x6216;&#x91cd;&#x590d;&#x533a;&#x57df;&#xff09;&#xff0c;&#x8fd9;&#x79cd;&#x6ce8;&#x610f;&#x529b;&#x9519;&#x4f4d;&#x662f;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x56e0;&#x7d20;&#x3002;","children":[],"payload":{"tag":"li","lines":"1160,1161"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;AVISC&#xff08;Attentional Vision Calibration&#xff09;&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x6821;&#x51c6;&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x6b65;&#x9aa4;&#xff1a;1) &#x5c42;&#x9009;&#x62e9;&#xff1a;&#x5206;&#x6790;&#x4e0d;&#x540c;&#x5c42;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x9009;&#x62e9;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x5360;&#x6bd4;&#x9ad8;&#x7684;&#x5c42;&#xff1b;2) &#x76f2;&#x4ee4;&#x724c;&#x8bc6;&#x522b;&#xff1a;&#x901a;&#x8fc7;&#x7edf;&#x8ba1;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x57fa;&#x4e8e;&#x5747;&#x503c;&#x548c;&#x6807;&#x51c6;&#x5dee;&#xff09;&#x8bc6;&#x522b;&#x51fa;&#x83b7;&#x5f97;&#x5f02;&#x5e38;&#x9ad8;&#x6ce8;&#x610f;&#x529b;&#x7684;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff1b;3) &#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff1a;&#x91c7;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x6bd4;&#x8f83;&#x4f7f;&#x7528;&#x6240;&#x6709;&#x4ee4;&#x724c;&#x751f;&#x6210;&#x7684;logits&#x4e0e;&#x5c4f;&#x853d;&#x76f2;&#x4ee4;&#x724c;&#x540e;&#x751f;&#x6210;&#x7684;logits&#xff0c;&#x6765;&#x8c03;&#x6574;&#x6700;&#x7ec8;&#x7684;&#x4ee4;&#x724c;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x4ece;&#x800c;&#x524a;&#x5f31;&#x76f2;&#x4ee4;&#x724c;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x589e;&#x5f3a;&#x76f8;&#x5173;&#x4ee4;&#x724c;&#x7684;&#x4f5c;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1161,1162"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x3001;MME&#x548c;AMBER&#x7b49;&#x591a;&#x4e2a;&#x6807;&#x51c6;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;AVISC&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x5b83;&#x5728;&#x4e0d;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#x4e86;&#x7b54;&#x6848;&#x7684;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#xff08;Factual Accuracy&#xff09;&#x548c;&#x63cf;&#x8ff0;&#x7684;&#x4e30;&#x5bcc;&#x6027;&#xff08;Descriptive Richness&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x6a21;&#x578b;&#x65e0;&#x5173;&#x6027;&#xff0c;&#x53ef;&#x4f5c;&#x4e3a;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x5e94;&#x7528;&#x4e8e;&#x73b0;&#x6709;LVLM&#x7cfb;&#x7edf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1162,1163"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLM&#x4e2d;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x9519;&#x4f4d;&#xff08;&#x5373;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x76f2;&#x4ee4;&#x724c;&#xff09;&#x662f;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x7684;&#x4e00;&#x4e2a;&#x91cd;&#x8981;&#x539f;&#x56e0;&#x3002;AVISC&#x901a;&#x8fc7;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x800c;&#x6709;&#x6548;&#x7684;&#x89e3;&#x7801;&#x65f6;&#x6821;&#x51c6;&#x673a;&#x5236;&#xff0c;&#x6210;&#x529f;&#x5730;&#x7f13;&#x89e3;&#x4e86;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9; grounding &#x80fd;&#x529b;&#x3002;&#x8fd9;&#x4e00;&#x5de5;&#x4f5c;&#x4e0d;&#x4ec5;&#x63ed;&#x793a;&#x4e86;LVLM&#x5185;&#x90e8;&#x7684;&#x4e00;&#x4e2a;&#x5173;&#x952e;&#x95ee;&#x9898;&#xff0c;&#x8fd8;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x5b9e;&#x7528;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x63d0;&#x9ad8;LVLM&#x5728;&#x771f;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1163,1165"}}],"payload":{"tag":"li","lines":"1159,1165","fold":1}}],"payload":{"tag":"h4","lines":"1157,1158"}},{"content":"PAI: Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;PAI&#xff0c;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x5e76;&#x51cf;&#x53bb;&#x7eaf;&#x6587;&#x672c;&#x5148;&#x9a8c;&#x7684;logits&#xff0c;&#x6765;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x548c;&#x6587;&#x672c;&#x60ef;&#x6027;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1166,1167"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x548c;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e4b;&#x95f4;&#x5b58;&#x5728;&#x89c4;&#x6a21;&#x5dee;&#x5f02;&#xff0c;&#x5bfc;&#x81f4;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5728;&#x591a;&#x6a21;&#x6001;&#x7406;&#x89e3;&#x4e2d;&#x5360;&#x636e;&#x4e3b;&#x5bfc;&#x5730;&#x4f4d;&#xff0c;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff09;&#x3002;&#x5177;&#x4f53;&#x8868;&#x73b0;&#x4e3a;&#x6a21;&#x578b;&#x5728;&#x6709;&#x6216;&#x65e0;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x65f6;&#x751f;&#x6210;&#x76f8;&#x540c;&#x7684;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#xff0c;&#x79f0;&#x4e3a;&#x201c;&#x6587;&#x672c;&#x60ef;&#x6027;&#x201d;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x963b;&#x788d;&#x5176;&#x5728;&#x5173;&#x952e;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1168,1169"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;PAI&#xff08;Paying Attention to Image&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x64cd;&#x4f5c;&#xff1a;1. <strong>&#x6ce8;&#x610f;&#x529b;&#x589e;&#x5f3a;</strong>&#xff1a;&#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x52a8;&#x6001;&#x653e;&#x5927;&#x81ea;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x4e2d;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6743;&#x91cd;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x66f4;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff1b;2. <strong>Logit&#x51cf;&#x6cd5;</strong>&#xff1a;&#x6784;&#x5efa;&#x7eaf;&#x6587;&#x672c;&#x8f93;&#x5165;&#xff08;&#x6307;&#x4ee4;+&#x5386;&#x53f2;&#x54cd;&#x5e94;&#xff09;&#xff0c;&#x5c06;&#x5176;logits&#x4ece;&#x591a;&#x6a21;&#x6001;&#x8f93;&#x5165;&#x7684;logits&#x4e2d;&#x51cf;&#x53bb;&#xff0c;&#x51cf;&#x5c11;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x504f;&#x5dee;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x4efb;&#x4f55;&#x89e3;&#x7801;&#x65b9;&#x5f0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1169,1170"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;COCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x4f7f;&#x7528;&#x591a;&#x4e2a;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;CHAIR&#x3001;GPT-4V&#x3001;POPE&#x3001;MMHal-Bench&#xff09;&#x9a8c;&#x8bc1;&#x6548;&#x679c;&#xff1a;1. &#x6587;&#x672c;&#x60ef;&#x6027;&#x5e7b;&#x89c9;&#x6bd4;&#x4f8b;&#x663e;&#x8457;&#x4e0b;&#x964d;&#xff1b;2. &#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x5e7b;&#x89c9;&#x9891;&#x7387;&#x5927;&#x5e45;&#x51cf;&#x5c11;&#xff08;&#x5177;&#x4f53;&#x6bd4;&#x4f8b;&#x56e0;&#x6a21;&#x578b;&#x548c;&#x6307;&#x6807;&#x800c;&#x5f02;&#xff09;&#xff1b;3. &#x5728;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#x548c;VQA&#x4efb;&#x52a1;&#x4e2d;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x597d;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5982;&#x56fe;1&#x6240;&#x793a;&#xff0c;PAI&#x7ea0;&#x6b63;&#x4e86;LLaVA-1.5&#x5c06;&#x201c;&#x6ed1;&#x96ea;&#x8bbe;&#x5907;&#x201d;&#x8bef;&#x5224;&#x4e3a;&#x201c;&#x80cc;&#x5305;&#x201d;&#x7684;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1170,1171"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: PAI&#x901a;&#x8fc7;&#x63a8;&#x7406;&#x5e72;&#x9884;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8bc1;&#x660e;&#x6587;&#x672c;&#x60ef;&#x6027;&#x662f;&#x5e7b;&#x89c9;&#x7684;&#x91cd;&#x8981;&#x6210;&#x56e0;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x4f4e;&#xff0c;&#x4e14;&#x9002;&#x7528;&#x4e8e;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x548c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x4e3a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x6548;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x591a;&#x6a21;&#x6001;&#x5e73;&#x8861;&#x673a;&#x5236;&#x53ca;&#x5176;&#x5728;&#x590d;&#x6742;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1171,1173"}}],"payload":{"tag":"li","lines":"1167,1173","fold":1}}],"payload":{"tag":"h4","lines":"1165,1166"}},{"content":"AGLA: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;AGLA&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x5373;&#x63d2;&#x5373;&#x7528;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x7ed3;&#x5408;&#x5168;&#x5c40;&#x548c;&#x5c40;&#x90e8;&#x6ce8;&#x610f;&#x529b;&#x6765;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x548c;&#x5224;&#x522b;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x8868;&#x73b0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1174,1175"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x56fe;&#x50cf;&#x4e2d;&#x5b9e;&#x9645;&#x7269;&#x4f53;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x201c;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#x6839;&#x672c;&#x539f;&#x56e0;&#x5728;&#x4e8e;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x4e0e;&#x63d0;&#x793a;&#x65e0;&#x5173;&#x7684;&#x5168;&#x5c40;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#xff0c;&#x800c;&#x5ffd;&#x89c6;&#x4e86;&#x4e0e;&#x63d0;&#x793a;&#x76f8;&#x5173;&#x7684;&#x5c40;&#x90e8;&#x7279;&#x5f81;&#xff0c;&#x5bfc;&#x81f4;&#x89c6;&#x89c9; grounding &#x80fd;&#x529b;&#x4e0d;&#x8db3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1176,1177"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;AGLA&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#xff1a;1) &#x56fe;&#x50cf;-&#x63d0;&#x793a;&#x5339;&#x914d;&#xff08;IPM&#xff09;&#x6a21;&#x5757;&#xff1a;&#x4f7f;&#x7528;GradCAM&#x6280;&#x672f;&#x8ba1;&#x7b97;&#x56fe;&#x50cf;&#x5757;&#x4e0e;&#x6587;&#x672c;&#x63d0;&#x793a;&#x7684;&#x76f8;&#x5173;&#x6027;&#x5f97;&#x5206;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x81ea;&#x9002;&#x5e94;&#x63a9;&#x7801;&#x7b56;&#x7565;&#x751f;&#x6210;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#xff0c;&#x7a81;&#x51fa;&#x76f8;&#x5173;&#x5c40;&#x90e8;&#x7279;&#x5f81;&#x5e76;&#x6291;&#x5236;&#x65e0;&#x5173;&#x5e72;&#x6270;&#xff1b;2) &#x5168;&#x5c40;&#x4e0e;&#x5c40;&#x90e8;&#x6ce8;&#x610f;&#x529b;&#x96c6;&#x6210;&#xff1a;&#x901a;&#x8fc7;logit&#x878d;&#x5408;&#x5c06;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x7684;&#x5168;&#x5c40;&#x751f;&#x6210;&#x7279;&#x5f81;&#x4e0e;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x7684;&#x5c40;&#x90e8;&#x5224;&#x522b;&#x7279;&#x5f81;&#x7ed3;&#x5408;&#xff0c;&#x751f;&#x6210;&#x6821;&#x51c6;&#x540e;&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x89e3;&#x7801;&#x65f6;&#x540c;&#x65f6;&#x5229;&#x7528;&#x751f;&#x6210;&#x548c;&#x5224;&#x522b;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1177,1178"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;AGLA&#x80fd;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x591a;&#x79cd;LVLM&#xff08;&#x5982;LLaVA&#x3001;InstructBLIP&#xff09;&#x5728;POPE&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff08;F1&#x5206;&#x6570;&#x63d0;&#x5347;&#xff09;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x5bf9;&#x6297;&#x6027;&#x8bbe;&#x7f6e;&#x4e0b;&#x6548;&#x679c;&#x660e;&#x663e;&#x3002;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x7684;&#x4f7f;&#x7528;&#x5355;&#x72ec;&#x4e5f;&#x80fd;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff0c;&#x800c;AGLA&#x7684;&#x96c6;&#x6210;&#x8fdb;&#x4e00;&#x6b65;&#x4f18;&#x5316;&#x4e86;&#x6548;&#x679c;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x79cd;&#x751f;&#x6210;&#x548c;&#x5224;&#x522b;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x5e7f;&#x6cdb;&#x9002;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1178,1179"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: AGLA&#x901a;&#x8fc7;&#x63ed;&#x793a;&#x6ce8;&#x610f;&#x529b;&#x7f3a;&#x9677;&#x7684;&#x6839;&#x6e90;&#x5e76;&#x63d0;&#x4f9b;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x8fd8;&#x4e3a;&#x7406;&#x89e3;&#x548c;&#x53d1;&#x5c55;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x8df5;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1179,1181"}}],"payload":{"tag":"li","lines":"1175,1181","fold":1}}],"payload":{"tag":"h4","lines":"1173,1174"}},{"content":"ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x89c6;&#x89c9;&#x653e;&#x5927;&#x878d;&#x5408;&#xff08;VAF&#xff09;&#x7684;&#x5373;&#x63d2;&#x5373;&#x7528;&#x6280;&#x672f;&#xff0c;&#x7528;&#x4e8e;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x4e2d;&#x95f4;&#x5c42;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x53f7;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x8fde;&#x8d2f;&#x6027;&#x3001;&#x51c6;&#x786e;&#x6027;&#x548c;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1182,1183"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;&#x5982;BLIP-2&#x3001;LLaVA&#x7b49;&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x201c;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x4e0e;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x5b9e;&#x9645;&#x7269;&#x4f53;&#x4e0d;&#x7b26;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff08;language priors&#xff09;&#xff0c;&#x800c;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x548c;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x7cbe;&#x5ea6;&#x5e94;&#x7528;&#x4e2d;&#x53ef;&#x80fd;&#x5e26;&#x6765;&#x98ce;&#x9669;&#x3002;&#x73b0;&#x6709;&#x7684;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#xff09;&#x867d;&#x80fd;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x4f1a;&#x635f;&#x5bb3;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x8fde;&#x8d2f;&#x6027;&#x3001;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x5e76;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x4e0d;&#x5f71;&#x54cd;&#x6027;&#x80fd;&#x4e14;&#x80fd;&#x9ad8;&#x6548;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1184,1185"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x653e;&#x5927;&#x878d;&#x5408;&#xff08;Visual Amplification Fusion, VAF&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x5173;&#x952e;&#x6d1e;&#x5bdf;&#x662f;&#xff1a;&#x5e7b;&#x89c9;&#x5e76;&#x975e;&#x6e90;&#x4e8e;&#x5bf9;&#x8bed;&#x8a00;&#x4fe1;&#x53f7;&#x7684;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#xff0c;&#x800c;&#x662f;&#x7531;&#x4e8e;&#x5728;&#x6a21;&#x6001;&#x878d;&#x5408;&#xff08;&#x4e3b;&#x8981;&#x53d1;&#x751f;&#x5728;&#x6a21;&#x578b;&#x4e2d;&#x95f4;&#x5c42;&#xff09;&#x8fc7;&#x7a0b;&#x4e2d;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#x4e0d;&#x8db3;&#x3002;VAF&#x662f;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x6280;&#x672f;&#xff0c;&#x5b83;&#x901a;&#x8fc7;&#x5728;&#x6a21;&#x578b;&#x7684;&#x4e2d;&#x95f4;&#x5c42;&#xff08;&#x6a21;&#x6001;&#x878d;&#x5408;&#x53d1;&#x751f;&#x7684;&#x4e3b;&#x8981;&#x4f4d;&#x7f6e;&#xff09;&#x7279;&#x5f02;&#x6027;&#x653e;&#x5927;&#x89c6;&#x89c9;&#x4fe1;&#x53f7;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x80fd;&#x66f4;&#x6709;&#x6548;&#x5730;&#x6355;&#x6349;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5bf9;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#x7684;&#x504f;&#x89c1;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x5904;&#x7406;&#x5bf9;&#x6bd4;&#x6837;&#x672c;&#xff08;&#x5982;VCD&#x9700;&#x8981;&#x5904;&#x7406;&#x539f;&#x59cb;&#x548c;&#x5931;&#x771f;&#x56fe;&#x50cf;&#xff09;&#xff0c;&#x56e0;&#x6b64;&#x4e0d;&#x5f71;&#x54cd;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1185,1186"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;VAF&#x5728;&#x591a;&#x4e2a;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5747;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x6548;&#x679c;&#xff1a;&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x7ea6;3%&#xff0c;&#x5728;MME&#x57fa;&#x51c6;&#x4e0a;&#x63d0;&#x5347;&#x7ea6;7%&#x3002;&#x5728;&#x4fdd;&#x6301;&#x5185;&#x5bb9;&#x8d28;&#x91cf;&#x65b9;&#x9762;&#xff0c;&#x5bf9;&#x6bd4;&#x65b9;&#x6cd5;VCD&#x5728;NoCaps&#x57fa;&#x51c6;&#x4e0a;&#x5bfc;&#x81f4;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x7ea6;19%&#xff0c;&#x800c;VAF&#x5219;&#x6ca1;&#x6709;&#x8d1f;&#x9762;&#x5f71;&#x54cd;&#x3002;&#x5728;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x65b9;&#x9762;&#xff0c;VCD&#x4f7f;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x964d;&#x4f4e;&#x4e86;50%&#xff0c;&#x800c;VAF&#x51e0;&#x4e4e;&#x4e0d;&#x5f71;&#x54cd;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1186,1187"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;VAF&#x65b9;&#x6cd5;&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x80fd;&#x591f;&#x663e;&#x8457;&#x51cf;&#x8f7b;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x8fde;&#x8d2f;&#x6027;&#x3001;&#x51c6;&#x786e;&#x6027;&#x548c;&#x6a21;&#x578b;&#x7684;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x90e8;&#x7f72;&#x9ad8;&#x7cbe;&#x5ea6;&#x3001;&#x9ad8;&#x6548;&#x7387;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#xff08;&#x5982;&#x533b;&#x7597;&#x548c;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x9886;&#x57df;&#xff09;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x9760;&#x7684;&#x6280;&#x672f;&#x652f;&#x6301;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x5177;&#x6709;&#x5f88;&#x597d;&#x7684;&#x901a;&#x7528;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1187,1189"}}],"payload":{"tag":"li","lines":"1183,1189","fold":1}}],"payload":{"tag":"h4","lines":"1181,1182"}},{"content":"TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;TARAC&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x7d2f;&#x79ef;&#x548c;&#x66f4;&#x65b0;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x6307;&#x6807;&#xff0c;&#x5e76;&#x517c;&#x5bb9;&#x4e0d;&#x540c;LVLM&#x67b6;&#x6784;&#x3002;","children":[],"payload":{"tag":"li","lines":"1190,1191"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x591a;&#x79cd;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x5e7b;&#x89c9;&#x53ef;&#x80fd;&#x6e90;&#x4e8e;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x5185;&#x5728;&#x7f3a;&#x9677;&#x3001;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x611f;&#x77e5;&#x9650;&#x5236;&#x6216;&#x591a;&#x6a21;&#x6001;&#x6570;&#x636e;&#x7684;&#x504f;&#x5dee;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x591a;&#x79cd;&#x65b9;&#x6cd5;&#x5c1d;&#x8bd5;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x95ee;&#x9898;&#x4f9d;&#x7136;&#x590d;&#x6742;&#x4e14;&#x6301;&#x4e45;&#x3002;&#x672c;&#x6587;&#x53d1;&#x73b0;&#x6ce8;&#x610f;&#x529b;&#x5728;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x4e0a;&#x7684;&#x8870;&#x51cf;&#x4e0e;&#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x5b58;&#x5728;&#x5f3a;&#x76f8;&#x5173;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x65e8;&#x5728;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1192,1193"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;TARAC&#xff08;Temporal Attention Real-time Accumulative Connection&#xff09;&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x3002;TARAC&#x5728;&#x6a21;&#x578b;&#x751f;&#x6210;&#x6bcf;&#x4e2a;&#x4ee4;&#x724c;&#x65f6;&#x6267;&#x884c;&#x4e09;&#x4e2a;&#x6b65;&#x9aa4;&#xff1a;1) &#x6355;&#x83b7;&#x5f53;&#x524d;&#x751f;&#x6210;&#x4ee4;&#x724c;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff1b;2) &#x5c06;&#x7d2f;&#x79ef;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x4ee5;&#x4e00;&#x5b9a;&#x6bd4;&#x4f8b;&#x6ce8;&#x5165;&#x5230;&#x5f53;&#x524d;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#xff1b;3) &#x91cd;&#x65b0;&#x5f52;&#x4e00;&#x5316;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4ee5;&#x4fdd;&#x6301;&#x5176;&#x6027;&#x8d28;&#x3002;&#x5177;&#x4f53;&#x5730;&#xff0c;&#x901a;&#x8fc7;&#x6700;&#x5927;&#x6c60;&#x5316;&#x538b;&#x7f29;&#x591a;&#x5934;&#x6ce8;&#x610f;&#x529b;&#x4e2d;&#x7684;&#x663e;&#x8457;&#x503c;&#xff0c;&#x5e76;&#x4f7f;&#x7528;&#x8bb0;&#x5fc6;&#x66f4;&#x65b0;&#x56e0;&#x5b50;&#x3b1;&#x52a8;&#x6001;&#x7d2f;&#x79ef;&#x5386;&#x53f2;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x901a;&#x8fc7;&#x6ce8;&#x5165;&#x7cfb;&#x6570;&#x3b2;&#x589e;&#x5f3a;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x5173;&#x6ce8;&#x3002;","children":[],"payload":{"tag":"li","lines":"1193,1194"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: TARAC&#x5728;&#x591a;&#x4e2a;&#x6a21;&#x578b;&#x548c;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x9a8c;&#x8bc1;&#x6709;&#x6548;&#x3002;&#x5728;LLaVA-1.5-7B&#x4e0a;&#xff0c;&#x76f8;&#x6bd4;OPERA&#x548c;VCD&#x65b9;&#x6cd5;&#xff0c;&#x5728;CHAIR&#x57fa;&#x51c6;&#x4e0a;&#x5206;&#x522b;&#x964d;&#x4f4e;&#x4e86;CS&#xff08;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x6027;&#xff09;17.2&#x548c;25.2&#xff0c;CI&#xff08;&#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x7387;&#xff09;5.4&#x548c;8.7&#x3002;&#x5728;Qwen2-VL-7B&#x4e0a;&#xff0c;&#x76f8;&#x6bd4;&#x8d2a;&#x5a6a;&#x641c;&#x7d22;&#xff0c;&#x5728;AMBER&#x57fa;&#x51c6;&#x4e0a;&#x964d;&#x4f4e;&#x4e86;CHAIR&#x7ea6;21.2%&#x548c;Hal&#xff08;&#x5e7b;&#x89c9;&#x7387;&#xff09;&#x7ea6;49%&#x3002;TARAC&#x8fd8;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x751f;&#x6210;&#x5f0f;&#x548c;&#x5224;&#x522b;&#x5f0f;&#x4efb;&#x52a1;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1194,1195"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: TARAC&#x901a;&#x8fc7;&#x589e;&#x5f3a;LVLM&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x56e0;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x8870;&#x51cf;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x5b83;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x517c;&#x5bb9;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x4e3a;LVLM&#x7684;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x9760;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x4f18;&#x5316;&#x5728;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1195,1197"}}],"payload":{"tag":"li","lines":"1191,1197","fold":1}}],"payload":{"tag":"h4","lines":"1189,1190"}},{"content":"SEVI: Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;SEVI&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4e0e;&#x4fe1;&#x606f;&#x6d41;&#x7684;&#x9519;&#x4f4d;&#x95ee;&#x9898;&#xff0c;&#x53d1;&#x73b0;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5728;&#x6df1;&#x5c42;&#x5df2;&#x878d;&#x5165;&#x8bed;&#x4e49;&#x8868;&#x5f81;&#xff0c;&#x4f46;&#x6a21;&#x578b;&#x672a;&#x5145;&#x5206;&#x5173;&#x6ce8;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x4fe1;&#x606f;&#xff0c;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x3002;SEVI&#x901a;&#x8fc7;&#x4e24;&#x9636;&#x6bb5;&#x4f18;&#x5316;&#xff0c;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x6838;&#x5fc3;&#x8bed;&#x4e49;&#x8868;&#x5f81;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x652f;&#x6301;&#x624b;&#x52a8;&#x8c03;&#x6574;&#x4fdd;&#x5b88;&#x7a0b;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1198,1199"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x4ea7;&#x751f;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5185;&#x5bb9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;LVLM&#x5728;&#x73b0;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#xff0c;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5e94;&#x7528;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1200,1201"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;SEVI&#xff08;Semantic-Enhanced Visual Interpretation&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x662f;&#x901a;&#x8fc7;&#x4e24;&#x9636;&#x6bb5;&#x4f18;&#x5316;&#x8303;&#x5f0f;&#x5bf9;&#x9f50;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4e0e;&#x4fe1;&#x606f;&#x6d41;&#x3002;&#x9996;&#x5148;&#xff0c;&#x57fa;&#x4e8e;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x8bc6;&#x522b;&#x51fa;&#x90a3;&#x4e9b;&#x5173;&#x6ce8;&#x6838;&#x5fc3;&#x8bed;&#x4e49;&#x8868;&#x5f81;&#x7684;&#x201c;&#x4f18;&#x52bf;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x201d;&#xff1b;&#x7136;&#x540e;&#xff0c;&#x4ee5;&#x8fd9;&#x4e9b;&#x5934;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4e3a;&#x76ee;&#x6807;&#xff0c;&#x901a;&#x8fc7;&#x5e73;&#x6ed1;&#x673a;&#x5236;&#x4f18;&#x5316;&#x5176;&#x4ed6;&#x5934;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x5df2;&#x6574;&#x5408;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x8bed;&#x4e49;&#x8868;&#x5f81;&#x7684;&#x5173;&#x6ce8;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x4e8e;&#x73b0;&#x6709;LVLM&#x3002;","children":[],"payload":{"tag":"li","lines":"1201,1202"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x4e94;&#x4e2a;LVLM&#xff08;&#x5982;LLaVA-1.5&#x3001;Qwen2-VL&#x7b49;&#xff09;&#x548c;&#x4e09;&#x4e2a;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x57fa;&#x51c6;&#xff08;CHAIR&#x3001;AMBER&#x3001;DetailCaps&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#x3002;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;SEVI&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#xff08;&#x4f8b;&#x5982;&#x5728;CHAIR&#x6307;&#x6807;&#x4e0a;&#x63d0;&#x5347;&#x660e;&#x663e;&#xff09;&#xff0c;&#x540c;&#x65f6;&#x53d1;&#x73b0;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x4e0e;&#x4fdd;&#x7559;&#x7ec6;&#x8282;&#x4e4b;&#x95f4;&#x5b58;&#x5728;&#x6743;&#x8861;&#xff1a;&#x6a21;&#x578b;&#x53ef;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x53c2;&#x6570;&#x5728;&#x201c;&#x4fdd;&#x5b88;&#x8f93;&#x51fa;&#x201d;&#xff08;&#x66f4;&#x5c11;&#x5e7b;&#x89c9;&#xff09;&#x548c;&#x201c;&#x4e30;&#x5bcc;&#x7ec6;&#x8282;&#x201d;&#x4e4b;&#x95f4;&#x7075;&#x6d3b;&#x5207;&#x6362;&#x3002;","children":[],"payload":{"tag":"li","lines":"1202,1203"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x5c06;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4e0e;&#x4fe1;&#x606f;&#x6d41;&#x5bf9;&#x9f50;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x63d0;&#x5347;LVLM&#x7684;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x5e76;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;SEVI&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x514d;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#x548c;&#x53ef;&#x63a7;&#x5236;&#x6027;&#xff0c;&#x5141;&#x8bb8;&#x6839;&#x636e;&#x5b9e;&#x9645;&#x9700;&#x6c42;&#x8c03;&#x6574;&#x6a21;&#x578b;&#x884c;&#x4e3a;&#xff0c;&#x8fd9;&#x5bf9;&#x63a8;&#x52a8;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x5230;&#x66f4;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"1203,1205"}}],"payload":{"tag":"li","lines":"1199,1205","fold":1}}],"payload":{"tag":"h4","lines":"1197,1198"}},{"content":"SPIN: Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;SPIN&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x6291;&#x5236;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x5bf9;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x4e0d;&#x8db3;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5728;&#x4e0d;&#x589e;&#x52a0;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x548c;&#x5ef6;&#x8fdf;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff08;&#x5373;&#x6587;&#x672c;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1206,1207"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x4f8b;&#x5982;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x9519;&#x8bef;&#x5c5e;&#x6027;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x901a;&#x8fc7;&#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#x6216;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6765;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x5f80;&#x5f80;&#x5e26;&#x6765;&#x663e;&#x8457;&#x7684;&#x5ef6;&#x8fdf;&#x589e;&#x52a0;&#x6216;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#xff0c;&#x4e14;&#x7f3a;&#x4e4f;&#x5bf9;&#x6a21;&#x578b;&#x7ec4;&#x4ef6;&#x5982;&#x4f55;&#x8d21;&#x732e;&#x4e8e;&#x5e7b;&#x89c9;&#x7684;&#x7cfb;&#x7edf;&#x6027;&#x7406;&#x89e3;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7cbe;&#x51c6;&#x6291;&#x5236;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x7684;&#x6a21;&#x578b;&#x7ec4;&#x4ef6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1208,1209"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;SPIN&#xff08;SuPpressing image INattentive heads&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x6ce8;&#x610f;&#x529b;&#x5f15;&#x5bfc;&#x7684;&#x5934;&#x6291;&#x5236;&#x7b56;&#x7565;&#x3002;&#x5173;&#x952e;&#x6b65;&#x9aa4;&#x5982;&#x4e0b;&#xff1a;1&#xff09;&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;LVLMs&#x7684;&#x5e7b;&#x89c9;&#x4e0e;&#x7279;&#x5b9a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x5bf9;&#x56fe;&#x50cf;token&#x5173;&#x6ce8;&#x4e0d;&#x8db3;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff1b;2&#xff09;&#x5bf9;&#x4e8e;&#x6bcf;&#x4e2a;&#x6587;&#x672c;&#x67e5;&#x8be2;token&#xff0c;&#x8ba1;&#x7b97;&#x5176;&#x5bf9;&#x6240;&#x6709;&#x56fe;&#x50cf;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5f97;&#x5206;&#xff1b;3&#xff09;&#x52a8;&#x6001;&#x8bc6;&#x522b;&#x5e76;&#x6291;&#x5236;&#x6bcf;&#x4e2a;&#x5c42;&#x4e2d;&#x5bf9;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x6700;&#x4f4e;&#x7684;&#x5934;&#x90e8;&#xff08;&#x4fdd;&#x7559;&#x6ce8;&#x610f;&#x529b;&#x6700;&#x9ad8;&#x7684;K&#x4e2a;&#x5934;&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x4e00;&#x4e2a;&#x4e8c;&#x503c;&#x63a9;&#x7801;&#x5b9e;&#x73b0;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x4e14;&#x4e0d;&#x5f15;&#x5165;&#x989d;&#x5916;&#x8ba1;&#x7b97;&#x6216;&#x5ef6;&#x8fdf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1209,1210"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e0a;&#x8fdb;&#x884c;&#x3002;&#x5173;&#x952e;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;SPIN&#x5c06;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#xff08;&#x5982;CHAIR&#x6307;&#x6807;&#xff09;&#x964d;&#x4f4e;&#x4e86;&#x9ad8;&#x8fbe;2.7&#x500d;&#xff1b;2&#xff09;&#x5728;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x4fdd;&#x6301;&#x4e86;F1&#x5206;&#x6570;&#x7b49;&#x6027;&#x80fd;&#x6307;&#x6807;&#xff1b;3&#xff09;&#x4e0e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;OPERA&#x3001;HALC&#xff09;&#x76f8;&#x6bd4;&#xff0c;&#x63a8;&#x7406;&#x541e;&#x5410;&#x91cf;&#x63d0;&#x9ad8;&#x4e86;1.8&#x500d;&#xff0c;&#x4e14;&#x65e0;&#x989d;&#x5916;&#x5ef6;&#x8fdf;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;SPIN&#x6210;&#x529f;&#x907f;&#x514d;&#x4e86;&#x751f;&#x6210;&#x50cf;&#x201c;&#x6905;&#x5b50;&#x201d;&#x8fd9;&#x6837;&#x7684;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#xff08;&#x5982;&#x56fe;1&#x6240;&#x793a;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1210,1211"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x53ef;&#x5f52;&#x56e0;&#x4e8e;&#x7279;&#x5b9a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x5bf9;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x7684;&#x5ffd;&#x89c6;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x6291;&#x5236;&#x8fd9;&#x4e9b;&#x5934;&#x53ef;&#x4ee5;&#x9ad8;&#x6548;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;SPIN&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x4efb;&#x52a1;&#x65e0;&#x5173;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x4e14;&#x96f6;&#x5f00;&#x9500;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6548;&#x7387;&#x3002;&#x8fd9;&#x4e00;&#x53d1;&#x73b0;&#x4e0d;&#x4ec5;&#x4e3a;&#x7406;&#x89e3;LVLMs&#x7684;&#x5185;&#x90e8;&#x673a;&#x5236;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x89c6;&#x89d2;&#xff0c;&#x800c;&#x4e14;&#x4e3a;&#x5728;&#x8d44;&#x6e90;&#x53d7;&#x9650;&#x73af;&#x5883;&#x4e2d;&#x90e8;&#x7f72;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x5b9e;&#x7528;&#x6280;&#x672f;&#xff0c;&#x5bf9;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1211,1213"}}],"payload":{"tag":"li","lines":"1207,1213","fold":1}}],"payload":{"tag":"h4","lines":"1205,1206"}},{"content":"*CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CLAIM&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8de8;&#x8bed;&#x8a00;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#x6765;&#x7f13;&#x89e3;&#x591a;&#x8bed;&#x8a00;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x8c03;&#x6574;&#x975e;&#x82f1;&#x8bed;&#x67e5;&#x8be2;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#xff0c;&#x4f7f;&#x5176;&#x4e0e;&#x82f1;&#x8bed;&#x6a21;&#x5f0f;&#x5bf9;&#x9f50;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x8bed;&#x8a00;&#x573a;&#x666f;&#x4e0b;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1214,1215"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5904;&#x7406;&#x975e;&#x82f1;&#x8bed;&#x67e5;&#x8be2;&#x65f6;&#xff0c;&#x6bd4;&#x82f1;&#x8bed;&#x67e5;&#x8be2;&#x66f4;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x54cd;&#x5e94;&#xff09;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x9884;&#x8bad;&#x7ec3;&#x6216;&#x5fae;&#x8c03;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x8017;&#x65f6;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLM&#x5728;&#x5168;&#x7403;&#x591a;&#x8bed;&#x8a00;&#x73af;&#x5883;&#x4e0b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x9002;&#x7528;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1216,1217"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: CLAIM&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6b65;&#x9aa4;&#xff1a;1) &#x8bc6;&#x522b;&#x8bed;&#x8a00;&#x7279;&#x5b9a;&#x7684;&#x8de8;&#x6a21;&#x6001;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff1a;&#x901a;&#x8fc7;&#x8bad;&#x7ec3;&#x63a2;&#x9488;&#x5206;&#x7c7b;&#x5668;&#xff0c;&#x627e;&#x51fa;&#x5728;&#x4e0d;&#x540c;&#x8bed;&#x8a00;&#x67e5;&#x8be2;&#x4e0b;&#x5bf9;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x6ce8;&#x610f;&#x529b;&#x5dee;&#x5f02;&#x663e;&#x8457;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff1b;2) &#x4f30;&#x8ba1;&#x8bed;&#x8a00;&#x504f;&#x79fb;&#x5411;&#x91cf;&#xff1a;&#x8ba1;&#x7b97;&#x82f1;&#x8bed;&#x548c;&#x76ee;&#x6807;&#x8bed;&#x8a00;&#x5728;&#x76f8;&#x540c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x67e5;&#x8be2;&#x4e0b;&#x6ce8;&#x610f;&#x529b;&#x8f93;&#x51fa;&#x7684;&#x5e73;&#x5747;&#x5dee;&#x5f02;&#x5411;&#x91cf;&#xff1b;3) &#x63a8;&#x7406;&#x9636;&#x6bb5;&#x5e72;&#x9884;&#xff1a;&#x5728;&#x8bc6;&#x522b;&#x51fa;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x4e0a;&#x5e94;&#x7528;&#x504f;&#x79fb;&#x5411;&#x91cf;&#xff0c;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x8f93;&#x51fa;&#xff0c;&#x4f7f;&#x5176;&#x66f4;&#x63a5;&#x8fd1;&#x82f1;&#x8bed;&#x6a21;&#x5f0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1217,1218"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;LLaVA-1.5&#x548c;Qwen-VL-Chat&#x6a21;&#x578b;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;CLAIM&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#x5e73;&#x5747;&#x63d0;&#x5347;13.56%&#xff08;&#x897f;&#x73ed;&#x7259;&#x8bed;&#x6700;&#x9ad8;&#x63d0;&#x5347;30%&#xff09;&#xff0c;&#x5728;MME&#x5e7b;&#x89c9;&#x5b50;&#x96c6;&#x4e0a;&#x5e73;&#x5747;&#x63d0;&#x5347;21.75%&#x3002;&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;&#x591a;&#x8bed;&#x8a00;&#x6ce8;&#x610f;&#x529b;&#x5dee;&#x5f02;&#x5728;&#x4e2d;&#x95f4;&#x5c42;&#x6700;&#x4e3a;&#x663e;&#x8457;&#x3002;","children":[],"payload":{"tag":"li","lines":"1218,1219"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CLAIM&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x591a;&#x8bed;&#x8a00;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x5347;LVLM&#x7684;&#x8de8;&#x8bed;&#x8a00;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x63ed;&#x793a;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5728;&#x591a;&#x8bed;&#x8a00;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x4f5c;&#x7528;&#xff0c;&#x5c24;&#x5176;&#x5f3a;&#x8c03;&#x4e86;&#x4e2d;&#x95f4;&#x5c42;&#x7684;&#x91cd;&#x8981;&#x6027;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1219,1222"}}],"payload":{"tag":"li","lines":"1215,1222","fold":1}}],"payload":{"tag":"h4","lines":"1213,1214"}},{"content":"LPS: Mitigating Object Hallucination via Robust Local Perception Search","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x5c40;&#x90e8;&#x611f;&#x77e5;&#x641c;&#x7d22;&#xff08;LPS&#xff09;&#x7684;&#x514d;&#x8bad;&#x7ec3;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5229;&#x7528;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x7684;&#x5c40;&#x90e8;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x80fd;&#x529b;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x6291;&#x5236;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x56fe;&#x50cf;&#x5b58;&#x5728;&#x566a;&#x58f0;&#x6216;&#x5bf9;&#x6297;&#x653b;&#x51fb;&#x65f6;&#x6548;&#x679c;&#x663e;&#x8457;&#x3002;","children":[],"payload":{"tag":"li","lines":"1223,1224"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x865a;&#x5047;&#x5bf9;&#x8c61;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x56fe;&#x50cf;&#x53d7;&#x5230;&#x5bf9;&#x6297;&#x653b;&#x51fb;&#x65f6;&#x66f4;&#x4e3a;&#x4e25;&#x91cd;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x6216;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x6cdb;&#x5316;&#x6027;&#x5dee;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x5f00;&#x53d1;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x76d1;&#x7763;&#x3001;&#x4ec5;&#x5229;&#x7528;&#x6a21;&#x578b;&#x5185;&#x5728;&#x80fd;&#x529b;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#x65b9;&#x6cd5;&#xff0c;&#x5728;&#x4fdd;&#x6301;&#x5e72;&#x51c0;&#x56fe;&#x50cf;&#x6027;&#x80fd;&#x7684;&#x540c;&#x65f6;&#x6709;&#x6548;&#x6291;&#x5236;&#x566a;&#x58f0;&#x4e0b;&#x7684;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1225,1226"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x5c40;&#x90e8;&#x611f;&#x77e5;&#x641c;&#x7d22;&#xff08;LPS&#xff09;&#xff0c;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x5229;&#x7528;MLLM&#x81ea;&#x8eab;&#x7684;&#x5c40;&#x90e8;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x751f;&#x6210;&#x5148;&#x9a8c;&#x4fe1;&#x606f;&#xff1a;1&#xff09;&#x5c06;&#x56fe;&#x50cf;&#x5206;&#x5272;&#x4e3a;&#x5c40;&#x90e8;&#x533a;&#x57df;&#xff08;&#x5982;&#x7f51;&#x683c; patches&#xff09;&#xff1b;2&#xff09;&#x901a;&#x8fc7;&#x6a21;&#x578b;&#x5bf9;&#x5c40;&#x90e8;&#x533a;&#x57df;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x8ba1;&#x7b97;&#x89c6;&#x89c9;&#x4e00;&#x81f4;&#x6027;&#x5206;&#x6570;&#xff08;Reward&#xff09;&#xff1b;3&#xff09;&#x5728;&#x675f;&#x641c;&#x7d22;&#xff08;Beam Search&#xff09;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x9009;&#x62e9;&#x4e0e;&#x5c40;&#x90e8;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x6700;&#x4e00;&#x81f4;&#x7684;&#x5019;&#x9009;&#x8f93;&#x51fa;&#xff0c;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#x6027;&#x63cf;&#x8ff0;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x6807;&#x6ce8;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#xff0c;&#x5b8c;&#x5168;&#x57fa;&#x4e8e;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x80fd;&#x529b;&#x5b9e;&#x73b0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1226,1227"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x5e7f;&#x6cdb;&#x4f7f;&#x7528;&#x7684;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#xff08;&#x5982;POPE&#xff09;&#x548c;&#x542b;&#x5bf9;&#x6297;&#x566a;&#x58f0;&#x7684;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;LPS&#x663e;&#x8457;&#x964d;&#x4f4e;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x7387;&#xff08;&#x5982;&#x76f8;&#x6bd4;&#x57fa;&#x7ebf;&#x6a21;&#x578b; hallucination &#x6307;&#x6807;&#x4e0b;&#x964d;&#x660e;&#x663e;&#xff09;&#xff1b;2&#xff09;&#x5728;&#x566a;&#x58f0;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#x4e0b;&#x8868;&#x73b0;&#x5c24;&#x4e3a;&#x7a81;&#x51fa;&#xff0c;&#x6297;&#x5e72;&#x6270;&#x80fd;&#x529b;&#x5f3a;&#xff1b;3&#xff09;&#x517c;&#x5bb9;&#x591a;&#x79cd;MLLM&#x67b6;&#x6784;&#xff08;&#x5982;LLaVA&#x3001;InstructBLIP&#xff09;&#xff0c;&#x4e14;&#x4e0d;&#x5f71;&#x54cd;&#x5e72;&#x51c0;&#x56fe;&#x50cf;&#x4e0a;&#x7684;&#x539f;&#x59cb;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1227,1228"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: LPS&#x901a;&#x8fc7;&#x6316;&#x6398;MLLM&#x56fa;&#x6709;&#x7684;&#x5c40;&#x90e8;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff0c;&#x4e3a;&#x6291;&#x5236;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x6548;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x9c81;&#x68d2;&#x6027;&#x5728;&#x566a;&#x58f0;&#x573a;&#x666f;&#x4e0b;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x63a8;&#x52a8;&#x4e86;MLLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;&#x4ee3;&#x7801;&#x5df2;&#x5f00;&#x6e90;&#xff0c;&#x5177;&#x6709;&#x9ad8;&#x5ea6;&#x7684;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1228,1232"}}],"payload":{"tag":"li","lines":"1224,1232","fold":1}}],"payload":{"tag":"h4","lines":"1222,1223"}},{"content":"MDSAM: Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;MDSAM&#x7684;&#x65e0;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x6355;&#x83b7;&#x548c;&#x4f18;&#x5316;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x914d;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1233,1234"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#xff0c;&#x4f8b;&#x5982;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff08;&#x5982;&apos;&#x96e8;&#x4f1e;&apos;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x73b0;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x6548;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x7f3a;&#x4e4f;&#x901a;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1235,1236"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: MDSAM&#xff08;Memory-Driven Sparse Attention Matrix&#xff09;&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x52a8;&#x6001;&#x6ce8;&#x610f;&#x529b;&#x4f18;&#x5316;&#x6846;&#x67b6;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x901a;&#x8fc7;&#x8bb0;&#x5fc6;&#x673a;&#x5236;&#x6355;&#x83b7;&#x6bcf;&#x4e00;&#x5c42;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x5bf9;&#x9f50;&#x64cd;&#x4f5c;&#x6fc0;&#x6d3b;&#x66f4;&#x65b0;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x5728;&#x6bcf;&#x4e00;&#x5c42;&#x63d0;&#x53d6;&#x6ce8;&#x610f;&#x529b;&#x77e9;&#x9635;&#xff1b;2) &#x5bf9;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x8fdb;&#x884c;&#x7a00;&#x758f;&#x5316;&#xff08;Top-K&#x9009;&#x62e9;&#xff09;&#x548c;&#x5f52;&#x4e00;&#x5316;&#x5904;&#x7406;&#xff1b;3) &#x5c06;&#x5386;&#x53f2;&#x5c42;&#x7684;&#x7a00;&#x758f;&#x6ce8;&#x610f;&#x529b;&#x4e0e;&#x5f53;&#x524d;&#x5c42;&#x52a0;&#x6743;&#x6574;&#x5408;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x5206;&#x5e03;&#xff0c;&#x589e;&#x5f3a;&#x76f8;&#x5173;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#x5ea6;&#xff0c;&#x540c;&#x65f6;&#x6291;&#x5236;&#x566a;&#x58f0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1236,1237"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;LVLM&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA-1.5-7B&#x3001;MiniGPT4&#x3001;DeepseekVL-7B&#xff09;&#x548c;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;CHAIR&#x3001;MME&#x3001;POPE&#x3001;MMHAL-BENCH&#xff09;&#x4e0a;&#x9a8c;&#x8bc1;&#xff1a;1) &#x5728;LLaVA-1.5-7B&#x4e0a;&#xff0c;CHAIR-S&#x6307;&#x6807;&#x4e0b;&#x964d;10.6&#xff0c;CHAIR-I&#x4e0b;&#x964d;3.2&#xff1b;2) &#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x51cf;&#x5c11;&#xff08;&#x5982;&#x56fe;1&#x793a;&#x4f8b;&#xff1a;&#x53bb;&#x9664;&#x865a;&#x6784;&#x7684;&apos;&#x96e8;&#x4f1e;&apos;&#x63cf;&#x8ff0;&#xff09;&#xff1b;3) &#x6ce8;&#x610f;&#x529b;&#x53ef;&#x89c6;&#x5316;&#x663e;&#x793a;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x5173;&#x6ce8;&#x6bd4;&#x4f8b;&#x63d0;&#x5347;&#xff0c;&#x566a;&#x58f0;&#x51cf;&#x5c11;&#x3002;","children":[],"payload":{"tag":"li","lines":"1237,1238"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MDSAM&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x5177;&#x6709;&#x5f3a;&#x901a;&#x7528;&#x6027;&#x548c;&#x4f4e;&#x6210;&#x672c;&#x4f18;&#x52bf;&#x3002;&#x5176;&#x52a8;&#x6001;&#x6ce8;&#x610f;&#x529b;&#x4f18;&#x5316;&#x673a;&#x5236;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1238,1240"}}],"payload":{"tag":"li","lines":"1234,1240","fold":1}}],"payload":{"tag":"h4","lines":"1232,1233"}},{"content":"CAAC: Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CAAC&#x7684;&#x81ea;&#x9002;&#x5e94;&#x6ce8;&#x610f;&#x529b;&#x6821;&#x51c6;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x4e24;&#x6b65;&#x6cd5;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff1a;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x6821;&#x51c6;&#xff08;VTC&#xff09;&#x5e73;&#x8861;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x81ea;&#x9002;&#x5e94;&#x6ce8;&#x610f;&#x529b;&#x91cd;&#x7f29;&#x653e;&#xff08;AAR&#xff09;&#x6839;&#x636e;&#x6a21;&#x578b;&#x7f6e;&#x4fe1;&#x5ea6;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;CAAC&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x957f;&#x6587;&#x672c;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#x3002;","children":[],"payload":{"tag":"li","lines":"1241,1242"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5373;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x6216;&#x5c5e;&#x6027;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5728;&#x5f00;&#x653e;&#x57df;&#x957f;&#x6587;&#x672c;&#x751f;&#x6210;&#x4efb;&#x52a1;&#x4e2d;&#x6548;&#x679c;&#x6709;&#x9650;&#xff0c;&#x4e3b;&#x8981;&#x56e0;&#x4e3a;&#x6a21;&#x578b;&#x5b58;&#x5728;&#x4e24;&#x79cd;&#x504f;&#x5dee;&#xff1a;&#x7a7a;&#x95f4;&#x611f;&#x77e5;&#x504f;&#x5dee;&#xff08;&#x6ce8;&#x610f;&#x529b;&#x8fc7;&#x5ea6;&#x96c6;&#x4e2d;&#x5728;&#x5c11;&#x6570;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#xff09;&#x548c;&#x6a21;&#x6001;&#x504f;&#x5dee;&#xff08;&#x968f;&#x7740;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x9010;&#x6e10;&#x5ffd;&#x7565;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x800c;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x5148;&#x9a8c;&#xff09;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLM&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1243,1244"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: CAAC&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x6821;&#x51c6;&#xff08;VTC&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x5e73;&#x6ed1;&#x89e3;&#x7801;&#x5668;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x9632;&#x6b62;&#x6ce8;&#x610f;&#x529b;&#x8fc7;&#x5ea6;&#x96c6;&#x4e2d;&#x5728;&#x7279;&#x5b9a;&#x56fe;&#x50cf;&#x533a;&#x57df;&#xff0c;&#x7f13;&#x89e3;&#x7a7a;&#x95f4;&#x611f;&#x77e5;&#x504f;&#x5dee;&#xff1b;2&#xff09;&#x81ea;&#x9002;&#x5e94;&#x6ce8;&#x610f;&#x529b;&#x91cd;&#x7f29;&#x653e;&#xff08;AAR&#xff09;&#xff1a;&#x6839;&#x636e;&#x6a21;&#x578b;&#x751f;&#x6210;&#x6bcf;&#x4e2a;&#x4ee4;&#x724c;&#x65f6;&#x7684;&#x7f6e;&#x4fe1;&#x5ea6;&#xff08;&#x901a;&#x8fc7;logit&#x6982;&#x7387;&#x8861;&#x91cf;&#xff09;&#xff0c;&#x52a8;&#x6001;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x3002;&#x5f53;&#x6a21;&#x578b;&#x7f6e;&#x4fe1;&#x5ea6;&#x8f83;&#x4f4e;&#x65f6;&#xff08;&#x8868;&#x660e;&#x53ef;&#x80fd;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x5f3a;&#x5236;&#x589e;&#x52a0;&#x5bf9;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x5173;&#x6ce8;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x76f4;&#x63a5;&#x4f5c;&#x7528;&#x4e8e;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"1244,1245"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;CHAIR&#x3001;AMBER&#x548c;POPE&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;CAAC&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5728;CHAIR&#x548c;AMBER&#x4e0a;&#x5206;&#x522b;&#x6bd4;&#x6700;&#x4f73;&#x57fa;&#x7ebf;&#x5e73;&#x5747;&#x964d;&#x4f4e;4%&#x548c;1.8%&#xff1b;2&#xff09;&#x5c24;&#x5176;&#x5728;&#x957f;&#x6587;&#x672c;&#x751f;&#x6210;&#xff08;&#x6700;&#x5927;&#x751f;&#x6210;&#x957f;&#x5ea6;512&#x4ee4;&#x724c;&#xff09;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x7a81;&#x51fa;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7ef4;&#x6301;&#x89c6;&#x89c9; grounding&#xff1b;3&#xff09;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x5e7b;&#x89c9;&#x4ee4;&#x724c;&#x901a;&#x5e38;&#x5177;&#x6709;&#x8f83;&#x4f4e;&#x7684;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x6027;&#x548c;&#x6a21;&#x578b;&#x7f6e;&#x4fe1;&#x5ea6;&#xff0c;&#x4e14;&#x591a;&#x51fa;&#x73b0;&#x5728;&#x751f;&#x6210;&#x5e8f;&#x5217;&#x7684;&#x540e;&#x671f;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x6a21;&#x6001;&#x504f;&#x5dee;&#x7684;&#x5b58;&#x5728;&#x3002;","children":[],"payload":{"tag":"li","lines":"1245,1246"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CAAC&#x901a;&#x8fc7;&#x7f6e;&#x4fe1;&#x5ea6;&#x9a71;&#x52a8;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6821;&#x51c6;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLM&#x7684;&#x4e24;&#x79cd;&#x6838;&#x5fc3;&#x504f;&#x5dee;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x957f;&#x6587;&#x672c;&#x751f;&#x6210;&#x4e2d;&#x5177;&#x6709;&#x663e;&#x8457;&#x4f18;&#x52bf;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x4e3a;&#x63d0;&#x5347;LVLM&#x5728;&#x5f00;&#x653e;&#x57df;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x5b9e;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5206;&#x6790;&#xff09;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x63a2;&#x7d22;&#x66f4;&#x7cbe;&#x7ec6;&#x7684;&#x7f6e;&#x4fe1;&#x5ea6;&#x5ea6;&#x91cf;&#x4e0e;&#x591a;&#x6a21;&#x6001;&#x504f;&#x5dee;&#x7684;&#x8054;&#x5408;&#x4f18;&#x5316;&#x3002;","children":[],"payload":{"tag":"li","lines":"1246,1248"}}],"payload":{"tag":"li","lines":"1242,1248","fold":1}}],"payload":{"tag":"h4","lines":"1240,1241"}},{"content":"When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinationsin Scene Text Spotting and Understanding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x9488;&#x5bf9;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5728;&#x5904;&#x7406;&#x573a;&#x666f;&#x6587;&#x672c;&#x65f6;&#x4ea7;&#x751f;&#x7684;&#x2018;&#x8bed;&#x4e49;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x7684;&#x6846;&#x67b6;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x5305;&#x542b;ZoomText&#x7c97;&#x5230;&#x7ec6;&#x7684;&#x6587;&#x672c;&#x5b9a;&#x4f4d;&#x7b56;&#x7565;&#x548c;&#x57fa;&#x4e8e;&#x6ce8;&#x610f;&#x529b;&#x7684;&#x5c42;&#x6821;&#x6b63;&#x673a;&#x5236;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x975e;&#x8bed;&#x4e49;&#x6587;&#x672c;&#x4e0a;&#x7684;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1249,1250"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5728;&#x573a;&#x666f;&#x6587;&#x672c;&#x8bc6;&#x522b;&#x548c;&#x7406;&#x89e3;&#x4e2d;&#x51fa;&#x73b0;&#x7684;&#x2018;&#x8bed;&#x4e49;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#x3002;&#x5f53;&#x6a21;&#x578b;&#x9047;&#x5230;&#x89c6;&#x89c9;&#x4e0a;&#x6a21;&#x7cca;&#x6216;&#x975e;&#x8bed;&#x4e49;&#x7684;&#x6587;&#x672c;&#xff08;&#x5982;&#x88ab;&#x7be1;&#x6539;&#x7684;&#x5b57;&#x7b26;&#xff09;&#x65f6;&#xff0c;&#x4f1a;&#x4f9d;&#x8d56;&#x5176;&#x5185;&#x90e8;&#x8bed;&#x4e49;&#x5148;&#x9a8c;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x89c6;&#x89c9;&#x4e0a;&#x9519;&#x8bef;&#x7684;&#x7b54;&#x6848;&#xff0c;&#x800c;&#x4e0d;&#x662f;&#x57fa;&#x4e8e;&#x5b9e;&#x9645;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x8fdb;&#x884c;&#x63a8;&#x7406;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x573a;&#x666f;&#x6587;&#x672c;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x4ea7;&#x54c1;&#x5206;&#x6790;&#x548c;&#x8f85;&#x52a9;&#x6280;&#x672f;&#x7b49;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x5e7f;&#x6cdb;&#x5b58;&#x5728;&#xff0c;&#x6a21;&#x578b;&#x7684;&#x4e0d;&#x53ef;&#x9760;&#x6027;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e0b;&#x6e38;&#x4efb;&#x52a1;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1251,1252"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;LLM&#x4e2d;&#x4e0d;&#x540c;&#x5c42;&#x5bf9;&#x6587;&#x672c;&#x533a;&#x57df;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x914d;&#x4e0e;&#x5e7b;&#x89c9;&#x503e;&#x5411;&#x76f8;&#x5173;&#xff1a;&#x6ce8;&#x610f;&#x529b;&#x66f4;&#x96c6;&#x4e2d;&#x4e8e;&#x771f;&#x5b9e;&#x6587;&#x672c;&#x533a;&#x57df;&#x7684;&#x5c42;&#x66f4;&#x5c11;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1) ZoomText&#xff1a;&#x4e00;&#x79cd;&#x4ece;&#x7c97;&#x5230;&#x7ec6;&#x7684;&#x6587;&#x672c;&#x533a;&#x57df;&#x5b9a;&#x4f4d;&#x7b56;&#x7565;&#xff0c;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x68c0;&#x6d4b;&#x5668;&#xff0c;&#x5148;&#x5b9a;&#x4f4d;&#x4e0a;&#x4e0b;&#x6587;&#x533a;&#x57df;&#xff0c;&#x518d;&#x7ec6;&#x5316;&#x5230;&#x6587;&#x672c;&#x533a;&#x57df;&#xff1b;2) Grounded Layer Correction (GLC)&#xff1a;&#x81ea;&#x9002;&#x5e94;&#x9009;&#x62e9;&#x6ce8;&#x610f;&#x529b;&#x6700;&#x96c6;&#x4e2d;&#x4e8e;&#x6587;&#x672c;&#x7684;Transformer&#x5c42;&#xff0c;&#x5c06;&#x5176;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x8868;&#x793a;&#x878d;&#x5408;&#x5230;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x4ee5;&#x7ea0;&#x6b63;&#x975e;&#x8bed;&#x4e49;&#x6837;&#x672c;&#x7684;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x8bed;&#x4e49;&#x6837;&#x672c;&#x7684;&#x6b63;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1252,1253"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1) &#x5728;&#x63d0;&#x51fa;&#x7684;TextHalu-Bench&#x57fa;&#x51c6;&#xff08;&#x5305;&#x542b;1,740&#x4e2a;&#x8bed;&#x4e49;&#x548c;&#x975e;&#x8bed;&#x4e49;&#x6837;&#x672c;&#xff09;&#x4e0a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5c06;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x7684;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x4e86;&#x7ea6;4%&#xff1b;2) &#x5728;&#x516c;&#x5171;&#x57fa;&#x51c6;&#x5982;ST-VQA&#x548c;TextVQA&#x4e0a;&#xff0c;&#x5e94;&#x7528;&#x8be5;&#x6846;&#x67b6;&#x540e;&#xff0c;Mini-Monkey&#x548c;Qwen2.5-VL&#x7b49;&#x6a21;&#x578b;&#x7684;&#x51c6;&#x786e;&#x6027;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#xff1b;3) &#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x57fa;&#x4e8e;&#x8bed;&#x4e49;&#x5148;&#x9a8c;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x6587;&#x672c;&#x7684;&#x63a5;&#x5730;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1253,1254"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;&#x6846;&#x67b6;&#x80fd;&#x591f;&#x6709;&#x6548;&#x7f13;&#x89e3;LMMs&#x5728;&#x573a;&#x666f;&#x6587;&#x672c;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x8bed;&#x4e49;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x4e2d;&#x3002;&#x8fd9;&#x4e00;&#x5de5;&#x4f5c;&#x63ed;&#x793a;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5728;&#x5e7b;&#x89c9;&#x4e2d;&#x7684;&#x5173;&#x952e;&#x4f5c;&#x7528;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x6539;&#x8fdb;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#xff0c;&#x63a8;&#x52a8;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x63a8;&#x7406;&#x7cfb;&#x7edf;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1254,1256"}}],"payload":{"tag":"li","lines":"1250,1256","fold":1}}],"payload":{"tag":"h4","lines":"1248,1249"}},{"content":"CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CAI&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x56de;&#x7b54;&#x63cf;&#x8ff0;&#x6027;&#x67e5;&#x8be2;&#xff08;caption query&#xff09;&#x548c;&#x975e;&#x63cf;&#x8ff0;&#x6027;&#x67e5;&#x8be2;&#x65f6;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#x7684;&#x5dee;&#x5f02;&#xff0c;&#x8bc6;&#x522b;&#x51fa;&#x5bf9;&#x63cf;&#x8ff0;&#x654f;&#x611f;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5e76;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x5bf9;&#x5176;&#x8fdb;&#x884c;&#x5e72;&#x9884;&#xff0c;&#x4ee5;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x4e14;&#x4ec5;&#x589e;&#x52a0;&#x6781;&#x5c0f;&#x7684;&#x63a8;&#x7406;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1257,1258"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x89e3;&#x91ca;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x2019;&#xff08;object hallucination&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x4f9d;&#x8d56;&#x6602;&#x8d35;&#x7684;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x548c;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#xff0c;&#x8981;&#x4e48;&#x663e;&#x8457;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x65f6;&#x95f4;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x8feb;&#x5207;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#xff0c;&#x4ee5;&#x63a8;&#x52a8;LVLMs&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1259,1260"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Caption-sensitive Attention Intervention (CAI)&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x9636;&#x6bb5;&#xff1a;1) &#x6700;&#x4f73;&#x63cf;&#x8ff0;&#x67e5;&#x8be2;&#x641c;&#x7d22;&#xff1a;&#x4ece;&#x5019;&#x9009;&#x67e5;&#x8be2;&#x4e2d;&#x627e;&#x51fa;&#x80fd;&#x6700;&#x5927;&#x7a0b;&#x5ea6;&#x6fc0;&#x6d3b;&#x6a21;&#x578b;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x4e14;&#x6240;&#x9700;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x53d8;&#x5316;&#x6700;&#x5c0f;&#x7684;&#x63cf;&#x8ff0;&#x6027;&#x67e5;&#x8be2;&#x3002;2) &#x654f;&#x611f;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x63a2;&#x6d4b;&#x4e0e;&#x504f;&#x79fb;&#x8ba1;&#x7b97;&#xff1a;&#x8bad;&#x7ec3;&#x4e8c;&#x5143;&#x5206;&#x7c7b;&#x5668;&#x6765;&#x8bc6;&#x522b;&#x5bf9;&#x63cf;&#x8ff0;&#x654f;&#x611f;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5e76;&#x8ba1;&#x7b97;&#x5176;&#x5728;&#x56de;&#x7b54;&#x63cf;&#x8ff0;&#x4e0e;&#x975e;&#x63cf;&#x8ff0;&#x67e5;&#x8be2;&#x65f6;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x8f93;&#x51fa;&#x504f;&#x79fb;&#x5411;&#x91cf;&#xff0c;&#x8be5;&#x5411;&#x91cf;&#x6307;&#x660e;&#x4e86;&#x89c6;&#x89c9;&#x4e2d;&#x5fc3;&#x7684;&#x4f18;&#x5316;&#x65b9;&#x5411;&#x3002;3) &#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#xff1a;&#x5c06;&#x9884;&#x8ba1;&#x7b97;&#x597d;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x504f;&#x79fb;&#x5411;&#x91cf;&#x4ee5;&#x4e00;&#x5b9a;&#x5f3a;&#x5ea6;&#x5e94;&#x7528;&#x4e8e;&#x8bc6;&#x522b;&#x51fa;&#x7684;top-K&#x4e2a;&#x654f;&#x611f;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5c06;&#x5176;&#x6ce8;&#x610f;&#x529b;&#x4ece;&#x2018;&#x4e0d;&#x5145;&#x5206;&#x2019;&#x72b6;&#x6001; refine &#x5230;&#x2018;&#x5145;&#x5206;&#x2019;&#x72b6;&#x6001;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1260,1261"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x8bc4;&#x4f30;&#x4e86;CAI&#x7684;&#x6709;&#x6548;&#x6027;&#xff1a;1) &#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x51c6;&#x786e;&#x7387;&#xff08;Accuracy&#xff09;&#x548c;F1&#x5206;&#x6570;&#x5e73;&#x5747;&#x63d0;&#x5347;&#x4e86;5.14%&#x548c;5.50%&#x3002;2) &#x5728;MME&#x57fa;&#x51c6;&#x7684;&#x5e7b;&#x89c9;&#x5b50;&#x96c6;&#x4e0a;&#xff0c;&#x5206;&#x6570;&#x5e73;&#x5747;&#x63d0;&#x5347;&#x4e86;64.3&#x5206;&#x3002;3) &#x5728;MMHalBench&#x57fa;&#x51c6;&#x4e0a;&#xff0c;&#x5e7b;&#x89c9;&#x7387;&#x4e0b;&#x964d;&#x4e86;7.8%&#xff0c;&#x540c;&#x65f6;&#x751f;&#x6210;&#x56de;&#x7b54;&#x7684;&#x4fe1;&#x606f;&#x91cf;&#x6709;&#x6240;&#x63d0;&#x5347;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;CAI&#x5728;&#x4ec5;&#x589e;&#x52a0;&#x6781;&#x5c0f;&#x63a8;&#x7406;&#x6210;&#x672c;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#xff08;SOTA&#xff09;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1261,1262"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLMs&#x5728;&#x56de;&#x7b54;&#x63cf;&#x8ff0;&#x6027;&#x67e5;&#x8be2;&#x65f6;&#x4f1a;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x5f3a;&#x7684;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x8fd9;&#x4e00;&#x73b0;&#x8c61;&#x53ef;&#x4ee5;&#x88ab;&#x6709;&#x6548;&#x5229;&#x7528;&#x3002;CAI&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5e72;&#x9884;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x7279;&#x5b9a;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x6210;&#x529f;&#x6fc0;&#x6d3b;&#x4e86;&#x6a21;&#x578b;&#x56fa;&#x6709;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff0c;&#x4ece;&#x800c;&#x663e;&#x8457;&#x51cf;&#x8f7b;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e0d;&#x4ec5;&#x4e3a;&#x7406;&#x89e3;LVLMs&#x7684;&#x5185;&#x90e8;&#x673a;&#x5236;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x89c6;&#x89d2;&#xff0c;&#x800c;&#x4e14;&#x4e3a;&#x672a;&#x6765;&#x5f00;&#x53d1;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x6280;&#x672f;&#x5f00;&#x8f9f;&#x4e86;&#x65b0;&#x7684;&#x9014;&#x5f84;&#xff0c;&#x5177;&#x6709;&#x91cd;&#x8981;&#x7684;&#x5b9e;&#x8df5;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1262,1264"}}],"payload":{"tag":"li","lines":"1258,1264","fold":1}}],"payload":{"tag":"h4","lines":"1256,1257"}},{"content":"ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;ONLY&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x4ec5;&#x9700;&#x5355;&#x6b21;&#x67e5;&#x8be2;&#x548c;&#x4e00;&#x5c42;&#x5e72;&#x9884;&#x5373;&#x53ef;&#x9ad8;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#x4e14;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x6781;&#x4f4e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1265,1266"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff09;&#x9700;&#x8981;&#x591a;&#x6b21;&#x67e5;&#x8be2;&#x6a21;&#x578b;&#xff0c;&#x5bfc;&#x81f4;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x5927;&#x5e45;&#x4e0b;&#x964d;&#xff0c;&#x65e0;&#x6cd5;&#x6ee1;&#x8db3;&#x5b9e;&#x65f6;&#x5e94;&#x7528;&#x7684;&#x9700;&#x6c42;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1267,1268"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;ONLY&#x65b9;&#x6cd5;&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x6bcf;&#x4e2a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x7684;&#x2018;&#x6587;&#x672c;-&#x89c6;&#x89c9;&#x71b5;&#x6bd4;&#x2019;&#xff08;TVER&#xff09;&#xff0c;&#x7b5b;&#x9009;&#x51fa;&#x66f4;&#x5173;&#x6ce8;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5e76;&#x4ec5;&#x5728;&#x8fd9;&#x4e9b;&#x5934;&#x4e0a;&#x8fdb;&#x884c;&#x4e00;&#x5c42;&#x989d;&#x5916;&#x7684;&#x5e72;&#x9884;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1) &#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x63d2;&#x5165;&#x4e00;&#x4e2a;&#x6587;&#x672c;&#x589e;&#x5f3a;&#x7684;&#x591a;&#x5934;&#x6ce8;&#x610f;&#x529b;&#x5c42;&#xff08;TE-MHA&#xff09;&#xff1b;2) &#x901a;&#x8fc7;&#x6b8b;&#x5dee;&#x8fde;&#x63a5;&#x5c06;&#x589e;&#x5f3a;&#x540e;&#x7684;&#x8f93;&#x51fa;&#x4e0e;&#x539f;&#x59cb;&#x8f93;&#x51fa;&#x7ed3;&#x5408;&#xff1b;3) &#x4f7f;&#x7528;&#x5bf9;&#x6bd4;&#x6216;&#x534f;&#x4f5c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7ed3;&#x679c;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4ec5;&#x9700;&#x5355;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x6781;&#x5c0f;&#xff08;&#x4ec5;&#x589e;&#x52a0;7%&#x7684;&#x63a8;&#x7406;&#x65f6;&#x95f4;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1268,1269"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;LLaVA-1.5&#x3001;InstructBLIP&#x548c;Qwen-VL&#x7b49;&#x6a21;&#x578b;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x4f7f;&#x7528;POPE&#x3001;CHAIR&#x3001;MME&#x7b49;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x3002;ONLY&#x5728;POPE&#x4e0a;&#x6bd4;&#x73b0;&#x6709;&#x6700;&#x4f73;&#x65b9;&#x6cd5;&#x63d0;&#x5347;3.14%&#xff0c;&#x5728;CHAIR&#x4e0a;&#x63d0;&#x5347;1.6%&#xff0c;&#x540c;&#x65f6;&#x63a8;&#x7406;&#x65f6;&#x95f4;&#x4ec5;&#x589e;&#x52a0;0.07&#x500d;&#xff08;&#x8fdc;&#x4f4e;&#x4e8e;&#x5176;&#x4ed6;&#x65b9;&#x6cd5;&#x7684;2&#x500d;&#x4ee5;&#x4e0a;&#xff09;&#xff0c;GPU&#x5185;&#x5b58;&#x5f00;&#x9500;&#x53ef;&#x5ffd;&#x7565;&#x4e0d;&#x8ba1;&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x663e;&#x793a;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x66f4;&#x51c6;&#x786e;&#x4e14;&#x8fde;&#x8d2f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1269,1270"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ONLY&#x901a;&#x8fc7;&#x6781;&#x4f4e;&#x7684;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5355;&#x5c42;&#x5e72;&#x9884;&#x548c;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x589e;&#x5f3a;&#x7684;&#x53ef;&#x884c;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x5b9e;&#x65f6;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x673a;&#x5668;&#x4eba;&#x4ea4;&#x4e92;&#xff09;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x6548;&#x53ef;&#x9760;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x540c;&#x65f6;&#x4e3a;&#x672a;&#x6765;&#x8f7b;&#x91cf;&#x7ea7;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x7684;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1270,1272"}}],"payload":{"tag":"li","lines":"1266,1272","fold":1}}],"payload":{"tag":"h4","lines":"1264,1265"}},{"content":"IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x751f;&#x6210;&#x957f;&#x6587;&#x672c;&#x65f6;&#xff0c;&#x5bf9;&#x56fe;&#x50cf;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x4f1a;&#x9010;&#x6e10;&#x4e0b;&#x964d;&#xff08;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x9000;&#x5316;&#xff09;&#xff0c;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x589e;&#x52a0;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;IKOD&#xff0c;&#x901a;&#x8fc7;&#x5408;&#x5e76;&#x77ed;&#x5e8f;&#x5217;&#x7684;&#x9ad8;&#x6ce8;&#x610f;&#x529b;Key-Value&#x7f13;&#x5b58;&#x6765;&#x7f13;&#x89e3;&#x6b64;&#x95ee;&#x9898;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1273,1274"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x957f;&#x5e8f;&#x5217;&#x65f6;&#x51fa;&#x73b0;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff09;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x975e;&#x5e38;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;LVLM&#x5728;&#x91d1;&#x878d;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x51b3;&#x7b56;&#x51c6;&#x786e;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1275,1276"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;IKOD&#xff08;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x5f15;&#x5bfc;&#x7684;&#x952e;&#x503c;&#x5408;&#x5e76;&#x534f;&#x4f5c;&#x89e3;&#x7801;&#xff09;&#x7b56;&#x7565;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6838;&#x5fc3;&#x662f;&#xff1a;1&#xff09;&#x89c2;&#x5bdf;&#x5230;&#x957f;&#x5e8f;&#x5217;&#x751f;&#x6210;&#x65f6;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x4f1a;&#x9000;&#x5316;&#xff1b;2&#xff09;&#x901a;&#x8fc7;&#x538b;&#x7f29;KV Cache&#xff0c;&#x4ece;&#x8f83;&#x77ed;&#x7684;&#x3001;&#x5177;&#x6709;&#x9ad8;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x7684;&#x5e8f;&#x5217;&#x4e2d;&#x83b7;&#x53d6;logits&#xff1b;3&#xff09;&#x5c06;&#x8fd9;&#x4e9b;logits&#x4e0e;&#x539f;&#x59cb;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x7684;logits&#x5408;&#x5e76;&#xff0c;&#x751f;&#x6210;&#x66f4;&#x4e13;&#x6ce8;&#x4e8e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x3002;&#x6b64;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x662f;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x7684;&#x89e3;&#x7801;&#x65f6;&#x4f18;&#x5316;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"1276,1277"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x5728;LLaVA-1.5&#x548c;InstructBLIP&#x7b49;&#x6a21;&#x578b;&#x4e0a;&#x5747;&#x89c2;&#x5bdf;&#x5230;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x968f;&#x5e8f;&#x5217;&#x53d8;&#x957f;&#x800c;&#x9000;&#x5316;&#x7684;&#x73b0;&#x8c61;&#xff0c;&#x4e14;&#x4e0e;&#x5e7b;&#x89c9;&#x589e;&#x52a0;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff1b;2&#xff09;IKOD&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x548c;&#x7efc;&#x5408;&#x80fd;&#x529b;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x90fd;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x80fd;&#x529b;&#xff1b;3&#xff09;&#x8be5;&#x65b9;&#x6cd5;&#x51e0;&#x4e4e;&#x6ca1;&#x6709;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x6210;&#x672c;&#xff0c;&#x4e14;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;LVLM&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1277,1278"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x9000;&#x5316;&#x662f;&#x5bfc;&#x81f4;LVLM&#x4ea7;&#x751f;&#x957f;&#x5e8f;&#x5217;&#x5e7b;&#x89c9;&#x7684;&#x4e00;&#x4e2a;&#x5173;&#x952e;&#x56e0;&#x7d20;&#x3002;&#x6240;&#x63d0;&#x51fa;&#x7684;IKOD&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x6ce8;&#x610f;&#x529b;&#x9000;&#x5316;&#x95ee;&#x9898;&#xff0c;&#x4ece;&#x800c;&#x6291;&#x5236;&#x4e86;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x9002;&#x7528;&#x6027;&#x5f3a;&#x7684;&#x7279;&#x70b9;&#xff0c;&#x4f7f;&#x5176;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#xff0c;&#x80fd;&#x589e;&#x5f3a;LVLM&#x5728;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1278,1281"}}],"payload":{"tag":"li","lines":"1274,1281","fold":1}}],"payload":{"tag":"h4","lines":"1272,1273"}},{"content":"D-LEAF: Localizing and Correcting Hallucinations in Multimodal LLMs via Layer-to-head Attention Diagnostics","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;D-LEAF&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x52a8;&#x6001;&#x5b9a;&#x4f4d;&#x548c;&#x4fee;&#x6b63;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x5c42;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x71b5;&#xff08;LIAE&#xff09;&#x548c;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x805a;&#x7126;&#xff08;IAF&#xff09;&#x4e24;&#x79cd;&#x8bca;&#x65ad;&#x5de5;&#x5177;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x7cbe;&#x51c6;&#x5b9a;&#x4f4d;&#x95ee;&#x9898;&#x5c42;&#x548c;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5e76;&#x9009;&#x62e9;&#x6027;&#x4fee;&#x6b63;&#xff0c;&#x4ece;&#x800c;&#x5927;&#x5e45;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e0a;&#x5b9e;&#x73b0;53%&#x7684;&#x76f8;&#x5bf9;&#x6539;&#x8fdb;&#xff0c;&#x4e14;&#x4ec5;&#x5e26;&#x6765;8%&#x7684;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"1282,1283"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x5bf9;&#x6240;&#x6709;&#x5c42;&#x548c;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x8fdb;&#x884c;&#x7edf;&#x4e00;&#x8c03;&#x6574;&#xff0c;&#x65e0;&#x6cd5;&#x7cbe;&#x786e;&#x5b9a;&#x4f4d;&#x9519;&#x8bef;&#x6765;&#x6e90;&#xff0c;&#x4e14;&#x53ef;&#x80fd;&#x6291;&#x5236;&#x6b63;&#x5e38;&#x5de5;&#x4f5c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5bfc;&#x81f4;&#x4fee;&#x6b63;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x51c6;&#x786e;&#x6027;&#x548c;&#x4e8b;&#x5b9e;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1284,1285"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x201c;&#x5148;&#x5b9a;&#x4f4d;&#x540e;&#x4fee;&#x6b63;&#x201d;&#x7b56;&#x7565;&#xff0c;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#x4e24;&#x4e2a;&#x8bca;&#x65ad;&#x5de5;&#x5177;&#xff1a;1&#xff09;&#x5c42;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x71b5;&#xff08;LIAE&#xff09;&#xff1a;&#x52a8;&#x6001;&#x68c0;&#x6d4b;&#x6ce8;&#x610f;&#x529b;&#x71b5;&#x5f02;&#x5e38;&#x9ad8;&#x7684;&#x4e0d;&#x53ef;&#x9760;&#x5c42;&#xff1b;2&#xff09;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x805a;&#x7126;&#xff08;IAF&#xff09;&#xff1a;&#x5728;&#x95ee;&#x9898;&#x5c42;&#x5185;&#x8bc4;&#x5206;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x8bc6;&#x522b;&#x9700;&#x8981;&#x4fee;&#x6b63;&#x7684;&#x4f4e;&#x805a;&#x7126;&#x5934;&#x3002;&#x57fa;&#x4e8e;&#x8fd9;&#x4e9b;&#x4fe1;&#x53f7;&#xff0c;&#x63d0;&#x51fa;&#x52a8;&#x6001;&#x5c42;&#x95f4;&#x71b5;&#x4e0e;&#x6ce8;&#x610f;&#x529b;&#x878d;&#x5408;&#xff08;D-LEAF&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x4ec5;&#x5bf9;&#x5b9a;&#x4f4d;&#x5230;&#x7684;&#x95ee;&#x9898;&#x5934;&#x6ce8;&#x5165;&#x878d;&#x5408;&#x6821;&#x6b63;&#x4fe1;&#x53f7;&#xff0c;&#x907f;&#x514d;&#x5168;&#x5c40;&#x6291;&#x5236;&#xff0c;&#x5b9e;&#x73b0;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x4fee;&#x6b63;&#x3002;","children":[],"payload":{"tag":"li","lines":"1285,1286"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x4e09;&#x79cd;&#x4e3b;&#x6d41;MLLM&#x67b6;&#x6784;&#x548c;&#x4e09;&#x4e2a;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x5bf9;&#x6bd4;&#x516d;&#x79cd;&#x5148;&#x8fdb;&#x65b9;&#x6cd5;&#x3002;D-LEAF&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#xff1a;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e0a;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;53%&#xff08;&#x76f8;&#x5bf9;&#x6539;&#x8fdb;&#xff09;&#xff0c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x51c6;&#x786e;&#x7387;&#x548c;F1&#x5206;&#x6570;&#x5747;&#x63d0;&#x5347;&#x7ea6;4%&#x3002;&#x540c;&#x65f6;&#xff0c;&#x4ec5;&#x5bfc;&#x81f4;8%&#x7684;&#x541e;&#x5410;&#x91cf;&#x4e0b;&#x964d;&#xff0c;&#x5728;&#x4e8b;&#x5b9e;&#x53ef;&#x9760;&#x6027;&#x3001;&#x63cf;&#x8ff0;&#x7ec6;&#x8282;&#x548c;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#x95f4;&#x8fbe;&#x5230;&#x6700;&#x4f18;&#x5e73;&#x8861;&#x3002;","children":[],"payload":{"tag":"li","lines":"1286,1287"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: D-LEAF&#x901a;&#x8fc7;&#x7cbe;&#x7ec6;&#x5b9a;&#x4f4d;&#x548c;&#x9009;&#x62e9;&#x6027;&#x4fee;&#x6b63;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;MLLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5c42;&#x95f4;&#x548c;&#x5934;&#x95f4;&#x5dee;&#x5f02;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5de5;&#x5177;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x4f4e;&#xff0c;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x9ad8;&#x6548;&#x7684;&#x591a;&#x6a21;&#x6001;&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x5b9e;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;MLLM&#x5728;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1287,1289"}}],"payload":{"tag":"li","lines":"1283,1289","fold":1}}],"payload":{"tag":"h4","lines":"1281,1282"}},{"content":"ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ICT&#x65b9;&#x6cd5;&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x5e72;&#x9884;&#x6280;&#x672f;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x9636;&#x6bb5;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x589e;&#x5f3a;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5bf9;&#x6574;&#x4f53;&#x56fe;&#x50cf;&#x548c;&#x7ec6;&#x7c92;&#x5ea6;&#x7269;&#x4f53;&#x7ec6;&#x8282;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x4e14;&#x4e0d;&#x635f;&#x5931;&#x6709;&#x7528;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1290,1291"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x7406;&#x89e3;&#x548c;&#x54cd;&#x5e94;&#x590d;&#x6742;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x4e0a;&#x4e0b;&#x6587;&#x65b9;&#x9762;&#x53d6;&#x5f97;&#x7a81;&#x7834;&#xff0c;&#x4f46;&#x5176;&#x56fa;&#x6709;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x6216;&#x5c5e;&#x6027;&#xff09;&#x9650;&#x5236;&#x4e86;&#x5728;&#x9ad8;&#x7cbe;&#x5ea6;&#x8981;&#x6c42;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x624b;&#x672f;&#xff09;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x9700;&#x8981;&#x6602;&#x8d35;&#x7684;&#x6570;&#x636e;&#x5fae;&#x8c03;&#xff0c;&#x8981;&#x4e48;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x5904;&#x7406;&#xff0c;&#x53ef;&#x80fd;&#x6d88;&#x9664;&#x6709;&#x76ca;&#x7684;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x5e76;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x5ef6;&#x8fdf;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x5e73;&#x8861;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x4fe1;&#x606f;&#x7684;&#x4f7f;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1292,1293"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;Image-Object Cross-Level Trusted Intervention (ICT)&#x65b9;&#x6cd5;&#x3002;&#x9996;&#x5148;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x6b63;&#x786e;&#x4e0e;&#x9519;&#x8bef;&#x54cd;&#x5e94;&#x65f6;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x7684;&#x6fc0;&#x6d3b;&#x6a21;&#x5f0f;&#xff0c;&#x8bc6;&#x522b;&#x51fa;&#x7f16;&#x7801;&#x6574;&#x4f53;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x548c;&#x9ad8;&#x7ec6;&#x7c92;&#x5ea6;&#x7269;&#x4f53;&#x7ec6;&#x8282;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x3002;&#x7136;&#x540e;&#xff0c;&#x4e3a;&#x6bcf;&#x4e2a;&#x5934;&#x8bad;&#x7ec3;&#x4e8c;&#x5143;&#x5206;&#x7c7b;&#x5668;&#x4ee5;&#x786e;&#x5b9a;&#x5176;&#x529f;&#x80fd;&#x7c7b;&#x578b;&#x3002;&#x5728;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x9636;&#x6bb5;&#xff0c;&#x6839;&#x636e;&#x5934;&#x7684;&#x7c7b;&#x578b;&#x9884;&#x8ba1;&#x7b97;&#x5e72;&#x9884;&#x65b9;&#x5411;&#x5411;&#x91cf;&#xff0c;&#x8c03;&#x6574;&#x8fd9;&#x4e9b;&#x5934;&#x7684;&#x6fc0;&#x6d3b;&#x503c;&#xff0c;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x76f8;&#x5173;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x51cf;&#x5c11;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x6709;&#x76ca;&#x7684;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x63a8;&#x7406;&#x5ef6;&#x8fdf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1293,1294"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;ICT&#x5728;LLaVA-v1.5&#x548c;Qwen-VL&#x6a21;&#x578b;&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff1a;&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#x5e73;&#x5747;&#x63d0;&#x5347;6.27%&#xff0c;&#x5728;MME&#x57fa;&#x51c6;&#x4e0a;&#x63d0;&#x5347;67.37&#x5206;&#x3002;&#x6b64;&#x5916;&#xff0c;ICT&#x8868;&#x73b0;&#x51fa;&#x826f;&#x597d;&#x7684;&#x8de8;&#x6570;&#x636e;&#x96c6;&#x548c;&#x8de8;&#x6a21;&#x578b;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#xff0c;&#x4ec5;&#x9700;&#x5c11;&#x91cf;&#x6570;&#x636e;&#x5373;&#x53ef;&#x5b9e;&#x73b0;&#x5f3a;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1294,1295"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ICT&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x524d;&#x5411;&#x4f20;&#x64ad;&#x9636;&#x6bb5;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#xff0c;&#x5728;&#x4e0d;&#x6d88;&#x9664;&#x6709;&#x76ca;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x524d;&#x63d0;&#x4e0b;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x5173;&#x6ce8;&#x3002;&#x5176;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x548c;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x7279;&#x6027;&#x4f7f;&#x5176;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#xff0c;&#x5e76;&#x4e3a;LVLMs&#x5728;&#x73b0;&#x5b9e;&#x9ad8;&#x98ce;&#x9669;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x548c;&#x63a7;&#x5236;&#x673a;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"1295,1297"}}],"payload":{"tag":"li","lines":"1291,1297","fold":1}}],"payload":{"tag":"h4","lines":"1289,1290"}},{"content":"UAC/DAC: Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#xff08;UAC&#x548c;DAC&#xff09;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;UAC&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x901a;&#x8fc7;&#x5355;&#x5f20;&#x65e0;&#x610f;&#x4e49;&#x56fe;&#x50cf;&#x4f30;&#x8ba1;&#x6ce8;&#x610f;&#x529b;&#x504f;&#x5dee;&#x5e76;&#x8fdb;&#x884c;&#x6821;&#x51c6;&#xff1b;DAC&#x901a;&#x8fc7;&#x53ef;&#x5b66;&#x4e60;&#x7684;&#x63d2;&#x4ef6;&#x6a21;&#x5757;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#x8fd9;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#x80fd;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1298,1299"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#xff0c;&#x964d;&#x4f4e;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#x3002;","children":[],"payload":{"tag":"li","lines":"1300,1301"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e24;&#x79cd;&#x6ce8;&#x610f;&#x529b;&#x6821;&#x51c6;&#x65b9;&#x6cd5;&#xff1a;1) &#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x5747;&#x5300;&#x6ce8;&#x610f;&#x529b;&#x6821;&#x51c6;&#xff08;UAC&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x4e00;&#x5f20;&#x65e0;&#x610f;&#x4e49;&#x7684;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#xff08;&#x5982;&#x7a7a;&#x767d;&#x56fe;&#x50cf;&#xff09;&#x4f30;&#x8ba1;&#x6a21;&#x578b;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x504f;&#x5dee;&#xff0c;&#x5e76;&#x5e94;&#x7528;&#x4e00;&#x4e2a;&#x6821;&#x51c6;&#x77e9;&#x9635;&#x6765;&#x4fee;&#x6b63;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x7684;&#x4e0d;&#x5e73;&#x8861;&#x3002;2) &#x52a8;&#x6001;&#x6ce8;&#x610f;&#x529b;&#x6821;&#x51c6;&#xff08;DAC&#xff09;&#xff1a;&#x4e00;&#x4e2a;&#x53ef;&#x5b66;&#x4e60;&#x7684;&#x5373;&#x63d2;&#x5373;&#x7528;&#x6a21;&#x5757;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x9f13;&#x52b1;&#x6a21;&#x578b;&#x5728;&#x4e0d;&#x540c;&#x7269;&#x4f53;&#x4f4d;&#x7f6e;&#x4e0b;&#x4ea7;&#x751f;&#x4e00;&#x81f4;&#x8f93;&#x51fa;&#xff0c;&#x4ece;&#x800c;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x3002;","children":[],"payload":{"tag":"li","lines":"1301,1302"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;MME&#x548c;LLaVA-Bench&#xff09;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;UAC&#x548c;DAC&#x80fd;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x9ad8;&#x6574;&#x4f53;&#x611f;&#x77e5;&#x80fd;&#x529b;&#x3002;&#x8fd9;&#x4e9b;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x79cd;LVLM&#x67b6;&#x6784;&#xff08;&#x5982;LLaVA-1.5&#x3001;mPLUG-Owl2&#x548c;LLaVA-NeXT&#xff09;&#x4e0a;&#x5747;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x5347;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1302,1303"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x6821;&#x51c6;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x504f;&#x5dee;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x5e76;&#x589e;&#x5f3a;LVLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;UAC&#x548c;DAC&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff0c;&#x80fd;&#x9002;&#x5e94;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x9ad8;LVLMs&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x5e76;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1303,1304"}}],"payload":{"tag":"li","lines":"1299,1304","fold":1}}],"payload":{"tag":"h4","lines":"1297,1298"}}],"payload":{"tag":"h3","lines":"1110,1111","fold":1}},{"content":"attention sinks","children":[{"content":"EAH: Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;EAH&#xff0c;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x6d45;&#x5c42;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x4e2d;&#x7684;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x6ce8;&#x610f;&#x529b;&#x6c47;&#x805a;&#xff08;vision sink&#xff09;&#x5bc6;&#x5ea6;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x4e3b;&#x6d41;&#x6a21;&#x578b;&#x4e0a;&#x9a8c;&#x8bc1;&#x6709;&#x6548;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"1307,1308"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#xff09;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x4fee;&#x6539;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3001;&#x5f15;&#x5165;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#x6216;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x6548;&#x7387;&#x4f4e;&#x3002;&#x672c;&#x6587;&#x53d1;&#x73b0;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x6c47;&#x805a;&#x6a21;&#x5f0f;&#x4e0e;&#x5e7b;&#x89c9;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff0c;&#x4f46;&#x8fd9;&#x4e00;&#x5173;&#x7cfb;&#x5c1a;&#x672a;&#x88ab;&#x5145;&#x5206;&#x63a2;&#x7d22;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x901a;&#x7528;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1309,1310"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;EAH&#xff08;Enhancing Attention Heads&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x6b65;&#x9aa4;&#x5982;&#x4e0b;&#xff1a;1. &#x5206;&#x6790;&#x6a21;&#x578b;&#x6d45;&#x5c42;&#xff08;&#x7b2c;1-2&#x5c42;&#xff09;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x4e2d;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x77e9;&#x9635;&#xff0c;&#x8bc6;&#x522b;&#x51fa;&#x5177;&#x6709;&#x9ad8;&#x5bc6;&#x5ea6;&#x6ce8;&#x610f;&#x529b;&#x6c47;&#x805a;&#xff08;dense vision sink&#xff09;&#x7684;&#x5934;&#xff1b;2. &#x5c06;&#x8be5;&#x5934;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x77e9;&#x9635;&#x5e7f;&#x64ad;&#xff08;broadcast&#xff09;&#x5230;&#x540c;&#x5c42;&#x7684;&#x5176;&#x4ed6;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5f3a;&#x5236;&#x6574;&#x4e2a;&#x5c42;&#x805a;&#x7126;&#x4e8e;&#x56fe;&#x50cf;&#x672c;&#x8eab;&#xff1b;3. &#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x5373;&#x63d2;&#x5373;&#x7528;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x4e0d;&#x540c;&#x67b6;&#x6784;&#x7684;MLLMs&#x3002;","children":[],"payload":{"tag":"li","lines":"1310,1311"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x57fa;&#x4e8e;LLaVA-1.5&#x3001;MiniGPT-4&#x3001;MiniGemini&#x548c;Intern-VL&#x7b49;&#x6a21;&#x578b;&#xff0c;&#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x8fdb;&#x884c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;VQA&#x4efb;&#x52a1;&#xff1a;1. &#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#x4e0e;&#x6d45;&#x5c42;&#x7a00;&#x758f;&#x6ce8;&#x610f;&#x529b;&#x6c47;&#x805a;&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff08;&#x5e7b;&#x89c9;&#x65f6;&#x5bc6;&#x96c6;&#x5934;&#x6bd4;&#x4f8b;&#x4f4e;&#x81f3;10%&#xff0c;&#x975e;&#x5e7b;&#x89c9;&#x65f6;&#x53ef;&#x8fbe;30%&#xff09;&#xff1b;2. EAH&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;CHAIRI&#x7b49;&#x6307;&#x6807;&#x6d4b;&#x5f97;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5728;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x548c;&#x4efb;&#x52a1;&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x4e00;&#x81f4;&#xff1b;3. &#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x504f;&#x5ea6;&#xff08;skewness&#xff09;&#x4e0e;&#x5e7b;&#x89c9;&#x7387;&#x8d1f;&#x76f8;&#x5173;&#xff0c;EAH&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x504f;&#x5ea6;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1311,1312"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#xff1a;1. &#x6d45;&#x5c42;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x7684;&#x5bc6;&#x96c6;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x6c47;&#x805a;&#x662f;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#xff1b;2. EAH&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x63d2;&#x4ef6;&#x5f0f;&#x65b9;&#x6cd5;&#xff0c;&#x5177;&#x6709;&#x5f3a;&#x901a;&#x7528;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#xff1b;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#xff1a;&#x4e3a;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x89c6;&#x89d2;&#x548c;&#x4f4e;&#x6210;&#x672c;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x6a21;&#x578b;&#x4f18;&#x5316;&#x548c;&#x5b9e;&#x65f6;&#x63a8;&#x7406;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1312,1314"}}],"payload":{"tag":"li","lines":"1308,1314","fold":1}}],"payload":{"tag":"h4","lines":"1306,1307"}},{"content":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: OPERA&#x662f;&#x4e00;&#x79cd;&#x65b0;&#x9896;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x8fc7;&#x4fe1;&#x4efb;&#x60e9;&#x7f5a;&#x548c;&#x56de;&#x6eaf;&#x91cd;&#x5206;&#x914d;&#x7b56;&#x7565;&#xff0c;&#x5728;&#x4e0d;&#x589e;&#x52a0;&#x6570;&#x636e;&#x3001;&#x77e5;&#x8bc6;&#x6216;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x6a21;&#x578b;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1315,1316"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x4ea7;&#x751f;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x6216;&#x4e0d;&#x51c6;&#x786e;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x963b;&#x788d;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x4f7f;&#x7528;&#xff0c;&#x56e0;&#x4e3a;&#x9519;&#x8bef;&#x7684;&#x5224;&#x65ad;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x989d;&#x5916;&#x7684;&#x6570;&#x636e;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x652f;&#x6301;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x8bba;&#x6587;&#x65e8;&#x5728;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6210;&#x672c;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x901a;&#x8fc7;&#x6539;&#x8fdb;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x6765;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1317,1318"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: OPERA&#x57fa;&#x4e8e;&#x5bf9;&#x81ea;&#x6ce8;&#x610f;&#x529b;&#x77e9;&#x9635;&#x4e2d;&#x77e5;&#x8bc6;&#x805a;&#x5408;&#x6a21;&#x5f0f;&#x7684;&#x89c2;&#x5bdf;&#xff0c;&#x53d1;&#x73b0;&#x5e7b;&#x89c9;&#x5f80;&#x5f80;&#x4e0e;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x5c11;&#x6570;&#x201c;&#x6458;&#x8981;&#x4ee4;&#x724c;&#x201d;&#xff08;&#x5982;&#x6807;&#x70b9;&#x7b26;&#x53f7;&#xff09;&#x800c;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x6709;&#x5173;&#x3002;&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#xff1a;1&#xff09;&#x8fc7;&#x4fe1;&#x4efb;&#x60e9;&#x7f5a;&#xff1a;&#x5728;&#x675f;&#x641c;&#x7d22;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x5bf9;&#x6a21;&#x578b;logits&#x5f15;&#x5165;&#x60e9;&#x7f5a;&#x9879;&#xff0c;&#x964d;&#x4f4e;&#x51fa;&#x73b0;&#x8fc7;&#x5ea6;&#x805a;&#x5408;&#x6a21;&#x5f0f;&#x7684;&#x5019;&#x9009;&#x4ee4;&#x724c;&#x7684;&#x4f18;&#x5148;&#x7ea7;&#xff1b;2&#xff09;&#x56de;&#x6eaf;&#x91cd;&#x5206;&#x914d;&#xff1a;&#x5f53;&#x68c0;&#x6d4b;&#x5230;&#x8fc7;&#x5ea6;&#x805a;&#x5408;&#x6a21;&#x5f0f;&#x65f6;&#xff0c;&#x56de;&#x6eaf;&#x5230;&#x6458;&#x8981;&#x4ee4;&#x724c;&#x7684;&#x4f4d;&#x7f6e;&#xff0c;&#x91cd;&#x65b0;&#x9009;&#x62e9;&#x66f4;&#x4f18;&#x7684;&#x5019;&#x9009;&#x4ee4;&#x724c;&#x4ee5;&#x907f;&#x514d;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1318,1319"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;OPERA&#x5728;&#x4e0d;&#x540c;MLLM&#x6a21;&#x578b;&#xff08;&#x5982;InstructBLIP&#x3001;LLaVA-1.5&#xff09;&#x548c;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR&#x5206;&#x6570;&#xff09;&#x4e0a;&#x5747;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;GPT-4/GPT-4V&#x7684;&#x8bc4;&#x4f30;&#x4e5f;&#x8bc1;&#x5b9e;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#xff0c;&#x4e14;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6210;&#x672c;&#x5373;&#x53ef;&#x5b9e;&#x73b0;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"1319,1320"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: OPERA&#x63ed;&#x793a;&#x4e86;MLLM&#x5e7b;&#x89c9;&#x4e0e;&#x81ea;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#x7684;&#x5185;&#x5728;&#x8054;&#x7cfb;&#xff0c;&#x5e76;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x4f4e;&#x6210;&#x672c;&#x3001;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x7801;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x901a;&#x7528;&#x6027;&#x548c;&#x6709;&#x6548;&#x6027;&#x4e3a;&#x672a;&#x6765;MLLM&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x5224;&#x65ad;&#x7684;&#x573a;&#x666f;&#x4e2d;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1320,1322"}}],"payload":{"tag":"li","lines":"1316,1322","fold":1}}],"payload":{"tag":"h4","lines":"1314,1315"}},{"content":"DAMRO: Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8bba;&#x6587;&#x63d0;&#x51fa;DAMRO&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;LVLM&#x4e2d;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;LLM&#x89e3;&#x7801;&#x5668;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x53d1;&#x73b0;&#x80cc;&#x666f;&#x4e2d;&#x7684;&#x5f02;&#x5e38;token&#x4f1a;&#x5bfc;&#x81f4;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x5229;&#x7528;ViT&#x7684;[CLS] token&#x8fc7;&#x6ee4;&#x5f02;&#x5e38;token&#x5e76;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x6291;&#x5236;&#x5176;&#x5f71;&#x54cd;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;LLaVA&#x7b49;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1323,1324"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x4e0e;&#x56fe;&#x50cf;&#x771f;&#x5b9e;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x5173;&#x6ce8;&#x6574;&#x4f53;&#x67b6;&#x6784;&#x6216;&#x7279;&#x5b9a;&#x6a21;&#x5757;&#x6539;&#x8fdb;&#xff0c;&#x4f46;&#x5ffd;&#x7565;&#x4e86;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff08;ViT&#xff09;&#x672c;&#x8eab;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x7f3a;&#x9677;&#x2014;&#x2014;&#x5176;&#x4e0e;LLM&#x89e3;&#x7801;&#x5668;&#x5747;&#x8fc7;&#x5ea6;&#x5173;&#x6ce8;&#x80cc;&#x666f;&#x4e2d;&#x7684;&#x5f02;&#x5e38;token&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x5ffd;&#x7565;&#x5c40;&#x90e8;&#x7269;&#x4f53;&#x4fe1;&#x606f;&#x4ece;&#x800c;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1325,1326"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x5206;&#x6790;&#x53d1;&#x73b0;ViT&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x548c;LLM&#x89e3;&#x7801;&#x5668;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x9ad8;&#x5ea6;&#x4e00;&#x81f4;&#xff0c;&#x5747;&#x805a;&#x7126;&#x4e8e;&#x80cc;&#x666f;&#x4e2d;&#x7684;&#x9ad8;&#x8303;&#x6570;&#x5f02;&#x5e38;token&#xff1b;2. &#x63d0;&#x51fa;DAMRO&#x65b9;&#x6cd5;&#xff1a;&#x5229;&#x7528;ViT&#x7684;[CLS] token&#x4f5c;&#x4e3a;&#x67e5;&#x8be2;&#x5411;&#x91cf;&#xff0c;&#x901a;&#x8fc7;&#x6ce8;&#x610f;&#x529b;&#x8ba1;&#x7b97;&#x7b5b;&#x9009;&#x51fa;top-K&#x5f02;&#x5e38;token&#x4f5c;&#x4e3a;&#x8d1f;&#x6837;&#x672c;&#xff1b;3. &#x5728;LLM&#x89e3;&#x7801;&#x9636;&#x6bb5;&#x91c7;&#x7528;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;contrastive decoding&#xff09;&#xff0c;&#x4ece;&#x6b63;&#x5e38;&#x751f;&#x6210;&#x6982;&#x7387;&#x5206;&#x5e03;&#x4e2d;&#x51cf;&#x53bb;&#x5f02;&#x5e38;token&#x5f15;&#x5bfc;&#x7684;&#x8d1f;&#x5206;&#x5e03;&#xff0c;&#x6291;&#x5236;&#x5176;&#x5bf9;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x7684;&#x5f71;&#x54cd;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1326,1327"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x5b9a;&#x91cf;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;&#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x65f6;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e0e;&#x89e3;&#x7801;&#x5668;&#x7684;top token&#x91cd;&#x53e0;&#x7387;&#xff08;Hi&#x6307;&#x6807;&#xff09;&#x66f4;&#x9ad8;&#xff08;&#x5bf9;&#x8c61;&#x7ea7;0.0605 vs 0.0551&#xff09;&#xff1b;2. &#x5728;POPE&#x3001;CHAIR&#x3001;MME&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;DAMRO&#x5728;LLaVA-1.5&#x3001;LLaVA-NeXT&#x548c;InstructBLIP&#x4e0a;&#x5747;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;&#xff1b;3. &#x5bf9;&#x6bd4;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;VCD&#x548c;M3ID&#xff0c;DAMRO&#x5728;&#x6574;&#x4f53;&#x6548;&#x679c;&#x548c;&#x6cdb;&#x5316;&#x6027;&#x4e0a;&#x8868;&#x73b0;&#x66f4;&#x4f18;&#x3002;","children":[],"payload":{"tag":"li","lines":"1327,1328"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ViT&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x7f3a;&#x9677;&#x662f;&#x5bfc;&#x81f4;LVLM&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x5185;&#x5728;&#x539f;&#x56e0;&#xff0c;&#x5176;&#x5f02;&#x5e38;token&#x4f1a;&#x8bef;&#x5bfc;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x3002;DAMRO&#x901a;&#x8fc7;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x6790;&#x548c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6709;&#x6548;&#x6291;&#x5236;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;LVLM&#x6a21;&#x578b;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x4e3a;&#x7406;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x673a;&#x5236;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x89c6;&#x89d2;&#xff0c;&#x5bf9;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1328,1332"}}],"payload":{"tag":"li","lines":"1324,1332","fold":1}}],"payload":{"tag":"h4","lines":"1322,1323"}},{"content":"Attention Hijackers: Detect and Disentangle Attention Hijacking in LVLMs for Hallucination Mitigation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e0d;&#x4ec5;&#x6e90;&#x4e8e;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x4e0d;&#x8db3;&#xff0c;&#x8fd8;&#x7531;&#x6307;&#x4ee4;&#x4ee4;&#x724c;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x5e72;&#x6270;&#x5f15;&#x8d77;&#xff08;&#x79f0;&#x4e3a;&#x201c;&#x6ce8;&#x610f;&#x529b;&#x52ab;&#x6301;&#x201d;&#xff09;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;AID&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x68c0;&#x6d4b;&#x5e76;&#x9694;&#x79bb;&#x201c;&#x6ce8;&#x610f;&#x529b;&#x52ab;&#x6301;&#x8005;&#x201d;&#x4ee4;&#x724c;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x79cd;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1333,1334"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x8f93;&#x51fa;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x95ee;&#x9898;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x5f52;&#x56e0;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x4ee4;&#x724c;&#x7684;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x4e0d;&#x8db3;&#xff0c;&#x4f46;&#x672c;&#x6587;&#x53d1;&#x73b0;&#x6307;&#x4ee4;&#x4ee4;&#x724c;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x4f1a;&#x6301;&#x7eed;&#x626d;&#x66f2;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#xff0c;&#x5c06;&#x6ce8;&#x610f;&#x529b;&#x5f15;&#x5411;&#x65e0;&#x5173;&#x56fe;&#x50cf;&#x533a;&#x57df;&#xff0c;&#x5bfc;&#x81f4;&#x9ad8;&#x8fbe;46.7%&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1335,1336"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;AID&#xff08;Attention Hijackers Detection and Disentanglement&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x7ec4;&#x4ef6;&#xff1a;1) <strong>&#x6ce8;&#x610f;&#x529b;&#x52ab;&#x6301;&#x8005;&#x68c0;&#x6d4b;</strong>&#xff1a;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x6307;&#x4ee4;&#x9a71;&#x52a8;&#x7684;&#x89c6;&#x89c9;&#x663e;&#x8457;&#x6027;&#xff08;instruction-driven visual salience&#xff09;&#xff0c;&#x8bc6;&#x522b;&#x5bf9;&#x540e;&#x7eed;&#x4ee4;&#x724c;&#x4ea7;&#x751f;&#x8fc7;&#x5ea6;&#x89c6;&#x89c9;&#x5f71;&#x54cd;&#x7684;&#x6307;&#x4ee4;&#x4ee4;&#x724c;&#xff08;&#x52ab;&#x6301;&#x8005;&#xff09;&#xff1b;2) <strong>&#x6ce8;&#x610f;&#x529b;&#x89e3;&#x79bb;</strong>&#xff1a;&#x5c4f;&#x853d;&#x5df2;&#x8bc6;&#x522b;&#x52ab;&#x6301;&#x8005;&#x7684;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x963b;&#x65ad;&#x5176;&#x5bf9;&#x540e;&#x7eed;&#x4ee4;&#x724c;&#x7684;&#x5e72;&#x6270;&#xff1b;3) <strong>&#x518d;&#x89e3;&#x79bb;</strong>&#xff1a;&#x91cd;&#x65b0;&#x5e73;&#x8861;&#x6307;&#x4ee4;&#x9a71;&#x52a8;&#x4e0e;&#x56fe;&#x50cf;&#x9a71;&#x52a8;&#x7684;&#x89c6;&#x89c9;&#x663e;&#x8457;&#x6027;&#xff0c;&#x907f;&#x514d;&#x8fc7;&#x5ea6;&#x5c4f;&#x853d;&#x5bfc;&#x81f4;&#x975e;&#x52ab;&#x6301;&#x4ee4;&#x724c;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e22;&#x5931;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#x3002;","children":[],"payload":{"tag":"li","lines":"1336,1337"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) &#x6ce8;&#x610f;&#x529b;&#x52ab;&#x6301;&#x662f;LVLM&#x5e7b;&#x89c9;&#x7684;&#x91cd;&#x8981;&#x6210;&#x56e0;&#xff0c;&#x6700;&#x9ad8;&#x5360;&#x6bd4;46.7%&#xff1b;2) AID&#x80fd;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x52ab;&#x6301;&#x8005;&#x4ee4;&#x724c;&#xff08;&#x5982;&#x6307;&#x4ee4;&#x4e2d;&#x7684;&#x201c;&#x63cf;&#x8ff0;&#x201d;&#x7b49;&#x89e6;&#x53d1;&#x8bcd;&#xff09;&#xff1b;3) &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;POPE&#x3001;CHAIR&#xff09;&#x4e0a;&#xff0c;AID&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x4e0d;&#x540c;LVLM&#xff08;&#x5982;BLIP&#x3001;LLaVA&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1337,1338"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#x6307;&#x4ee4;&#x4ee4;&#x724c;&#x5f15;&#x53d1;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x52ab;&#x6301;&#x662f;LVLM&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x673a;&#x5236;&#x4e4b;&#x4e00;&#x3002;AID&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x68c0;&#x6d4b;&#x548c;&#x89e3;&#x79bb;&#x52ab;&#x6301;&#x8005;&#xff0c;&#x6062;&#x590d;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff0c;&#x4e3a;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x63d0;&#x4f9b;&#x4e86;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b0;&#x8303;&#x5f0f;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6f5c;&#x529b;&#x5728;&#x4e8e;&#x53ef;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;LVLM&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x542f;&#x53d1;&#x5bf9;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5e72;&#x6270;&#x56e0;&#x7d20;&#x7684;&#x8fdb;&#x4e00;&#x6b65;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"1338,1340"}}],"payload":{"tag":"li","lines":"1334,1340","fold":1}}],"payload":{"tag":"h4","lines":"1332,1333"}},{"content":"TAI &amp; HAI: Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;VisFlow&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x8f7b;&#x91cf;&#x7ea7;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x53cc;&#x5c42;&#x7ea7;&#xff08;Token&#x7ea7;&#x548c;Head&#x7ea7;&#xff09;&#x6ce8;&#x610f;&#x529b;&#x8c03;&#x5236;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1341,1342"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5904;&#x7406;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x65f6;&#xff0c;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x2019;&#xff08;Visual Hallucination, VH&#xff09;&#xff0c;&#x5373;&#x751f;&#x6210;&#x81ea;&#x4fe1;&#x4f46;&#x4e0d;&#x51c6;&#x786e;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x9700;&#x8981;&#x6602;&#x8d35;&#x7684;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#xff0c;&#x8981;&#x4e48;&#x5f15;&#x5165;&#x989d;&#x5916;&#x5ef6;&#x8fdf;&#x548c;&#x590d;&#x6742;&#x5ea6;&#xff0c;&#x8981;&#x4e48;&#x4f9d;&#x8d56;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7b49;&#x4f4e;&#x6548;&#x7b56;&#x7565;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x76f4;&#x63a5;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1343,1344"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x540d;&#x4e3a;VisFlow&#x7684;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;","children":[{"content":"1. <strong>Token&#x7ea7;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#xff08;TAI&#xff09;</strong>&#xff1a;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x4e4b;&#x95f4;&#x7684;&#x5185;&#x90e8;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#xff0c;&#x8bc6;&#x522b;&#x51fa;&#x5173;&#x952e;&#x7684;&#x2018;&#x89c6;&#x89c9;&#x6c47;&#x805a;&#x4ee4;&#x724c;&#x2019;&#xff08;Visual Sink Token&#xff09;&#x548c;&#x2018;&#x663e;&#x8457;&#x4ee4;&#x724c;&#x2019;&#xff08;Salient Token&#xff09;&#x3002;TAI&#x9009;&#x62e9;&#x6027;&#x5730;&#x589e;&#x5f3a;&#x5bf9;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x6ce8;&#x610f;&#x529b;&#xff0c;&#x5e76;&#x7ea0;&#x6b63;&#x7531;&#x65cb;&#x8f6c;&#x4f4d;&#x7f6e;&#x7f16;&#x7801;&#xff08;RoPE&#xff09;&#x5f15;&#x5165;&#x7684;&#x4f4d;&#x7f6e;&#x504f;&#x5dee;&#x3002;","children":[],"payload":{"tag":"li","lines":"1345,1346","listIndex":1}},{"content":"2. <strong>Head&#x7ea7;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#xff08;HAI&#xff09;</strong>&#xff1a;&#x8bc6;&#x522b;&#x51fa;&#x4e0e;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff08;linguistic priors&#xff09;&#x8fc7;&#x5ea6;&#x76f8;&#x5173;&#x7684;&#x6587;&#x672c;&#x6ce8;&#x610f;&#x529b;&#x5934;&#xff0c;&#x5e76;&#x6291;&#x5236;&#x8fd9;&#x4e9b;&#x5934;&#x5bf9;&#x7cfb;&#x7edf;&#x63d0;&#x793a;&#x548c;&#x76f8;&#x90bb;&#x6587;&#x672c;&#x4ee4;&#x724c;&#x7684;&#x5f02;&#x5e38;&#x5173;&#x6ce8;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x6a21;&#x578b;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x76f4;&#x63a5;&#x8c03;&#x5236;&#x5176;&#x5185;&#x90e8;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x591a;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x6781;&#x5c0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1346,1347","listIndex":2}}],"payload":{"tag":"li","lines":"1344,1347"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x6a21;&#x578b;&#x548c;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;POPE&#xff09;&#x4e0a;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;","children":[{"content":"1. <strong>&#x6709;&#x6548;&#x6027;</strong>&#xff1a;VisFlow&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff0c;&#x5176;&#x6027;&#x80fd;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x7684;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x3001;&#x8f85;&#x52a9;&#x6a21;&#x5757;&#x548c;&#x89e3;&#x7801;&#x65f6;&#x5e72;&#x9884;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1348,1349","listIndex":1}},{"content":"2. <strong>&#x6548;&#x7387;</strong>&#xff1a;&#x7531;&#x4e8e;&#x76f4;&#x63a5;&#x4fee;&#x6539;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#xff0c;&#x907f;&#x514d;&#x4e86;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x6240;&#x9700;&#x7684;&#x591a;&#x8f6e;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff0c;VisFlow&#x7684;&#x63a8;&#x7406;&#x901f;&#x5ea6;&#xff08;Tokens Per Second&#xff09;&#x663e;&#x8457;&#x66f4;&#x5feb;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x6781;&#x5c0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1349,1350","listIndex":2}},{"content":"3. <strong>&#x6d1e;&#x5bdf;&#x9a8c;&#x8bc1;</strong>&#xff1a;&#x5b9e;&#x9a8c;&#x9a8c;&#x8bc1;&#x4e86;&#x2018;&#x5e76;&#x975e;&#x6240;&#x6709;&#x4ee4;&#x724c;&#x548c;&#x5934;&#x90fd;&#x540c;&#x7b49;&#x91cd;&#x8981;&#x2019;&#x7684;&#x6838;&#x5fc3;&#x6d1e;&#x5bdf;&#xff0c;&#x4f8b;&#x5982;&#xff0c;&#x786e;&#x8ba4;&#x4e86;&#x89c6;&#x89c9;&#x6c47;&#x805a;&#x4ee4;&#x724c;&#x5bf9;&#x5168;&#x5c40;&#x8bed;&#x4e49;&#x611f;&#x77e5;&#x7684;&#x5173;&#x952e;&#x4f5c;&#x7528;&#xff0c;&#x8fd9;&#x4e0e;&#x4e4b;&#x524d;&#x67d0;&#x4e9b;&#x65b9;&#x6cd5;&#x7684;&#x505a;&#x6cd5;&#x76f8;&#x53cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1350,1351","listIndex":3}}],"payload":{"tag":"li","lines":"1347,1351"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x7cbe;&#x7ec6;&#x5206;&#x6790;&#x5e76;&#x5e72;&#x9884;LVLM&#x5185;&#x90e8;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x53ef;&#x4ee5;&#x9ad8;&#x6548;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5730;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;VisFlow&#x7684;&#x6210;&#x529f;&#x8868;&#x660e;&#xff0c;&#x7406;&#x89e3;&#x5e76;&#x4fee;&#x6b63;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x7684;&#x4fe1;&#x606f;&#x6d41;&#x548c;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x5e03;&#x662f;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x95ee;&#x9898;&#x7684;&#x6709;&#x6548;&#x9014;&#x5f84;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5c24;&#x5176;&#x9002;&#x7528;&#x4e8e;&#x5bf9;&#x5ef6;&#x8fdf;&#x548c;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x654f;&#x611f;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x573a;&#x666f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1351,1353"}}],"payload":{"tag":"li","lines":"1342,1353","fold":1}}],"payload":{"tag":"h4","lines":"1340,1341"}}],"payload":{"tag":"h3","lines":"1304,1305","fold":1}},{"content":"agent","children":[{"content":"Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;Hydra&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x667a;&#x80fd;&#x4f53;&#x63a8;&#x7406;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x63a8;&#x7406;&#x548c;&#x8de8;&#x6a21;&#x578b;&#x9a8c;&#x8bc1;&#xff0c;&#x540c;&#x65f6;&#x63d0;&#x5347;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x7684;&#x6297;&#x5bf9;&#x6297;&#x653b;&#x51fb;&#x80fd;&#x529b;&#x548c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1356,1357"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5728;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#xff08;&#x5982;&#x56fd;&#x9632;&#x3001;&#x533b;&#x7597;&#xff09;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#xff0c;&#x9762;&#x4e34;&#x4e24;&#x5927;&#x5173;&#x952e;&#x6311;&#x6218;&#xff1a;1&#xff09;&#x5bf9;&#x6297;&#x6027;&#x653b;&#x51fb;&#x7684;&#x8106;&#x5f31;&#x6027;&#xff0c;&#x5373;&#x8f93;&#x5165;&#x88ab;&#x5fae;&#x5c0f;&#x6270;&#x52a8;&#x540e;&#x53ef;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x9519;&#x8bef;&#xff1b;2&#xff09;&#x5185;&#x5728;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x4f1a;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x9519;&#x8bef;&#x6216;&#x8bef;&#x5bfc;&#x6027;&#x4fe1;&#x606f;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x53ea;&#x5355;&#x72ec;&#x89e3;&#x51b3;&#x5176;&#x4e2d;&#x4e00;&#x4e2a;&#x95ee;&#x9898;&#xff0c;&#x7f3a;&#x4e4f;&#x4e00;&#x4e2a;&#x7edf;&#x4e00;&#x7684;&#x6846;&#x67b6;&#x6765;&#x540c;&#x65f6;&#x5e94;&#x5bf9;&#x8fd9;&#x4e24;&#x79cd;&#x5a01;&#x80c1;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;VLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1358,1359"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x540d;&#x4e3a;Hydra&#x7684;&#x667a;&#x80fd;&#x4f53;&#x63a8;&#x7406;&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x4e00;&#x4e2a;&#x57fa;&#x4e8e;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x7684;&#x667a;&#x80fd;&#x4f53;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x91c7;&#x7528;&#x201c;&#x884c;&#x52a8;-&#x6279;&#x5224;&#x5faa;&#x73af;&#x201d;&#xff08;Action-Critique Loop&#xff09;&#x6765;&#x52a8;&#x6001;&#x4f18;&#x5316;VLM&#x7684;&#x8f93;&#x51fa;&#xff1a;1&#xff09;<strong>&#x884c;&#x52a8;&#xff08;Action&#xff09;</strong>&#xff1a;&#x667a;&#x80fd;&#x4f53;&#x901a;&#x8fc7;&#x63d0;&#x793a;&#xff08;prompting&#xff09;&#x64cd;&#x7eb5;&#x4e00;&#x4e2a;&#x63d2;&#x4ef6;&#x5f0f;VLM&#xff0c;&#x5e76;&#x5229;&#x7528;&#x4e0a;&#x4e0b;&#x6587;&#x5b66;&#x4e60;&#xff08;ICL&#xff09;&#x548c;&#x601d;&#x7ef4;&#x94fe;&#xff08;CoT&#xff09;&#x6280;&#x672f;&#x8fdb;&#x884c;&#x521d;&#x6b65;&#x63a8;&#x7406;&#x3002;2&#xff09;<strong>&#x6279;&#x5224;&#xff08;Critique&#xff09;</strong>&#xff1a;&#x667a;&#x80fd;&#x4f53;&#x8c03;&#x7528;&#x4e00;&#x5957;&#x5305;&#x542b;&#x591a;&#x79cd;&#x89c6;&#x89c9;AI&#x6a21;&#x578b;&#xff08;&#x5982;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x5668;&#x3001;&#x5176;&#x4ed6;VLM&#xff09;&#x7684;&#x5de5;&#x5177;&#x5957;&#x4ef6;&#xff0c;&#x68c0;&#x7d22;&#x5e76;&#x4ea4;&#x53c9;&#x9a8c;&#x8bc1;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x4ee5;&#x8bc6;&#x522b;&#x8f93;&#x51fa;&#x4e2d;&#x7684;&#x4e0d;&#x4e00;&#x81f4;&#x6027;&#xff08;&#x5982;&#x5bf9;&#x6297;&#x6270;&#x52a8;&#x5bfc;&#x81f4;&#x7684;&#x9519;&#x8bef;&#x6216;&#x5e7b;&#x89c9;&#xff09;&#x3002;3&#xff09;** refinement&#xff08; refinement&#xff09;**&#xff1a;&#x57fa;&#x4e8e;&#x6279;&#x5224;&#x7ed3;&#x679c;&#xff0c;&#x667a;&#x80fd;&#x4f53;&#x8fed;&#x4ee3;&#x5730;&#x4fee;&#x6b63;&#x5176;&#x63a8;&#x7406;&#x548c;&#x8f93;&#x51fa;&#xff0c;&#x76f4;&#x5230;&#x8fbe;&#x6210;&#x4e00;&#x81f4;&#x4e14;&#x51c6;&#x786e;&#x7684;&#x7ed3;&#x679c;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x5373;&#x63d2;&#x5373;&#x7528;&#x5730;&#x589e;&#x5f3a;&#x73b0;&#x6709;VLM&#x3002;","children":[],"payload":{"tag":"li","lines":"1359,1360"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x8bba;&#x6587;&#x5728;&#x56db;&#x4e2a;&#x4e0d;&#x540c;&#x7684;VLM&#x3001;&#x4e09;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x548c;&#x4e24;&#x79cd;&#x5bf9;&#x6297;&#x653b;&#x51fb;&#x7b56;&#x7565;&#x4e0a;&#x8fdb;&#x884c;&#x4e86;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x3002;&#x5173;&#x952e;&#x7ed3;&#x679c;&#x663e;&#x793a;&#xff1a;1&#xff09;&#x5728;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x65b9;&#x9762;&#xff0c;Hydra&#x5728;&#x6240;&#x6709;&#x6d4b;&#x8bd5;&#x7684;VLM&#x4e0a;&#x5747;&#x4f18;&#x4e8e;&#x6700;&#x5148;&#x8fdb;&#x7684;&#xff08;SOTA&#xff09;&#x540e;&#x5904;&#x7406;&#x53bb;&#x5e7b;&#x89c9;&#x65b9;&#x6cd5;&#xff08;&#x5982;Woodpecker&#xff09;&#x3002;2&#xff09;&#x5728;&#x5bf9;&#x6297;&#x9c81;&#x68d2;&#x6027;&#x65b9;&#x9762;&#xff0c;&#x5373;&#x4f7f;&#x6ca1;&#x6709;&#x4f7f;&#x7528;&#x663e;&#x5f0f;&#x7684;&#x5bf9;&#x6297;&#x9632;&#x5fa1;&#x65b9;&#x6cd5;&#xff0c;Hydra&#x5728;&#x9762;&#x5bf9;&#x5bf9;&#x6297;&#x6027;&#x6270;&#x52a8;&#x8f93;&#x5165;&#x65f6;&#xff0c;&#x5176;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x8fdc;&#x5c0f;&#x4e8e;&#x57fa;&#x7ebf;VLM&#x548c;&#x5bf9;&#x6bd4;&#x65b9;&#x6cd5;&#xff0c;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x5f3a;&#x7684;&#x62b5;&#x6297;&#x529b;&#x3002;3&#xff09;Hydra&#x6210;&#x529f;&#x5730;&#x5c06;&#x5bf9;&#x6297;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x80fd;&#x529b;&#x7ed3;&#x5408;&#x5728;&#x4e00;&#x4e2a;&#x7edf;&#x4e00;&#x6846;&#x67b6;&#x4e2d;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x7b56;&#x7565;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1360,1361"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;Hydra&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x5176;&#x667a;&#x80fd;&#x4f53;&#x63a8;&#x7406;&#x8303;&#x5f0f;&#xff0c;&#x9996;&#x6b21;&#x4e3a;VLM&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x7edf;&#x4e00;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x4e14;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x80fd;&#x540c;&#x65f6;&#x6709;&#x6548;&#x5e94;&#x5bf9;&#x5bf9;&#x6297;&#x6027;&#x653b;&#x51fb;&#x548c;&#x5185;&#x5728;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x5f25;&#x5408;&#x4e86;VLM&#x5b89;&#x5168;&#x6027;&#x548c;&#x4e8b;&#x5b9e;&#x53ef;&#x9760;&#x6027;&#x4e4b;&#x95f4;&#x7684;&#x7814;&#x7a76;&#x7a7a;&#x767d;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x4e14;&#x5065;&#x58ee;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x94fa;&#x5e73;&#x4e86;&#x9053;&#x8def;&#xff0c;&#x5c24;&#x5176;&#x9002;&#x7528;&#x4e8e;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x7f51;&#x7edc;&#x5b89;&#x5168;&#x548c;&#x533b;&#x7597;AI&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x3002;","children":[],"payload":{"tag":"li","lines":"1361,1363"}}],"payload":{"tag":"li","lines":"1357,1363","fold":1}}],"payload":{"tag":"h4","lines":"1355,1356"}},{"content":"Toward Robust Hyper-Detailed Image Captioning: A Multiagent Approach and Dual Evaluation Metrics for Factuality and Coverage","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;CapMAS&#x7684;&#x591a;&#x667a;&#x80fd;&#x4f53;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;LLM&#x548c;MLLM&#x534f;&#x4f5c;&#x4fee;&#x6b63;&#x8d85;&#x8be6;&#x7ec6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x4e86;&#x540c;&#x65f6;&#x8bc4;&#x4f30;&#x4e8b;&#x5b9e;&#x6027;&#x548c;&#x8986;&#x76d6;&#x5ea6;&#x7684;&#x53cc;&#x6307;&#x6807;&#x6846;&#x67b6;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#x63cf;&#x8ff0;&#x7684;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x751a;&#x81f3;&#x6539;&#x8fdb;GPT-4V&#x751f;&#x6210;&#x7684;&#x7ed3;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1364,1365"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x8d85;&#x8be6;&#x7ec6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x4ea7;&#x751f;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5982;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#xff09;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;MLLMs&#x5728;&#x8f85;&#x52a9;&#x89c6;&#x969c;&#x4eba;&#x58eb;&#x7b49;&#x5b9e;&#x9645;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1366,1367"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x591a;&#x667a;&#x80fd;&#x4f53;&#x534f;&#x4f5c;&#x4fee;&#x6b63;&#x6d41;&#x7a0b;&#xff1a;LLM&#x5c06;&#x7ed9;&#x5b9a;&#x63cf;&#x8ff0;&#x5206;&#x89e3;&#x4e3a;&#x539f;&#x5b50;&#x547d;&#x9898; &#x2192; MLLM&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x9a8c;&#x8bc1;&#x6bcf;&#x4e2a;&#x547d;&#x9898;&#x7684;&#x771f;&#x5b9e;&#x6027; &#x2192; LLM&#x636e;&#x6b64;&#x4fee;&#x6b63;&#x63cf;&#x8ff0;&#x3002;2. &#x8bc4;&#x4f30;&#x6846;&#x67b6;&#xff1a;&#x63d0;&#x51fa;&#x57fa;&#x4e8e;GPT&#x7684;&#x4e8b;&#x5b9e;&#x6027;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#xff0c;&#x5e76;&#x6784;&#x5efa;&#x4eba;&#x673a;&#x534f;&#x4f5c;&#x7684;&#x8be6;&#x7ec6;VQA&#x6570;&#x636e;&#x96c6;&#x8bc4;&#x4f30;&#x63cf;&#x8ff0;&#x8986;&#x76d6;&#x5ea6;&#xff08;&#x4ec5;&#x901a;&#x8fc7;&#x63cf;&#x8ff0;&#x80fd;&#x5426;&#x51c6;&#x786e;&#x56de;&#x7b54;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x95ee;&#x9898;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1367,1368"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x73b0;&#x6709;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x65b9;&#x6cd5;&#xff08;&#x5982;Confidence&#x548c;Consistency&#xff09;&#x5bf9;&#x957f;&#x63cf;&#x8ff0;&#x672b;&#x5c3e;&#x7684;&#x5e7b;&#x89c9;&#x5931;&#x6548;&#xff1b;2. CapMAS&#x663e;&#x8457;&#x63d0;&#x5347;&#x63cf;&#x8ff0;&#x4e8b;&#x5b9e;&#x6027;&#xff08;&#x5305;&#x62ec;GPT-4V&#x751f;&#x6210;&#x7684;&#x7ed3;&#x679c;&#xff09;&#xff1b;3. &#x4f20;&#x7edf;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;BLEU/ROUGE&#x7b49;&#xff09;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x8bc4;&#x4f30;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#xff1b;4. MLLMs&#x5728;VQA&#x4efb;&#x52a1;&#x7684;&#x8868;&#x73b0;&#x4e0e;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x7684;&#x80fd;&#x529b;&#x65e0;&#x76f8;&#x5173;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1368,1369"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CapMAS&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x5177;&#x901a;&#x7528;&#x6027;&#xff1b;&#x63d0;&#x51fa;&#x7684;&#x53cc;&#x6307;&#x6807;&#x8bc4;&#x4f30;&#x6846;&#x67b6;&#x66f4;&#x7b26;&#x5408;&#x4eba;&#x7c7b;&#x5224;&#x65ad;&#xff1b;&#x63ed;&#x793a;&#x4e86;&#x5f53;&#x524d;VQA&#x4e2d;&#x5fc3;&#x5316;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x7684;&#x5c40;&#x9650;&#x6027;&#xff0c;&#x4e3a;&#x53ef;&#x9760;&#x591a;&#x6a21;&#x6001;&#x7cfb;&#x7edf;&#x5f00;&#x53d1;&#x63d0;&#x4f9b;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1369,1371"}}],"payload":{"tag":"li","lines":"1365,1371","fold":1}}],"payload":{"tag":"h4","lines":"1363,1364"}}],"payload":{"tag":"h3","lines":"1353,1354","fold":1}},{"content":"&#x4e2d;&#x95f4;&#x5c42;","children":[{"content":"EVA: Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;EVA&#x7684;&#x8bad;&#x7ec3;&#x65e0;&#x5173;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x95f4;&#x5c42;&#x4e2d;&#x89c6;&#x89c9;&#x4e8b;&#x5b9e;&#x4fe1;&#x606f;&#x6700;&#x4e30;&#x5bcc;&#x7684;&#x5c42;&#xff0c;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x591a;&#x6a21;&#x6001;&#x8f93;&#x5165;&#x548c;&#x7eaf;&#x6587;&#x672c;&#x8f93;&#x5165;&#x7684;&#x6982;&#x7387;&#x5206;&#x5e03;&#x5dee;&#x5f02;&#xff0c;&#x63d0;&#x53d6;&#x89c6;&#x89c9;&#x4e8b;&#x5b9e;&#x77e5;&#x8bc6;&#x5e76;&#x4fee;&#x6b63;&#x6700;&#x7ec8;&#x5c42;&#x7684;&#x8f93;&#x51fa;&#x903b;&#x8f91;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;MLLMs&#x7684;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1374,1375"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x5b58;&#x5728;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x4f1a;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x63cf;&#x8ff0;&#xff0c;&#x800c;&#x4ec5;&#x4f9d;&#x8d56;&#x5185;&#x90e8;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5728;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x548c;&#x4eba;&#x673a;&#x4ea4;&#x4e92;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x5148;&#x524d;&#x7814;&#x7a76;&#x53d1;&#x73b0;MLLMs&#x7684;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#xff08;&#x5c24;&#x5176;&#x662f;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x6587;&#x672c;&#x5148;&#x9a8c;&#xff09;&#x5728;&#x6df1;&#x5c42;&#x7f51;&#x7edc;&#x4e2d;&#x6291;&#x5236;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x4f46;&#x4e2d;&#x95f4;&#x5c42;&#x4e2d;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#x5982;&#x4f55;&#x6291;&#x5236;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x673a;&#x5236;&#x5c1a;&#x4e0d;&#x660e;&#x786e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1376,1377"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;EVA&#xff08;Decoding by Extracting Visual FActs&#xff09;&#x65b9;&#x6cd5;&#xff1a;1. &#x4f7f;&#x7528;&#x65e9;&#x9000;&#x6cd5;&#xff08;early-exit&#xff09;&#x83b7;&#x53d6;&#x6bcf;&#x4e2a;&#x4e2d;&#x95f4;&#x5c42;&#x5bf9;&#x539f;&#x59cb;&#x591a;&#x6a21;&#x6001;&#x8f93;&#x5165;&#x548c;&#x7eaf;&#x6587;&#x672c;&#x8f93;&#x5165;&#x7684;&#x4e0b;&#x4e00;&#x8bcd;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff1b;2. &#x901a;&#x8fc7;Jensen-Shannon&#xff08;JS&#xff09;&#x6563;&#x5ea6;&#x91cf;&#x5316;&#x4e24;&#x79cd;&#x5206;&#x5e03;&#x7684;&#x5dee;&#x5f02;&#xff0c;&#x9009;&#x62e9;JS&#x6563;&#x5ea6;&#x6700;&#x5927;&#x7684;&#x4e2d;&#x95f4;&#x5c42;&#xff08;&#x89c6;&#x89c9;&#x4e8b;&#x5b9e;&#x4fe1;&#x606f;&#x6700;&#x4e30;&#x5bcc;&#xff09;&#xff1b;3. &#x5bf9;&#x6bd4;&#x8be5;&#x5c42;&#x7684;&#x591a;&#x6a21;&#x6001;&#x8f93;&#x51fa;&#x548c;&#x7eaf;&#x6587;&#x672c;&#x8f93;&#x51fa;&#xff0c;&#x63d0;&#x53d6;&#x89c6;&#x89c9;&#x4e8b;&#x5b9e;&#x77e5;&#x8bc6;&#xff0c;&#x6309;&#x6bd4;&#x4f8b;&#x878d;&#x5165;&#x6700;&#x7ec8;&#x5c42;&#x7684;&#x8f93;&#x51fa;&#x903b;&#x8f91;&#x4e2d;&#xff0c;&#x4fee;&#x6b63;&#x5e7b;&#x89c9;&#x751f;&#x6210;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x517c;&#x5bb9;&#x8d2a;&#x5a6a;&#x641c;&#x7d22;&#x3001;&#x6838;&#x91c7;&#x6837;&#x548c;&#x675f;&#x641c;&#x7d22;&#x7b49;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x5e76;&#x53ef;&#x5e94;&#x7528;&#x4e8e;&#x4e0d;&#x540c;MLLMs&#x67b6;&#x6784;&#x3002;","children":[],"payload":{"tag":"li","lines":"1377,1378"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x5e7f;&#x6cdb;&#x4f7f;&#x7528;&#x7684;&#x57fa;&#x51c6;&#xff08;&#x5982;POPE&#x3001;MME&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;EVA&#x5728;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#xff1a;1. &#x5e7b;&#x89c9;&#x7387;&#x663e;&#x8457;&#x964d;&#x4f4e;&#xff1b;2. &#x4e0e;&#x56db;&#x79cd;&#x4ee3;&#x8868;&#x6027;MLLMs&#xff08;InstructBLIP&#x3001;MiniGPT-4&#x3001;LLaVA-1.5&#x548c;Qwen-VL&#xff09;&#x548c;&#x4e09;&#x79cd;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#xff0c;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x4f18;&#x8d8a;&#x6027;&#x80fd;&#xff1b;3. &#x9a8c;&#x8bc1;&#x4e86;&#x4e2d;&#x95f4;&#x5c42;JS&#x6563;&#x5ea6;&#x4e0e;&#x89c6;&#x89c9;&#x4e8b;&#x5b9e;&#x77e5;&#x8bc6;&#x6f14;&#x5316;&#x8d8b;&#x52bf;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1378,1379"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: EVA&#x901a;&#x8fc7;&#x4ece;&#x4e2d;&#x95f4;&#x5c42;&#x63d0;&#x53d6;&#x89c6;&#x89c9;&#x4e8b;&#x5b9e;&#x77e5;&#x8bc6;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6a21;&#x578b;&#x4fee;&#x6539;&#xff0c;&#x5177;&#x6709;&#x9ad8;&#x901a;&#x7528;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x3002;&#x8fd9;&#x4e00;&#x53d1;&#x73b0;&#x63ed;&#x793a;&#x4e86;&#x4e2d;&#x95f4;&#x5c42;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x6291;&#x5236;&#x7684;&#x673a;&#x5236;&#xff0c;&#x4e3a;&#x672a;&#x6765;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x4f18;&#x5316;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1379,1381"}}],"payload":{"tag":"li","lines":"1375,1381","fold":1}}],"payload":{"tag":"h4","lines":"1373,1374"}},{"content":"LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: LISA&#x662f;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x5c42;&#x8c03;&#x5236;&#x548c;&#x591a;&#x5c42;&#x878d;&#x5408;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x63d0;&#x5347;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x771f;&#x5b9e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1382,1383"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e2d;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x4f9d;&#x8d56;&#x6602;&#x8d35;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5dee;&#x7684;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#xff0c;&#x8981;&#x4e48;&#x5728;&#x89e3;&#x7801;&#x65f6;&#x672a;&#x80fd;&#x5145;&#x5206;&#x8003;&#x8651;Transformer&#x5c42;&#x4e4b;&#x95f4;&#x7684;&#x529f;&#x80fd;&#x5206;&#x5c42;&#x7279;&#x6027;&#xff0c;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;","children":[],"payload":{"tag":"li","lines":"1384,1385"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: LISA&#x7684;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#x57fa;&#x4e8e;&#x5bf9;Transformer&#x5c42;&#x529f;&#x80fd;&#x5206;&#x5c42;&#x7684;&#x89c2;&#x5bdf;&#xff1a;&#x6d45;&#x5c42;&#x8d1f;&#x8d23;&#x89c6;&#x89c9; grounding&#xff0c;&#x4e2d;&#x5c42;&#x7f16;&#x7801;&#x8bed;&#x4e49;&#xff0c;&#x6df1;&#x5c42;&#x6613;&#x653e;&#x5927;&#x865a;&#x5047;&#x4fe1;&#x53f7;&#x3002;&#x5b83;&#x91c7;&#x7528;&#x4e24;&#x79cd;&#x5173;&#x952e;&#x6280;&#x672f;&#xff1a;1&#xff09;&#x5206;&#x5c42;&#x9891;&#x8c31;&#x8c03;&#x5236;&#xff08;Layer-specific Spectral Modulation&#xff09;&#xff1a;&#x5728;&#x6df1;&#x5c42;&#x6291;&#x5236;&#x8fc7;&#x6fc0;&#x6d3b;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x4fe1;&#x53f7;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x6d45;&#x5c42;&#x7684;&#x5bf9;&#x9f50;&#x7ebf;&#x7d22;&#xff1b;2&#xff09;&#x57fa;&#x4e8e;&#x951a;&#x70b9;&#x7684;&#x591a;&#x5c42;logit&#x878d;&#x5408;&#xff08;Anchor-based Logit Fusion&#xff09;&#xff1a;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x4ee3;&#x8868;&#x6027;&#x5c42;&#xff08;&#x951a;&#x5c42;&#xff09;&#x5e76;&#x8fdb;&#x884c;token&#x7ea7;&#x522b;&#x7684;logit&#x878d;&#x5408;&#xff0c;&#x4ee5;&#x81ea;&#x9002;&#x5e94;&#x5730;&#x6574;&#x5408;&#x53ef;&#x9760;&#x4fe1;&#x606f;&#x3002;&#x6574;&#x4e2a;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x5d4c;&#x5165;&#x73b0;&#x6709;MLLMs&#xff08;&#x5982;Qwen2.5-VL&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1385,1386"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;CHAIR&#x3001;POPE&#x3001;AMBER&#x3001;MME&#xff09;&#x4e0a;&#xff0c;LISA&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#xff1a;CHAIR&#x6307;&#x6807;&#x4e0a;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x6700;&#x591a;&#x8fbe;53.6%&#xff0c;POPE F1&#x5206;&#x6570;&#x63d0;&#x5347;4.5%&#x3002;&#x5b9e;&#x9a8c;&#x8fd8;&#x8868;&#x660e;&#xff0c;LISA&#x5177;&#x6709;&#x826f;&#x597d;&#x7684;&#x6cdb;&#x5316;&#x6027;&#xff0c;&#x80fd;&#x517c;&#x5bb9;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x548c;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x4e14;&#x4e0d;&#x5f71;&#x54cd;&#x6b63;&#x5e38;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1386,1387"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: LISA&#x901a;&#x8fc7;&#x663e;&#x5f0f;&#x5229;&#x7528;Transformer&#x5c42;&#x7684;&#x529f;&#x80fd;&#x5206;&#x5c42;&#x7279;&#x6027;&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x5e7b;&#x89c9;&#x6291;&#x5236;&#x65b9;&#x6848;&#x3002;&#x5176;&#x7ed3;&#x8bba;&#x662f;&#xff1a;&#x6df1;&#x5ea6;&#x611f;&#x77e5;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x751f;&#x6210;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x67b6;&#x6784;&#x6216;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x672a;&#x6765;&#x57fa;&#x4e8e;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x7ed3;&#x6784;&#x7684;&#x89e3;&#x7801;&#x4f18;&#x5316;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;MLLMs&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x79ef;&#x6781;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1387,1389"}}],"payload":{"tag":"li","lines":"1383,1389","fold":1}}],"payload":{"tag":"h4","lines":"1381,1382"}},{"content":"SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;SAVER&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x98ce;&#x683c;&#x5316;&#x56fe;&#x50cf;&#x5bfc;&#x81f4;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4ea7;&#x751f;&#x66f4;&#x591a;&#x5e7b;&#x89c9;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5229;&#x7528;&#x65e9;&#x671f;&#x5c42;&#x7684;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x8f93;&#x51fa;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1390,1391"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5904;&#x7406;&#x98ce;&#x683c;&#x5316;&#x56fe;&#x50cf;&#xff08;&#x5982;&#x5361;&#x901a;&#x3001;&#x7d20;&#x63cf;&#x7b49;&#xff09;&#x65f6;&#x4ea7;&#x751f;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#x7684;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x98ce;&#x683c;&#x5316;&#x56fe;&#x50cf;&#x5728;&#x6e38;&#x620f;&#x573a;&#x666f;&#x7406;&#x89e3;&#x3001;&#x827a;&#x672f;&#x6559;&#x80b2;&#x3001;&#x533b;&#x5b66;&#x5206;&#x6790;&#x7b49;&#x5173;&#x952e;&#x573a;&#x666f;&#x4e2d;&#x626e;&#x6f14;&#x91cd;&#x8981;&#x89d2;&#x8272;&#xff0c;&#x5e7b;&#x89c9;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x7684;&#x5b89;&#x5168;&#x95ee;&#x9898;&#x548c;&#x516c;&#x4f17;&#x5371;&#x5bb3;&#xff0c;&#x9650;&#x5236;LVLM&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1392,1393"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x6784;&#x5efa;&#x4e86;&#x4e00;&#x4e2a;&#x5305;&#x542b;&#x539f;&#x59cb;&#x6444;&#x5f71;&#x56fe;&#x50cf;&#x53ca;&#x5176;&#x4e94;&#x79cd;&#x98ce;&#x683c;&#x5316;&#x7248;&#x672c;&#xff08;&#x5361;&#x901a;&#x3001;&#x6e38;&#x620f;&#x3001;&#x6d82;&#x9e26;&#x3001;&#x7ed8;&#x753b;&#x3001;&#x7d20;&#x63cf;&#xff09;&#x7684;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5e76;&#x6807;&#x6ce8;&#x4e86;&#x8be6;&#x7ec6;&#x7684;&#x6807;&#x9898;&#x6807;&#x7b7e;&#x3002;&#x7136;&#x540e;&#xff0c;&#x4ed6;&#x4eec;&#x63d0;&#x51fa;SAVER&#xff08;Style-Aware Visual Early Revision&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x3002;SAVER&#x901a;&#x8fc7;&#x5206;&#x6790;token&#x7ea7;&#x522b;&#x7684;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x6a21;&#x5f0f;&#xff0c;&#x53d1;&#x73b0;&#x65e9;&#x671f;&#x5c42;&#x7684;&#x89c6;&#x89c9;&#x8868;&#x793a;&#x6a21;&#x5f0f;&#x66f4;&#x96c6;&#x4e2d;&#x3001;&#x7f6e;&#x4fe1;&#x5ea6;&#x66f4;&#x9ad8;&#xff0c;&#x56e0;&#x6b64;&#x52a8;&#x6001;&#x9009;&#x62e9;&#x8fd9;&#x4e9b;&#x65e9;&#x671f;&#x5c42;&#x7684;&#x53cd;&#x9988;&#x6765;&#x4fee;&#x6b63;&#x6700;&#x7ec8;&#x8f93;&#x51fa;&#x7684;token logits&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1393,1394"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x98ce;&#x683c;&#x5316;&#x56fe;&#x50cf;&#x76f8;&#x6bd4;&#x539f;&#x59cb;&#x6444;&#x5f71;&#x56fe;&#x50cf;&#x663e;&#x8457;&#x589e;&#x52a0;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff1b;2&#xff09;&#x6b63;&#x786e;token&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x7684;&#x76f8;&#x5173;&#x6a21;&#x5f0f;&#x5728;&#x65e9;&#x671f;&#x5c42;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x5bc6;&#x96c6;&#x7684;&#x89c6;&#x89c9;&#x6fc0;&#x6d3b;&#xff1b;3&#xff09;SAVER&#x5728;&#x591a;&#x79cd;&#x6a21;&#x578b;&#x3001;&#x6570;&#x636e;&#x96c6;&#x548c;&#x4efb;&#x52a1;&#x4e0a; consistently &#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"1394,1395"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#x98ce;&#x683c;&#x5316;&#x56fe;&#x50cf;&#x4f1a;&#x663e;&#x8457;&#x52a0;&#x5267;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x800c;SAVER&#x901a;&#x8fc7;&#x5229;&#x7528;&#x65e9;&#x671f;&#x5c42;&#x7684;&#x89c6;&#x89c9;&#x53cd;&#x9988;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x8f93;&#x51fa;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x9ad8;LVLM&#x5728;&#x98ce;&#x683c;&#x5316;&#x56fe;&#x50cf;&#x5904;&#x7406;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4fc3;&#x8fdb;&#x5176;&#x5728;&#x827a;&#x672f;&#x3001;&#x6559;&#x80b2;&#x3001;&#x533b;&#x7597;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x66f4;&#x5b89;&#x5168;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1395,1397"}}],"payload":{"tag":"li","lines":"1391,1397","fold":1}}],"payload":{"tag":"h4","lines":"1389,1390"}},{"content":"LayerCD: Mitigating Hallucination in Multimodal LLMs with Layer Contrastive Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;LayerCD&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e2d;&#x6d45;&#x5c42;&#x548c;&#x6df1;&#x5c42;&#x7279;&#x5f81;&#x751f;&#x6210;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;(MLLM)&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1398,1399"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;(MLLM)&#x5728;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x95ee;&#x9898;&#xff0c;&#x4f8b;&#x5982;&#x9519;&#x8bef;&#x8bc6;&#x522b;&#x7269;&#x4f53;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x3002;&#x8fd9;&#x79cd;&#x5e7b;&#x89c9;&#x4f1a;&#x9650;&#x5236;&#x6a21;&#x578b;&#x5bf9;&#x56fe;&#x50cf;&#x7684;&#x51c6;&#x786e;&#x7406;&#x89e3;&#xff0c;&#x5f71;&#x54cd;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x90e8;&#x7f72;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1400,1401"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;Layer Contrastive Decoding (LayerCD)&#xff0c;&#x8fd9;&#x662f;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x6539;&#x53d8;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x7684;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x65b9;&#x6cd5;&#x3002;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x5229;&#x7528;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x4e2d;&#x4e0d;&#x540c;&#x5c42;&#x6b21;&#x7279;&#x5f81;&#xff08;&#x6d45;&#x5c42;&#x4e0e;&#x6df1;&#x5c42;&#xff09;&#x7684;&#x8f93;&#x51fa;&#x5206;&#x5e03;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#xff1a;&#x6d45;&#x5c42;&#x7279;&#x5f81;&#xff08;&#x5982;&#x7b2c;&#x4e00;&#x5c42;&#xff09;&#x4e3b;&#x8981;&#x5305;&#x542b;&#x8fb9;&#x7f18;&#x3001;&#x989c;&#x8272;&#x7b49;&#x4f4e;&#x9636;&#x4fe1;&#x606f;&#xff0c;&#x5bb9;&#x6613;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#xff1b;&#x6df1;&#x5c42;&#x7279;&#x5f81;&#xff08;&#x5982;&#x6700;&#x540e;&#x4e00;&#x5c42;&#xff09;&#x5305;&#x542b;&#x66f4;&#x591a;&#x8bed;&#x4e49;&#x4fe1;&#x606f;&#xff0c;&#x66f4;&#x53ef;&#x9760;&#x3002;LayerCD&#x901a;&#x8fc7;&#x516c;&#x5f0f;&#x3c3;[(1+&#x3b1;)f(x,zd) - &#x3b1;f(x,zs)]&#x8ba1;&#x7b97;&#x5bf9;&#x6bd4;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x5176;&#x4e2d;&#x3b1;&#x63a7;&#x5236;&#x5bf9;&#x6bd4;&#x5f3a;&#x5ea6;&#xff0c;zs&#x548c;zd&#x5206;&#x522b;&#x4ee3;&#x8868;&#x6d45;&#x5c42;&#x548c;&#x6df1;&#x5c42;&#x7279;&#x5f81;&#x3002;&#x540c;&#x65f6;&#x5f15;&#x5165;&#x81ea;&#x9002;&#x5e94;&#x5408;&#x7406;&#x6027;&#x7ea6;&#x675f;&#xff0c;&#x8fc7;&#x6ee4;&#x6389;&#x6df1;&#x5c42;&#x7279;&#x5f81;&#x4e2d;&#x4f4e;&#x7f6e;&#x4fe1;&#x5ea6;&#x7684;token&#xff0c;&#x786e;&#x4fdd;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x5408;&#x7406;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1401,1402"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;POPE&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5305;&#x542b;MSCOCO&#x3001;A-OKVQA&#x548c;GQA&#x6570;&#x636e;&#x96c6;&#xff09;&#x4e0a;&#xff0c;LayerCD&#x5728;&#x4e09;&#x79cd;&#x91c7;&#x6837;&#x8bbe;&#x7f6e;&#xff08;&#x968f;&#x673a;&#x3001;&#x9ad8;&#x9891;&#x548c;&#x5bf9;&#x6297;&#xff09;&#x4e0b;&#x5747;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x5e38;&#x89c4;&#x89e3;&#x7801;&#x548c;VCD&#x57fa;&#x7ebf;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;LLaVA1.5&#x6a21;&#x578b;&#x4e0a;&#xff0c;LayerCD&#x5728;&#x968f;&#x673a;&#x8bbe;&#x7f6e;&#x4e0b;&#x7684;F1&#x5206;&#x6570;&#x8fbe;&#x5230;83.93&#xff0c;&#x6bd4;&#x5e38;&#x89c4;&#x89e3;&#x7801;&#xff08;81.20&#xff09;&#x548c;VCD&#xff08;79.36&#xff09;&#x66f4;&#x9ad8;&#xff1b;&#x5728;Cambrian&#x548c;MoLmo&#x6a21;&#x578b;&#x4e0a;&#x4e5f;&#x89c2;&#x5bdf;&#x5230;&#x7c7b;&#x4f3c;&#x63d0;&#x5347;&#xff0c;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x5bf9;&#x4e0d;&#x540c;&#x67b6;&#x6784;&#x7684;MLLM&#x5747;&#x6709;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"1402,1403"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: LayerCD&#x901a;&#x8fc7;&#x5229;&#x7528;&#x6d45;&#x5c42;&#x548c;&#x6df1;&#x5c42;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7684;&#x5bf9;&#x6bd4;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;MLLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x6216;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;&#x4e3b;&#x8981;&#x5c40;&#x9650;&#x6027;&#x662f;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x8f83;&#x9ad8;&#xff08;&#x9700;&#x8981;&#x4e24;&#x6b21;&#x524d;&#x5411;&#x4f20;&#x64ad;&#xff09;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x4e13;&#x6ce8;&#x4e8e;&#x5f00;&#x53d1;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x5bf9;&#x6bd4;&#x6846;&#x67b6;&#xff0c;&#x4ee5;&#x9002;&#x7528;&#x4e8e;&#x8d85;&#x5927;&#x89c4;&#x6a21;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1403,1405"}}],"payload":{"tag":"li","lines":"1399,1405","fold":1}}],"payload":{"tag":"h4","lines":"1397,1398"}},{"content":"HELPD: Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;HELPD&#x7684;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x5c42;&#x53cd;&#x9988;&#x5b66;&#x4e60;&#x548c;&#x89c6;&#x89c9;&#x589e;&#x5f3a;&#x60e9;&#x7f5a;&#x89e3;&#x7801;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x5bf9;&#x8c61;&#x7ea7;&#x548c;&#x53e5;&#x5b50;&#x7ea7;&#x7684;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x53cd;&#x9988;&#xff0c;&#x5e76;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x589e;&#x5f3a;&#x56fe;&#x50cf;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x5b9e;&#x9a8c;&#x8bc1;&#x660e;&#x80fd;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1406,1407"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x4e0e;&#x56fe;&#x50cf;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff08;&#x5982;&#x751f;&#x6210;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4e13;&#x6ce8;&#x4e8e;&#x68c0;&#x6d4b;&#x5bf9;&#x8c61;&#x662f;&#x5426;&#x5b58;&#x5728;&#xff0c;&#x800c;&#x5ffd;&#x7565;&#x4e86;&#x5bf9;&#x8c61;&#x4e0e;&#x8bed;&#x4e49;&#x4e4b;&#x95f4;&#x7684;&#x5173;&#x8054;&#xff0c;&#x5bfc;&#x81f4;&#x8bef;&#x5224;&#x6216;&#x6548;&#x679c;&#x4e0d;&#x8db3;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x6a21;&#x578b;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x5df2;&#x751f;&#x6210;&#x6587;&#x672c;&#xff08;&#x2018;&#x8fc7;&#x5ea6;&#x4fe1;&#x4efb;&#x2019;&#xff09;&#xff0c;&#x800c;&#x5ffd;&#x89c6;&#x4e86;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x7684;&#x5f71;&#x54cd;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x5168;&#x9762;&#x7684;&#x65b9;&#x6cd5;&#x6765;&#x540c;&#x65f6;&#x5904;&#x7406;&#x5bf9;&#x8c61;&#x548c;&#x8bed;&#x4e49;&#x5c42;&#x9762;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x5e73;&#x8861;&#x6587;&#x672c;&#x4e0e;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x4f7f;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1408,1409"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;HELPD&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#xff1a;1. &#x5206;&#x5c42;&#x53cd;&#x9988;&#x5b66;&#x4e60;&#xff08;Hierarchical Feedback Learning&#xff09;&#xff1a;&#x5728;&#x8bad;&#x7ec3;&#x540e;&#x671f;&#x5f15;&#x5165;&#x5c11;&#x91cf;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x901a;&#x8fc7;&#x91c7;&#x6837;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x53e5;&#x5b50;&#x548c;&#x6807;&#x7b7e;&#x53e5;&#x5b50;&#xff0c;&#x5206;&#x522b;&#x63d0;&#x53d6;&#x5bf9;&#x8c61;&#x96c6;&#xff08;&#x4f7f;&#x7528;NLTK&#x5de5;&#x5177;&#xff09;&#x8ba1;&#x7b97;F1&#x5206;&#x6570;&#x4f5c;&#x4e3a;&#x5bf9;&#x8c61;&#x7ea7;&#x53cd;&#x9988;&#xff08;R_obj&#xff09;&#xff0c;&#x540c;&#x65f6;&#x5229;&#x7528;GPT-4&#x8fdb;&#x884c;&#x5c11;&#x6837;&#x672c;&#x63a8;&#x7406;&#xff0c;&#x5bf9;&#x6bd4;&#x53e5;&#x5b50;&#x8bed;&#x4e49;&#x5f97;&#x5230;&#x53e5;&#x5b50;&#x7ea7;&#x53cd;&#x9988;&#xff08;R_sen&#xff09;&#x3002;&#x8fd9;&#x4e24;&#x79cd;&#x53cd;&#x9988;&#x901a;&#x8fc7;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#xff08;REINFORCE&#x7b97;&#x6cd5;&#xff09;&#x6574;&#x5408;&#x5230;&#x8bad;&#x7ec3;&#x4e2d;&#x3002;2. &#x89c6;&#x89c9;&#x589e;&#x5f3a;&#x60e9;&#x7f5a;&#x89e3;&#x7801;&#xff08;Vision-enhanced Penalty Decoding&#xff09;&#xff1a;&#x5728;&#x89e3;&#x7801;&#x65f6;&#xff0c;&#x4e0d;&#x4ec5;&#x8003;&#x8651;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e2d;&#x7684;&#x2018;&#x8fc7;&#x5ea6;&#x4fe1;&#x4efb;&#x60e9;&#x7f5a;&#x2019;&#xff08;&#x57fa;&#x4e8e;&#x6587;&#x672c;&#x6ce8;&#x610f;&#x529b;&#x7684;&#x60e9;&#x7f5a;&#xff09;&#xff0c;&#x8fd8;&#x589e;&#x52a0;&#x4e86;&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x7a97;&#x53e3;&#x7684;&#x89c6;&#x89c9;&#x60e9;&#x7f5a;&#xff0c;&#x4f7f;&#x6700;&#x7ec8;&#x8f93;&#x51fa;&#x7684;logits&#x66f4;&#x504f;&#x5411;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#xff0c;&#x51cf;&#x5c11;&#x5bf9;&#x6587;&#x672c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x3002;","children":[],"payload":{"tag":"li","lines":"1409,1410"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;HELPD&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8868;&#x73b0;&#x4f18;&#x5f02;&#xff1a;1. &#x4ec5;&#x9700;&#x5c11;&#x91cf;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x51cf;&#x5c11;&#x8d85;&#x8fc7;15%&#x7684;&#x5e7b;&#x89c9;&#xff1b;2. &#x5728;&#x4e0d;&#x540c;LVLM&#x6a21;&#x578b;&#xff08;&#x5982;InstructBLIP&#x3001;mPLUG-Owl&#x7b49;&#xff09;&#x4e0a;&#x5747;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff1b;3. &#x540c;&#x65f6;&#x63d0;&#x9ad8;&#x4e86;&#x6587;&#x672c;&#x751f;&#x6210;&#x7684;&#x8d28;&#x91cf;&#xff1b;4. &#x6ce8;&#x610f;&#x529b;&#x53ef;&#x89c6;&#x5316;&#x663e;&#x793a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x6210;&#x529f;&#x589e;&#x5f3a;&#x4e86;&#x56fe;&#x50cf;&#x5728;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x5f71;&#x54cd;&#x529b;&#xff0c;&#x5e73;&#x8861;&#x4e86;&#x6587;&#x672c;&#x4e0e;&#x89c6;&#x89c9;&#x6a21;&#x6001;&#x3002;","children":[],"payload":{"tag":"li","lines":"1410,1411"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: HELPD&#x901a;&#x8fc7;&#x5206;&#x5c42;&#x53cd;&#x9988;&#x548c;&#x89c6;&#x89c9;&#x589e;&#x5f3a;&#x89e3;&#x7801;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;LVLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x5177;&#x6709;&#x901a;&#x7528;&#x6027;&#xff0c;&#x53ef;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;&#x4efb;&#x4f55;LVLM&#x4e2d;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x66f4;&#x7cbe;&#x7ec6;&#x7684;&#x8bed;&#x4e49;&#x5173;&#x8054;&#x548c;&#x8de8;&#x6a21;&#x6001;&#x5e73;&#x8861;&#x673a;&#x5236;&#xff0c;&#x63a8;&#x52a8;&#x53ef;&#x9760;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1411,1413"}}],"payload":{"tag":"li","lines":"1407,1413","fold":1}}],"payload":{"tag":"h4","lines":"1405,1406"}},{"content":"Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x5176;&#x4e2d;&#x95f4;&#x5c42;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5904;&#x7406;&#x4e0d;&#x5f53;&#x3002;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x53d1;&#x73b0;&#x771f;&#x5b9e;&#x5bf9;&#x8c61;&#x5728;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x9636;&#x6bb5;&#x83b7;&#x5f97;&#x66f4;&#x9ad8;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x800c;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#x5219;&#x56e0;&#x591a;&#x5934;&#x6ce8;&#x610f;&#x529b;&#x4e0d;&#x4e00;&#x81f4;&#x5bfc;&#x81f4;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x8c03;&#x6574;&#x6ce8;&#x610f;&#x529b;&#x4ee5;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1414,1415"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x63cf;&#x8ff0;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x53ef;&#x80fd;&#x5e26;&#x6765;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x5148;&#x524d;&#x7814;&#x7a76;&#x591a;&#x4ece;&#x8bed;&#x8a00;&#x504f;&#x5dee;&#x89d2;&#x5ea6;&#x5206;&#x6790;&#xff0c;&#x4f46;&#x5ffd;&#x89c6;&#x4e86;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5904;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x95ee;&#x9898;&#x3002;&#x672c;&#x7814;&#x7a76;&#x65e8;&#x5728;&#x6df1;&#x5165;&#x63a2;&#x7a76;LVLM&#x5904;&#x7406;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5185;&#x90e8;&#x673a;&#x5236;&#xff0c;&#x7279;&#x522b;&#x662f;&#x4e2d;&#x95f4;&#x5c42;&#x5982;&#x4f55;&#x5bfc;&#x81f4;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#xff0c;&#x4ee5;&#x586b;&#x8865;&#x8fd9;&#x4e00;&#x7814;&#x7a76;&#x7a7a;&#x767d;&#x3002;","children":[],"payload":{"tag":"li","lines":"1416,1417"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x91c7;&#x7528;&#x4e86;&#x4e00;&#x79cd;&#x591a;&#x89d2;&#x5ea6;&#x7684;&#x5206;&#x6790;&#x65b9;&#x6cd5;&#xff1a;1) &#x63d0;&#x51fa;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x6bd4;&#x7387;&#xff08;VAR&#xff09;&#x6307;&#x6807;&#xff0c;&#x91cf;&#x5316;&#x751f;&#x6210;token&#x4e0e;&#x56fe;&#x50cf;token&#x7684;&#x6ce8;&#x610f;&#x529b;&#x4ea4;&#x4e92;&#x7a0b;&#x5ea6;&#xff0c;&#x7528;&#x4ee5;&#x8bc6;&#x522b;&#x5173;&#x952e;&#x5904;&#x7406;&#x5c42;&#xff1b;2) &#x5229;&#x7528;Logit Lens&#x6280;&#x672f;&#x89e3;&#x7801;&#x56fe;&#x50cf;token&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff0c;&#x5c06;&#x5176;&#x6620;&#x5c04;&#x5230;&#x8bcd;&#x6c47;&#x7a7a;&#x95f4;&#x4ee5;&#x89e3;&#x91ca;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x8bed;&#x4e49;&#x7406;&#x89e3;&#xff1b;3) &#x901a;&#x8fc7;&#x53ef;&#x89c6;&#x5316;&#x591a;&#x5934;&#x6ce8;&#x610f;&#x529b;&#xff08;MHSA&#xff09;&#x7684;&#x70ed;&#x529b;&#x56fe;&#xff0c;&#x5206;&#x6790;&#x4e0d;&#x540c;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x7684;&#x884c;&#x4e3a;&#x6a21;&#x5f0f;&#x3002;&#x57fa;&#x4e8e;&#x8fd9;&#x4e9b;&#x5206;&#x6790;&#xff0c;&#x4ed6;&#x4eec;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x5e72;&#x9884;&#x65b9;&#x6cd5;&#xff1a;&#x5728;&#x5173;&#x952e;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x9636;&#x6bb5;&#xff0c;&#x6574;&#x5408;&#x591a;&#x4e2a;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x7684;&#x4fe1;&#x606f;&#x6765;&#x8c03;&#x6574;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff0c;&#x4ece;&#x800c;&#x6291;&#x5236;&#x5e7b;&#x89c9;token&#x7684;&#x751f;&#x6210;&#x3002;","children":[],"payload":{"tag":"li","lines":"1417,1418"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x53d1;&#x73b0;&#x5305;&#x62ec;&#xff1a;1) &#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5904;&#x7406;&#x4e3b;&#x8981;&#x96c6;&#x4e2d;&#x5728;&#x6a21;&#x578b;&#x7684;&#x4e2d;&#x95f4;&#x5c42;&#xff0c;&#x8be5;&#x5c42;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x5206;&#x4e3a;&#x2018;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x2019;&#x548c;&#x2018;&#x8bed;&#x4e49;&#x7cbe;&#x70bc;&#x2019;&#x4e24;&#x4e2a;&#x5b50;&#x9636;&#x6bb5;&#xff1b;2) &#x5728;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e30;&#x5bcc;&#x9636;&#x6bb5;&#xff0c;&#x771f;&#x5b9e;&#x5bf9;&#x8c61;token&#x83b7;&#x5f97;&#x7684;&#x56fe;&#x50cf;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#xff08;VAR&#xff09;&#x663e;&#x8457;&#x9ad8;&#x4e8e;&#x5e7b;&#x89c9;token&#xff1b;3) &#x5e7b;&#x89c9;token&#x7684;&#x4ea7;&#x751f;&#x5e38;&#x6e90;&#x4e8e;&#x591a;&#x5934;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x4e2d;&#x4e0d;&#x540c;&#x5934;&#x5173;&#x6ce8;&#x4e86;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x7269;&#x4f53;&#xff0c;&#x5bfc;&#x81f4;&#x4fe1;&#x606f;&#x6df7;&#x6dc6;&#x3002;&#x57fa;&#x4e8e;VAR&#x7684;&#x68c0;&#x6d4b;&#x65b9;&#x6cd5;&#x5728;LLaVA-1.5-7B&#x6a21;&#x578b;&#x4e0a;&#x8fbe;&#x5230;&#x4e86;74%&#x7684;AUROC&#x548c;88%&#x7684;mAP&#x3002;&#x6240;&#x63d0;&#x51fa;&#x7684;&#x5e72;&#x9884;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x4e3b;&#x6d41;LVLM&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x5e73;&#x5747;&#x4f7f;CHAIRI&#x548c;CHAIRS&#x6307;&#x6807;&#x5206;&#x522b;&#x4e0b;&#x964d;&#x4e86;6.3&#x548c;24.1&#x4e2a;&#x70b9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x63cf;&#x8ff0;&#x7684;&#x7ec6;&#x8282;&#x6c34;&#x5e73;&#x3002;","children":[],"payload":{"tag":"li","lines":"1418,1419"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;LVLM&#x7684;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x5e76;&#x975e;&#x4ec5;&#x6e90;&#x4e8e;&#x8bed;&#x8a00;&#x504f;&#x5dee;&#xff0c;&#x5176;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x5904;&#x7406;&#x4e2d;&#x95f4;&#x5c42;&#x7684;&#x4e0d;&#x5f53;&#x6ce8;&#x610f;&#x529b;&#x5206;&#x914d;&#x662f;&#x5173;&#x952e;&#x539f;&#x56e0;&#x3002;&#x901a;&#x8fc7;&#x7406;&#x89e3;&#x5e76;&#x5e72;&#x9884;&#x4e2d;&#x95f4;&#x5c42;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x68c0;&#x6d4b;&#x548c;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x9879;&#x7814;&#x7a76;&#x4e0d;&#x4ec5;&#x589e;&#x8fdb;&#x4e86;&#x5bf9;LVLM&#x5185;&#x90e8;&#x5de5;&#x4f5c;&#x673a;&#x5236;&#x7684;&#x7406;&#x89e3;&#xff0c;&#x800c;&#x4e14;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x3001;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x7684;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#xff0c;&#x5177;&#x6709;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#xff0c;&#x80fd;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1419,1421"}}],"payload":{"tag":"li","lines":"1415,1421","fold":1}}],"payload":{"tag":"h4","lines":"1413,1414"}},{"content":"CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;CHiP&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8de8;&#x6a21;&#x6001;&#x5206;&#x5c42;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff0c;&#x540c;&#x65f6;&#x5229;&#x7528;&#x6587;&#x672c;&#x548c;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1422,1423"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5982;LLaVA&#x548c;GPT-4V&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x5c3d;&#x7ba1;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x5728;&#x6587;&#x672c;&#x6a21;&#x6001;&#x4e2d;&#x6709;&#x6548;&#xff0c;&#x4f46;&#x76f4;&#x63a5;&#x6269;&#x5c55;&#x5230;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#x65f6;&#xff0c;&#x65e0;&#x6cd5;&#x5145;&#x5206;&#x5bf9;&#x9f50;&#x56fe;&#x50cf;&#x548c;&#x6587;&#x672c;&#x8868;&#x793a;&#xff0c;&#x4e5f;&#x96be;&#x4ee5;&#x533a;&#x5206;&#x5e7b;&#x89c9;&#x4e0e;&#x975e;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1424,1425"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;CHiP&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;1) &#x5206;&#x5c42;&#x6587;&#x672c;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff1a;&#x5728;&#x54cd;&#x5e94;&#x3001;&#x6bb5;&#x843d;&#x548c;&#x8bcd;&#x5143;&#x4e09;&#x4e2a;&#x7c92;&#x5ea6;&#x4e0a;&#x8ba1;&#x7b97;&#x504f;&#x597d;&#x5956;&#x52b1;&#xff0c;&#x7ec6;&#x5316;&#x5bf9;&#x9f50;&#x8fc7;&#x7a0b;&#xff1b;2) &#x89c6;&#x89c9;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff1a;&#x6784;&#x5efa;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x5bf9;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x9009;&#x62e9;&#xff09;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x80fd;&#x540c;&#x65f6;&#x4ece;&#x6587;&#x672c;&#x548c;&#x89c6;&#x89c9;&#x6a21;&#x6001;&#x5b66;&#x4e60;&#x504f;&#x597d;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;DPO&#x57fa;&#x7840;&#x4e0a;&#x5f15;&#x5165;&#x591a;&#x7c92;&#x5ea6;&#x5956;&#x52b1;&#x8ba1;&#x7b97;&#x548c;&#x8de8;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x673a;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"1425,1426"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;ObjHal&#x3001;MMHal&#x3001;AMBER&#xff09;&#x4e0a;&#xff0c;CHiP&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;&#x3002;&#x57fa;&#x4e8e;Muffin&#x548c;LLaVA&#x6a21;&#x578b;&#xff0c;&#x5728;ObjHal&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x76f8;&#x6bd4;DPO&#x5206;&#x522b;&#x76f8;&#x5bf9;&#x63d0;&#x5347;52.7%&#x548c;55.5%&#x3002;&#x53ef;&#x89c6;&#x5316;&#x5206;&#x6790;&#x663e;&#x793a;CHiP&#x80fd;&#x66f4;&#x597d;&#x5bf9;&#x9f50;&#x56fe;&#x50cf;&#x4e0e;&#x771f;&#x5b9e;&#x63cf;&#x8ff0;&#xff0c;&#x5e76;&#x589e;&#x5927;&#x4e0e;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#x7684;&#x8868;&#x5f81;&#x8ddd;&#x79bb;&#x3002;","children":[],"payload":{"tag":"li","lines":"1426,1427"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CHiP&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x8de8;&#x6a21;&#x6001;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x6709;&#x671b;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x3002;&#x4ee3;&#x7801;&#x548c;&#x6570;&#x636e;&#x96c6;&#x5df2;&#x5f00;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"1427,1429"}}],"payload":{"tag":"li","lines":"1423,1429","fold":1}}],"payload":{"tag":"h4","lines":"1421,1422"}},{"content":"DCLA: Mitigating Hallucinations via Inter-Layer ConsistencyAggregation in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;DCLA&#x7684;&#x65b0;&#x578b;&#x89e3;&#x7801;&#x673a;&#x5236;&#xff0c;&#x901a;&#x8fc7;&#x805a;&#x5408;Transformer&#x4e0d;&#x540c;&#x5c42;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x6765;&#x6784;&#x5efa;&#x52a8;&#x6001;&#x8bed;&#x4e49;&#x53c2;&#x8003;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x5fae;&#x8c03;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1430,1431"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x5176;&#x5728;&#x533b;&#x7597;&#x62a5;&#x544a;&#x751f;&#x6210;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x57fa;&#x4e8e;&#x5fae;&#x8c03;&#x6216;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#x7684;&#x65b9;&#x6cd5;&#xff09;&#x5b58;&#x5728;&#x6027;&#x80fd;&#x4e0d;&#x7a33;&#x5b9a;&#x3001;&#x5bf9;&#x8d85;&#x53c2;&#x6570;&#x654f;&#x611f;&#x6216;&#x8d44;&#x6e90;&#x6d88;&#x8017;&#x5927;&#x7b49;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x672a;&#x80fd;&#x5145;&#x5206;&#x8003;&#x8651;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x5c42;&#x95f4;&#x8bed;&#x4e49;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x52a8;&#x6001;&#x7279;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1432,1433"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;DCLA&#xff08;Decoding with Inter-layer Consistency via Layer Aggregation&#xff09;&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x901a;&#x8fc7;&#x52a0;&#x6743;&#x805a;&#x5408;&#x5f53;&#x524d;&#x5c42;&#x4e4b;&#x524d;&#x6240;&#x6709;&#x5c42;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff08;&#x5305;&#x62ec;&#x672a;&#x7ecf;&#x4fee;&#x6b63;&#x7684;&#x539f;&#x59cb;&#x72b6;&#x6001;&#x548c;&#x5df2;&#x4fee;&#x6b63;&#x7684;&#x72b6;&#x6001;&#xff09;&#xff0c;&#x6784;&#x5efa;&#x4e00;&#x4e2a;&#x52a8;&#x6001;&#x7684;&#x8bed;&#x4e49;&#x53c2;&#x8003;&#x3002;&#x8be5;&#x53c2;&#x8003;&#x7528;&#x4e8e;&#x4fee;&#x6b63;&#x540e;&#x7eed;&#x5c42;&#x4e2d;&#x8bed;&#x4e49;&#x504f;&#x79bb;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#xff0c;&#x4ece;&#x800c;&#x5f3a;&#x5236;&#x5c42;&#x95f4;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x52a0;&#x6743;&#x7b56;&#x7565;&#x91c7;&#x7528;&#x6307;&#x6570;&#x8870;&#x51cf;&#x65b9;&#x5f0f;&#xff0c;&#x66f4;&#x5173;&#x6ce8;&#x63a5;&#x8fd1;&#x5f53;&#x524d;&#x5c42;&#x7684;&#x5c42;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x5e95;&#x5c42;&#x8bed;&#x4e49;&#x7684;&#x7a33;&#x5b9a;&#x6027;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x5fae;&#x8c03;&#x6216;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#x3002;","children":[],"payload":{"tag":"li","lines":"1433,1434"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;MME&#x548c;POPE&#x7b49;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#xff0c;DCLA&#x5728;LLaVA1.5-7b&#x3001;LLaVA1.5-13b&#x3001;LLaVA-NEXT&#x548c;mPLUG-Owl2&#x7b49;&#x591a;&#x4e2a;LVLM&#x4e0a;&#x5747;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x5728;VizWiz&#x548c;MM-Vet&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x7ed3;&#x679c;&#x4e5f;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x66f4;&#x5e7f;&#x6cdb;&#x7684;&#x9002;&#x7528;&#x6027;&#xff0c;&#x4e0d;&#x4ec5;&#x9650;&#x4e8e;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x80fd;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x4e00;&#x822c;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1434,1435"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#x5f3a;&#x5236;&#x5c42;&#x95f4;&#x4e00;&#x81f4;&#x6027;&#x662f;&#x7f13;&#x89e3;LVLM&#x5e7b;&#x89c9;&#x7684;&#x6709;&#x6548;&#x9014;&#x5f84;&#x3002;DCLA&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x5177;&#x6709;&#x666e;&#x9002;&#x6027;&#x3001;&#x9ad8;&#x6548;&#x6027;&#x548c;&#x6613;&#x7528;&#x6027;&#xff0c;&#x4e3a;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5e76;&#x63ed;&#x793a;&#x4e86;&#x5c42;&#x95f4;&#x52a8;&#x6001;&#x4ea4;&#x4e92;&#x5bf9;&#x4e8e;&#x6a21;&#x578b;&#x7a33;&#x5b9a;&#x6027;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1435,1437"}}],"payload":{"tag":"li","lines":"1431,1437","fold":1}}],"payload":{"tag":"h4","lines":"1429,1430"}}],"payload":{"tag":"h3","lines":"1371,1372","fold":1}},{"content":"COT","children":[{"content":"KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: KAM-CoT&#x662f;&#x4e00;&#x4e2a;&#x7ed3;&#x5408;&#x4e86;&#x601d;&#x7ef4;&#x94fe;&#x3001;&#x77e5;&#x8bc6;&#x56fe;&#x8c31;&#x548c;&#x591a;&#x6a21;&#x6001;&#x6570;&#x636e;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x590d;&#x6742;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x8868;&#x73b0;&#x3002;&#x5b83;&#x5728;ScienceQA&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x4ee5;&#x4ec5;2.8&#x4ebf;&#x53c2;&#x6570;&#x5b9e;&#x73b0;&#x4e86;93.87%&#x7684;&#x51c6;&#x786e;&#x7387;&#xff0c;&#x663e;&#x8457;&#x8d85;&#x8d8a;GPT-3.5&#x548c;GPT-4&#xff0c;&#x4e14;&#x66f4;&#x9ad8;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"1440,1441"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5728;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x5b58;&#x5728;&#x7684;&#x4e24;&#x4e2a;&#x4e3b;&#x8981;&#x95ee;&#x9898;&#xff1a;1) &#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#xff0c;&#x9700;&#x8981;&#x5927;&#x91cf;&#x786c;&#x4ef6;&#x8d44;&#x6e90;&#xff1b;2) &#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#xff0c;&#x5373;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x5b9e;&#x9645;&#x9519;&#x8bef;&#x7684;&#x63a8;&#x7406;&#x548c;&#x7b54;&#x6848;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x53ef;&#x9760;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x662f;&#x8fc8;&#x5411;&#x901a;&#x7528;&#x4eba;&#x5de5;&#x667a;&#x80fd;&#x7684;&#x5173;&#x952e;&#x4e00;&#x6b65;&#xff0c;&#x5728;&#x6559;&#x80b2;&#x548c;&#x79d1;&#x5b66;&#x95ee;&#x7b54;&#x7b49;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1442,1443"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;KAM-CoT&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x4ee5;&#x4e0b;&#x65b9;&#x6cd5;&#x89e3;&#x51b3;&#x95ee;&#x9898;&#xff1a;1) <strong>&#x4e24;&#x9636;&#x6bb5;&#x8bad;&#x7ec3;</strong>&#xff1a;&#x9996;&#x5148;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x751f;&#x6210;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff08;Rationale&#xff09;&#xff0c;&#x7136;&#x540e;&#x5229;&#x7528;&#x8be5;&#x4f9d;&#x636e;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7b54;&#x6848;&#x3002;2) <strong>&#x591a;&#x6a21;&#x6001;&#x878d;&#x5408;</strong>&#xff1a;&#x6a21;&#x578b;&#x5305;&#x542b;&#x8bed;&#x8a00;&#x7f16;&#x7801;&#x5668;&#x3001;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x548c;&#x56fe;&#x795e;&#x7ecf;&#x7f51;&#x7edc;&#xff08;GNN&#xff09;&#xff0c;&#x5206;&#x522b;&#x5904;&#x7406;&#x6587;&#x672c;&#x3001;&#x56fe;&#x50cf;&#x548c;&#x4ece;&#x77e5;&#x8bc6;&#x56fe;&#x8c31;&#xff08;&#x5982;ConceptNet&#xff09;&#x4e2d;&#x63d0;&#x53d6;&#x7684;&#x5b50;&#x56fe;&#x4e09;&#x5143;&#x7ec4;&#x3002;3) <strong>&#x77e5;&#x8bc6;&#x589e;&#x5f3a;</strong>&#xff1a;&#x5229;&#x7528;&#x4ea4;&#x53c9;&#x6ce8;&#x610f;&#x529b;&#xff08;Cross-Attention&#xff09;&#x673a;&#x5236;&#x5c06;&#x6587;&#x672c;&#x3001;&#x56fe;&#x50cf;&#x548c;&#x77e5;&#x8bc6;&#x56fe;&#x8c31;&#x7684;&#x7279;&#x5f81;&#x8fdb;&#x884c;&#x6df1;&#x5ea6;&#x878d;&#x5408;&#xff0c;&#x4e3a;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x652f;&#x6491;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;4) <strong>&#x53c2;&#x6570;&#x9ad8;&#x6548;</strong>&#xff1a;&#x6574;&#x4e2a;&#x6a21;&#x578b;&#x4ec5;&#x9700;&#x8bad;&#x7ec3;2.8&#x4ebf;&#x53c2;&#x6570;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x9636;&#x6bb5;&#x548c;&#x6a21;&#x5757;&#x5316;&#x8bbe;&#x8ba1;&#x964d;&#x4f4e;&#x4e86;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1443,1444"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;ScienceQA&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5982;&#x4e0b;&#xff1a;1) <strong>&#x51c6;&#x786e;&#x7387;</strong>&#xff1a;&#x5e73;&#x5747;&#x8fbe;&#x5230;93.87%&#xff0c;&#x8fdc;&#x8d85;GPT-3.5&#xff08;75.17%&#xff09;18&#x4e2a;&#x767e;&#x5206;&#x70b9;&#x548c;GPT-4&#xff08;83.99%&#xff09;10&#x4e2a;&#x767e;&#x5206;&#x70b9;&#x3002;2) <strong>&#x6548;&#x7387;</strong>&#xff1a;&#x4ec5;&#x4f7f;&#x7528;280M&#xff08;2.8&#x4ebf;&#xff09;&#x53ef;&#x8bad;&#x7ec3;&#x53c2;&#x6570;&#x5c31;&#x5b9e;&#x73b0;&#x4e86;&#x4e0a;&#x8ff0;&#x6027;&#x80fd;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x5353;&#x8d8a;&#x7684;&#x6210;&#x672c;&#x6548;&#x76ca;&#x3002;3) <strong>&#x6d88;&#x878d;&#x5b9e;&#x9a8c;</strong>&#xff1a;&#x9a8c;&#x8bc1;&#x4e86;&#x77e5;&#x8bc6;&#x56fe;&#x8c31;&#x878d;&#x5408;&#x548c;&#x4e24;&#x9636;&#x6bb5;&#x63a8;&#x7406;&#x7b56;&#x7565;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3001;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x4f5c;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1444,1445"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x5c06;&#x77e5;&#x8bc6;&#x56fe;&#x8c31;&#x4e0e;&#x591a;&#x6a21;&#x6001;&#x601d;&#x7ef4;&#x94fe;&#x63a8;&#x7406;&#x76f8;&#x7ed3;&#x5408;&#xff0c;KAM-CoT&#x6846;&#x67b6;&#x80fd;&#x591f;&#x4ee5;&#x66f4;&#x5c0f;&#x7684;&#x6a21;&#x578b;&#x89c4;&#x6a21;&#x5b9e;&#x73b0;&#x66f4;&#x51c6;&#x786e;&#x3001;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x63a8;&#x7406;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#xff1a;1) &#x4e3a;&#x6784;&#x5efa;&#x9ad8;&#x6548;&#x3001;&#x8f7b;&#x91cf;&#x5316;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#xff0c;&#x964d;&#x4f4e;&#x4e86;&#x90e8;&#x7f72;&#x6210;&#x672c;&#xff1b;2) &#x77e5;&#x8bc6;&#x589e;&#x5f3a;&#x7684;&#x673a;&#x5236;&#x4e3a;&#x51cf;&#x5c11;&#x5927;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff1b;3) &#x5728;&#x79d1;&#x5b66;&#x6559;&#x80b2;&#x3001;&#x667a;&#x80fd;&#x95ee;&#x7b54;&#x7b49;&#x9886;&#x57df;&#x5177;&#x6709;&#x76f4;&#x63a5;&#x7684;&#x5e94;&#x7528;&#x524d;&#x666f;&#x3002;&#x672a;&#x6765;&#x5de5;&#x4f5c;&#x53ef;&#x63a2;&#x7d22;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x77e5;&#x8bc6;&#x96c6;&#x6210;&#x65b9;&#x6cd5;&#x548c;&#x66f4;&#x5e7f;&#x6cdb;&#x7684;&#x6a21;&#x6001;&#x652f;&#x6301;&#x3002;","children":[],"payload":{"tag":"li","lines":"1445,1447"}}],"payload":{"tag":"li","lines":"1441,1447","fold":1}}],"payload":{"tag":"h4","lines":"1439,1440"}},{"content":"Thinking Before Looking: Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x94fe;&#xff08;VIC&#xff09;&#x7684;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x2018;&#x5148;&#x601d;&#x8003;&#x518d;&#x770b;&#x56fe;&#x2019;&#xff08;thinking before looking&#xff09;&#x7684;&#x8303;&#x5f0f;&#xff0c;&#x5728;&#x5f15;&#x5165;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x524d;&#x5148;&#x4ec5;&#x7528;&#x6587;&#x672c;&#x4e0a;&#x4e0b;&#x6587;&#x6784;&#x5efa;&#x63a8;&#x7406;&#x94fe;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1448,1449"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x6574;&#x5408;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#x65b9;&#x9762;&#x53d6;&#x5f97;&#x4e86;&#x8fdb;&#x5c55;&#xff0c;&#x4f46;&#x5728;&#x8fdb;&#x884c;&#x591a;&#x6a21;&#x6001;&#x601d;&#x7ef4;&#x94fe;&#xff08;CoT&#xff09;&#x63a8;&#x7406;&#x65f6;&#xff0c;&#x7531;&#x4e8e;&#x91c7;&#x7528;&#x2018;&#x8fb9;&#x770b;&#x8fb9;&#x601d;&#x8003;&#x2019;&#xff08;thinking while looking&#xff09;&#x7684;&#x65b9;&#x5f0f;&#xff0c;&#x5373;&#x5728;&#x751f;&#x6210;&#x63a8;&#x7406;&#x94fe;&#x7684;&#x540c;&#x65f6;&#x5904;&#x7406;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#xff0c;&#x5bb9;&#x6613;&#x56e0;&#x8bef;&#x5bfc;&#x6027;&#x56fe;&#x50cf;&#x800c;&#x4ea7;&#x751f;&#x4e25;&#x91cd;&#x7684;&#x8de8;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#xff0c;&#x4f8b;&#x5982;&#x751f;&#x6210;&#x4e0d;&#x5b58;&#x5728;&#x5bf9;&#x8c61;&#x6216;&#x9519;&#x8bef;&#x89e3;&#x8bfb;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x4e86;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x56de;&#x7b54;&#x7684;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x4e00;&#x79cd;&#x65b0;&#x65b9;&#x6cd5;&#x6765;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x3001;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1450,1451"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x94fe;&#xff08;VIC&#xff09;&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x2018;&#x5148;&#x601d;&#x8003;&#x518d;&#x770b;&#x56fe;&#x2019;&#x8303;&#x5f0f;&#x3002;&#x5177;&#x4f53;&#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x4e24;&#x6b65;&#xff1a;&#x9996;&#x5148;&#xff0c;&#x4ec5;&#x57fa;&#x4e8e;&#x6587;&#x672c;&#x95ee;&#x9898;&#xff08;Q&#xff09;&#x548c;&#x7279;&#x5b9a;&#x63d0;&#x793a;&#xff08;P_vic&#xff09;&#xff0c;&#x5229;&#x7528;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x751f;&#x6210;&#x4e00;&#x7cfb;&#x5217;&#x4e2d;&#x95f4;&#x63a8;&#x7406;&#x6b65;&#x9aa4;&#xff08;{s_n}&#xff09;&#xff0c;&#x5b8c;&#x5168;&#x6392;&#x9664;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x5f71;&#x54cd;&#xff1b;&#x7136;&#x540e;&#xff0c;&#x518d;&#x5f15;&#x5165;&#x56fe;&#x50cf;&#xff08;I&#xff09;&#xff0c;&#x901a;&#x8fc7;MLLM&#x63d0;&#x53d6;&#x89c6;&#x89c9;&#x4f9d;&#x636e;&#xff08;R&#xff09;&#xff0c;&#x5e76;&#x7ed3;&#x5408;&#x4e4b;&#x524d;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x63a8;&#x7406;&#x6b65;&#x9aa4;&#xff0c;&#x6700;&#x7ec8;&#x751f;&#x6210;&#x7b54;&#x6848;&#xff08;A&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5c06;&#x6587;&#x672c;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x89e3;&#x8026;&#xff0c;&#x51cf;&#x5c11;&#x4e86;&#x8de8;&#x6a21;&#x6001;&#x504f;&#x5dee;&#xff0c;&#x5229;&#x7528;&#x4e86;LLM&#x5f3a;&#x5927;&#x7684;&#x5148;&#x9a8c;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1451,1452"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: VIC&#x5728;&#x591a;&#x4e2a;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x96f6;&#x6837;&#x672c;&#x6027;&#x80fd;&#x3002;&#x5728;&#x5e7b;&#x89c9;&#x4e13;&#x9879;&#x8bc4;&#x6d4b;&#x4e2d;&#xff0c;Gemini 1.5 Pro&#x5728;MMVP&#x4e0a;&#x7684;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x5347;&#x4e86;31.74%&#xff0c;GPT-4o mini&#x63d0;&#x5347;&#x4e86;16.59%&#x3002;&#x5728;&#x901a;&#x7528;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#xff08;&#x5982;MME&#x3001;MathVista&#x3001;SEED-Bench&#xff09;&#x4e0a;&#xff0c;GPT&#x7cfb;&#x5217;&#x6a21;&#x578b;&#x5e73;&#x5747;&#x63d0;&#x5347;8.02%&#xff0c;Gemini&#x7cfb;&#x5217;&#x5e73;&#x5747;&#x63d0;&#x5347;7.19%&#x3002;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;VIC&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x9ad8;&#x63a8;&#x7406;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x4e14;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1452,1453"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x2018;&#x5148;&#x601d;&#x8003;&#x518d;&#x770b;&#x56fe;&#x2019;&#x7684;&#x8303;&#x5f0f;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x4f20;&#x7edf;&#x7684;&#x2018;&#x8fb9;&#x770b;&#x8fb9;&#x601d;&#x8003;&#x2019;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;VIC&#x6846;&#x67b6;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x3001;&#x7075;&#x6d3b;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x53ef;&#x5e7f;&#x6cdb;&#x5e94;&#x7528;&#x4e8e;&#x5404;&#x79cd;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x3002;&#x8fd9;&#x4e00;&#x6210;&#x679c;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6027;&#x80fd;&#xff0c;&#x4e5f;&#x4e3a;&#x672a;&#x6765;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5f3a;&#x8c03;&#x4e86;&#x5728;&#x590d;&#x6742;&#x8de8;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x89e3;&#x8026;&#x548c;&#x5904;&#x7406;&#x4e0d;&#x540c;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1453,1455"}}],"payload":{"tag":"li","lines":"1449,1455","fold":1}}],"payload":{"tag":"h4","lines":"1447,1448"}},{"content":"Socratic Questioning: Learn to Self-guide Multimodal Reasoning in the Wild","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x2018;&#x82cf;&#x683c;&#x62c9;&#x5e95;&#x5f0f;&#x63d0;&#x95ee;&#x2019;&#xff08;SQ&#xff09;&#x7684;&#x521b;&#x65b0;&#x591a;&#x8f6e;&#x8bad;&#x7ec3;&#x4e0e;&#x63a8;&#x7406;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x63d0;&#x5347;&#x8f7b;&#x91cf;&#x7ea7;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x590d;&#x6742;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x6027;&#x80fd;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x542f;&#x53d1;&#x5f0f;&#x81ea;&#x95ee;&#x81ea;&#x7b54;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x5173;&#x952e;&#x89c6;&#x89c9;&#x7ebf;&#x7d22;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5e76;&#x5728;&#x4ec5;&#x4f7f;&#x7528;&#x5c0f;&#x578b;&#x6570;&#x636e;&#x96c6;CapQA&#x5fae;&#x8c03;&#x540e;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5b9e;&#x73b0;&#x4e86;&#x96f6;&#x6837;&#x672c;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x7684;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff0c;&#x540c;&#x65f6;&#x5927;&#x5e45;&#x964d;&#x4f4e;&#x4e86;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1456,1457"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x590d;&#x6742;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x4e2d;&#x7684;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x95ee;&#x9898;&#xff1a;1) &#x5982;&#x4f55;&#x6709;&#x673a;&#x878d;&#x5408;&#x2018;&#x601d;&#x7ef4;&#x94fe;&#x2019;&#xff08;CoT&#xff09;&#x548c;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x4e24;&#x79cd;&#x6210;&#x719f;&#x65b9;&#x6cd5;&#xff0c;&#x4ee5;&#x53d1;&#x6325;&#x5176;&#x4e92;&#x8865;&#x4f18;&#x52bf;&#xff1b;2) &#x5982;&#x4f55;&#x6709;&#x6548;&#x7f13;&#x89e3;MLLM&#x4e2d;&#x5e38;&#x89c1;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#xff08;&#x5373;&#x6a21;&#x578b;&#x5ffd;&#x7565;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4ea7;&#x751f;&#x865a;&#x5047;&#x5185;&#x5bb9;&#xff09;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x964d;&#x4f4e;&#x9ad8;&#x6602;&#x7684;&#x8bad;&#x7ec3;&#x548c;&#x6807;&#x6ce8;&#x6210;&#x672c;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x53ef;&#x9760;&#x7684;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x662f;&#x8bb8;&#x591a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x7684;&#x5173;&#x952e;&#xff0c;&#x800c;&#x5f53;&#x524d;&#x65b9;&#x6cd5;&#x5728;&#x7cbe;&#x5ea6;&#x548c;&#x53ef;&#x7528;&#x6027;&#x4e0a;&#x4ecd;&#x5b58;&#x5728;&#x663e;&#x8457;&#x7f3a;&#x9677;&#x3002;","children":[],"payload":{"tag":"li","lines":"1458,1459"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x2018;&#x82cf;&#x683c;&#x62c9;&#x5e95;&#x5f0f;&#x63d0;&#x95ee;&#x2019;&#xff08;SQ&#xff09;&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x4e00;&#x4e2a;&#x56db;&#x6b65;&#x591a;&#x8f6e;&#x81ea;&#x5f15;&#x5bfc;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#xff1a;1) &#x81ea;&#x95ee;&#xff08;Self-ask&#xff09;&#xff1a;&#x6a21;&#x578b;&#x9488;&#x5bf9;&#x4e3b;&#x95ee;&#x9898;&#x751f;&#x6210;&#x4e00;&#x7cfb;&#x5217;&#x7ec6;&#x7c92;&#x5ea6;&#x5b50;&#x95ee;&#x9898;&#xff1b;2) &#x81ea;&#x7b54;&#xff08;Self-answer&#xff09;&#xff1a;&#x6a21;&#x578b;&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x56de;&#x7b54;&#x8fd9;&#x4e9b;&#x5b50;&#x95ee;&#x9898;&#xff0c;&#x5f62;&#x6210;&#x95ee;&#x7b54;&#x5bf9;&#x4f5c;&#x4e3a;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff1b;3) &#x6574;&#x5408;&#x4e0e;&#x7ec4;&#x7ec7;&#xff08;Consolidate &amp; Organize&#xff09;&#xff1a;&#x5c06;&#x95ee;&#x7b54;&#x5bf9;&#x4fe1;&#x606f;&#x6574;&#x5408;&#x6210;&#x8fde;&#x8d2f;&#x7684;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#xff1b;4) &#x603b;&#x7ed3;&#x4e0e;&#x6d53;&#x7f29;&#xff08;Summarize &amp; Condense&#xff09;&#xff1a;&#x751f;&#x6210;&#x4fdd;&#x7559;&#x6838;&#x5fc3;&#x5143;&#x7d20;&#x7684;&#x6458;&#x8981;&#x5f0f;&#x63cf;&#x8ff0;&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x88ab;&#x7ec4;&#x7ec7;&#x6210;&#x6307;&#x4ee4;&#x5bf9;&#x8bdd;&#x683c;&#x5f0f;&#xff0c;&#x6784;&#x5efa;&#x4e86;&#x4e00;&#x4e2a;&#x540d;&#x4e3a;CapQA&#x7684;&#x5c0f;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6570;&#x636e;&#x96c6;&#xff08;&#x5305;&#x542b;1k&#x5f20;&#x56fe;&#x50cf;&#xff09;&#xff0c;&#x7528;&#x4e8e;&#x9ad8;&#x6548;&#x5fae;&#x8c03;&#x8f7b;&#x91cf;&#x7ea7;MLLM&#xff08;&#x4f7f;&#x7528;ViT-L/14&#x4f5c;&#x4e3a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff0c;Vicuna&#x4f5c;&#x4e3a;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x4e24;&#x5c42;MLP&#x9002;&#x914d;&#x5668;&#x5bf9;&#x9f50;&#x6a21;&#x6001;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1459,1460"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1) &#x5728;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x4e2d;&#xff0c;SQ&#x65b9;&#x6cd5;&#x4f7f;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;31.2%&#xff1b;2) &#x5728;&#x591a;&#x4e2a;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;&#x4ec5;&#x4f7f;&#x7528;CapQA&#x5fae;&#x8c03;&#x7684;&#x6a21;&#x578b;&#x8868;&#x73b0;&#x51fa;&#x5f3a;&#x5927;&#x7684;&#x96f6;&#x6837;&#x672c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#xff0c;&#x7efc;&#x5408;&#x8bc6;&#x522b;&#x548c;&#x63a8;&#x7406;&#x6027;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff1b;3) &#x901a;&#x8fc7;GPT-4&#x8bc4;&#x4f30;&#x8bc1;&#x5b9e;&#xff0c;SQ&#x751f;&#x6210;&#x7684;&#x81ea;&#x95ee;&#x95ee;&#x9898;&#x8d28;&#x91cf;&#x66f4;&#x9ad8;&#xff0c;&#x4e14;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#x5927;&#x5e45;&#x51cf;&#x5c11;&#xff1b;4) &#x65b9;&#x6cd5;&#x517c;&#x5bb9;&#x8f7b;&#x91cf;&#x7ea7;&#x6a21;&#x578b;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x8ba1;&#x7b97;&#x548c;&#x6807;&#x6ce8;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1460,1461"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;SQ&#x6846;&#x67b6;&#x6210;&#x529f;&#x878d;&#x5408;&#x4e86;CoT&#x548c;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x7684;&#x4f18;&#x52bf;&#xff0c;&#x901a;&#x8fc7;&#x542f;&#x53d1;&#x5f0f;&#x81ea;&#x95ee;&#x81ea;&#x7b54;&#x673a;&#x5236;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#x5ea6;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x5b9e;&#x73b0;&#x4e86;&#x4f4e;&#x6210;&#x672c;&#x9ad8;&#x6548;&#x8bad;&#x7ec3;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#xff1a;1) &#x4e3a;&#x8f7b;&#x91cf;&#x7ea7;MLLM&#x7684;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x884c;&#x65b9;&#x6848;&#xff1b;2) CapQA&#x6570;&#x636e;&#x96c6;&#x53ef;&#x4f5c;&#x4e3a;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x7684;&#x65b0;&#x57fa;&#x51c6;&#xff1b;3) &#x81ea;&#x5f15;&#x5bfc;&#x63a8;&#x7406;&#x8303;&#x5f0f;&#x53ef;&#x80fd;&#x63a8;&#x52a8;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1461,1463"}}],"payload":{"tag":"li","lines":"1457,1463","fold":1}}],"payload":{"tag":"h4","lines":"1455,1456"}},{"content":"MMCoT: Multimodal Chain-of-Thought Reasoning in Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Multimodal-CoT&#xff0c;&#x4e00;&#x4e2a;&#x7ed3;&#x5408;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#x7684;&#x4e24;&#x9636;&#x6bb5;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x63d0;&#x5347;&#x5c0f;&#x53c2;&#x6570;&#x6a21;&#x578b;&#x7684;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x9996;&#x5148;&#x751f;&#x6210;&#x591a;&#x6a21;&#x6001; rationale&#xff0c;&#x7136;&#x540e;&#x57fa;&#x4e8e;&#x6b64;&#x63a8;&#x7406;&#x7b54;&#x6848;&#xff0c;&#x5728;ScienceQA&#x57fa;&#x51c6;&#x4e0a;&#x53d6;&#x5f97;&#x4e86;SOTA&#x6027;&#x80fd;&#xff0c;&#x5e76;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1464,1465"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x73b0;&#x6709;Chain-of-Thought (CoT) &#x63a8;&#x7406;&#x7814;&#x7a76;&#x4e3b;&#x8981;&#x96c6;&#x4e2d;&#x4e8e;&#x7eaf;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#xff0c;&#x5ffd;&#x7565;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x573a;&#x666f;&#xff08;&#x5982;&#x89c6;&#x89c9;+&#x8bed;&#x8a00;&#xff09;&#x3002;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x5bf9;&#x4eba;&#x7c7b;&#x8ba4;&#x77e5;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x800c;&#x5c0f;&#x53c2;&#x6570;&#x6a21;&#x578b;&#xff08;&lt;10B&#xff09;&#x5728;&#x591a;&#x6a21;&#x6001;CoT&#x63a8;&#x7406;&#x4e2d;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x751f;&#x6210;&#x9519;&#x8bef;&#x63a8;&#x7406;&#x94fe;&#xff09;&#xff0c;&#x5bfc;&#x81f4;&#x7b54;&#x6848;&#x9519;&#x8bef;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x5b9e;&#x73b0;&#x9ad8;&#x6548;&#x3001;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x63a8;&#x7406;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1466,1467"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x4e24;&#x9636;&#x6bb5;&#x6846;&#x67b6;&#xff1a;1.  rationale&#x751f;&#x6210;&#x9636;&#x6bb5;&#xff1a;&#x6a21;&#x578b;&#x63a5;&#x6536;&#x95ee;&#x9898;&#x6587;&#x672c;&#x548c;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#xff0c;&#x751f;&#x6210;&#x57fa;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x4fe1;&#x606f;&#x7684;&#x4e2d;&#x95f4;&#x63a8;&#x7406;&#x6b65;&#x9aa4;&#xff08;rationale&#xff09;&#x3002;2. &#x7b54;&#x6848;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#xff1a;&#x6a21;&#x578b;&#x5c06;&#x7b2c;&#x4e00;&#x9636;&#x6bb5;&#x751f;&#x6210;&#x7684;rationale&#x4e0e;&#x539f;&#x59cb;&#x95ee;&#x9898;&#xff08;&#x6587;&#x672c;&#x548c;&#x56fe;&#x50cf;&#xff09;&#x7ed3;&#x5408;&#xff0c;&#x63a8;&#x7406;&#x51fa;&#x6700;&#x7ec8;&#x7b54;&#x6848;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5fae;&#x8c03;T5&#x7b49;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;&lt;1B&#x53c2;&#x6570;&#xff09;&#x6765;&#x5b9e;&#x73b0;&#xff0c;&#x878d;&#x5408;&#x4e86;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x7684;&#x7279;&#x5f81;&#x8868;&#x793a;&#xff0c;&#x800c;&#x975e;&#x4f9d;&#x8d56;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLMs&#xff09;&#x7684;&#x63d0;&#x793a;&#x5de5;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1467,1468"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;ScienceQA&#x548c;A-OKVQA&#x57fa;&#x51c6;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x6a21;&#x578b;&#x5728;ScienceQA&#x4e0a;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#xff08;state-of-the-art&#xff09;&#x6027;&#x80fd;&#x3002;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5c0f;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6536;&#x655b;&#x901f;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1468,1469"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Multimodal-CoT&#x662f;&#x9996;&#x4e2a;&#x5728;&#x79d1;&#x5b66;&#x8bc4;&#x5ba1;&#x6587;&#x732e;&#x4e2d;&#x7814;&#x7a76;&#x591a;&#x6a21;&#x6001;CoT&#x63a8;&#x7406;&#x7684;&#x5de5;&#x4f5c;&#x3002;&#x7ed3;&#x8bba;&#x8868;&#x660e;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x79bb; rationale&#x751f;&#x6210;&#x4e0e;&#x7b54;&#x6848;&#x63a8;&#x7406;&#x4e24;&#x9636;&#x6bb5;&#xff0c;&#x5e76;&#x878d;&#x5165;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#xff0c;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#x5c0f;&#x53c2;&#x6570;&#x6a21;&#x578b;&#x7684;&#x591a;&#x6b65;&#x63a8;&#x7406;&#x80fd;&#x529b;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x8303;&#x5f0f;&#xff0c;&#x5176;&#x9ad8;&#x6548;&#x6027;&#x4f7f;&#x5f97;&#x5728;&#x6d88;&#x8d39;&#x7ea7;GPU&#x4e0a;&#x90e8;&#x7f72;&#x9ad8;&#x6027;&#x80fd;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x6a21;&#x578b;&#x6210;&#x4e3a;&#x53ef;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1469,1472"}}],"payload":{"tag":"li","lines":"1465,1472","fold":1}}],"payload":{"tag":"h4","lines":"1463,1464"}}],"payload":{"tag":"h3","lines":"1437,1438","fold":1}}],"payload":{"tag":"h2","lines":"930,931"}},{"content":"&#x68c0;&#x7d22;&#x65b9;&#x6cd5;","children":[{"content":"&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x65b9;&#x6cd5;&#xff08;RAG&#xff09;","children":[{"content":"ARA: Alleviating Hallucination in Large Vision-Language Models with Active Retrieval Augmentation","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;ARA&#xff08;&#x4e3b;&#x52a8;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#xff09;&#x7684;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x7ed3;&#x5408;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x68c0;&#x7d22;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5206;&#x5c42;&#x68c0;&#x7d22;&#x76ee;&#x6807;&#x3001;&#x9009;&#x62e9;&#x6700;&#x4f18;&#x68c0;&#x7d22;&#x7b56;&#x7565;&#x4ee5;&#x53ca;&#x5728;&#x6a21;&#x578b;&#x4f4e;&#x7f6e;&#x4fe1;&#x5ea6;&#x65f6;&#x89e6;&#x53d1;&#x68c0;&#x7d22;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x9a8c;&#x8bc1;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1477,1478"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x7406;&#x89e3;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x7ecf;&#x5e38;&#x751f;&#x6210;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x4e8b;&#x5b9e;&#x9519;&#x8bef;&#x7684;&#x56de;&#x7b54;&#xff0c;&#x5373;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x533b;&#x7597;&#x3001;&#x673a;&#x5668;&#x4eba;&#x7b49;&#x9700;&#x8981;&#x9ad8;&#x51c6;&#x786e;&#x6027;&#x9886;&#x57df;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x867d;&#x7136;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x5728;&#x5927;&#x578b;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x4e2d;&#x5df2;&#x8bc1;&#x660e;&#x662f;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x6709;&#x6548;&#x65b9;&#x6cd5;&#xff0c;&#x4f46;&#x5728;LVLM&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x4ecd;&#x663e;&#x4e0d;&#x8db3;&#xff0c;&#x4e14;&#x76f4;&#x63a5;&#x79fb;&#x690d;&#x53ef;&#x80fd;&#x52a0;&#x5267;&#x5e7b;&#x89c9;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x4e13;&#x95e8;&#x9488;&#x5bf9;LVLM&#x7684;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x65b9;&#x6cd5;&#x6765;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1479,1480"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e3b;&#x52a8;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;ARA&#xff09;&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x5173;&#x952e;&#x7ef4;&#x5ea6;&#xff1a;1&#xff09;&#x57fa;&#x4e8e;&#x56fe;&#x50cf;&#x5c42;&#x6b21;&#x7ed3;&#x6784;&#x5206;&#x89e3;&#x68c0;&#x7d22;&#x76ee;&#x6807;&#xff0c;&#x9996;&#x5148;&#x4ece;&#x67e5;&#x8be2;&#x4e2d;&#x63d0;&#x53d6;&#x76ee;&#x6807;&#x5bf9;&#x8c61;&#xff0c;&#x5e76;&#x5728;&#x56fe;&#x50cf;&#x4e2d;&#x5b9a;&#x4f4d;&#x7279;&#x5b9a;&#x533a;&#x57df;&#xff0c;&#x91c7;&#x7528;&#x4ece;&#x7c97;&#x5230;&#x7ec6;&#x7684;&#x68c0;&#x7d22;&#x8303;&#x5f0f;&#xff08;&#x540c;&#x65f6;&#x8fdb;&#x884c;&#x5168;&#x56fe;&#x50cf;&#x548c;&#x7279;&#x5b9a;&#x533a;&#x57df;&#x7684;&#x68c0;&#x7d22;&#xff09;&#xff1b;2&#xff09;&#x8bc6;&#x522b;&#x6700;&#x6709;&#x6548;&#x7684;&#x68c0;&#x7d22;&#x65b9;&#x6cd5;&#xff08;&#x5206;&#x6790;&#x4e86;&#x591a;&#x79cd;&#x68c0;&#x7d22;&#x7b56;&#x7565;&#xff09;&#x5e76;&#x8fc7;&#x6ee4;&#x4e0d;&#x53ef;&#x9760;&#x7684;&#x68c0;&#x7d22;&#x7ed3;&#x679c;&#xff08;&#x4f7f;&#x7528;&#x91cd;&#x6392;&#x5e8f;&#x7b56;&#x7565;&#xff09;&#xff1b;3&#xff09;&#x4ec5;&#x5728;LVLM&#x7f6e;&#x4fe1;&#x5ea6;&#x4f4e;&#x65f6;&#x89e6;&#x53d1;&#x68c0;&#x7d22;&#xff08;&#x57fa;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x8f93;&#x5165;&#x4e4b;&#x95f4;&#x7684;&#x4e92;&#x4fe1;&#x606f;&#x8ba1;&#x7b97;&#x96be;&#x5ea6;&#x6307;&#x6807;&#xff09;&#xff0c;&#x907f;&#x514d;&#x9ad8;&#x7f6e;&#x4fe1;&#x5ea6;&#x65f6;&#x7684;&#x4e0d;&#x5fc5;&#x8981;&#x68c0;&#x7d22;&#x3002;","children":[],"payload":{"tag":"li","lines":"1480,1481"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x4f7f;&#x7528;&#x4e86;&#x4e09;&#x79cd;&#x5e7f;&#x6cdb;&#x4f7f;&#x7528;&#x7684;LVLM&#x6a21;&#x578b;&#xff08;LLaVA-1.5&#x3001;Qwen-VL&#x548c;mPLUG-Owl2&#xff09;&#x548c;&#x56db;&#x4e2a;&#x5e7b;&#x89c9;&#x76f8;&#x5173;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x3002;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;&#x901a;&#x8fc7;&#x91c7;&#x7528;&#x5408;&#x9002;&#x7684;&#x68c0;&#x7d22;&#x673a;&#x5236;&#x548c;&#x8c28;&#x614e;&#x7684;&#x68c0;&#x7d22;&#x65f6;&#x673a;&#xff0c;ARA&#x6846;&#x67b6;&#x80fd;&#x591f;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x9002;&#x5ea6;&#x7684;&#x68c0;&#x7d22;&#x9891;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"1481,1482"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x4f18;&#x5316;&#x68c0;&#x7d22;&#x8bbe;&#x7f6e;&#xff08;&#x5305;&#x62ec;&#x76ee;&#x6807;&#x5206;&#x89e3;&#x3001;&#x65b9;&#x6cd5;&#x9009;&#x62e9;&#x548c;&#x65f6;&#x673a;&#x63a7;&#x5236;&#xff09;&#xff0c;&#x53ef;&#x4ee5;&#x66f4;&#x6709;&#x6548;&#x5730;&#x5229;&#x7528;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x6765;&#x51cf;&#x5c11;LVLM&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x8fd9;&#x4e00;&#x7814;&#x7a76;&#x4e3a;&#x5982;&#x4f55;&#x9002;&#x5e94;LVLM&#x7684;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x63d0;&#x4f9b;&#x4e86;&#x66f4;&#x6df1;&#x5165;&#x7684;&#x89c1;&#x89e3;&#xff0c;&#x5177;&#x6709;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x51c6;&#x786e;&#x6027;&#x7684;&#x5b9e;&#x9645;&#x573a;&#x666f;&#x4e2d;&#x63d0;&#x5347;LVLM&#x53ef;&#x4fe1;&#x5ea6;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1482,1485"}}],"payload":{"tag":"li","lines":"1478,1485","fold":1}}],"payload":{"tag":"h4","lines":"1476,1477"}},{"content":"RVCD: Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;RVCD&#xff08;&#x68c0;&#x7d22;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;OH&#xff09;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x68c0;&#x7d22;&#x4ee3;&#x8868;&#x5355;&#x4e00;&#x6982;&#x5ff5;&#x7684;AI&#x751f;&#x6210;&#x56fe;&#x50cf;&#xff0c;&#x5728;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x4e2d;&#x5229;&#x7528;&#x6b63;&#x8d1f;&#x56fe;&#x50cf;&#x7684;logits&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x8c03;&#x8282;&#xff0c;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1486,1487"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;OH&#xff09;&#x662f;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x4e00;&#x4e2a;&#x6301;&#x7eed;&#x5b58;&#x5728;&#x7684;&#x4e25;&#x91cd;&#x95ee;&#x9898;&#xff0c;&#x8868;&#x73b0;&#x4e3a;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3001;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x6216;&#x5173;&#x7cfb;&#x3002;&#x8fd9;&#x4f1a;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x5176;&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;&#x5c3d;&#x7ba1;&#x5df2;&#x6709;&#x7814;&#x7a76;&#x5c1d;&#x8bd5;&#x89e3;&#x51b3;&#xff0c;&#x4f46;OH&#x95ee;&#x9898;&#x4ecd;&#x672a;&#x5b8c;&#x5168;&#x89e3;&#x51b3;&#xff0c;&#x4e14;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x672a;&#x5145;&#x5206;&#x5229;&#x7528;&#x5916;&#x90e8;&#x56fe;&#x50cf;&#x8d44;&#x6e90;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x7684;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1488,1489"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;RVCD&#x65b9;&#x6cd5;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x6d41;&#x7a0b;&#x5982;&#x4e0b;&#xff1a;1) &#x4f7f;&#x7528;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#xff08;&#x5982;YOLO&#xff09;&#x5206;&#x6790;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x548c;LVLM&#x751f;&#x6210;&#x7684;&#x521d;&#x59cb;&#x63cf;&#x8ff0;&#xff08;draft caption&#xff09;&#xff0c;&#x8bc6;&#x522b;&#x51fa;&#x201c;&#x8d1f;&#x5bf9;&#x8c61;&#x201d;&#xff08;&#x63cf;&#x8ff0;&#x4e2d;&#x5b58;&#x5728;&#x4f46;&#x56fe;&#x50cf;&#x4e2d;&#x5b9e;&#x9645;&#x4e0d;&#x5b58;&#x5728;&#xff09;&#x548c;&#x201c;&#x6b63;&#x5bf9;&#x8c61;&#x201d;&#xff08;&#x63cf;&#x8ff0;&#x548c;&#x56fe;&#x50cf;&#x4e2d;&#x5747;&#x5b58;&#x5728;&#xff09;&#xff1b;2) &#x4ece;&#x4e00;&#x4e2a;&#x9884;&#x6784;&#x5efa;&#x7684;&#x6570;&#x636e;&#x5e93;&#x4e2d;&#x68c0;&#x7d22;&#x4ee3;&#x8868;&#x8fd9;&#x4e9b;&#x6b63;&#x8d1f;&#x5bf9;&#x8c61;&#x7684;&#x5355;&#x4e00;&#x6982;&#x5ff5;AI&#x751f;&#x6210;&#x56fe;&#x50cf;&#xff08;&#x4f7f;&#x7528;FLUX&#x6a21;&#x578b;&#x751f;&#x6210;&#xff0c;&#x5e76;&#x7ecf;LVLM&#x548c;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x6a21;&#x578b;&#x53cc;&#x91cd;&#x9a8c;&#x8bc1;&#xff09;&#xff1b;3) &#x5728;&#x6bcf;&#x4e00;&#x6b65;&#x89e3;&#x7801;&#x65f6;&#xff0c;&#x540c;&#x65f6;&#x8ba1;&#x7b97;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x3001;&#x6b63;&#x8d1f;&#x53c2;&#x8003;&#x56fe;&#x50cf;&#x7684;logits&#xff1b;4) &#x901a;&#x8fc7;&#x4e00;&#x4e2a;&#x516c;&#x5f0f;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x539f;&#x59cb;logits&#xff1a;&#x589e;&#x5f3a;&#x4e0e;&#x6b63;&#x5bf9;&#x8c61;&#x76f8;&#x5173;&#x7684;logits&#xff0c;&#x6291;&#x5236;&#x4e0e;&#x8d1f;&#x5bf9;&#x8c61;&#x76f8;&#x5173;&#x7684;logits&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x4e8e;&#x5f00;&#x6e90;LVLM&#xff08;&#x5982;LLaVA, MiniGPT-4&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1489,1490"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1) RVCD&#x5728;&#x964d;&#x4f4e;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;OH&#xff09;&#x65b9;&#x9762;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x6240;&#x6709;&#x73b0;&#x6709;&#x7684;&#x57fa;&#x4e8e;&#x89e3;&#x7801;&#x7684;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD, Zoom-Decode&#xff09;&#x3002;2) &#x5728;&#x6807;&#x51c6;OH&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR-I, CHAIR-S, POPE&#xff09;&#x4e0a;&#x53d6;&#x5f97;&#x4e86;&#x6700;&#x4f73;&#x6027;&#x80fd;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;3) &#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6587;&#x672c;&#x751f;&#x6210;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#xff08;&#x7528;BLEU&#x5206;&#x6570;&#x8861;&#x91cf;&#xff09;&#xff0c;&#x8868;&#x660e;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#x6ca1;&#x6709;&#x727a;&#x7272;&#x8bed;&#x8a00;&#x6d41;&#x7545;&#x6027;&#x3002;4) &#x5b9e;&#x9a8c;&#x8bc1;&#x660e;&#x4e86;&#x5916;&#x90e8;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x6a21;&#x578b;&#xff08;YOLO&#xff09;&#x5728;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#x4e0a;&#x6bd4;LVLM&#x81ea;&#x8eab;&#x66f4;&#x51c6;&#x786e;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x501f;&#x52a9;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x4fe1;&#x606f;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1490,1491"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;RVCD&#x901a;&#x8fc7;&#x68c0;&#x7d22;&#x548c;&#x5229;&#x7528;&#x5916;&#x90e8;&#x663e;&#x5f0f;&#x6982;&#x5ff5;&#x56fe;&#x50cf;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff0c;&#x662f;&#x4e00;&#x79cd;&#x5f3a;&#x5927;&#x4e14;&#x5b9e;&#x7528;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x6291;&#x5236;LVLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x8f93;&#x51fa;&#x6587;&#x672c;&#x8d28;&#x91cf;&#x3002;&#x5176;&#x8d21;&#x732e;&#x5728;&#x4e8e;&#xff1a;1) &#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65b0;&#x7684;&#x5373;&#x63d2;&#x5373;&#x7528;&#x5f0f;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff1b;2) &#x521b;&#x5efa;&#x4e86;&#x4e00;&#x4e2a;&#x516c;&#x5f00;&#x7684;&#x3001;&#x7ecf;&#x8fc7;&#x53cc;&#x91cd;&#x9a8c;&#x8bc1;&#x7684;&#x5355;&#x4e00;&#x6982;&#x5ff5;&#x56fe;&#x50cf;&#x6570;&#x636e;&#x5e93;&#xff0c;&#x589e;&#x5f3a;&#x4e86;&#x65b9;&#x6cd5;&#x7684;&#x53ef;&#x89e3;&#x91ca;&#x6027;&#x5e76;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x8d44;&#x6e90;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x8868;&#x660e;&#xff0c;&#x5145;&#x5206;&#x5229;&#x7528;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#x548c;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x662f;&#x89e3;&#x51b3;LVLM&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x7684;&#x6709;&#x6548;&#x9014;&#x5f84;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;LVLM&#x5728;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1491,1493"}}],"payload":{"tag":"li","lines":"1487,1493","fold":1}}],"payload":{"tag":"h4","lines":"1485,1486"}},{"content":"RE-ALIGN: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: RE-ALIGN&#x662f;&#x4e00;&#x4e2a;&#x65b0;&#x9896;&#x7684;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5bf9;&#x9f50;&#x6846;&#x67b6;&#xff0c;&#x5b83;&#x901a;&#x8fc7;&#x7ed3;&#x5408;&#x56fe;&#x50cf;&#x68c0;&#x7d22;&#x548c;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x6765;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5229;&#x7528;&#x68c0;&#x7d22;&#x5230;&#x7684;&#x56fe;&#x50cf;&#x6784;&#x5efa;&#x5305;&#x542b;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x504f;&#x597d;&#x7684;&#x53cc;&#x91cd;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x6269;&#x5c55;&#x7684;rDPO&#x76ee;&#x6807;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x548c;&#x63d0;&#x5347;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x4efb;&#x52a1;&#x6027;&#x80fd;&#x65b9;&#x9762;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff0c;&#x4e14;&#x5177;&#x6709;&#x826f;&#x597d;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1494,1495"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x4e2d;&#x4e25;&#x91cd;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e0d;&#x7b26;&#x7684;&#x865a;&#x5047;&#x6216;&#x9519;&#x8bef;&#x7ec6;&#x8282;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x975e;&#x5e38;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;VLM&#x5728;&#x73b0;&#x5b9e;&#x4e16;&#x754c;&#x8de8;&#x6a21;&#x6001;&#x5e94;&#x7528;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x6709;&#x6548;&#x6027;&#xff0c;&#x963b;&#x788d;&#x5176;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1496,1497"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;RE-ALIGN&#x6846;&#x67b6;&#x6765;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x4e24;&#x6b65;&#xff1a;1. <strong>&#x504f;&#x597d;&#x6570;&#x636e;&#x751f;&#x6210;</strong>&#xff1a;&#x9996;&#x5148;&#xff0c;&#x4f7f;&#x7528;&#x4e00;&#x4e2a;&#x5148;&#x8fdb;&#x7684;VLM&#x4e3a;&#x8bad;&#x7ec3;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x2018;&#x88ab;&#x9009;&#x4e2d;&#x7684;&#x2019;&#x6b63;&#x786e;&#x54cd;&#x5e94;&#xff08;yw&#xff09;&#x3002;&#x63a5;&#x7740;&#xff0c;&#x901a;&#x8fc7;&#x2018;&#x7b56;&#x7565;&#x6027;&#x63a9;&#x7801;&#x2019;&#x8fc7;&#x7a0b;&#xff0c;&#x906e;&#x76d6;&#x6389;&#x54cd;&#x5e94;&#x4e2d;&#x4e0e;&#x56fe;&#x50cf;&#x4e2d;&#x7269;&#x4f53;&#x3001;&#x5c5e;&#x6027;&#x6216;&#x903b;&#x8f91;&#x5173;&#x7cfb;&#x76f8;&#x5173;&#x7684;&#x5173;&#x952e;&#x8bcd;&#x6bb5;&#xff0c;&#x5f62;&#x6210;&#x63a9;&#x7801;&#x540e;&#x7684;&#x54cd;&#x5e94;&#xff08;ym&#xff09;&#x3002;&#x7136;&#x540e;&#xff0c;&#x901a;&#x8fc7;&#x56fe;&#x50cf;&#x68c0;&#x7d22;&#x4ece;&#x8bad;&#x7ec3;&#x96c6;&#x4e2d;&#x4e3a;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#xff08;vi&#xff09;&#x68c0;&#x7d22;top-k&#x4e2a;&#x6700;&#x76f8;&#x4f3c;&#x7684;&#x56fe;&#x50cf;&#x3002;&#x6700;&#x540e;&#xff0c;&#x7528;&#x8fd9;&#x4e9b;&#x68c0;&#x7d22;&#x5230;&#x7684;&#x56fe;&#x50cf;&#x548c;&#x63a9;&#x7801;&#x540e;&#x7684;&#x54cd;&#x5e94;&#x63d0;&#x793a;VLM&#x751f;&#x6210;&#x8865;&#x5168;&#x5185;&#x5bb9;&#xff0c;&#x82e5;&#x8865;&#x5168;&#x540e;&#x7684;&#x54cd;&#x5e94;&#xff08;yc&#xff09;&#x4e0e;&#x539f;&#x59cb;&#x6b63;&#x786e;&#x54cd;&#x5e94;&#x7684;&#x8bed;&#x4e49;&#x76f8;&#x4f3c;&#x5ea6;&#x4f4e;&#x4e8e;&#x9608;&#x503c;&#xff08;0.95&#xff09;&#xff0c;&#x5219;&#x5c06;&#x5176;&#x6807;&#x8bb0;&#x4e3a;&#x2018;&#x88ab;&#x62d2;&#x7edd;&#x7684;&#x2019;&#x5e7b;&#x89c9;&#x54cd;&#x5e94;&#xff08;yl&#xff09;&#xff0c;&#x4ece;&#x800c;&#x6784;&#x5efa;&#x51fa;&#xff08;x, v, yw, yl&#xff09;&#x504f;&#x597d;&#x5bf9;&#x3002;2. <strong>&#x504f;&#x597d;&#x4f18;&#x5316;</strong>&#xff1a;&#x63d0;&#x51fa;&#x4e86;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x7684;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;rDPO&#xff09;&#x76ee;&#x6807;&#x51fd;&#x6570;L_rDPO&#xff0c;&#x5b83;&#x662f;&#x6807;&#x51c6;DPO&#x635f;&#x5931;&#xff08;L_DPO&#xff09;&#x548c;&#x4e00;&#x4e2a;&#x65b0;&#x589e;&#x7684;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x4f18;&#x5316;&#x635f;&#x5931;&#xff08;L_vDPO&#xff09;&#x7684;&#x52a0;&#x6743;&#x7ec4;&#x5408;&#x3002;L_vDPO&#x9f13;&#x52b1;&#x6a21;&#x578b;&#x5bf9;&#x4e8e;&#x540c;&#x4e00;&#x6587;&#x672c;&#x6307;&#x4ee4;&#xff0c;&#x5728;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0b;&#x7684;&#x54cd;&#x5e94;&#x6982;&#x7387;&#x5e94;&#x9ad8;&#x4e8e;&#x5728;&#x68c0;&#x7d22;&#x5230;&#x7684;&#x76f8;&#x4f3c;&#x56fe;&#x50cf;&#x4e0b;&#x7684;&#x54cd;&#x5e94;&#x6982;&#x7387;&#xff0c;&#x4ece;&#x800c;&#x5c06;&#x89c6;&#x89c9;&#x504f;&#x597d;&#x4fe1;&#x53f7;&#x76f4;&#x63a5;&#x878d;&#x5165;&#x5bf9;&#x9f50;&#x8fc7;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1497,1498"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x53d1;&#x73b0;&#xff1a;1. <strong>&#x5728;&#x51cf;&#x8f7b;&#x5e7b;&#x89c9;&#x65b9;&#x9762;</strong>&#xff1a;RE-ALIGN&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;VLM&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5176;&#x6548;&#x679c;&#x4f18;&#x4e8e;&#x4e4b;&#x524d;&#x7684;&#x6240;&#x6709;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#x3002;2. <strong>&#x5728;&#x901a;&#x7528;VQA&#x4efb;&#x52a1;&#x6027;&#x80fd;&#x65b9;&#x9762;</strong>&#xff1a;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#xff0c;&#x8fd8;&#x5e26;&#x6765;&#x4e86;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x4efb;&#x52a1;&#x6027;&#x80fd;&#x7684;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff0c;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#xff08;state-of-the-art&#xff09;&#x6c34;&#x5e73;&#x3002;3. <strong>&#x5728;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x65b9;&#x9762;</strong>&#xff1a;RE-ALIGN&#x5728;&#x4e0d;&#x540c;&#x89c4;&#x6a21;&#xff08;&#x5927;&#x5c0f;&#xff09;&#x548c;&#x4e0d;&#x540c;&#x67b6;&#x6784;&#xff08;&#x5982;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x6a21;&#x578b;&#x3001;&#x7edf;&#x4e00;&#x6a21;&#x578b;&#xff09;&#x7684;VLM&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x4e00;&#x81f4;&#x7684;&#x6709;&#x6548;&#x6027;&#x548c;&#x7a33;&#x5065;&#x6027;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x5e7f;&#x6cdb;&#x7684;&#x9002;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1498,1499"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;RE-ALIGN&#x901a;&#x8fc7;&#x521b;&#x65b0;&#x6027;&#x5730;&#x5229;&#x7528;&#x56fe;&#x50cf;&#x68c0;&#x7d22;&#x6765;&#x6784;&#x5efa;&#x53cc;&#x91cd;&#x504f;&#x597d;&#x6570;&#x636e;&#x96c6;&#x5e76;&#x6269;&#x5c55;DPO&#x76ee;&#x6807;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x66f4;&#x6709;&#x6548;&#x548c;&#x53ef;&#x9760;&#x7684;&#x65b9;&#x6cd5;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4ee3;&#x8868;&#x7740;&#x5728;&#x5bf9;&#x9f50;&#x591a;&#x6a21;&#x6001;LLMs&#x65b9;&#x9762;&#x5411;&#x524d;&#x8fc8;&#x51fa;&#x4e86;&#x91cd;&#x8981;&#x4e00;&#x6b65;&#xff0c;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x662f;&#x4e3a;&#x5f00;&#x53d1;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x6709;&#x6548;&#x7684;&#x8de8;&#x6a21;&#x6001;&#x5e94;&#x7528;&#xff08;&#x5982;&#x66f4;&#x53ef;&#x4fe1;&#x7684;AI&#x52a9;&#x624b;&#x3001;&#x66f4;&#x51c6;&#x786e;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7cfb;&#x7edf;&#xff09;&#x94fa;&#x5e73;&#x4e86;&#x9053;&#x8def;&#x3002;","children":[],"payload":{"tag":"li","lines":"1499,1501"}}],"payload":{"tag":"li","lines":"1495,1501","fold":1}}],"payload":{"tag":"h4","lines":"1493,1494"}},{"content":"OPA-DPA: Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x5217;&#x8868;&#x805a;&#x7126;&#x4e8e;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x591a;&#x79cd;&#x6280;&#x672f;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x3001;&#x5de5;&#x5177;&#x8c03;&#x7528;&#x3001;&#x6ce8;&#x610f;&#x529b;&#x63a7;&#x5236;&#x7b49;&#xff09;&#x6765;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#xff0c;&#x5e76;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x9a8c;&#x8bc1;&#x4e86;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1502,1503"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x5c55;&#x73b0;&#x51fa;&#x5f3a;&#x5927;&#x591a;&#x6a21;&#x6001;&#x80fd;&#x529b;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x4e5f;&#x7ee7;&#x627f;&#x4e86;&#x5176;&#x5e95;&#x5c42;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x6216;&#x865a;&#x6784;&#x5185;&#x5bb9;&#xff09;&#x7684;&#x503e;&#x5411;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3001;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5728;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#xff08;&#x5982;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x7684;&#x5e94;&#x7528;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x89e3;&#x51b3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1504,1505"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x8bba;&#x6587;&#x5217;&#x8868;&#x4e2d;&#x7684;&#x65b9;&#x6cd5;&#x591a;&#x6837;&#xff0c;&#x4e3b;&#x8981;&#x5305;&#x62ec;&#xff1a;1. &#x89c6;&#x89c9;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;Visual Contrastive Decoding&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0e;&#x5176;&#x589e;&#x5f3a;&#x7248;&#x672c;&#xff08;&#x5982;&#x88c1;&#x526a;&#x3001;&#x65cb;&#x8f6c;&#xff09;&#x7684;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#xff0c;&#x6291;&#x5236;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x65e0;&#x5173;&#x7684;&#x5e7b;&#x89c9;&#x751f;&#x6210;&#x3002;2. &#x667a;&#x80fd;&#x4f53;&#x6570;&#x636e;&#x7ba1;&#x9053;&#xff08;Agentic Data Pipeline&#xff09;&#xff1a;&#x5982;Omni-Detective&#xff0c;&#x901a;&#x8fc7;&#x5de5;&#x5177;&#x8c03;&#x7528;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x4e14;&#x5e7b;&#x89c9;&#x6700;&#x5c11;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x6570;&#x636e;&#x3002;3. &#x6ce8;&#x610f;&#x529b;&#x63a7;&#x5236;&#xff08;Attention Control&#xff09;&#xff1a;&#x5982;FlexAC&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x5e76;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x4e2d;&#x5bfc;&#x81f4;&#x5173;&#x8054;&#x6027;&#x5e7b;&#x89c9;&#x7684;&#x7279;&#x5b9a;&#x5c42;&#x8868;&#x793a;&#x6765;&#x8c03;&#x63a7;&#x63a8;&#x7406;&#x5f3a;&#x5ea6;&#x3002;4. &#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x751f;&#x6210;&#xff08;RAG&#xff09;&#xff1a;&#x5982;VisRAG&#xff0c;&#x5f15;&#x5165;&#x5916;&#x90e8;&#x89c6;&#x89c9;&#x77e5;&#x8bc6;&#x5e93;&#x6765; grounding &#x63a8;&#x7406;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;5. &#x591a;&#x6a21;&#x6001;&#x7f6e;&#x4fe1;&#x5ea6;&#x63a8;&#x7406;&#xff08;Multimodal Deep Confidence&#xff09;&#xff1a;&#x901a;&#x8fc7;&#x63a2;&#x7d22;&#x591a;&#x6761;&#x89c6;&#x89c9;&#x63a8;&#x7406;&#x8def;&#x5f84;&#x6765;&#x8bc4;&#x4f30;&#x7b54;&#x6848;&#x7f6e;&#x4fe1;&#x5ea6;&#xff0c;&#x9009;&#x62e9;&#x6700;&#x53ef;&#x9760;&#x7684;&#x8def;&#x5f84;&#x3002;","children":[],"payload":{"tag":"li","lines":"1505,1506"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1. &#x6240;&#x63d0;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x5f00;&#x6e90;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x5982;&#x6587;&#x672c;&#x56fe;&#x50cf;&#x7406;&#x89e3;&#x3001;&#x63a8;&#x7406;&#x3001;&#x591a;&#x56fe;&#x50cf;&#x7406;&#x89e3;&#x3001;&#x901a;&#x7528;VQA&#x3001;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x7b49;&#xff09;&#x4e0a;&#xff0c;&#x6027;&#x80fd;&#x8fbe;&#x5230;&#x6216;&#x8d85;&#x8fc7;&#x4e86;&#x540c;&#x89c4;&#x6a21;&#x7684;&#x6700;&#x5148;&#x8fdb;&#x6a21;&#x578b;&#x3002;2. &#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#x548c;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x4e0e;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x3002;3. &#x5728;&#x653e;&#x5c04;&#x5b66;&#x7b49;&#x4e13;&#x4e1a;&#x9886;&#x57df;&#xff0c;&#x901a;&#x8fc7;&#x79bb;&#x6563;&#x8bed;&#x4e49;&#x71b5;&#xff08;DSE&#xff09;&#x8fc7;&#x6ee4;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#x4e86;&#x9ed1;&#x76d2;&#x6a21;&#x578b;&#x7684;&#x56de;&#x7b54;&#x51c6;&#x786e;&#x6027;&#x3002;4. &#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x5177;&#x6709;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x9ad8;&#x6548;&#x7684;&#x7279;&#x70b9;&#xff08;&#x5982;1+N LoRA&#x67b6;&#x6784;&#xff09;&#xff0c;&#x9002;&#x5408;&#x79fb;&#x52a8;&#x7aef;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1506,1507"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x4e00;&#x7cfb;&#x5217;&#x521b;&#x65b0;&#x7684;&#x6280;&#x672f;&#x624b;&#x6bb5;&#xff08;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#x3001;&#x6570;&#x636e;&#x7ba1;&#x9053;&#x3001;&#x6ce8;&#x610f;&#x529b;&#x8c03;&#x63a7;&#x3001;&#x68c0;&#x7d22;&#x589e;&#x5f3a;&#x7b49;&#xff09;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x5730;&#x51cf;&#x8f7b;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x9ad8;&#x5176;&#x8f93;&#x51fa;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x771f;&#x5b9e;&#x6027;&#x3002;&#x8fd9;&#x4e9b;&#x5de5;&#x4f5c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x3001;&#x66f4;&#x7a33;&#x5065;&#x7684;&#x591a;&#x6a21;&#x6001;&#x4eba;&#x5de5;&#x667a;&#x80fd;&#x7cfb;&#x7edf;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#xff0c;&#x5e76;&#x63a8;&#x52a8;&#x4e86;LVLM&#x5728;&#x533b;&#x7597;&#x3001;&#x5b89;&#x5168;&#x68c0;&#x6d4b;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x900f;&#x660e;&#x5ea6;&#x3001;&#x589e;&#x5f3a;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#xff0c;&#x5e76;&#x4e3a;&#x89e3;&#x51b3;AI&#x5b89;&#x5168;&#x6027;&#x4e0e;&#x53ef;&#x9760;&#x6027;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x601d;&#x8def;&#x3002;","children":[],"payload":{"tag":"li","lines":"1507,1509"}}],"payload":{"tag":"li","lines":"1503,1509","fold":1}}],"payload":{"tag":"h4","lines":"1501,1502"}},{"content":"DEHALL: Combating Multimodal LLM Hallucination via Bottom-Up Holistic Reasoning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x81ea;&#x5e95;&#x5411;&#x4e0a;&#x7684;&#x6574;&#x4f53;&#x63a8;&#x7406;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x7ed3;&#x5408;&#x573a;&#x666f;&#x56fe;&#x548c;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#xff0c;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5728;&#x89c6;&#x89c9;&#x3001;&#x6587;&#x672c;&#x8f93;&#x5165;&#x548c;&#x5e38;&#x8bc6;&#x5c42;&#x9762;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1510,1511"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x8f93;&#x5165;&#x6570;&#x636e;&#x4e0d;&#x7b26;&#x7684;&#x9519;&#x8bef;&#x8f93;&#x51fa;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x611f;&#x77e5;&#x5c42;&#x9762;&#x7684;&#x9519;&#x8bef;&#xff08;&#x5982;&#x7269;&#x4f53;&#x548c;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x4f46;&#x5ffd;&#x7565;&#x4e86;&#x8ba4;&#x77e5;&#x5c42;&#x9762;&#x9700;&#x8981;&#x4e8b;&#x5b9e;&#x5e38;&#x8bc6;&#x7684;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff0c;&#x4ee5;&#x53ca;&#x6587;&#x672c;&#x8f93;&#x5165;&#x672c;&#x8eab;&#x53ef;&#x80fd;&#x5b58;&#x5728;&#x7684;&#x9519;&#x8bef;&#x5f15;&#x5bfc;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;MLLMs&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x5168;&#x9762;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1512,1513"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x516d;&#x6b65;&#x81ea;&#x5e95;&#x5411;&#x4e0a;&#x63a8;&#x7406;&#x6846;&#x67b6;&#xff1a;1&#xff09;&#x76ee;&#x6807;&#x8bc6;&#x522b;&#x4e0e;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#xff1a;&#x4f7f;&#x7528;&#x573a;&#x666f;&#x56fe;&#xff08;Scene Graph&#xff09;&#x7ed3;&#x6784;&#x5316;&#x8868;&#x793a;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#xff08;&#x7269;&#x4f53;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#xff09;&#xff1b;2&#xff09;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x9a8c;&#x8bc1;&#xff1a;&#x901a;&#x8fc7;&#x5916;&#x90e8;&#x5de5;&#x5177;&#x9a8c;&#x8bc1;&#x548c;&#x4fee;&#x6b63;&#x573a;&#x666f;&#x56fe;&#xff1b;3&#xff09;&#x95ee;&#x9898;&#x9a8c;&#x8bc1;&#x4e0e;&#x8c03;&#x6574;&#xff1a;&#x68c0;&#x67e5;&#x8f93;&#x5165;&#x6587;&#x672c;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x7684;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x4fee;&#x6b63;&#x51b2;&#x7a81;&#xff1b;4&#xff09;&#x5e38;&#x8bc6;&#x8bf1;&#x5bfc;&#xff1a;&#x9488;&#x5bf9;&#x8ba4;&#x77e5;&#x5c42;&#x9762;&#x95ee;&#x9898;&#xff0c;&#x751f;&#x6210;&#x5fc5;&#x8981;&#x7684;&#x5e38;&#x8bc6;&#x58f0;&#x660e;&#xff1b;5&#xff09;&#x5e38;&#x8bc6;&#x9a8c;&#x8bc1;&#xff1a;&#x901a;&#x8fc7;&#x5916;&#x90e8;&#x77e5;&#x8bc6;&#x5e93;&#x9a8c;&#x8bc1;&#x5e38;&#x8bc6;&#x58f0;&#x660e;&#x7684;&#x6b63;&#x786e;&#x6027;&#xff1b;6&#xff09;&#x95ee;&#x7b54;&#x6574;&#x5408;&#xff1a;&#x7efc;&#x5408;&#x6240;&#x6709;&#x9a8c;&#x8bc1;&#x540e;&#x7684;&#x4fe1;&#x606f;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7b54;&#x6848;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x53ef;&#x9002;&#x914d;&#x73b0;&#x6709;MLLMs&#x3002;","children":[],"payload":{"tag":"li","lines":"1513,1514"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x516d;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x5177;&#x4f53;&#x5305;&#x62ec;&#xff1a;&#x6709;&#x6548;&#x5904;&#x7406;&#x4e86;&#x6587;&#x672c;&#x8f93;&#x5165;&#x4e2d;&#x7684;&#x51b2;&#x7a81;&#xff08;47.8%&#x7684;&#x7528;&#x6237;&#x8f93;&#x5165;&#x5b58;&#x5728;&#x6b64;&#x7c7b;&#x95ee;&#x9898;&#xff09;&#xff0c;&#x89e3;&#x51b3;&#x4e86;&#x8ba4;&#x77e5;&#x5c42;&#x9762;&#x5e7b;&#x89c9;&#xff08;&#x5360;&#x95ee;&#x9898;&#x7684;51%&#xff09;&#xff0c;&#x5e76;&#x5728;&#x7269;&#x4f53;&#x3001;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x51fa;&#x6539;&#x8fdb;&#x3002;&#x53ef;&#x89c6;&#x5316;&#x5206;&#x6790;&#x8bc1;&#x5b9e;&#x4e86;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x51cf;&#x5c11;&#x9519;&#x8bef;&#x8f93;&#x51fa;&#x3002;","children":[],"payload":{"tag":"li","lines":"1514,1515"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x6a21;&#x4eff;&#x4eba;&#x7c7b;&#x4ece;&#x611f;&#x77e5;&#x5230;&#x8ba4;&#x77e5;&#x7684;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#xff0c;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x5168;&#x9762;&#x89e3;&#x51b3;MLLM&#x5e7b;&#x89c9;&#x7684;&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x521b;&#x65b0;&#x70b9;&#x5728;&#x4e8e;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x5904;&#x7406;&#x6587;&#x672c;&#x8f93;&#x5165;&#x5f15;&#x53d1;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x573a;&#x666f;&#x56fe;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x8868;&#x793a;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#xff0c;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x5b9e;&#x8df5;&#x5e94;&#x7528;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1515,1517"}}],"payload":{"tag":"li","lines":"1511,1517","fold":1}}],"payload":{"tag":"h4","lines":"1509,1510"}}],"payload":{"tag":"h3","lines":"1474,1475","fold":1}},{"content":"&#x5176;&#x4ed6;","children":[{"content":"Dentist: A Unified Hallucination Mitigation Framework for Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;Dentist&#x7684;&#x7edf;&#x4e00;&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x5c06;&#x67e5;&#x8be2;&#x5206;&#x7c7b;&#x4e3a;&#x611f;&#x77e5;&#x578b;&#x6216;&#x63a8;&#x7406;&#x578b;&#xff0c;&#x5e76;&#x9488;&#x5bf9;&#x4e0d;&#x540c;&#x7c7b;&#x578b;&#x91c7;&#x7528;&#x4e0d;&#x540c;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x7b56;&#x7565;&#xff08;&#x5982;&#x5b50;&#x95ee;&#x9898;&#x9a8c;&#x8bc1;&#x6216;&#x601d;&#x7ef4;&#x94fe;&#x63a8;&#x7406;&#xff09;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1520,1521"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x957f;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x90e8;&#x5206;&#x4e0d;&#x4e00;&#x81f4;&#xff09;&#xff0c;&#x8fd9;&#x4f1a;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x4f20;&#x64ad;&#x548c;&#x7528;&#x6237;&#x4f53;&#x9a8c;&#x4e0b;&#x964d;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x91c7;&#x7528;&#x56fa;&#x5b9a;&#x7684;&#x9a8c;&#x8bc1;&#x7b56;&#x7565;&#xff0c;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x5904;&#x7406;&#x4e0d;&#x540c;&#x7c7b;&#x578b;&#x7684;&#x67e5;&#x8be2;&#xff08;&#x5982;&#x611f;&#x77e5;&#x578b;&#x4e0e;&#x63a8;&#x7406;&#x578b;&#xff09;&#x53ca;&#x5176;&#x5f15;&#x53d1;&#x7684;&#x5e7b;&#x89c9;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x80fd;&#x591f;&#x6839;&#x636e;&#x67e5;&#x8be2;&#x7c7b;&#x578b;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x7f13;&#x89e3;&#x7b56;&#x7565;&#x7684;&#x7edf;&#x4e00;&#x6846;&#x67b6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1522,1523"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Dentist&#x6846;&#x67b6;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x6f5c;&#x5728;&#x5e7b;&#x89c9;&#x5206;&#x7c7b;&#xff1a;&#x4f7f;&#x7528;GPT&#x7b49;&#x5de5;&#x5177;&#x5c06;&#x67e5;&#x8be2;&#x5206;&#x7c7b;&#x4e3a;&#x611f;&#x77e5;&#x578b;&#xff08;&#x5982;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x8bc6;&#x522b;&#xff09;&#x6216;&#x63a8;&#x7406;&#x578b;&#xff08;&#x5982;&#x903b;&#x8f91;&#x63a8;&#x7406;&#xff09;&#xff1b;2&#xff09;&#x5206;&#x6cbb;&#x5904;&#x7406;&#xff1a;&#x5bf9;&#x611f;&#x77e5;&#x578b;&#x67e5;&#x8be2;&#xff0c;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x5b50;&#x95ee;&#x9898;&#x5e76;&#x9a8c;&#x8bc1;&#x5b50;&#x7b54;&#x6848;&#x6765;&#x4fee;&#x6b63;&#x5e7b;&#x89c9;&#xff1b;&#x5bf9;&#x63a8;&#x7406;&#x578b;&#x67e5;&#x8be2;&#xff0c;&#x5229;&#x7528;&#x601d;&#x7ef4;&#x94fe;&#xff08;CoT&#xff09;&#x63d0;&#x793a;&#x8fdb;&#x884c;&#x903b;&#x8f91;&#x9a8c;&#x8bc1;&#xff1b;3&#xff09;&#x9a8c;&#x8bc1;&#x5faa;&#x73af;&#xff1a;&#x91cd;&#x590d;&#x4e0a;&#x8ff0;&#x8fc7;&#x7a0b;&#x76f4;&#x81f3;&#x751f;&#x6210;&#x7b54;&#x6848;&#x8bed;&#x4e49;&#x7a33;&#x5b9a;&#x6216;&#x8fbe;&#x5230;&#x5faa;&#x73af;&#x4e0a;&#x9650;&#xff0c;&#x786e;&#x4fdd;&#x5e7b;&#x89c9;&#x88ab;&#x5f7b;&#x5e95;&#x6d88;&#x9664;&#x3002;","children":[],"payload":{"tag":"li","lines":"1523,1524"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;MMbench&#x3001;POPE&#x3001;CHAIR&#x548c;LLaVA-QA90&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#xff0c;Dentist&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff08;InstructBLIP/LLaVA/VisualGLM&#xff09;&#x7684;&#x51c6;&#x786e;&#x6027;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;MMbench&#x7684;Image Quality&#xff08;&#x7c97;&#x7c92;&#x5ea6;&#x611f;&#x77e5;&#x4efb;&#x52a1;&#xff09;&#x4e0a;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x5206;&#x522b;&#x63d0;&#x5347;&#x4e86;13.44%&#x3001;10.2%&#x548c;15.8%&#x3002;&#x4e0e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;Woodpecker&#x76f8;&#x6bd4;&#xff0c;Dentist&#x5728;&#x591a;&#x9879;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x66f4;&#x4f18;&#x7684;&#x6548;&#x679c;&#x548c;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1524,1525"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: Dentist&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x5206;&#x7c7b;&#x67e5;&#x8be2;&#x7c7b;&#x578b;&#x5e76;&#x5b9a;&#x5236;&#x5316;&#x7f13;&#x89e3;&#x7b56;&#x7565;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;LVLM&#x4e2d;&#x591a;&#x6837;&#x5316;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x6a21;&#x5757;&#x5316;&#x8bbe;&#x8ba1;&#x6613;&#x4e8e;&#x96c6;&#x6210;&#x5230;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x4e2d;&#xff0c;&#x5e76;&#x4e3a;&#x672a;&#x6765;&#x6269;&#x5c55;&#x65b0;&#x7684;&#x5206;&#x7c7b;&#x548c;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x63d0;&#x4f9b;&#x4e86;&#x4fbf;&#x5229;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5bf9;&#x51cf;&#x5c11;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x4f20;&#x64ad;&#x5177;&#x6709;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1525,1527"}}],"payload":{"tag":"li","lines":"1521,1527","fold":1}}],"payload":{"tag":"h4","lines":"1519,1520"}},{"content":"VisVM: Scaling Inference-Time Search with Vision Value Model  for Improved Visual Comprehension","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;Vision Value Model (VisVM)&#xff0c;&#x4e00;&#x79cd;&#x901a;&#x8fc7;&#x63a8;&#x7406;&#x65f6;&#x641c;&#x7d22;&#x6765;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x7684;&#x65b9;&#x6cd5;&#x3002;VisVM&#x80fd;&#x9884;&#x6d4b;&#x751f;&#x6210;&#x53e5;&#x5b50;&#x7684;&#x957f;&#x671f;&#x4ef7;&#x503c;&#xff0c;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x589e;&#x52a0;&#x7ec6;&#x8282;&#xff0c;&#x751f;&#x6210;&#x7684;&#x63cf;&#x8ff0;&#x8fd8;&#x53ef;&#x7528;&#x4e8e;&#x81ea;&#x8bad;&#x7ec3;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5728;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1528,1529"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x5b58;&#x5728;&#x4e24;&#x4e2a;&#x4e3b;&#x8981;&#x95ee;&#x9898;&#xff1a;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff08;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x63cf;&#x8ff0;&#xff09;&#x548c;&#x5ffd;&#x7565;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x663e;&#x8457;&#x7684;&#x533a;&#x57df;&#x7ec6;&#x8282;&#x3002;&#x867d;&#x7136;&#x901a;&#x8fc7;&#x589e;&#x52a0;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x89c4;&#x6a21;&#x548c;&#x8d28;&#x91cf;&#x7684;&#x4f20;&#x7edf;&#x65b9;&#x6cd5;&#x53ef;&#x4ee5;&#x7f13;&#x89e3;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x5dee;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x8bba;&#x6587;&#x65e8;&#x5728;&#x63a2;&#x7d22;&#x4e00;&#x79cd;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#xff08;inference-time&#xff09;&#x901a;&#x8fc7;&#x589e;&#x52a0;&#x8ba1;&#x7b97;&#x6765;&#x63d0;&#x5347;VLM&#x54cd;&#x5e94;&#x8d28;&#x91cf;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x8fd9;&#x4e9b;&#x9ad8;&#x8d28;&#x91cf;&#x54cd;&#x5e94;&#x8fdb;&#x4e00;&#x6b65;&#x63a8;&#x52a8;VLM&#x7684;&#x81ea;&#x6211;&#x6539;&#x8fdb;&#xff08;self-improving&#xff09;&#xff0c;&#x8fd9;&#x5bf9;&#x4e8e;VLMs&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1530,1531"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;Vision Value Model (VisVM)&#xff0c;&#x4e00;&#x4e2a;&#x4ef7;&#x503c;&#x7f51;&#x7edc;&#x6a21;&#x578b;&#xff0c;&#x7528;&#x4e8e;&#x5728;VLM&#x8fdb;&#x884c;&#x9010;&#x6b65;&#x751f;&#x6210;&#xff08;&#x6bcf;&#x6b65;&#x751f;&#x6210;&#x4e00;&#x4e2a;&#x53e5;&#x5b50;&#xff09;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x641c;&#x7d22;&#x4e2d;&#x63d0;&#x4f9b;&#x6307;&#x5bfc;&#x3002;","children":[{"content":"1. <strong>&#x6838;&#x5fc3;&#x601d;&#x60f3;</strong>&#xff1a;&#x5c06;VLM&#x7684;&#x6587;&#x672c;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x5efa;&#x6a21;&#x4e3a;&#x4e00;&#x4e2a;&#x9a6c;&#x5c14;&#x53ef;&#x592b;&#x51b3;&#x7b56;&#x8fc7;&#x7a0b;&#xff08;MDP&#xff09;&#x3002;VisVM&#x7684;&#x72ec;&#x7279;&#x4e4b;&#x5904;&#x5728;&#x4e8e;&#x5176;&#x5177;&#x6709;<strong>&#x524d;&#x77bb;&#x6027;&#x4e00;&#x81f4;&#x6027;&#xff08;Forward-looking coherence&#xff09;<strong>&#x548c;</strong>&#x5168;&#x9762;&#x89c6;&#x89c9; grounding&#xff08;Comprehensive visual grounding&#xff09;</strong>&#x3002;","children":[],"payload":{"tag":"li","lines":"1532,1533","listIndex":1}},{"content":"2. <strong>&#x6280;&#x672f;&#x7ec6;&#x8282;</strong>&#xff1a;","children":[{"content":"<strong>&#x8f93;&#x5165;</strong>&#xff1a;&#x5f53;&#x524d;&#x751f;&#x6210;&#x7684;&#x53e5;&#x5b50;&#x548c;&#x56fe;&#x50cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1534,1535"}},{"content":"<strong>&#x8f93;&#x51fa;</strong>&#xff1a;&#x9884;&#x6d4b;&#x4e00;&#x4e2a;&#x4ee3;&#x8868;&#x957f;&#x671f;&#x4ef7;&#x503c;&#x7684;&#x6807;&#x91cf;&#xff08;V&#x503c;&#xff09;&#x3002;&#x8fd9;&#x4e2a;&#x503c;&#x4e0d;&#x4ec5;&#x8bc4;&#x4f30;&#x5f53;&#x524d;&#x53e5;&#x5b50;&#x7684;&#x8d28;&#x91cf;&#xff0c;&#x8fd8;&#x9884;&#x4f30;&#x5f53;&#x524d;&#x9009;&#x62e9;&#x5bf9;&#x540e;&#x7eed;&#x53e5;&#x5b50;&#x8d28;&#x91cf;&#xff08;&#x5982;&#x662f;&#x5426;&#x5f15;&#x53d1;&#x5e7b;&#x89c9;&#xff09;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1535,1536"}},{"content":"<strong>&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;</strong>&#xff1a;&#x4f7f;&#x7528;<strong>&#x65f6;&#x5e8f;&#x5dee;&#x5206;&#x5b66;&#x4e60;&#xff08;Temporal Difference (TD) learning&#xff09;</strong> &#x8fdb;&#x884c;&#x8bad;&#x7ec3;&#x3002;&#x5176;&#x635f;&#x5931;&#x51fd;&#x6570;&#x786e;&#x4fdd;&#x5f53;&#x524d;&#x72b6;&#x6001;&#x7684;&#x9884;&#x6d4b;&#x503c;&#x7b49;&#x4e8e;&#x5f53;&#x524d;&#x83b7;&#x5f97;&#x7684;&#x5b9e;&#x9645;&#x5956;&#x52b1;&#x52a0;&#x4e0a;&#x4e0b;&#x4e00;&#x4e2a;&#x72b6;&#x6001;&#x7684;&#x6298;&#x6263;&#x9884;&#x6d4b;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1536,1537"}},{"content":"<strong>&#x5956;&#x52b1;&#x4fe1;&#x53f7;</strong>&#xff1a;&#x5229;&#x7528;CLIP&#x7684;&#x6587;&#x672c;-&#x56fe;&#x50cf;&#x76f8;&#x4f3c;&#x5ea6;&#x5ea6;&#x91cf;&#x4f5c;&#x4e3a;&#x57fa;&#x7840;&#x5956;&#x52b1;&#x4fe1;&#x53f7;&#xff08;Process Reward Model, PRM&#xff09;&#xff0c;&#x4ee5;&#x6355;&#x6349;&#x4e30;&#x5bcc;&#x7684;&#x89c6;&#x89c9;&#x8bed;&#x4e49;&#x5e76;&#x786e;&#x4fdd;&#x6587;&#x672c;&#x4e0e;&#x56fe;&#x50cf;&#x7684;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"1537,1538"}}],"payload":{"tag":"li","lines":"1533,1538","listIndex":2}},{"content":"3. <strong>&#x63a8;&#x7406;&#x641c;&#x7d22;</strong>&#xff1a;&#x5728;VLM&#x751f;&#x6210;&#x6bcf;&#x4e2a;&#x53e5;&#x5b50;&#x65f6;&#xff0c;&#x5e76;&#x884c;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x5019;&#x9009;&#x53e5;&#x5b50;&#xff0c;&#x5e76;&#x7531;VisVM&#x4e3a;&#x6bcf;&#x4e2a;&#x5019;&#x9009;&#x9884;&#x6d4b;&#x957f;&#x671f;&#x4ef7;&#x503c;&#xff08;V&#x503c;&#xff09;&#x3002;&#x6a21;&#x578b;&#x9009;&#x62e9;&#x5177;&#x6709;&#x6700;&#x9ad8;V&#x503c;&#x7684;&#x5019;&#x9009;&#x53e5;&#x5b50;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x6bcf;&#x4e00;&#x6b65;&#x90fd;&#x505a;&#x51fa;&#x66f4;&#x4f18;&#x7684;&#x9009;&#x62e9;&#xff0c;&#x5f15;&#x5bfc;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x6700;&#x7ec8;&#x4ea7;&#x751f;&#x66f4;&#x9ad8;&#x8d28;&#x91cf;&#x7684;&#x6574;&#x4f53;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1538,1539","listIndex":3}}],"payload":{"tag":"li","lines":"1531,1539"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8bc1;&#x5b9e;&#x4e86;VisVM&#x7684;&#x6709;&#x6548;&#x6027;&#xff1a;","children":[{"content":"1. <strong>&#x63a8;&#x7406;&#x65f6;&#x641c;&#x7d22;&#xff08;Inference-time search&#xff09;</strong>&#xff1a;&#x5728;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x6027;&#x56fe;&#x50cf;&#x6807;&#x9898;&#xff08;descriptive caption&#xff09;&#x7684;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x4e0e;&#x8d2a;&#x5a6a;&#x89e3;&#x7801;&#xff08;greedy decoding&#xff09;&#x3001;Best-of-N&#x89e3;&#x7801;&#x4ee5;&#x53ca;&#x4f7f;&#x7528;CLIP&#x4f5c;&#x4e3a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#xff08;CLIP-PRM&#xff09;&#x7684;&#x641c;&#x7d22;&#x65b9;&#x6cd5;&#x76f8;&#x6bd4;&#xff0c;<strong>VisVM&#x5f15;&#x5bfc;&#x7684;&#x641c;&#x7d22;&#x751f;&#x6210;&#x7684;&#x6807;&#x9898;&#x5728;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x5305;&#x542b;&#x4e86;&#x66f4;&#x4e30;&#x5bcc;&#x7684;&#x89c6;&#x89c9;&#x7ec6;&#x8282;</strong>&#x3002;&#x5177;&#x4f53;&#x8868;&#x73b0;&#x4e3a;&#xff1a;","children":[{"content":"&#x5728;&#x8861;&#x91cf;&#x5e7b;&#x89c9;&#x7684;&#x6307;&#x6807;&#xff08;CHAIRs&#x548c;MMHal&#xff09;&#x4e0a;&#x5f97;&#x5206;&#x66f4;&#x4f18;&#x3002;","children":[],"payload":{"tag":"li","lines":"1541,1542"}},{"content":"&#x5373;&#x4f7f;&#x5728;&#x641c;&#x7d22;&#x9884;&#x7b97;&#x66f4;&#x5c0f;&#xff08;&#x641c;&#x7d22;&#x5bbd;&#x5ea6;&#x4e3a;6&#xff09;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x5176;&#x6548;&#x679c;&#x4e5f;&#x4f18;&#x4e8e;&#x641c;&#x7d22;&#x5bbd;&#x5ea6;&#x4e3a;30&#x7684;Best-of-N&#x65b9;&#x6cd5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1542,1543"}},{"content":"&#x5728;GPT&#x548c;&#x4eba;&#x7c7b;&#x8bc4;&#x4f30;&#x4e2d;&#xff0c;VisVM&#x751f;&#x6210;&#x7684;&#x6807;&#x9898;&#x5728;74%&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#x88ab;&#x504f;&#x597d;&#x4e8e;&#x8d2a;&#x5a6a;&#x89e3;&#x7801;&#x4ea7;&#x751f;&#x7684;&#x6807;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1543,1544"}}],"payload":{"tag":"li","lines":"1540,1544","listIndex":1}},{"content":"2. <strong>&#x81ea;&#x8bad;&#x7ec3;&#x6539;&#x8fdb;&#xff08;Self-improvement training&#xff09;</strong>&#xff1a;&#x5c06;VisVM&#x5f15;&#x5bfc;&#x751f;&#x6210;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x63cf;&#x8ff0;&#x6807;&#x9898;&#x4f5c;&#x4e3a;&#x76d1;&#x7763;&#x5fae;&#x8c03;&#xff08;SFT&#xff09;&#x6570;&#x636e;&#xff0c;&#x7528;&#x4e8e;&#x8bad;&#x7ec3;&#x539f;&#x59cb;VLM&#xff08;LLaVA-Next-7B&#x548c;Qwen2-VL-7B&#xff09;&#x3002;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;&#x7ecf;&#x8fc7;&#x81ea;&#x8bad;&#x7ec3;&#x540e;&#xff0c;&#x539f;&#x59cb;VLM&#x5728;<strong>&#x4e5d;&#x4e2a;&#x6807;&#x51c6;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x5e73;&#x5747;&#x5206;&#x522b;&#x63d0;&#x5347;&#x4e86;10.8%&#x548c;7.3%</strong>&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x4f5c;&#x4e3a;&#x81ea;&#x6211;&#x6539;&#x8fdb;&#x7ba1;&#x9053;&#x7684;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1544,1545","listIndex":2}}],"payload":{"tag":"li","lines":"1539,1545"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;VisVM&#x6210;&#x529f;&#x5730;&#x4e3a;VLM&#x7684;&#x63a8;&#x7406;&#x65f6;&#x641c;&#x7d22;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x6709;&#x6548;&#x7684;&#x3001;&#x5177;&#x6709;&#x957f;&#x8fdc;&#x773c;&#x5149;&#x7684;&#x4ef7;&#x503c;&#x6a21;&#x578b;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x7684;&#x8d28;&#x91cf;&#xff08;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3001;&#x589e;&#x52a0;&#x7ec6;&#x8282;&#xff09;&#x3002;&#x66f4;&#x91cd;&#x8981;&#x7684;&#x662f;&#xff0c;&#x8fd9;&#x79cd;&#x65b9;&#x6cd5;&#x5f00;&#x542f;&#x4e86;&#x4e00;&#x6761;&#x901a;&#x5f80;<strong>&#x81ea;&#x6211;&#x6539;&#x8fdb;&#x7684;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;self-improving VLMs&#xff09;</strong> &#x7684;&#x9053;&#x8def;&#xff1a;&#x901a;&#x8fc7;&#x63a8;&#x7406;&#x65f6;&#x8ba1;&#x7b97;&#x751f;&#x6210;&#x7684;&#x9ad8;&#x8d28;&#x91cf;&#x6570;&#x636e;&#x53ef;&#x4ee5;&#x7528;&#x4e8e;&#x6a21;&#x578b;&#x8bad;&#x7ec3;&#xff0c;&#x4ece;&#x800c;&#x5f62;&#x6210;&#x4e00;&#x4e2a;&#x6027;&#x80fd;&#x6301;&#x7eed;&#x63d0;&#x5347;&#x7684;&#x6b63;&#x5411;&#x5faa;&#x73af;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x8868;&#x660e;&#xff0c;&#x589e;&#x52a0;&#x63a8;&#x7406;&#x65f6;&#x8ba1;&#x7b97;&#x662f;&#x589e;&#x5f3a;VLM&#x80fd;&#x529b;&#x7684;&#x4e00;&#x4e2a;&#x6709;&#x6548;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x65b9;&#x5411;&#xff0c;&#x5f25;&#x8865;&#x4e86;&#x4ec5;&#x4f9d;&#x8d56;&#x6269;&#x5927;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x7684;&#x4e0d;&#x8db3;&#xff0c;&#x5bf9;&#x672a;&#x6765;VLMs&#x7684;&#x53d1;&#x5c55;&#x5177;&#x6709;&#x91cd;&#x8981;&#x7684;&#x542f;&#x793a;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1545,1547"}}],"payload":{"tag":"li","lines":"1529,1547","fold":1}}],"payload":{"tag":"h4","lines":"1527,1528"}}],"payload":{"tag":"h3","lines":"1517,1518","fold":1}}],"payload":{"tag":"h2","lines":"1472,1473"}},{"content":"&#x7279;&#x6b8a;","children":[{"content":"&#x65b0;&#x53d1;&#x73b0;","children":[{"content":"API Cutoff: The Role of Background Information in Reducing Object Hallucination in Vision-Language Models: Insights from Cutoff API Prompting","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;&#x5728;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x4e2d;&#xff0c;&#x4fdd;&#x7559;&#x56fe;&#x50cf;&#x80cc;&#x666f;&#x4fe1;&#x606f;&#x5bf9;&#x4e8e;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x201c;Cutoff API Prompting&#x201d;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x65b9;&#x6cd5;&#xff0c;&#x5728;&#x7a81;&#x51fa;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x7684;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x80cc;&#x666f;&#x4e0a;&#x4e0b;&#x6587;&#xff0c;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x77db;&#x76fe;&#x6587;&#x672c;&#x7684;&#x9519;&#x8bef;&#x3002;","children":[],"payload":{"tag":"li","lines":"1552,1553"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;&#x5982;CLIP&#x3001;LLaVA&#xff09;&#x5728;&#x5904;&#x7406;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x65f6;&#xff0c;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x77db;&#x76fe;&#x7684;&#x8f93;&#x51fa;&#xff0c;&#x5373;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x73b0;&#x8c61;&#xff0c;&#x5c24;&#x5176;&#x662f;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x9519;&#x8bef;&#x5730;&#x58f0;&#x79f0;&#x56fe;&#x50cf;&#x4e2d;&#x5b58;&#x5728;&#x6216;&#x4e0d;&#x5b58;&#x5728;&#x67d0;&#x7269;&#x4f53;&#xff09;&#x3002;&#x8fd9;&#x4e25;&#x91cd;&#x9650;&#x5236;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x7b49;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x5c1d;&#x8bd5;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff08;&#x5982;API Prompting&#xff09;&#x6765;&#x6291;&#x5236;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x5176;&#x6548;&#x679c;&#x5206;&#x6790;&#x4e0d;&#x8db3;&#xff0c;&#x7279;&#x522b;&#x662f;&#x80cc;&#x666f;&#x4fe1;&#x606f;&#x5728;&#x7269;&#x4f53;&#x8bc6;&#x522b;&#x4e2d;&#x7684;&#x4f5c;&#x7528;&#x5c1a;&#x4e0d;&#x660e;&#x786e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1554,1555"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x91c7;&#x7528;&#x4e86;&#x4e00;&#x79cd;&#x6539;&#x8fdb;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x65b9;&#x6cd5;&#x201c;Cutoff API Prompting&#x201d;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x9996;&#x5148;&#x4ece;Heatmap VLM&#xff08;H-VLM&#xff0c;&#x5982;CLIP&#x6216;LLaVA&#xff09;&#x4e2d;&#x63d0;&#x53d6;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x70ed;&#x56fe;&#xff08;Visual Attention Heatmap&#xff09;&#xff0c;&#x4ee5;&#x8bc6;&#x522b;&#x56fe;&#x50cf;&#x4e2d;&#x5bf9;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x8d21;&#x732e;&#x6700;&#x5927;&#x7684;&#x533a;&#x57df;&#x3002;&#x5173;&#x952e;&#x521b;&#x65b0;&#x5728;&#x4e8e;&#x5f15;&#x5165;&#x4e86;&#x201c;&#x6700;&#x5c0f;&#x622a;&#x65ad;&#xff08;Cutoff&#xff09;&#x201d;&#x64cd;&#x4f5c;&#xff1a;&#x5c06;&#x70ed;&#x56fe;&#x4e2d;&#x4f4e;&#x4e8e;&#x9608;&#x503c;&#xff08;&#x5982;0.5&#xff09;&#x7684;&#x503c;&#x66ff;&#x6362;&#x4e3a;&#x8be5;&#x9608;&#x503c;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x7a81;&#x51fa;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x4fdd;&#x7559;&#x66f4;&#x591a;&#x80cc;&#x666f;&#x4fe1;&#x606f;&#xff08;&#x800c;&#x975e;&#x5b8c;&#x5168;&#x906e;&#x853d;&#x80cc;&#x666f;&#xff09;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x4f5c;&#x8005;&#x4f7f;&#x7528;MSCOCO&#x6570;&#x636e;&#x96c6;&#x53ca;&#x5176;&#x771f;&#x5b9e;&#x5206;&#x5272;&#x6807;&#x6ce8;&#xff08;ground truth segmentation&#xff09;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x5b9e;&#x9a8c;&#xff0c;&#x5e76;&#x901a;&#x8fc7;POPE&#x57fa;&#x51c6;&#x8bc4;&#x4f30;&#x5bf9;&#x8c61;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x8ba1;&#x7b97;&#x4e86;&#x7cbe;&#x786e;&#x5ea6;&#x3001;&#x53ec;&#x56de;&#x7387;&#x3001;IoU&#x7b49;&#x6307;&#x6807;&#x6765;&#x91cf;&#x5316;&#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x4e0e;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x7684;&#x5bf9;&#x9f50;&#x7a0b;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1555,1556"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. Cutoff API Prompting&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#xff1a;&#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#xff0c;&#x4f7f;&#x7528;Cutoff&#x540e;&#xff0c;Recall&#x6307;&#x6807;&#x63d0;&#x5347;&#x4e86;&#x7ea6;3%&#xff0c;&#x603b;&#x4f53;&#x51c6;&#x786e;&#x7387;&#xff08;Acc.&#xff09;&#x548c;F1&#x5206;&#x6570;&#x5747;&#x6709;&#x63d0;&#x9ad8;&#xff08;&#x4f8b;&#x5982;CLIP w/ Cutoff&#x7684;Acc.&#x4ece;86.52%&#x5347;&#x81f3;88.59%&#xff09;&#x3002;2. &#x80cc;&#x666f;&#x4fe1;&#x606f;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff1a;&#x5f53;&#x4f7f;&#x7528;&#x771f;&#x5b9e;&#x5206;&#x5272;&#x63a9;&#x7801;&#x5b8c;&#x5168;&#x906e;&#x853d;&#x80cc;&#x666f;&#xff08;API-Seg.&#xff09;&#x65f6;&#xff0c;Recall&#x9aa4;&#x964d;&#x81f3;71.78%&#xff1b;&#x800c;&#x5e94;&#x7528;Cutoff&#x540e;&#xff08;&#x4fdd;&#x7559;&#x90e8;&#x5206;&#x80cc;&#x666f;&#xff09;&#xff0c;Recall&#x5927;&#x5e45;&#x56de;&#x5347;&#x81f3;89.24%&#x3002;3. &#x89c6;&#x89c9;&#x6ce8;&#x610f;&#x529b;&#x5bf9;&#x9f50;&#x5f71;&#x54cd;&#x6548;&#x679c;&#xff1a;&#x5f53;&#x6ce8;&#x610f;&#x529b;&#x70ed;&#x56fe;&#x4e0e;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x5bf9;&#x9f50;&#x826f;&#x597d;&#xff08;IoU&#x9ad8;&#xff09;&#x65f6;&#xff0c;API Prompting&#x66f4;&#x6709;&#x6548;&#xff1b;&#x53cd;&#x4e4b;&#x5219;&#x6548;&#x679c;&#x8f83;&#x5dee;&#x3002;4. &#x5c0f;&#x7269;&#x4f53;&#x66f4;&#x4f9d;&#x8d56;&#x80cc;&#x666f;&#xff1a;&#x5b9e;&#x9a8c;&#x53d1;&#x73b0;&#xff0c;&#x5f53;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x8f83;&#x5c0f;&#x65f6;&#xff0c;&#x5b8c;&#x5168;&#x79fb;&#x9664;&#x80cc;&#x666f;&#x4f1a;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x663e;&#x8457;&#x4e0b;&#x964d;&#xff0c;&#x8868;&#x660e;&#x5c0f;&#x7269;&#x4f53;&#x7684;&#x8bc6;&#x522b;&#x66f4;&#x9700;&#x8981;&#x4e0a;&#x4e0b;&#x6587;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1556,1557"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x80cc;&#x666f;&#x4fe1;&#x606f;&#x5bf9;&#x4e8e;VLM&#x51c6;&#x786e;&#x5224;&#x65ad;&#x7269;&#x4f53;&#x662f;&#x5426;&#x5b58;&#x5728;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;&#x5b8c;&#x5168;&#x906e;&#x853d;&#x56fe;&#x50cf;&#x90e8;&#x5206;&#x533a;&#x57df;&#xff08;&#x5373;&#x4f7f;&#x662f;&#x80cc;&#x666f;&#xff09;&#x7684;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x65b9;&#x6cd5;&#x4f1a;&#x52a0;&#x5267;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x800c;Cutoff API Prompting&#x901a;&#x8fc7;&#x5728;&#x7a81;&#x51fa;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x7684;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x80cc;&#x666f;&#x4e0a;&#x4e0b;&#x6587;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;&#x672a;&#x6765;&#x7814;&#x7a76;&#x5e94;&#x4e13;&#x6ce8;&#x4e8e;&#x589e;&#x5f3a;&#x76ee;&#x6807;&#x7269;&#x4f53;&#x7684;&#x53ef;&#x89c1;&#x6027;&#xff0c;&#x800c;&#x975e;&#x906e;&#x853d;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x5b89;&#x5168;&#x7684;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x65b9;&#x5411;&#xff0c;&#x4f46;&#x5f53;&#x524d;&#x7814;&#x7a76;&#x4ec5;&#x9650;&#x4e8e;LLaVA&#x6a21;&#x578b;&#x548c;MSCOCO&#x6570;&#x636e;&#x96c6;&#xff0c;&#x6cdb;&#x5316;&#x6027;&#x548c;&#x5bf9;&#x4e0d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x7684;&#x5206;&#x6790;&#x4ecd;&#x9700;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x3002;","children":[],"payload":{"tag":"li","lines":"1557,1559"}}],"payload":{"tag":"li","lines":"1553,1559","fold":1}}],"payload":{"tag":"h4","lines":"1551,1552"}},{"content":"Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x7814;&#x7a76;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x53d1;&#x73b0;&#x73b0;&#x6709;&#x5148;&#x8fdb;&#x6a21;&#x578b;&#x666e;&#x904d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x9488;&#x5bf9;&#x540d;&#x8bcd;&#x5e7b;&#x89c9;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x5bf9;&#x52a8;&#x8bcd;&#x65e0;&#x6548;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x57fa;&#x4e8e;&#x52a8;&#x8bcd;&#x77e5;&#x8bc6;&#x7684;&#x8c03;&#x4f18;&#x65b9;&#x6cd5;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x6027;&#x80fd;&#x4ecd;&#x6709;&#x63d0;&#x5347;&#x7a7a;&#x95f4;&#x3002;","children":[],"payload":{"tag":"li","lines":"1560,1561"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#xff08;Verb Hallucination&#xff09;&#x95ee;&#x9898;&#x3002;&#x5c3d;&#x7ba1;MLLMs&#x5728;OCR&#x3001;VQA&#x3001;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x7b49;&#x4efb;&#x52a1;&#x4e0a;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x4e8b;&#x5b9e;&#x4e0d;&#x7b26;&#x3001;&#x65e0;&#x5173;&#x6216;&#x8352;&#x8c2c;&#x7684;&#x5185;&#x5bb9;&#xff09;&#x4ecd;&#x7136;&#x662f;&#x4e00;&#x4e2a;&#x6301;&#x7eed;&#x5b58;&#x5728;&#x7684;&#x95ee;&#x9898;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x548c;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x96c6;&#x4e2d;&#x4e8e;&#x540d;&#x8bcd;/&#x7269;&#x4f53;&#x76f8;&#x5173;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x800c;&#x52a8;&#x8bcd;&#x4f5c;&#x4e3a;&#x7406;&#x89e3;&#x4eba;&#x7c7b;&#x884c;&#x4e3a;&#x7684;&#x5173;&#x952e;&#x6982;&#x5ff5;&#xff0c;&#x5176;&#x76f8;&#x5173;&#x7684;&#x5e7b;&#x89c9;&#x4e00;&#x76f4;&#x88ab;&#x5ffd;&#x89c6;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x52a8;&#x8bcd;&#x662f;&#x63cf;&#x8ff0;&#x52a8;&#x6001;&#x884c;&#x4e3a;&#x548c;&#x4ea4;&#x4e92;&#x7684;&#x6838;&#x5fc3;&#xff0c;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5bf9;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x573a;&#x666f;&#x7684;&#x7406;&#x89e3;&#x548c;&#x63cf;&#x8ff0;&#x7684;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x4ece;&#x800c;&#x9650;&#x5236;MLLMs&#x5728;&#x5173;&#x952e;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x4eba;&#x673a;&#x4ea4;&#x4e92;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1562,1563"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x591a;&#x79cd;&#x89c6;&#x89d2;&#x7cfb;&#x7edf;&#x5730;&#x63a2;&#x7a76;&#x4e86;MLLMs&#x7684;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff1a;1. <strong>&#x6784;&#x5efa;&#x9996;&#x4e2a;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;</strong>&#xff1a;&#x57fa;&#x4e8e;HICO&#x548c;CharadesEgo&#x6570;&#x636e;&#x96c6;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#xff0c;&#x6784;&#x5efa;&#x4e86;&#x5305;&#x542b;&#x591a;&#x9009;&#x9898;&#xff08;MCQ&#xff09;&#x548c;&#x662f;&#x975e;&#x9898;&#xff08;YN&#xff09;&#x7684;&#x8bc4;&#x6d4b;&#x57fa;&#x51c6;&#x3002;2. <strong>&#x591a;&#x7ef4;&#x5ea6;&#x63a2;&#x6d4b;</strong>&#xff1a;&#x4ece;&#x4e0d;&#x540c;&#x67e5;&#x8be2;&#x6761;&#x4ef6;&#xff08;&#x5982;&#x95ee;&#x9898;&#x683c;&#x5f0f;&#x3001;&#x662f;&#x5426;&#x5173;&#x8054;&#x7269;&#x4f53;&#xff09;&#x3001;&#x4e0d;&#x540c;&#x6210;&#x50cf;&#x6761;&#x4ef6;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x8d28;&#x91cf;&#x3001;&#x89c6;&#x89d2;&#xff09;&#x548c;&#x4e0d;&#x540c;&#x8bed;&#x4e49;&#x6761;&#x4ef6;&#xff08;&#x5982;&#x52a8;&#x8bcd;&#x7a00;&#x6709;&#x5ea6;&#x3001;&#x5185;&#x5bb9;&#x6a21;&#x7cca;&#x6027;&#xff09;&#x5168;&#x9762;&#x6d4b;&#x8bd5;&#x6a21;&#x578b;&#x3002;3. <strong>&#x8bc4;&#x4f30;&#x73b0;&#x6709;&#x65b9;&#x6cd5;</strong>&#xff1a;&#x6d4b;&#x8bd5;&#x4e86;&#x73b0;&#x6709;&#x7684;&#x4f4e;&#x6210;&#x672c;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x5bf9;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x7684;&#x6548;&#x679c;&#x3002;4. <strong>&#x63d0;&#x51fa;&#x65b0;&#x65b9;&#x6cd5;</strong>&#xff1a;&#x4e3a;&#x4e86;&#x7f13;&#x89e3;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x4e30;&#x5bcc;&#x52a8;&#x8bcd;&#x77e5;&#x8bc6;&#xff08;verb structure knowledge&#xff09;&#x7684;&#x53c2;&#x6570;&#x9ad8;&#x6548;&#x5fae;&#x8c03;&#xff08;parameter-efficient fine-tuning&#xff09;&#x65b9;&#x6cd5;&#x4f5c;&#x4e3a;&#x57fa;&#x7ebf;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1563,1564"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x548c;&#x53d1;&#x73b0;&#x5305;&#x62ec;&#xff1a;1. <strong>&#x666e;&#x904d;&#x4e14;&#x4e25;&#x91cd;&#x7684;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;</strong>&#xff1a;&#x6240;&#x6709;&#x6d4b;&#x8bd5;&#x7684;&#x5148;&#x8fdb;MLLMs&#xff08;&#x5982;GPT-4-Turbo, Gemini-1.5-Flash, LLaVA, InstructBLIP&#x7b49;&#xff09;&#x5728;&#x52a8;&#x8bcd;&#x7406;&#x89e3;&#x4e0a;&#x90fd;&#x8868;&#x73b0;&#x4e0d;&#x4f73;&#xff0c;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x3002;2. <strong>&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x65e0;&#x6548;</strong>&#xff1a;&#x9488;&#x5bf9;&#x540d;&#x8bcd;&#x5e7b;&#x89c9;&#x8bbe;&#x8ba1;&#x7684;&#x73b0;&#x6709;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x5728;&#x7f13;&#x89e3;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x4e0a;&#x57fa;&#x672c;&#x65e0;&#x6548;&#x3002;3. <strong>&#x5bf9;&#x7269;&#x4f53;&#x7684;&#x4e25;&#x91cd;&#x4f9d;&#x8d56;</strong>&#xff1a;MLLMs&#x7684;&#x52a8;&#x8bcd;&#x7406;&#x89e3;&#x4e25;&#x91cd;&#x4f9d;&#x8d56;&#x4e8e;&#x95ee;&#x9898;&#x4e2d;&#x662f;&#x5426;&#x63d0;&#x53ca;&#x7269;&#x4f53;&#xff08;Object Correlation&#xff09;&#x3002;&#x5728;&#x6ca1;&#x6709;&#x7269;&#x4f53;&#x53c2;&#x8003;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff08;&#x5982;&#x201c;&#x6709;&#x4eba;&#x5728;&#x5403;&#x4e1c;&#x897f;&#x5417;&#xff1f;&#x201d;&#xff09;&#xff0c;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x6025;&#x5267;&#x4e0b;&#x964d;&#x3002;4. <strong>&#x65e0;&#x6cd5;&#x62d2;&#x7edd;&#x503e;&#x5411;</strong>&#xff1a;&#x5728;&#x662f;&#x975e;&#x9898;&#x4e2d;&#xff0c;&#x6240;&#x6709;&#x6a21;&#x578b;&#x90fd;&#x8868;&#x73b0;&#x51fa;&#x9ad8;&#x53ec;&#x56de;&#x7387;&#x4f46;&#x4f4e;&#x7cbe;&#x786e;&#x5ea6;&#xff0c;&#x610f;&#x5473;&#x7740;&#x6a21;&#x578b;&#x503e;&#x5411;&#x4e8e;&#x56de;&#x7b54;&#x201c;&#x662f;&#x201d;&#xff0c;&#x65e0;&#x8bba;&#x8be5;&#x52a8;&#x8bcd;&#x662f;&#x5426;&#x771f;&#x7684;&#x5728;&#x56fe;&#x50cf;&#x4e2d;&#x51fa;&#x73b0;&#x3002;5. <strong>&#x5bf9;&#x56fe;&#x50cf;&#x8d28;&#x91cf;&#x654f;&#x611f;</strong>&#xff1a;&#x5f53;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x8d28;&#x91cf;&#x4e0b;&#x964d;&#xff08;&#x5982;&#x6dfb;&#x52a0;&#x6912;&#x76d0;&#x566a;&#x58f0;&#xff09;&#x65f6;&#xff0c;&#x6240;&#x6709;&#x6a21;&#x578b;&#x7684;&#x52a8;&#x8bcd;&#x7406;&#x89e3;&#x6027;&#x80fd;&#x90fd;&#x660e;&#x663e;&#x4e0b;&#x964d;&#xff0c;&#x4e14;&#x4e00;&#x4e9b;&#x9ad8;&#x6027;&#x80fd;&#x6a21;&#x578b;&#x7684;&#x9519;&#x8bef;&#x4e00;&#x81f4;&#x6027;&#xff08;error consistency&#xff09;&#x8f83;&#x4f4e;&#xff0c;&#x8bf4;&#x660e;&#x5176;&#x5e7b;&#x89c9;&#x66f4;&#x5bb9;&#x6613;&#x88ab;&#x89c6;&#x89c9;&#x5931;&#x771f;&#x8bf1;&#x53d1;&#x3002;6. <strong>&#x65b0;&#x65b9;&#x6cd5;&#x7684;&#x6709;&#x6548;&#x6027;</strong>&#xff1a;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x7684;&#x57fa;&#x4e8e;&#x52a8;&#x8bcd;&#x77e5;&#x8bc6;&#x7684;&#x5fae;&#x8c03;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x52a8;&#x8bcd;&#x76f8;&#x5173;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x5176;&#x6027;&#x80fd;&#x4ecd;&#x8fdc;&#x672a;&#x8fbe;&#x5230;&#x4ee4;&#x4eba;&#x6ee1;&#x610f;&#x7684;&#x7a0b;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1564,1565"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff1a;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x662f;MLLMs&#x4e2d;&#x4e00;&#x4e2a;&#x666e;&#x904d;&#x5b58;&#x5728;&#x4f46;&#x6b64;&#x524d;&#x88ab;&#x5ffd;&#x89c6;&#x7684;&#x4e25;&#x91cd;&#x95ee;&#x9898;&#x3002;&#x73b0;&#x6709;&#x7684;&#x3001;&#x4e13;&#x6ce8;&#x4e8e;&#x540d;&#x8bcd;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#x4e3a;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x6709;&#x6548;&#x7684;&#x57fa;&#x7ebf;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5f15;&#x5165;&#x52a8;&#x8bcd;&#x7279;&#x5f02;&#x6027;&#x77e5;&#x8bc6;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x8fd9;&#x4e00;&#x7814;&#x7a76;&#x63ed;&#x793a;&#x4e86;MLLMs&#x5728;&#x7406;&#x89e3;&#x52a8;&#x6001;&#x89c6;&#x89c9;&#x6982;&#x5ff5;&#x65b9;&#x9762;&#x7684;&#x5c40;&#x9650;&#x6027;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x5f7b;&#x5e95;&#x6d88;&#x9664;&#x52a8;&#x8bcd;&#x5e7b;&#x89c9;&#x7684;&#x7814;&#x7a76;&#x94fa;&#x5e73;&#x4e86;&#x9053;&#x8def;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x547c;&#x5401;&#x793e;&#x533a;&#x66f4;&#x591a;&#x5730;&#x5173;&#x6ce8;&#x52a8;&#x8bcd;&#x53ca;&#x76f8;&#x5173;&#x52a8;&#x6001;&#x6982;&#x5ff5;&#x7684;&#x7406;&#x89e3;&#xff0c;&#x63a8;&#x52a8;&#x5f00;&#x53d1;&#x66f4;&#x5168;&#x9762;&#x7684;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#x548c;&#x7f13;&#x89e3;&#x6280;&#x672f;&#xff0c;&#x4ece;&#x800c;&#x63d0;&#x5347;MLLMs&#x5728;&#x590d;&#x6742;&#x573a;&#x666f;&#x4e2d;&#x7406;&#x89e3;&#x548c;&#x63cf;&#x8ff0;&#x884c;&#x4e3a;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1565,1567"}}],"payload":{"tag":"li","lines":"1561,1567","fold":1}}],"payload":{"tag":"h4","lines":"1559,1560"}},{"content":"SKIP \\N: A Simple Method to Reduce Hallucination in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x6bb5;&#x843d;&#x5206;&#x9694;&#x7b26;&#x2018;\\n\\n&#x2019;&#x4f1a;&#x5f15;&#x53d1;&#x8bed;&#x4e49;&#x504f;&#x79fb;&#x504f;&#x5dee;&#xff0c;&#x5bfc;&#x81f4;&#x540e;&#x7eed;&#x63cf;&#x8ff0;&#x51fa;&#x73b0;&#x66f4;&#x591a;&#x5e7b;&#x89c9;&#xff08;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#xff09;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x7b80;&#x5355;&#x65b9;&#x6cd5;&#xff1a;&#x4fee;&#x6539;&#x8f93;&#x5165;&#x63d0;&#x793a;&#xff08;&#x8981;&#x6c42;&#x5355;&#x6bb5;&#x843d;&#x63cf;&#x8ff0;&#xff09;&#x6216;&#x8c03;&#x6574;&#x8f93;&#x51fa;&#x903b;&#x8f91;&#xff08;&#x6291;&#x5236;&#x2018;\\n&#x2019;&#x751f;&#x6210;&#xff09;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1568,1569"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x63cf;&#x8ff0;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;LVLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#xff09;&#x7684;&#x53ef;&#x9760;&#x90e8;&#x7f72;&#xff0c;&#x4f46;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x672a;&#x6df1;&#x5165;&#x63a2;&#x7d22;&#x5176;&#x6839;&#x672c;&#x539f;&#x56e0;&#x3002;&#x672c;&#x6587;&#x9996;&#x6b21;&#x4ece;&#x6a21;&#x578b;&#x56fa;&#x6709;&#x504f;&#x5dee;&#x7684;&#x89d2;&#x5ea6;&#x7cfb;&#x7edf;&#x6027;&#x5206;&#x6790;&#x5e7b;&#x89c9;&#x6210;&#x56e0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1570,1571"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x7684;&#x65b9;&#x6cd5;&#xff1a;1. &#x8f93;&#x5165;&#x4fa7; mitigation&#xff08;MiHI&#xff09;&#xff1a;&#x4fee;&#x6539;&#x63d0;&#x793a;&#x8bcd;&#xff0c;&#x4f8b;&#x5982;&#x5c06;&#x201c;&#x8bf7;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x6b64;&#x56fe;&#x50cf;&#x201d;&#x6539;&#x4e3a;&#x201c;&#x8bf7;&#x7528;&#x4e00;&#x4e2a;&#x6bb5;&#x843d;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x6b64;&#x56fe;&#x50cf;&#x201d;&#xff0c;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x751f;&#x6210;&#x8fde;&#x7eed;&#x6587;&#x672c;&#x907f;&#x514d;&#x5206;&#x6bb5;&#xff1b;2. &#x8f93;&#x51fa;&#x4fa7; mitigation&#xff08;MiHO&#xff09;&#xff1a;&#x5728;&#x89e3;&#x7801;&#x65f6;&#x5bf9;&#x2018;\\n&#x2019;&#x6807;&#x8bb0;&#x7684;logits&#x65bd;&#x52a0;&#x60e9;&#x7f5a;&#xff08;&#x516c;&#x5f0f;&#xff1a;L&#x302; = L &#x2212; &#x3bb;&#xb7;1_\\n&#xff09;&#xff0c;&#x6291;&#x5236;&#x5176;&#x751f;&#x6210;&#x6982;&#x7387;&#x3002;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#x53ef;&#x72ec;&#x7acb;&#x6216;&#x7ec4;&#x5408;&#x4f7f;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1571,1572"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x9a8c;&#x8bc1;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x53d1;&#x73b0;&#xff1a;1. &#x2018;\\n\\n&#x2019;&#x540e;&#x7684;&#x5185;&#x5bb9;&#x5e7b;&#x89c9;&#x663e;&#x8457;&#x589e;&#x52a0;&#xff08;Cs&#x6307;&#x6807;&#x5e73;&#x5747;&#x4ece;<sub>25%&#x5347;&#x81f3;</sub>58%&#xff09;&#xff1b;2. &#x4e3b;&#x52a8;&#x63d2;&#x5165;&#x2018;\\n\\n&#x2019;&#x53ef;&#x8bf1;&#x53d1;&#x66f4;&#x591a;&#x5e7b;&#x89c9;&#xff08;&#x653b;&#x51fb;&#x6709;&#x6548;&#x6027;&#x968f;&#x63d2;&#x5165;&#x4f4d;&#x7f6e;&#x540e;&#x79fb;&#x800c;&#x589e;&#x5f3a;&#xff09;&#xff1b;3. &#x6240;&#x63d0;&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#xff08;MiHO&#x4f7f;Cs&#x5e73;&#x5747;&#x4e0b;&#x964d;&#x7ea6;10%&#xff0c;&#x7ec4;&#x5408;&#x65b9;&#x6cd5;&#x6548;&#x679c;&#x66f4;&#x4f18;&#xff09;&#xff0c;&#x4f46;MiHI&#x5bf9;&#x672a;&#x7ecf;&#x8fc7;&#x6307;&#x4ee4;&#x5fae;&#x8c03;&#x7684;&#x6a21;&#x578b;&#xff08;&#x5982;Fuyu-8B&#xff09;&#x6548;&#x679c;&#x6709;&#x9650;&#x3002;","children":[],"payload":{"tag":"li","lines":"1572,1573"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;LVLM&#x7684;&#x56fa;&#x6709;&#x504f;&#x5dee;&#xff08;&#x5982;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e2d;&#x2018;\\n\\n&#x2019;&#x5f15;&#x53d1;&#x7684;&#x8bed;&#x4e49;&#x7a81;&#x53d8;&#xff09;&#x662f;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x6210;&#x56e0;&#x3002;&#x63d0;&#x51fa;&#x7684;&#x7b80;&#x5355;&#x65b9;&#x6cd5;&#x80fd;&#x4f4e;&#x6210;&#x672c;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#xff0c;&#x4e3a;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x65b0;&#x89c6;&#x89d2;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#xff1a;&#x4e3a;LVLM&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x5b9e;&#x7528;&#x5de5;&#x5177;&#xff0c;&#x542f;&#x53d1;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x4ece;&#x6570;&#x636e;&#x504f;&#x5dee;&#x89d2;&#x5ea6;&#x5206;&#x6790;&#x6a21;&#x578b;&#x7f3a;&#x9677;&#xff0c;&#x5e76;&#x63ed;&#x793a;&#x4e86;&#x2018;\\n\\n&#x2019;&#x53ef;&#x80fd;&#x88ab;&#x7528;&#x4f5c;&#x5bf9;&#x6297;&#x653b;&#x51fb;&#x7684;&#x65b0;&#x624b;&#x6bb5;&#x3002;","children":[],"payload":{"tag":"li","lines":"1573,1575"}}],"payload":{"tag":"li","lines":"1569,1575","fold":1}}],"payload":{"tag":"h4","lines":"1567,1568"}},{"content":"OHD-Caps: Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models","children":[{"content":"","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x7814;&#x7a76;&#x53d1;&#x73b0;CLIP&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x81ea;&#x8eab;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x800c;&#x975e;&#x4ec5;&#x7531;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x5f15;&#x8d77;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;OHD-Caps&#x57fa;&#x51c6;&#x548c;&#x7ec6;&#x7c92;&#x5ea6;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x65b9;&#x6cd5;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;CLIP&#x53ca;&#x5176;&#x4e0b;&#x6e38;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1576,1577"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff09;&#xff0c;&#x4f46;&#x6b64;&#x524d;&#x7814;&#x7a76;&#x672a;&#x80fd;&#x660e;&#x786e;&#x95ee;&#x9898;&#x6839;&#x6e90;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x9a8c;&#x8bc1;CLIP&#x6a21;&#x578b;&#x4f5c;&#x4e3a;LVLMs&#x7684;&#x6838;&#x5fc3;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x662f;&#x5426;&#x672c;&#x8eab;&#x5c31;&#x662f;&#x5e7b;&#x89c9;&#x6e90;&#xff0c;&#x8fd9;&#x5bf9;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1578,1579"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6784;&#x5efa;OHD-Caps&#x8bc4;&#x4f30;&#x57fa;&#x51c6;&#xff1a;&#x4ece;COCO&#x3001;Flickr30K&#x548c;Nocaps&#x6570;&#x636e;&#x96c6;&#x4e2d;&#x521b;&#x5efa;&#x6837;&#x672c;&#xff0c;&#x6bcf;&#x4e2a;&#x6837;&#x672c;&#x5305;&#x542b;&#x4e00;&#x5f20;&#x56fe;&#x50cf;&#x3001;&#x4e00;&#x4e2a;&#x6b63;&#x786e;&#x63cf;&#x8ff0;&#x548c;27&#x4e2a;&#x901a;&#x8fc7;GPT-4&#x63d2;&#x5165;&#x865a;&#x5047;&#x7269;&#x4f53;&#x6216;&#x5220;&#x9664;&#x771f;&#x5b9e;&#x7269;&#x4f53;&#x751f;&#x6210;&#x7684;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1579,1580"}}],"payload":{"tag":"li","lines":"1577,1580","fold":1}}],"payload":{"tag":"ul","lines":"1576,1580"}},{"content":"","children":[{"content":"2. &#x63d0;&#x51fa;&#x7ec6;&#x7c92;&#x5ea6;&#x7269;&#x4f53;&#x7ea7;&#x5bf9;&#x6bd4;&#x635f;&#x5931;&#xff1a;&#x901a;&#x8fc7;&#x8d1f;&#x6837;&#x672c;&#x589e;&#x5f3a;&#x8bad;&#x7ec3;&#xff0c;&#x4f7f;CLIP&#x80fd;&#x533a;&#x5206;&#x7ec6;&#x5fae;&#x7684;&#x7269;&#x4f53;&#x7ea7;&#x5dee;&#x5f02;&#x3002;","children":[],"payload":{"tag":"li","lines":"1580,1581","listIndex":2}},{"content":"3. &#x7528;&#x5fae;&#x8c03;&#x540e;&#x7684;CLIP&#x4f5c;&#x4e3a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x589e;&#x5f3a;LLaVA-1.5&#x6a21;&#x578b;&#x3002;","children":[{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. CLIP&#x6a21;&#x578b;&#x666e;&#x904d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#xff1a;ViT-L/14&#x4ec5;&#x5728;19.0%&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#x6b63;&#x786e;&#x9009;&#x62e9;&#x65e0;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1582,1583"}}],"payload":{"tag":"li","lines":"1581,1583","listIndex":3}},{"content":"4. &#x5fae;&#x8c03;&#x540e;&#x6027;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#xff1a;CLIP ViT-B/32&#x7684;&#x51c6;&#x786e;&#x7387;&#x4ece;14.3%&#x63d0;&#x5347;&#x81f3;82.5%&#x3002;","children":[],"payload":{"tag":"li","lines":"1583,1584","listIndex":4}},{"content":"5. &#x4e0b;&#x6e38;&#x6a21;&#x578b;&#x6539;&#x5584;&#xff1a;LLaVA-1.5&#x5728;Nocaps&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4ece;80.2&#x6539;&#x5584;&#x81f3;83.2&#x3002;","children":[{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CLIP&#x6a21;&#x578b;&#x672c;&#x8eab;&#x662f;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x91cd;&#x8981;&#x6765;&#x6e90;&#xff0c;&#x800c;&#x975e;&#x4ec5;&#x7531;&#x591a;&#x6a21;&#x6001;&#x4ea4;&#x4e92;&#x5f15;&#x8d77;&#x3002;&#x901a;&#x8fc7;&#x8d1f;&#x6837;&#x672c;&#x589e;&#x5f3a;&#x548c;&#x7ec6;&#x7c92;&#x5ea6;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x3002;&#x5fae;&#x8c03;&#x540e;&#x7684;CLIP&#x80fd;&#x4f5c;&#x4e3a;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e0b;&#x6e38;LVLMs&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x4fe1;&#x7684;&#x591a;&#x6a21;&#x6001;&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"1585,1587"}}],"payload":{"tag":"li","lines":"1584,1587","listIndex":5}}],"payload":{"tag":"ol","lines":"1580,1587"}}],"payload":{"tag":"h4","lines":"1575,1576"}},{"content":"VPFC: Two Causes, Not One: Rethinking Omission and Fabrication Hallucinations in MLLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x6311;&#x6218;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x5355;&#x4e00;&#x6210;&#x56e0;&#x5047;&#x8bbe;&#xff0c;&#x63d0;&#x51fa;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#x548c;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#x7531;&#x4e0d;&#x540c;&#x673a;&#x5236;&#x5f15;&#x8d77;&#x3002;&#x9057;&#x6f0f;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x6620;&#x5c04;&#x5230;&#x8bed;&#x8a00;&#x65f6;&#x4fe1;&#x5fc3;&#x4e0d;&#x8db3;&#xff0c;&#x634f;&#x9020;&#x5219;&#x56e0;&#x8de8;&#x6a21;&#x6001;&#x8868;&#x793a;&#x7a7a;&#x95f4;&#x4e2d;&#x7684;&#x865a;&#x5047;&#x5173;&#x8054;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x6ce8;&#x610f;&#x529b;&#x52bf;&#x573a;&#x6982;&#x5ff5;&#x548c;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x6821;&#x51c6;&#x65b9;&#x6cd5;VPFC&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x9057;&#x6f0f;&#x4e14;&#x4e0d;&#x5f15;&#x53d1;&#x989d;&#x5916;&#x634f;&#x9020;&#x3002;","children":[],"payload":{"tag":"li","lines":"1588,1589"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;Object Hallucination&#xff09;&#x7684;&#x95ee;&#x9898;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x5047;&#x8bbe;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#xff08;omission hallucination&#xff0c;&#x6a21;&#x578b;&#x672a;&#x80fd;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#x548c;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#xff08;fabrication hallucination&#xff0c;&#x6a21;&#x578b;&#x63cf;&#x8ff0;&#x4e86;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#x5177;&#x6709;&#x76f8;&#x540c;&#x7684;&#x6210;&#x56e0;&#xff08;&#x5982;&#x5bf9;&#x7edf;&#x8ba1;&#x504f;&#x5dee;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#xff09;&#xff0c;&#x5e76;&#x91c7;&#x7528;&#x5355;&#x4e00;&#x7b56;&#x7565;&#x540c;&#x65f6;&#x5904;&#x7406;&#x4e24;&#x8005;&#x3002;&#x7136;&#x800c;&#xff0c;&#x8fd9;&#x79cd;&#x65b9;&#x6cd5;&#x5728;&#x51cf;&#x5c11;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x5f80;&#x5f80;&#x4f1a;&#x52a0;&#x5267;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#xff0c;&#x8868;&#x660e;&#x5f53;&#x524d;&#x5bf9;&#x5e95;&#x5c42;&#x673a;&#x5236;&#x7684;&#x7406;&#x89e3;&#x5b58;&#x5728;&#x6839;&#x672c;&#x7f3a;&#x9677;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;MLLMs&#x8f93;&#x51fa;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#xff0c;&#x963b;&#x788d;&#x4e86;&#x5176;&#x5728;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x573a;&#x666f;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1590,1591"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x7cfb;&#x7edf;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x5e72;&#x9884;&#x5b9e;&#x9a8c;&#x5206;&#x6790;&#x4e86;&#x4e24;&#x79cd;&#x5e7b;&#x89c9;&#x7684;&#x6839;&#x6e90;&#x3002;1. &#x5bf9;&#x4e8e;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#xff1a;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;&#x5373;&#x4f7f;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x6210;&#x529f;&#x6355;&#x6349;&#x5230;&#x4e86;&#x7269;&#x4f53;&#x7684;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#xff08;&#x901a;&#x8fc7;&#x5b9a;&#x4f4d;&#x6ce8;&#x610f;&#x529b;&#x5934;&#x8bc1;&#x5b9e;&#xff09;&#xff0c;&#x4f46;&#x5728;&#x5c06;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x6620;&#x5c04;&#x5230;&#x8bed;&#x4e49;&#x6982;&#x5ff5;&#xff08;&#x8bed;&#x8a00;&#x8868;&#x8fbe;&#xff09;&#x7684;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x6a21;&#x578b;&#x5bf9;&#x8fd9;&#x4e9b;&#x8bc1;&#x636e;&#x7684;&#x4fe1;&#x5fc3;&#x4e0d;&#x8db3;&#xff0c;&#x5bfc;&#x81f4;&#x5176;&#x4e0d;&#x6562;&#x786e;&#x8ba4;&#x7269;&#x4f53;&#x7684;&#x5b58;&#x5728;&#x3002;2. &#x5bf9;&#x4e8e;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#xff1a;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#xff0c;&#x7531;&#x4e8e;&#x8bad;&#x7ec3;&#x8bed;&#x6599;&#x4e2d;&#x67d0;&#x4e9b;&#x7269;&#x4f53;&#xff08;&#x5982;&#x201c;&#x9a6c;&#x6876;&#x201d;&#x548c;&#x201c;&#x6d17;&#x624b;&#x53f0;&#x201d;&#xff09;&#x9891;&#x7e41;&#x5171;&#x73b0;&#xff0c;&#x6a21;&#x578b;&#x5728;&#x8de8;&#x6a21;&#x6001;&#x8868;&#x793a;&#x7a7a;&#x95f4;&#x4e2d;&#x5efa;&#x7acb;&#x4e86;&#x9519;&#x8bef;&#x7684;&#x5f3a;&#x5173;&#x8054;&#xff0c;&#x4ece;&#x800c;&#x5c06;&#x90e8;&#x5206;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x9519;&#x8bef;&#x5730;&#x9ad8;&#x7f6e;&#x4fe1;&#x5ea6;&#x5730;&#x6620;&#x5c04;&#x5230;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x4e0a;&#x3002;&#x57fa;&#x4e8e;&#x8fd9;&#x4e9b;&#x53d1;&#x73b0;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x201c;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x6ce8;&#x610f;&#x529b;&#x52bf;&#x573a;&#x201d;&#xff08;Visual-Semantic Attention Potential Field&#xff09;&#x7684;&#x6982;&#x5ff5;&#x6846;&#x67b6;&#xff0c;&#x5c06;&#x6bcf;&#x4e2a;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x89c6;&#x4e3a;&#x5904;&#x4e8e;&#x4e00;&#x4e2a;&#x52bf;&#x573a;&#x4e2d;&#xff1a;&#x9ad8;&#x53ef;&#x4fe1;&#x5ea6;&#x89c6;&#x89c9;&#x533a;&#x57df;&#x4f4d;&#x4e8e;&#x52bf;&#x9631;&#xff0c;&#x6613;&#x4e8e;&#x786e;&#x8ba4;&#x7269;&#x4f53;&#xff1b;&#x4f4e;&#x53ef;&#x4fe1;&#x5ea6;&#x89c6;&#x89c9;&#x533a;&#x57df;&#x4f4d;&#x4e8e;&#x52bf;&#x5cf0;&#xff0c;&#x96be;&#x4ee5;&#x786e;&#x8ba4;&#x7269;&#x4f53;&#x3002;&#x5229;&#x7528;&#x8be5;&#x6846;&#x67b6;&#xff0c;&#x4f5c;&#x8005;&#x8bbe;&#x8ba1;&#x4e86;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x201c;&#x89c6;&#x89c9;&#x52bf;&#x573a;&#x6821;&#x51c6;&#x201d;&#xff08;Visual Potential Field Calibration, VPFC&#xff09;&#x3002;VPFC&#x901a;&#x8fc7;&#x5728;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x5230;&#x8bed;&#x4e49;&#x6982;&#x5ff5;&#x7684;&#x6620;&#x5c04;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x4e13;&#x95e8;&#x9488;&#x5bf9;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#xff0c;&#x91cd;&#x65b0;&#x6821;&#x51c6;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x7684;&#x4fe1;&#x5fc3;&#x5206;&#x914d;&#xff0c;&#x4ece;&#x800c;&#x63d0;&#x5347;&#x5bf9;&#x6b63;&#x786e;&#x7279;&#x5f81;&#x7684;&#x4fe1;&#x5fc3;&#xff08;&#x51cf;&#x5c11;&#x9057;&#x6f0f;&#xff09;&#x5e76;&#x6291;&#x5236;&#x5bf9;&#x9519;&#x8bef;&#x7279;&#x5f81;&#x7684;&#x9ad8;&#x4fe1;&#x5fc3;&#xff08;&#x907f;&#x514d;&#x634f;&#x9020;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1591,1592"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1. &#x6210;&#x56e0;&#x5206;&#x6790;&#xff1a;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x8bc1;&#x5b9e;&#xff0c;&#x5728;&#x53d1;&#x751f;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x5176;&#x5b9e;&#x5173;&#x6ce8;&#x5230;&#x4e86;&#x6b63;&#x786e;&#x7269;&#x4f53;&#x533a;&#x57df;&#xff08;&#x5982;&#x52fa;&#x5b50;&#xff09;&#xff0c;&#x4f46;&#x4fe1;&#x5fc3;&#x4e0d;&#x8db3;&#xff1b;&#x5728;&#x53d1;&#x751f;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x9519;&#x8bef;&#x5730;&#x5c06;&#x5173;&#x8054;&#x7269;&#x4f53;&#x7684;&#x7279;&#x5f81;&#xff08;&#x5982;&#x9a6c;&#x6876;&#x7684;&#x90e8;&#x5206;&#x7279;&#x5f81;&#xff09;&#x9ad8;&#x7f6e;&#x4fe1;&#x5ea6;&#x5730;&#x5173;&#x8054;&#x5230;&#x76ee;&#x6807;&#x7269;&#x4f53;&#xff08;&#x6d17;&#x624b;&#x53f0;&#xff09;&#x3002;2. &#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x7f3a;&#x9677;&#xff1a;&#x5b9e;&#x9a8c;&#xff08;&#x4f7f;&#x7528;Visual Contrastive Decoding, VCD&#xff09;&#x663e;&#x793a;&#xff0c;&#x57fa;&#x4e8e;&#x7edf;&#x4e00;&#x6210;&#x56e0;&#x5047;&#x8bbe;&#x7684;&#x65b9;&#x6cd5;&#x5728;&#x51cf;&#x5c11;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x663e;&#x8457;&#x52a0;&#x5267;&#x4e86;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#xff08;&#x5c24;&#x5176;&#x5728;&#x5bf9;&#x6297;&#x6027;&#x6570;&#x636e;&#x96c6;&#x5b50;&#x96c6;&#x4e0a;&#xff09;&#xff0c;&#x603b;&#x4f53;&#x8f93;&#x51fa;&#x8d28;&#x91cf;&#x4e0b;&#x964d;&#x3002;3. VPFC&#x6709;&#x6548;&#x6027;&#xff1a;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;POPE, MM-Hallucination, CHAIR, LLaVA-Bench&#xff09;&#x4e0a;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;VPFC&#x65b9;&#x6cd5;&#x5728;&#x4e0d;&#x5f15;&#x5165;&#x989d;&#x5916;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#x7684;&#x524d;&#x63d0;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#xff0c;&#x5728;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff08;training-free&#xff09;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x4e2d;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#x7684;&#xff08;State-of-the-Art&#xff09;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1592,1593"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x6700;&#x7ec8;&#x7ed3;&#x8bba;&#x662f;&#xff1a;&#x9057;&#x6f0f;&#x5e7b;&#x89c9;&#x548c;&#x634f;&#x9020;&#x5e7b;&#x89c9;&#x5177;&#x6709;&#x622a;&#x7136;&#x4e0d;&#x540c;&#x7684;&#x6839;&#x672c;&#x539f;&#x56e0;&#xff0c;&#x800c;&#x975e;&#x5171;&#x4eab;&#x540c;&#x4e00;&#x6210;&#x56e0;&#x3002;&#x9057;&#x6f0f;&#x6e90;&#x4e8e;&#x6620;&#x5c04;&#x8fc7;&#x7a0b;&#x4e2d;&#x7684;&#x4fe1;&#x5fc3;&#x4e0d;&#x8db3;&#xff0c;&#x634f;&#x9020;&#x6e90;&#x4e8e;&#x8de8;&#x6a21;&#x6001;&#x7a7a;&#x95f4;&#x4e2d;&#x7684;&#x9519;&#x8bef;&#x7edf;&#x8ba1;&#x5173;&#x8054;&#x3002;&#x8fd9;&#x4e00;&#x53d1;&#x73b0;&#x63ed;&#x793a;&#x4e86;&#x5f53;&#x524d;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7814;&#x7a76;&#x4e2d;&#x7684;&#x4e00;&#x4e2a;&#x5173;&#x952e;&#x758f;&#x5ffd;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#x63d0;&#x51fa;&#x7684;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x6ce8;&#x610f;&#x529b;&#x52bf;&#x573a;&#x6982;&#x5ff5;&#x548c;VPFC&#x65b9;&#x6cd5;&#xff0c;&#x4e3a;&#x5f00;&#x53d1;&#x66f4;&#x9c81;&#x68d2;&#x3001;&#x66f4;&#x5e73;&#x8861;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x7b56;&#x7565;&#x6307;&#x660e;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#xff1a;1. &#x7406;&#x8bba;&#x5c42;&#x9762;&#xff1a;&#x98a0;&#x8986;&#x4e86;&#x9886;&#x57df;&#x5185;&#x5bf9;&#x5e7b;&#x89c9;&#x6210;&#x56e0;&#x7684;&#x4f20;&#x7edf;&#x8ba4;&#x77e5;&#xff0c;&#x63a8;&#x52a8;&#x4e86;&#x66f4;&#x7cbe;&#x7ec6;&#x7684;MLLM&#x884c;&#x4e3a;&#x673a;&#x7406;&#x7814;&#x7a76;&#x3002;2. &#x5b9e;&#x8df5;&#x5c42;&#x9762;&#xff1a;VPFC&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x6709;&#x6548;&#x5de5;&#x5177;&#xff0c;&#x53ef;&#x76f4;&#x63a5;&#x63d0;&#x5347;&#x73b0;&#x6709;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4e3a;&#x5176;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x548c;&#x53ef;&#x9760;&#x6027;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1593,1595"}}],"payload":{"tag":"li","lines":"1589,1595","fold":1}}],"payload":{"tag":"h4","lines":"1587,1588"}},{"content":"CGC: Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x53d1;&#x73b0;&#x57fa;&#x4e8e;&#x79bb;&#x6563;&#x56fe;&#x50cf;&#x5206;&#x8bcd;&#x5668;&#x7684;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4f1a;&#x56e0;&#x56fe;&#x50cf;token&#x7684;&#x5171;&#x73b0;&#x6a21;&#x5f0f;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x63cf;&#x8ff0;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;CGC&#x65b9;&#x6cd5;&#x805a;&#x7c7b;&#x9ad8;&#x5171;&#x73b0;token&#xff0c;&#x5e76;&#x8bbe;&#x8ba1;VTD&#x65b9;&#x6cd5;&#x5728;&#x751f;&#x6210;&#x65f6;&#x6291;&#x5236;&#x6f5c;&#x5728;&#x7a7a;&#x95f4;&#x4e2d;&#x7f3a;&#x5931;token&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1596,1597"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x57fa;&#x4e8e;&#x79bb;&#x6563;&#x56fe;&#x50cf;&#x5206;&#x8bcd;&#x5668;&#x7684;LVLM&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7684;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x771f;&#x5b9e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x963b;&#x788d;&#x6b64;&#x7c7b;&#x7edf;&#x4e00;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#x7684;&#x90e8;&#x7f72;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5927;&#x591a;&#x9488;&#x5bf9;&#x6587;&#x672c;&#x6a21;&#x6001;&#x6216;&#x8fde;&#x7eed;&#x7279;&#x5f81;&#x6a21;&#x578b;&#xff0c;&#x5bf9;&#x65b0;&#x5174;&#x7684;&#x79bb;&#x6563;token&#x5316;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7814;&#x7a76;&#x4e0d;&#x8db3;&#xff0c;&#x4e14;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#x65e7;&#x65b9;&#x6cd5;&#x6548;&#x679c;&#x4e0d;&#x4f73;&#x3002;","children":[],"payload":{"tag":"li","lines":"1598,1599"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x91c7;&#x7528;&#x4e86;&#x4e00;&#x4e2a;&#x4e24;&#x9636;&#x6bb5;&#x6846;&#x67b6;&#xff1a;1. <strong>&#x4e0a;&#x4e0b;&#x6587;&#x5f15;&#x5bfc;&#x805a;&#x7c7b;&#xff08;CGC&#xff09;</strong>&#xff1a;&#x9996;&#x5148;&#x5229;&#x7528;&#x5206;&#x5272;&#x6570;&#x636e;&#x96c6;&#x6784;&#x5efa;&#x56fe;&#x50cf;token&#x7684;&#x5171;&#x73b0;&#x56fe;&#xff08;&#x8282;&#x70b9;&#x4e3a;token&#xff0c;&#x8fb9;&#x6743;&#x91cd;&#x7531;&#x7a7a;&#x95f4;&#x90bb;&#x8fd1;&#x6027;&#x548c;&#x8bed;&#x4e49;&#x5173;&#x8054;&#x6027;&#x51b3;&#x5b9a;&#xff09;&#xff0c;&#x7136;&#x540e;&#x7528;&#x56fe;&#x795e;&#x7ecf;&#x7f51;&#x7edc;&#xff08;GNN&#xff09;&#x8fdb;&#x884c;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x5f97;&#x5230;token&#x5d4c;&#x5165;&#xff0c;&#x6700;&#x540e;&#x901a;&#x8fc7;K-means&#x805a;&#x7c7b;&#x5c06;&#x9891;&#x7e41;&#x5171;&#x73b0;&#x7684;token&#x5206;&#x7ec4;&#xff0c;&#x4ee5;&#x91cf;&#x5316;&#x89c6;&#x89c9;&#x5148;&#x9a8c;&#x3002;2. <strong>&#x89c6;&#x89c9;token&#x53bb;&#x6c61;&#xff08;VTD&#xff09;</strong>&#xff1a;&#x5728;&#x6a21;&#x578b;&#x81ea;&#x56de;&#x5f52;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x5b9e;&#x65f6;&#x8bc6;&#x522b;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e2d;&#x7f3a;&#x5931;&#x4f46;&#x5c5e;&#x4e8e;&#x4e3b;&#x5bfc;&#x805a;&#x7c7b;&#xff08;&#x5728;&#x56fe;&#x50cf;&#x4e2d;&#x51fa;&#x73b0;&#x6700;&#x9891;&#x7e41;&#x7684;&#x805a;&#x7c7b;&#xff09;&#x7684;token&#xff0c;&#x5e76;&#x5c06;&#x8fd9;&#x4e9b;&#x7f3a;&#x5931;token&#x6295;&#x5f71;&#x5230;&#x6a21;&#x578b;&#x7684;&#x6f5c;&#x5728;&#x7a7a;&#x95f4;&#x4e2d;&#xff0c;&#x4ece;&#x5176;&#x9690;&#x85cf;&#x5c42;&#x8868;&#x793a;&#x4e2d;&#x51cf;&#x53bb;&#x5b83;&#x4eec;&#x7684;&#x8d21;&#x732e;&#xff0c;&#x4ece;&#x800c;&#x6291;&#x5236;&#x5176;&#x5f15;&#x53d1;&#x5e7b;&#x89c9;&#x7684;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1599,1600"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x53d1;&#x73b0;&#xff1a;1. &#x5e7b;&#x89c9;&#x4e0e;&#x56fe;&#x50cf;&#x4e2d;&#x7f3a;&#x5931;&#x4f46;&#x5c5e;&#x4e8e;&#x524d;&#x51e0;&#x4e2a;&#x4e3b;&#x5bfc;&#x805a;&#x7c7b;&#x7684;token&#x9ad8;&#x5ea6;&#x76f8;&#x5173;&#xff0c;&#x8fd9;&#x4e9b;&#x7f3a;&#x5931;token&#x4e0e;&#x5df2;&#x51fa;&#x73b0;token&#x9891;&#x7e41;&#x5171;&#x73b0;&#x3002;2. &#x63d0;&#x51fa;&#x7684;VTD&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x5728;&#x591a;&#x9879;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x8868;&#x8fbe;&#x80fd;&#x529b;&#x548c;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x3002;3. &#x4e0e;&#x73b0;&#x6709;&#x7684;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CD&#xff09;&#x7b49;&#x65b9;&#x6cd5;&#x76f8;&#x6bd4;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x79bb;&#x6563;token&#x5316;&#x7684;LVLM&#x4e0a;&#x8868;&#x73b0;&#x66f4;&#x4f18;&#xff0c;&#x4e14;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x4f4e;&#xff0c;&#x65e0;&#x9700;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1600,1601"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x79bb;&#x6563;&#x56fe;&#x50cf;&#x5206;&#x8bcd;&#x5668;&#x4e2d;&#x7684;token&#x5171;&#x73b0;&#x6a21;&#x5f0f;&#x662f;LVLM&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x7684;&#x4e00;&#x4e2a;&#x65b0;&#x6839;&#x6e90;&#x3002;&#x6240;&#x63d0;&#x51fa;&#x7684;CGC&#x548c;VTD&#x6846;&#x67b6;&#x6210;&#x529f;&#x6355;&#x83b7;&#x4e86;&#x8fd9;&#x79cd;&#x89c6;&#x89c9;&#x5148;&#x9a8c;&#x5e76;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x65b0;&#x4e00;&#x4ee3;&#x57fa;&#x4e8e;&#x79bb;&#x6563;&#x8868;&#x793a;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x3001;&#x65e0;&#x9700;&#x91cd;&#x8bad;&#x7ec3;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x5de5;&#x5177;&#xff0c;&#x63a8;&#x52a8;&#x4e86;&#x66f4;&#x53ef;&#x9760;LVLM&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1601,1603"}}],"payload":{"tag":"li","lines":"1597,1603","fold":1}}],"payload":{"tag":"h4","lines":"1595,1596"}},{"content":"Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x8fc7;&#x4e8e;&#x8be6;&#x7ec6;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x65e0;&#x6cd5;&#x9002;&#x65f6;&#x7ec8;&#x6b62;&#x751f;&#x6210;&#xff08;EOS&#x51b3;&#x7b56;&#xff09;&#x3002;&#x901a;&#x8fc7;&#x5206;&#x6790;EOS&#x51b3;&#x7b56;&#x673a;&#x5236;&#xff0c;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#xff1a;&#x9009;&#x62e9;&#x6027;EOS&#x76d1;&#x7763;&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#x548c;&#x6570;&#x636e;&#x8fc7;&#x6ee4;&#x7b56;&#x7565;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1604,1605"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x7684;&#x5185;&#x5bb9;&#xff08;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x4f8b;&#x5982;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x4f20;&#x7edf;&#x7814;&#x7a76;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7f3a;&#x9677;&#x6216;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7b49;&#x95ee;&#x9898;&#xff0c;&#x4f46;&#x672c;&#x6587;&#x53d1;&#x73b0;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x8fc7;&#x4e8e;&#x8be6;&#x7ec6;&#xff08;&#x8d85;&#x51fa;&#x6a21;&#x578b;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff09;&#x662f;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x7684;&#x5173;&#x952e;&#x56e0;&#x7d20;&#xff0c;&#x56e0;&#x4e3a;&#x6a21;&#x578b;&#x4e3a;&#x62df;&#x5408;&#x8be6;&#x7ec6;&#x6807;&#x6ce8;&#x4f1a;&#x751f;&#x6210;&#x65e0;&#x6cd5;&#x786e;&#x8ba4;&#x7684;&#x5185;&#x5bb9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1606,1607"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x4ece;EOS&#xff08;&#x53e5;&#x5b50;&#x7ed3;&#x675f;&#x6807;&#x8bb0;&#xff09;&#x51b3;&#x7b56;&#x89d2;&#x5ea6;&#x5206;&#x6790;&#x95ee;&#x9898;&#xff1a;1. &#x4f7f;&#x7528;&#x663e;&#x8457;&#x6027;&#x5206;&#x6790;&#x65b9;&#x6cd5;&#x53d1;&#x73b0;&#x6a21;&#x578b;&#x4f9d;&#x8d56;&#x6574;&#x4e2a;&#x751f;&#x6210;&#x5e8f;&#x5217;&#xff08;&#x800c;&#x975e;&#x5f53;&#x524d;&#x53e5;&#x5b50;&#xff09;&#x8bc4;&#x4f30;&#x5b8c;&#x6574;&#x6027;&#xff1b;2. &#x901a;&#x8fc7;&#x64cd;&#x7eb5;&#x8f93;&#x5165;&#x4e0a;&#x4e0b;&#x6587;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x964d;&#x566a;&#x3001;&#x6587;&#x672c;&#x63a9;&#x7801;&#xff09;&#x9a8c;&#x8bc1;&#x6a21;&#x578b;&#x57fa;&#x4e8e;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x8bed;&#x4e49;&#x6bd4;&#x8f83;&#x51b3;&#x5b9a;&#x7ec8;&#x6b62;&#x751f;&#x6210;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff1a;1. &#x9009;&#x62e9;&#x6027;EOS&#x76d1;&#x7763;&#x8bad;&#x7ec3;&#x76ee;&#x6807;&#xff1a;&#x4fee;&#x6539;&#x6700;&#x5927;&#x4f3c;&#x7136;&#x4f30;&#x8ba1;&#xff08;MLE&#xff09;&#xff0c;&#x8ba9;&#x6a21;&#x578b;&#x4ece;&#x5e38;&#x89c4;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x5b66;&#x4e60;&#x9002;&#x65f6;&#x7ec8;&#x6b62;&#xff1b;2. &#x6570;&#x636e;&#x8fc7;&#x6ee4;&#x7b56;&#x7565;&#xff1a;&#x8bbe;&#x8ba1;&#x6307;&#x6807;&#x8bc4;&#x4f30;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x5bf9;EOS&#x503e;&#x5411;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x8fc7;&#x6ee4;&#x6709;&#x5bb3;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1607,1608"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1. &#x9009;&#x62e9;&#x6027;EOS&#x76d1;&#x7763;&#x65b9;&#x6cd5;&#x5728;LLaVA-1.5&#x6a21;&#x578b;&#x4e0a;&#x8fdb;&#x4e00;&#x6b65;&#x8bad;&#x7ec3;&#x540e;&#xff0c;&#x53e5;&#x5b50;&#x7ea7;&#x548c;&#x5b9e;&#x4f8b;&#x7ea7;&#x5e7b;&#x89c9;&#x5206;&#x522b;&#x51cf;&#x5c11;26%&#x548c;27%&#xff1b;2. &#x6570;&#x636e;&#x8fc7;&#x6ee4;&#x7b56;&#x7565;&#x4ec5;&#x79fb;&#x9664;&#x5c11;&#x91cf;&#x6570;&#x636e;&#x5373;&#x53ef;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x3002;&#x4e24;&#x8005;&#x5747;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x77e5;&#x8bc6;&#xff0c;&#x4e14;&#x9a8c;&#x8bc1;&#x4e86;&#x201c;&#x6a21;&#x578b;&#x5177;&#x5907;&#x57fa;&#x4e8e;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7ec8;&#x6b62;&#x751f;&#x6210;&#x7684;&#x6f5c;&#x529b;&#x201d;&#x7684;&#x5047;&#x8bbe;&#x3002;","children":[],"payload":{"tag":"li","lines":"1608,1609"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff1a;LVLM&#x672c;&#x8eab;&#x5177;&#x6709;&#x901a;&#x8fc7;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x6bd4;&#x8f83;&#x8bc4;&#x4f30;&#x751f;&#x6210;&#x5b8c;&#x6574;&#x6027;&#x7684;&#x80fd;&#x529b;&#xff0c;&#x8fc7;&#x5ea6;&#x8be6;&#x7ec6;&#x7684;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4f1a;&#x6291;&#x5236;&#x8fd9;&#x79cd;&#x80fd;&#x529b;&#x3002;&#x901a;&#x8fc7;&#x589e;&#x5f3a;EOS&#x51b3;&#x7b56;&#x53ef;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x63d0;&#x51fa;&#x7684;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#x7b80;&#x5355;&#x9ad8;&#x6548;&#xff0c;&#x4e3a;&#x6539;&#x5584;LVLM&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x89c6;&#x89d2;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x5b89;&#x5168;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1609,1611"}}],"payload":{"tag":"li","lines":"1605,1611","fold":1}}],"payload":{"tag":"h4","lines":"1603,1604"}},{"content":"Residual Visual Decoding: Investigating and Mitigating the Multimodal Hallucination Snowballing in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5b58;&#x5728;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x6eda;&#x96ea;&#x7403;&#x6548;&#x5e94;&#xff1a;&#x5148;&#x524d;&#x751f;&#x6210;&#x7684;&#x5e7b;&#x89c9;&#x4f1a;&#x8bef;&#x5bfc;&#x6a21;&#x578b;&#x540e;&#x7eed;&#x56de;&#x7b54;&#xff0c;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x7d2f;&#x79ef;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;MMHalSnowball&#x8bc4;&#x4f30;&#x6846;&#x67b6;&#x548c;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x6b8b;&#x5dee;&#x89c6;&#x89c9;&#x89e3;&#x7801;&#xff08;RVD&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;24%&#x7684;&#x5e7b;&#x89c9;&#x4f20;&#x64ad;&#x3002;","children":[],"payload":{"tag":"li","lines":"1612,1613"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;LVLMs&#x5728;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#x4e2d;&#x56e0;&#x5148;&#x524d;&#x751f;&#x6210;&#x7684;&#x5e7b;&#x89c9;&#x800c;&#x6301;&#x7eed;&#x4ea7;&#x751f;&#x9519;&#x8bef;&#x56de;&#x7b54;&#x7684;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x81f3;&#x5173;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x89c6;&#x89c9;&#x8f85;&#x52a9;&#x7cfb;&#x7edf;&#xff09;&#x4e2d;&#xff0c;&#x5e7b;&#x89c9;&#x7684;&#x7d2f;&#x79ef;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x51b3;&#x7b56;&#x9519;&#x8bef;&#xff08;&#x4f8b;&#x5982;&#x9519;&#x8bef;&#x6307;&#x5bfc;&#x76f2;&#x4eba;&#x8fc7;&#x9a6c;&#x8def;&#xff09;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1614,1615"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e24;&#x79cd;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#xff1a;1) MMHalSnowball&#x8bc4;&#x4f30;&#x6846;&#x67b6;&#xff1a;&#x901a;&#x8fc7;&#x4eba;&#x5de5;&#x6784;&#x9020;&#x5305;&#x542b;&#x5b58;&#x5728;&#x6027;&#x3001;&#x5c5e;&#x6027;&#x3001;&#x5173;&#x7cfb;&#x548c;&#x60f3;&#x8c61;&#x56db;&#x7c7b;&#x5e7b;&#x89c9;&#x7684;&#x5bf9;&#x8bdd;&#x4e0a;&#x4e0b;&#x6587;&#xff0c;&#x6d4b;&#x8bd5;&#x6a21;&#x578b;&#x662f;&#x5426;&#x88ab;&#x5e7b;&#x89c9;&#x8bef;&#x5bfc;&#xff1b;2) &#x6b8b;&#x5dee;&#x89c6;&#x89c9;&#x89e3;&#x7801;&#xff08;RVD&#xff09;&#xff1a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x5e72;&#x9884;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x5f53;&#x524d;&#x6307;&#x4ee4;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x7684;&#x6b8b;&#x5dee;&#x5206;&#x5e03;&#x6765;&#x4fee;&#x6b63;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#xff0c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x76f4;&#x63a5;&#x8bbf;&#x95ee;&#x3002;","children":[],"payload":{"tag":"li","lines":"1615,1616"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x53d1;&#x73b0;&#x5305;&#x62ec;&#xff1a;1) &#x5f53;&#x5bf9;&#x8bdd;&#x4e2d;&#x5305;&#x542b;&#x76f8;&#x5173;&#x5e7b;&#x89c9;&#x65f6;&#xff0c;&#x5f00;&#x6e90;LVLMs&#x7684;&#x6027;&#x80fd;&#x4e0b;&#x964d;&#x81f3;&#x5c11;31%&#xff1b;2) 59.1%&#x7684;&#x539f;&#x672c;&#x80fd;&#x6b63;&#x786e;&#x56de;&#x7b54;&#x7684;&#x95ee;&#x9898;&#x5728;&#x5e7b;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x4e2d;&#x88ab;&#x8bef;&#x5bfc;&#xff1b;3) RVD&#x65b9;&#x6cd5;&#x5728;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x80fd;&#x529b;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x51cf;&#x5c11;&#x4e86;24%&#x7684;&#x5e7b;&#x89c9;&#x6eda;&#x96ea;&#x7403;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1616,1617"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#x591a;&#x6a21;&#x6001;&#x5e7b;&#x89c9;&#x6eda;&#x96ea;&#x7403;&#x662f;LVLMs&#x7684;&#x666e;&#x904d;&#x7f3a;&#x9677;&#xff0c;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5bf9;&#x8bed;&#x8a00;&#x4e0a;&#x4e0b;&#x6587;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x3002;RVD&#x65b9;&#x6cd5;&#x8bc1;&#x660e;&#x901a;&#x8fc7;&#x5f3a;&#x5316;&#x89c6;&#x89c9;&#x4fe1;&#x53f7;&#x53ef;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x9879;&#x7814;&#x7a76;&#x5bf9;&#x63d0;&#x5347;LVLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#xff0c;&#x4e3a;&#x540e;&#x7eed;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x8bc4;&#x4f30;&#x548c;&#x5e72;&#x9884;&#x7684;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1617,1619"}}],"payload":{"tag":"li","lines":"1613,1619","fold":1}}],"payload":{"tag":"h4","lines":"1611,1612"}}],"payload":{"tag":"h3","lines":"1548,1549","fold":1}},{"content":"&#x65b0;&#x65b9;&#x6cd5;","children":[{"content":"GACD: Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x68af;&#x5ea6;&#x7684;&#x81ea;&#x53cd;&#x601d;&#x65b9;&#x6cd5;GACD&#xff0c;&#x7528;&#x4e8e;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;token&#x5bf9;&#x8f93;&#x51fa;&#x7684;&#x8d21;&#x732e;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x52a8;&#x6001;&#x8c03;&#x6574;token&#x6743;&#x91cd;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x8f85;&#x52a9;&#x6a21;&#x578b;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7684;&#x89c6;&#x89c9;&#x57fa;&#x7840;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1621,1622"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff08;hallucination&#xff09;&#xff0c;&#x5373;&#x8f93;&#x51fa;&#x5185;&#x5bb9;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x7b26;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x4e24;&#x79cd;&#x504f;&#x5dee;&#xff1a;&#x6587;&#x672c;-&#x89c6;&#x89c9;&#x504f;&#x5dee;&#xff08;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x63d0;&#x793a;&#x800c;&#x5ffd;&#x7565;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff09;&#x548c;&#x5171;&#x73b0;&#x504f;&#x5dee;&#xff08;&#x57fa;&#x4e8e;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e2d;&#x7684;&#x865a;&#x5047;&#x5bf9;&#x8c61;&#x5173;&#x8054;&#xff09;&#x3002;&#x8fd9;&#x4e9b;&#x504f;&#x5dee;&#x4f1a;&#x964d;&#x4f4e;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x963b;&#x788d;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1623,1624"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x68af;&#x5ea6;&#x611f;&#x77e5;&#x7ea6;&#x675f;&#x89e3;&#x7801;&#xff08;GACD&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x4f7f;&#x7528;&#x4e00;&#x9636;&#x6cf0;&#x52d2;&#x68af;&#x5ea6;&#x5206;&#x6790;&#x6bcf;&#x4e2a;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;token&#x5bf9;&#x5f53;&#x524d;&#x8f93;&#x51fa;&#x7684;&#x8d21;&#x732e;&#x5ea6;&#xff1b;2&#xff09;&#x901a;&#x8fc7;&#x4e24;&#x4e2a;&#x6a21;&#x5757;&#x7f13;&#x89e3;&#x504f;&#x5dee;&#xff1a;&#x6291;&#x5236;&#x4e0e;&#x5f53;&#x524d;&#x8f93;&#x51fa;&#x5bf9;&#x8c61;&#x865a;&#x5047;&#x76f8;&#x5173;&#x7684;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#xff08;&#x89e3;&#x51b3;&#x5171;&#x73b0;&#x504f;&#x5dee;&#xff09;&#xff0c;&#x4ee5;&#x53ca;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x76f8;&#x5bf9;&#x4e8e;&#x6587;&#x672c;&#x7684;&#x6743;&#x91cd;&#xff08;&#x89e3;&#x51b3;&#x6587;&#x672c;-&#x89c6;&#x89c9;&#x504f;&#x5dee;&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x76f4;&#x63a5;&#x5e94;&#x7528;&#xff0c;&#x65e0;&#x9700;&#x5fae;&#x8c03;&#x6216;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1624,1625"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;GACD&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff1a;AMBER&#x603b;&#x4f53;&#x5f97;&#x5206;&#x63d0;&#x5347;8%&#xff0c;POPE&#x7684;F1&#x5206;&#x6570;&#x63d0;&#x9ad8;8%&#xff0c;LLaVA-QA90&#x7684;&#x7ec6;&#x8282;&#x4e30;&#x5bcc;&#x5ea6;&#x63d0;&#x5347;45%&#x3001;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x9ad8;92%&#x3002;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x5b8c;&#x6574;&#x6027;&#xff0c;&#x672a;&#x727a;&#x7272;&#x4fe1;&#x606f;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1625,1626"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: GACD&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3001;&#x57fa;&#x4e8e;&#x68af;&#x5ea6;&#x5206;&#x6790;&#x7684;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x89e3;&#x65b9;&#x6848;&#xff0c;&#x80fd;&#x591f;&#x7ec6;&#x7c92;&#x5ea6;&#x5730;&#x8bc6;&#x522b;&#x548c;&#x8c03;&#x6574;&#x591a;&#x6a21;&#x6001;&#x504f;&#x5dee;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;MLLM&#x8f93;&#x51fa;&#x7684;&#x89c6;&#x89c9;&#x57fa;&#x7840;&#x6027;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x591a;&#x6a21;&#x6001;&#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x4e14;&#x6613;&#x4e8e;&#x8fc1;&#x79fb;&#x5230;&#x73b0;&#x6709;&#x6a21;&#x578b;&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"1626,1628"}}],"payload":{"tag":"li","lines":"1622,1628","fold":1}}],"payload":{"tag":"h4","lines":"1620,1621"}},{"content":"VORD: Visual Ordinal Calibration for Mitigating Object Hallucinations in Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;VORD&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5229;&#x7528;&#x56fe;&#x50cf;&#x4fee;&#x6539;&#x524d;&#x540e;&#x7684;&#x5e8f;&#x6570;&#x5173;&#x7cfb;&#x6765;&#x6821;&#x51c6;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x7684;token&#x7f6e;&#x4fe1;&#x5ea6;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5305;&#x542b;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x548c;&#x53ef;&#x8bad;&#x7ec3;&#x7684;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6821;&#x51c6;&#x6548;&#x679c;&#x5e76;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1629,1630"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;(LVLMs)&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&apos;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&apos;&#xff0c;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x4e2d;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x73b0;&#x6709;&#x7814;&#x7a76;&#x591a;&#x5173;&#x6ce8;&#x63d0;&#x5347;&#x6574;&#x4f53;&#x51c6;&#x786e;&#x6027;&#xff0c;&#x5374;&#x5ffd;&#x89c6;&#x4e86;&#x6a21;&#x578b;&#x7f6e;&#x4fe1;&#x5ea6;&#x6821;&#x51c6;&#x8fd9;&#x4e00;&#x5173;&#x952e;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1631,1632"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;VORD&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x79cd;&#x5f62;&#x5f0f;&#xff1a;1) VORD Decoding&#xff1a;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x8f7b;&#x91cf;&#x7ea7;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x5bf9;&#x6bd4;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x4e0e;&#x4fee;&#x6539;&#x540e;&#x56fe;&#x50cf;(&#x5982;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#x3001;&#x6df7;&#x5408;&#x56fe;&#x50cf;)&#x7684;token&#x6982;&#x7387;&#xff0c;&#x60e9;&#x7f5a;&#x90a3;&#x4e9b;&#x5728;&#x4fee;&#x6539;&#x540e;&#x56fe;&#x50cf;&#x4e2d;&#x6982;&#x7387;&#x5f02;&#x5e38;&#x9ad8;&#x7684;&#x4e0d;&#x5408;&#x7406;token&#xff1b;2) VORD Loss&#xff1a;&#x4e00;&#x79cd;&#x53ef;&#x8bad;&#x7ec3;&#x7684;&#x5e8f;&#x6570;&#x6392;&#x5e8f;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff0c;&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x89c6;&#x89c9;&#x76f8;&#x4f3c;&#x5ea6;&#x8fb9;&#x9645;&#x6765;&#x5f3a;&#x5316;token&#x6982;&#x7387;&#x7684;&#x5e8f;&#x6570;&#x5173;&#x7cfb;&#x3002;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x9075;&#x5faa;&apos;&#x4fee;&#x6539;&#x7a0b;&#x5ea6;&#x8d8a;&#x5927;&#xff0c;&#x76f8;&#x5173;token&#x6982;&#x7387;&#x5e94;&#x8d8a;&#x4f4e;&#xff0c;&#x4e0d;&#x76f8;&#x5173;token&#x6982;&#x7387;&#x5e94;&#x8d8a;&#x9ad8;&apos;&#x7684;&#x5e8f;&#x6570;&#x7ea6;&#x675f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1632,1633"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) VORD&#x80fd;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x591a;&#x79cd;LVLM&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff1b;2) &#x6539;&#x5584;&#x4e86;&#x6a21;&#x578b;&#x6821;&#x51c6;&#xff0c;&#x4f7f;&#x7f6e;&#x4fe1;&#x5ea6;&#x4e0e;&#x6b63;&#x786e;&#x6027;&#x66f4;&#x4e00;&#x81f4;&#xff1b;3) &#x751f;&#x6210;&#x4e86;&#x66f4;&#x7b80;&#x77ed;&#x7684;&#x6587;&#x672c;&#x5e8f;&#x5217;&#xff1b;4) &#x4e24;&#x79cd;&#x5f62;&#x5f0f;(VORD Decoding&#x548c;VORD Loss)&#x5747;&#x6709;&#x6548;&#xff0c;&#x540e;&#x8005;&#x5728;&#x53ef;&#x7528;&#x8bad;&#x7ec3;&#x8d44;&#x6e90;&#x65f6;&#x6027;&#x80fd;&#x66f4;&#x4f18;&#x3002;","children":[],"payload":{"tag":"li","lines":"1633,1634"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: VORD&#x901a;&#x8fc7;&#x7b80;&#x5355;&#x7684;&#x5e8f;&#x6570;&#x6821;&#x51c6;&#x539f;&#x7406;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLMs&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e3a;&#x6539;&#x5584;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x6c9f;&#x901a;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x5927;&#x91cf;&#x8d44;&#x6e90;&#x5373;&#x53ef;&#x90e8;&#x7f72;&#xff0c;&#x5bf9;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;LVLMs&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1634,1636"}}],"payload":{"tag":"li","lines":"1630,1636","fold":1}}],"payload":{"tag":"h4","lines":"1628,1629"}},{"content":"MFP: Mitigating Object Hallucinations in MLLMs via Multi-Frequency Perturbations","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x591a;&#x9891;&#x7387;&#x6270;&#x52a8;&#xff08;MFP&#xff09;&#x7684;&#x7b80;&#x5355;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x3001;&#x53ef;&#x63d2;&#x62d4;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x540c;&#x65f6;&#x5229;&#x7528;&#x56fe;&#x50cf;&#x7684;&#x9ad8;&#x9891;&#x548c;&#x4f4e;&#x9891;&#x7279;&#x5f81;&#x6765;&#x6270;&#x52a8;&#x89c6;&#x89c9;&#x8868;&#x793a;&#xff0c;&#x5e76;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x663e;&#x5f0f;&#x6291;&#x5236;&#x5197;&#x4f59;&#x9891;&#x57df;&#x7279;&#x5f81;&#xff0c;&#x4ece;&#x800c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5e76;&#x53ef;&#x4e0e;&#x5176;&#x4ed6;&#x63a8;&#x7406;&#x65f6;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1637,1638"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5176;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x7684;&#x771f;&#x5b9e;&#x6027;&#x5e38;&#x53d7;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x63cf;&#x8ff0;&#xff09;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x635f;&#x5bb3;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x6027;&#x80fd;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x8bba;&#x6587;&#x53d1;&#x73b0;&#xff0c;&#x5bfc;&#x81f4;&#x8be5;&#x95ee;&#x9898;&#x7684;&#x5173;&#x952e;&#x539f;&#x56e0;&#x662f;&#x6a21;&#x578b;&#x5728;&#x68c0;&#x6d4b;&#x7269;&#x4f53;&#x65f6;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x56fe;&#x50cf;&#x7684;&#x7279;&#x5b9a;&#x9891;&#x7387;&#x7279;&#x5f81;&#xff08;&#x5982;&#x9ad8;&#x9891;&#x8fb9;&#x7f18;&#x6216;&#x4f4e;&#x9891;&#x8f6e;&#x5ed3;&#xff09;&#xff0c;&#x800c;&#x5ffd;&#x7565;&#x4e86;&#x56fe;&#x50cf;&#x7684;&#x5b9e;&#x9645;&#x5185;&#x5bb9;&#x3002;&#x89e3;&#x51b3;&#x6b64;&#x95ee;&#x9898;&#x5bf9;&#x4e8e;&#x63d0;&#x5347;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1639,1640"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x591a;&#x9891;&#x7387;&#x6270;&#x52a8;&#xff08;MFP&#xff09;&#x65b9;&#x6cd5;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#x4e24;&#x4e2a;&#x7ec4;&#x4ef6;&#xff1a;1&#xff09;&#x591a;&#x9891;&#x7387;&#x7279;&#x5f81;&#x63d0;&#x53d6;&#xff1a;&#x4f7f;&#x7528;&#x9ad8;&#x65af;&#x9ad8;&#x901a;&#x548c;&#x4f4e;&#x901a;&#x6ee4;&#x6ce2;&#x5668;&#x5bf9;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x8fdb;&#x884c;&#x5085;&#x91cc;&#x53f6;&#x53d8;&#x6362;&#xff0c;&#x5206;&#x79bb;&#x51fa;&#x9ad8;&#x9891;&#x548c;&#x4f4e;&#x9891;&#x7279;&#x5f81;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff08;&#x5982;CLIP&#xff09;&#x5c06;&#x5176;&#x8f6c;&#x6362;&#x4e3a;&#x5bf9;&#x5e94;&#x7684;&#x89c6;&#x89c9;token&#x5e8f;&#x5217;&#xff1b;2&#xff09;&#x7ec6;&#x7c92;&#x5ea6;&#x9891;&#x7387;&#x7279;&#x5f81;&#x878d;&#x5408;&#xff1a;&#x5c06;&#x539f;&#x59cb;&#x56fe;&#x50cf;token&#x5e8f;&#x5217;&#x4e0e;&#x9ad8;&#x3001;&#x4f4e;&#x9891;token&#x5e8f;&#x5217;&#x901a;&#x8fc7;&#x4ea4;&#x53c9;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#x5728;token&#x7ea7;&#x522b;&#x8fdb;&#x884c;&#x878d;&#x5408;&#xff0c;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x7684;&#x6270;&#x52a8;&#x89c6;&#x89c9;token&#x5e8f;&#x5217;&#x3002;&#x5728;&#x63a8;&#x7406;&#x65f6;&#xff0c;&#x5bf9;&#x9ad8;&#x3001;&#x4f4e;&#x9891;&#x7279;&#x5f81;&#x6270;&#x52a8;&#x5e94;&#x7528;&#x8870;&#x51cf;&#xff0c;&#x4ee5;&#x6291;&#x5236;&#x5197;&#x4f59;&#x9891;&#x57df;&#x4fe1;&#x606f;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x53ef;&#x4f5c;&#x4e3a;&#x8bad;&#x7ec3;&#x65f6;&#x65b9;&#x6cd5;&#x4f7f;&#x7528;&#xff0c;&#x4e14;&#x6613;&#x4e8e;&#x4e0e;&#x5176;&#x4ed6;&#x63a8;&#x7406;&#x65f6;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x3002;","children":[],"payload":{"tag":"li","lines":"1640,1641"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;MFP&#x80fd;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x591a;&#x79cd;MLLM&#x67b6;&#x6784;&#xff08;&#x4e0d;&#x540c;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x3001;LLM&#x9aa8;&#x5e72;&#x3001;&#x5206;&#x8fa8;&#x7387;&#x6216;&#x89c4;&#x6a21;&#xff09;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x7387;&#x3002;&#x5728;MSCOCO&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x5b9a;&#x91cf;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x4ec5;&#x4fdd;&#x7559;&#x90e8;&#x5206;&#x9891;&#x7387;&#x7279;&#x5f81;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x53e5;&#x5b50;&#x6bd4;&#x4f8b;&#x4ece;15.0%&#x5927;&#x5e45;&#x4e0a;&#x5347;&#xff08;&#x9ad8;&#x9891;&#x4fdd;&#x7559;&#x51cf;&#x5c11;&#x65f6;&#x8fbe;32.1%&#xff0c;&#x4f4e;&#x9891;&#x4fdd;&#x7559;&#x51cf;&#x5c11;&#x65f6;&#x8fbe;24.4%&#xff09;&#xff0c;&#x800c;MFP&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x8fd9;&#x4e00;&#x73b0;&#x8c61;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x4f5c;&#x4e3a;&#x8bad;&#x7ec3;&#x65f6;&#x65b9;&#x6cd5;&#xff0c;MFP&#x4e0e;&#x63a8;&#x7406;&#x65f6;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x540e;&#xff0c;&#x5728;CHAIR&#x57fa;&#x51c6;&#x4e0a;&#x8fbe;&#x5230;&#x4e86;&#x6700;&#x5148;&#x8fdb;&#xff08;SOTA&#xff09;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1641,1642"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;MLLM&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x5bf9;&#x9891;&#x57df;&#x7279;&#x5f81;&#x7684;&#x8fc7;&#x5ea6;&#x654f;&#x611f;&#xff0c;&#x800c;MFP&#x901a;&#x8fc7;&#x6270;&#x52a8;&#x548c;&#x6291;&#x5236;&#x5197;&#x4f59;&#x9891;&#x7387;&#x7279;&#x5f81;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x8f93;&#x51fa;&#x771f;&#x5b9e;&#x6027;&#x3002;&#x8fd9;&#x4e00;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x5f3a;&#x6cdb;&#x5316;&#x80fd;&#x529b;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;&#x6a21;&#x578b;&#x67b6;&#x6784;&#xff0c;&#x4e14;&#x53ef;&#x4e0e;&#x5176;&#x4ed6;&#x6280;&#x672f;&#x7ed3;&#x5408;&#x8fdb;&#x4e00;&#x6b65;&#x4f18;&#x5316;&#x6027;&#x80fd;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x4e3a;MLLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x65b0;&#x89c6;&#x89d2;&#xff08;&#x9891;&#x57df;&#x5206;&#x6790;&#xff09;&#xff0c;&#x63a8;&#x52a8;&#x66f4;&#x53ef;&#x4fe1;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x53d1;&#x5c55;&#xff0c;&#x5e76;&#x4e3a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x63d0;&#x4f9b;&#x66f4;&#x5b89;&#x5168;&#x7684;&#x6a21;&#x578b;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"1642,1646"}}],"payload":{"tag":"li","lines":"1638,1646","fold":1}}],"payload":{"tag":"h4","lines":"1636,1637"}},{"content":"MAP: Mitigating Hallucinations in Large Vision-Language Models with Map-Level Attention Processing","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;MAP&#xff08;Map-Level Attention Processing&#xff09;&#x7684;&#x65b0;&#x578b;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5c06;&#x6a21;&#x578b;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x89c6;&#x4e3a;&#x4e8c;&#x7ef4;&#x8bed;&#x4e49;&#x56fe;&#xff0c;&#x901a;&#x8fc7;&#x5c42;&#x95f4;&#x548c;&#x5c42;&#x5185;&#x7684;&#x4ea4;&#x53c9;&#x6ce8;&#x610f;&#x529b;&#x64cd;&#x4f5c;&#x4ee5;&#x53ca;&#x5168;&#x5c40;-&#x5c40;&#x90e8;&#x903b;&#x8f91;&#x878d;&#x5408;&#xff0c;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e8b;&#x5b9e;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x9ad8;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1647,1648"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#xff0c;&#x5373;&#x751f;&#x6210;&#x8bed;&#x6cd5;&#x6b63;&#x786e;&#x4f46;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x4fe1;&#x606f;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x8f93;&#x51fa;&#x7684;&#x9886;&#x57df;&#xff08;&#x5982;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x4e2d;&#x5c24;&#x4e3a;&#x4e25;&#x91cd;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x5c40;&#x9650;&#x4e8e;&#x5355;&#x7ef4;&#x5ea6;&#xff08;&#x5c42;&#x95f4;&#x6216;&#x5c42;&#x5185;&#xff09;&#x5904;&#x7406;&#xff0c;&#x672a;&#x80fd;&#x5145;&#x5206;&#x5229;&#x7528;&#x5206;&#x6563;&#x5728;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x4e2d;&#x7684;&#x4e8b;&#x5b9e;&#x4fe1;&#x53f7;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x5168;&#x9762;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1649,1650"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;MAP&#x65b9;&#x6cd5;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1. <strong>&#x4e8c;&#x7ef4;&#x8bed;&#x4e49;&#x56fe;&#x64cd;&#x4f5c;</strong>&#xff1a;&#x5c06;&#x5168;&#x90e8;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x89c6;&#x4e3a;&#x4e8c;&#x7ef4;&#x7ed3;&#x6784;&#xff08;&#x5c42;&#x7ef4;&#x5ea6;&#xd7;&#x65f6;&#x95f4;&#x7ef4;&#x5ea6;&#xff09;&#xff0c;&#x805a;&#x5408;&#x5206;&#x6563;&#x7684;&#x4e8b;&#x5b9e;&#x4fe1;&#x606f;&#xff1b;2. <strong>&#x5c42;&#x95f4;&#x4ea4;&#x53c9;&#x6ce8;&#x610f;&#x529b;&#xff08;Layer-Wise Criss-Cross Attention&#xff09;</strong>&#xff1a;&#x5728;&#x6bcf;&#x4e00;&#x89e3;&#x7801;&#x5c42;&#x540c;&#x65f6;&#x6355;&#x83b7;&#x5c42;&#x95f4;&#x548c;&#x5c42;&#x5185;&#x4f9d;&#x8d56;&#x5173;&#x7cfb;&#xff0c;&#x901a;&#x8fc7;&#x5e7f;&#x64ad;&#x673a;&#x5236;&#x7ec6;&#x5316;&#x8bcd;&#x5143;&#x8868;&#x793a;&#xff1b;3. <strong>&#x5168;&#x5c40;-&#x5c40;&#x90e8;&#x903b;&#x8f91;&#x878d;&#x5408;&#xff08;Global-Local Logit Fusion&#xff09;</strong>&#xff1a;&#x7ed3;&#x5408;&#x5168;&#x5c40;&#x6ce8;&#x610f;&#x529b;&#x524d;&#x540e;&#x7684;&#x903b;&#x8f91;&#x8f93;&#x51fa;&#xff0c;&#x5e73;&#x8861;&#x7ec6;&#x7c92;&#x5ea6;&#x5c40;&#x90e8;&#x8bc1;&#x636e;&#x548c;&#x5168;&#x5c40;&#x4e0a;&#x4e0b;&#x6587;&#x4fe1;&#x606f;&#xff0c;&#x8fdb;&#x4e00;&#x6b65;&#x63d0;&#x5347;&#x9884;&#x6d4b;&#x51c6;&#x786e;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#xff0c;&#x4ec5;&#x5e72;&#x9884;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1650,1651"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;POPE&#x3001;MME&#x548c;MMHal-Bench&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x7ed3;&#x679c;&#x663e;&#x793a;&#xff1a;1. MAP&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x4e8b;&#x5b9e;&#x4e00;&#x81f4;&#x6027;&#xff1b;2. &#x5728;MME&#x7ec6;&#x5206;&#x7c7b;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x989c;&#x8272;&#x3001;&#x8ba1;&#x6570;&#x3001;&#x573a;&#x666f;&#x8bc6;&#x522b;&#xff09;&#x4e2d;&#x6027;&#x80fd;&#x5168;&#x9762;&#x63d0;&#x5347;&#xff1b;3. &#x7edf;&#x8ba1;&#x8bc1;&#x636e;&#x8868;&#x660e;&#xff0c;&#x56fe;&#x50cf;&#x5185;&#x5bf9;&#x8c61;&#x7684;&#x8bcd;&#x5143;&#x6982;&#x7387;&#x59cb;&#x7ec8;&#x9ad8;&#x4e8e;&#x5e7b;&#x89c9;&#x5bf9;&#x8c61;&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x8bed;&#x4e49;&#x56fe;&#x4e2d;&#x4e8b;&#x5b9e;&#x4fe1;&#x53f7;&#x7684;&#x5e7f;&#x6cdb;&#x5206;&#x5e03;&#xff1b;4. &#x65b9;&#x6cd5;&#x5728;&#x4e0d;&#x540c;LVLM&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA1.5&#xff09;&#x4e0a;&#x5747;&#x6709;&#x6548;&#xff0c;&#x5177;&#x6cdb;&#x5316;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1651,1652"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MAP&#x901a;&#x8fc7;&#x4e8c;&#x7ef4;&#x8bed;&#x4e49;&#x56fe;&#x89c6;&#x89d2;&#x548c;&#x521b;&#x65b0;&#x7684;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x4e3a;LVLM&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x9ad8;&#x6548;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x7279;&#x6027;&#x964d;&#x4f4e;&#x4e86;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x5b9e;&#x9645;&#x573a;&#x666f;&#x3002;&#x672a;&#x6765;&#x53ef;&#x63a2;&#x7d22;&#x8be5;&#x6846;&#x67b6;&#x5728;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x8fdb;&#x4e00;&#x6b65;&#x5e94;&#x7528;&#xff0c;&#x5982;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x6216;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7cfb;&#x7edf;&#xff0c;&#x4ee5;&#x63d0;&#x5347;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1652,1655"}}],"payload":{"tag":"li","lines":"1648,1655","fold":1}}],"payload":{"tag":"h4","lines":"1646,1647"}},{"content":"Obliviate: Analyzing and Mitigating Object Hallucination: A Training Bias Perspective","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5b58;&#x5728;&#x8bad;&#x7ec3;&#x504f;&#x5dee;&#xff0c;&#x5bfc;&#x81f4;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5373;&#x4f7f;&#x5bf9;&#x8bad;&#x7ec3;&#x4e2d;&#x89c1;&#x8fc7;&#x7684;&#x56fe;&#x50cf;&#x8fdb;&#x884c;&#x5fae;&#x5c0f;&#x4fee;&#x6539;&#xff08;&#x5982;&#x906e;&#x6321;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x6a21;&#x578b;&#x4ecd;&#x4f1a;&#x9519;&#x8bef;&#x5730;&#x80af;&#x5b9a;&#x7269;&#x4f53;&#x7684;&#x5b58;&#x5728;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x7684;&#x53bb;&#x504f;&#x65b9;&#x6cd5;Obliviate&#xff0c;&#x4ec5;&#x66f4;&#x65b0;2%&#x7684;&#x53c2;&#x6570;&#x5373;&#x53ef;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1656,1657"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x6587;&#x672c;&#x3002;&#x5c3d;&#x7ba1;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x89c4;&#x6a21;&#x4e0d;&#x65ad;&#x6269;&#x5927;&#xff0c;&#x8be5;&#x95ee;&#x9898;&#x4f9d;&#x7136;&#x5b58;&#x5728;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x73b0;&#x5b9e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x7814;&#x7a76;&#x65e8;&#x5728;&#x63a2;&#x7a76;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x5982;&#x4f55;&#x5bfc;&#x81f4;&#x8fd9;&#x79cd;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x9a8c;&#x8bc1;&#x6a21;&#x578b;&#x662f;&#x5426;&#x771f;&#x6b63;&#x5b66;&#x4f1a;&#x4e86;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#xff0c;&#x8fd8;&#x662f;&#x4ec5;&#x4ec5;&#x5b66;&#x4e60;&#x4e86;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e2d;&#x7684;&#x865a;&#x5047;&#x5173;&#x8054;&#x3002;","children":[],"payload":{"tag":"li","lines":"1658,1659"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x6784;&#x5efa;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x57fa;&#x51c6;POPEv2&#xff0c;&#x5305;&#x542b;&#x4ece;LVLM&#x8bad;&#x7ec3;&#x6570;&#x636e;&#xff08;MSCOCO&#xff09;&#x4e2d;&#x91c7;&#x6837;&#x7684;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x53cd;&#x4e8b;&#x5b9e;&#x56fe;&#x50cf;&#xff08;&#x906e;&#x6321;&#x7279;&#x5b9a;&#x7269;&#x4f53;&#xff09;&#xff0c;&#x7528;&#x4e8e;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x8bc1;&#x636e;&#x7684;&#x4f9d;&#x8d56;&#x3002;&#x901a;&#x8fc7;&#x63a2;&#x6d4b;&#x5b9e;&#x9a8c;&#x5206;&#x6790;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x7ec4;&#x4ef6;&#xff0c;&#x53d1;&#x73b0;&#x8bed;&#x8a00;&#x5efa;&#x6a21;&#x5934;&#xff08;LM Head&#xff09;&#x662f;&#x504f;&#x5dee;&#x7684;&#x4e3b;&#x8981;&#x6765;&#x6e90;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x63d0;&#x51fa;&#x4e86;Obliviate&#x65b9;&#x6cd5;&#xff1a;&#x5c06;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x4e0a;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x4e0e;&#x771f;&#x5b9e;&#x6807;&#x7b7e;&#x7684;&#x5dee;&#x5f02;&#x4f5c;&#x4e3a;&#x504f;&#x5dee;&#x4ee3;&#x7406;&#xff0c;&#x4ec5;&#x5bf9;LM Head&#x8fdb;&#x884c;&#x68af;&#x5ea6;&#x4e0a;&#x5347;&#x66f4;&#x65b0;&#xff08;&#x53c2;&#x6570;&#x66f4;&#x65b0;&#x7ea6;2%&#xff09;&#xff0c;&#x4ee5;&#x964d;&#x4f4e;&#x504f;&#x5dee;&#x8f93;&#x51fa;&#x7684;&#x6982;&#x7387;&#xff0c;&#x5b9e;&#x73b0;&#x9ad8;&#x6548;&#x53bb;&#x504f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1659,1660"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x53d1;&#x73b0;&#x5305;&#x62ec;&#xff1a;1&#xff09;LVLM&#x5728;POPEv2&#x4e0a;&#x8868;&#x73b0;&#x4e0d;&#x4f73;&#xff08;F1&#x5206;&#x6570;&#x7ea6;80%&#xff09;&#xff0c;&#x5373;&#x4f7f;&#x56fe;&#x50cf;&#x6765;&#x81ea;&#x8bad;&#x7ec3;&#x96c6;&#xff0c;&#x53cd;&#x4e8b;&#x5b9e;&#x56fe;&#x50cf;&#x4e0a;&#x7684;&#x771f;&#x8d1f;&#x7387;&#xff08;TNR&#xff09;&#x591a;&#x6570;&#x4f4e;&#x4e8e;70%&#xff0c;&#x8868;&#x660e;&#x6a21;&#x578b;&#x503e;&#x5411;&#x4e8e;&#x9519;&#x8bef;&#x80af;&#x5b9a;&#xff1b;2&#xff09;&#x6a21;&#x578b;&#x89c4;&#x6a21;&#x6269;&#x5927;&#x5bf9;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x6709;&#x9650;&#xff08;&#x5982;Qwen2-VL-72B&#x53cd;&#x800c;&#x6bd4;2B&#x7248;&#x672c;&#x5dee;&#xff09;&#xff0c;&#x800c;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#xff08;&#x5982;&#x52a8;&#x6001;&#x5206;&#x8fa8;&#x7387;&#xff09;&#x66f4;&#x6709;&#x6548;&#xff1b;3&#xff09;&#x63a2;&#x6d4b;&#x5b9e;&#x9a8c;&#x663e;&#x793a;&#xff0c;&#x4e2d;&#x95f4;&#x5c42;&#x80fd;&#x51c6;&#x786e;&#x7f16;&#x7801;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x4f46;LM Head&#x65e0;&#x6cd5;&#x6b63;&#x786e;&#x8f6c;&#x6362;&#x4e3a;&#x6587;&#x672c;&#x8f93;&#x51fa;&#xff1b;4&#xff09;Obliviate&#x4ec5;&#x7528;1.5%&#x7684;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x548c;2%&#x7684;&#x53c2;&#x6570;&#xff0c;&#x5c31;&#x5728;&#x591a;&#x79cd;LVLM&#xff08;2B-72B&#xff09;&#x4e0a;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x4e14;&#x6cdb;&#x5316;&#x5230;&#x975e;&#x7269;&#x4f53;&#x7ea7;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1660,1661"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff0c;LVLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x4e3b;&#x8981;&#x6e90;&#x4e8e;&#x8bad;&#x7ec3;&#x504f;&#x5dee;&#xff08;&#x5c24;&#x5176;&#x662f;LM Head&#x7684;&#x7f3a;&#x9677;&#xff09;&#xff0c;&#x800c;&#x975e;&#x6570;&#x636e;&#x6216;&#x89c4;&#x6a21;&#x4e0d;&#x8db3;&#x3002;Obliviate&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x53c2;&#x6570;&#x548c;&#x6570;&#x636e;&#x9ad8;&#x6548;&#x7684;&#x53bb;&#x504f;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x4e14;&#x5177;&#x6709;&#x5f3a;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x548c;&#x6cdb;&#x5316;&#x6027;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x5f3a;&#x8c03;&#x4e86;&#x91cd;&#x65b0;&#x601d;&#x8003;LVLM&#x8bad;&#x7ec3;&#x8303;&#x5f0f;&#x7684;&#x5fc5;&#x8981;&#x6027;&#xff0c;&#x5e76;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x7a33;&#x5065;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x5b9e;&#x7528;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1661,1663"}}],"payload":{"tag":"li","lines":"1657,1663","fold":1}}],"payload":{"tag":"h4","lines":"1655,1656"}},{"content":"DBD: Do More Details Always Introduce More Hallucinations in LVLM-based Image Captioning?","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x6311;&#x6218;&#x4e86;&#x201c;&#x7ec6;&#x8282;&#x8d8a;&#x591a;&#xff0c;&#x5e7b;&#x89c9;&#x8d8a;&#x591a;&#x201d;&#x7684;&#x4f20;&#x7edf;&#x89c2;&#x70b9;&#xff0c;&#x6307;&#x51fa;&#x73b0;&#x6709;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR&#xff09;&#x5b58;&#x5728;&#x7f3a;&#x9677;&#xff0c;&#x9ad8;&#x4f30;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x5e76;&#x884c;&#x89e3;&#x7801;&#x7b56;&#x7565;DBD&#x548c;&#x57fa;&#x4e8e;CLIP&#x7684;&#x65b0;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff0c;&#x5b9e;&#x9a8c;&#x8bc1;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x751f;&#x6210;&#x66f4;&#x8be6;&#x7ec6;&#x7684;&#x63cf;&#x8ff0;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4f4e;&#x5e7b;&#x89c9;&#x6c34;&#x5e73;&#x3002;","children":[],"payload":{"tag":"li","lines":"1664,1665"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x751f;&#x6210;&#x4e2d;&#x7ec6;&#x8282;&#x4e0e;&#x5e7b;&#x89c9;&#x4e4b;&#x95f4;&#x7684;&#x6743;&#x8861;&#x95ee;&#x9898;&#x3002;&#x4f20;&#x7edf;&#x89c2;&#x70b9;&#x8ba4;&#x4e3a;&#x751f;&#x6210;&#x66f4;&#x591a;&#x7ec6;&#x8282;&#x4f1a;&#x5bfc;&#x81f4;&#x66f4;&#x591a;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff08;Object Hallucination, OH&#xff09;&#xff0c;&#x4f46;&#x8be5;&#x7814;&#x7a76;&#x6307;&#x51fa;&#x73b0;&#x6709;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff08;&#x5982;CHAIR&#xff09;&#x5b58;&#x5728;&#x6280;&#x672f;&#x7f3a;&#x9677;&#xff0c;&#x5bfc;&#x81f4;&#x7ed3;&#x8bba;&#x4e0d;&#x53ef;&#x9760;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5728;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x5c24;&#x4e3a;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x51c6;&#x786e;&#x4e14;&#x5168;&#x9762;&#x7684;&#x63cf;&#x8ff0;&#x662f;&#x5fc5;&#x4e0d;&#x53ef;&#x5c11;&#x7684;&#x3002;","children":[],"payload":{"tag":"li","lines":"1666,1667"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#x2014;&#x2014;&#x5dee;&#x5f02;&#x5316;&#x675f;&#x641c;&#x7d22;&#x89e3;&#x7801;&#xff08;Differentiated Beam Decoding, DBD&#xff09;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x5c06;&#x56fe;&#x50cf;&#x4e2d;&#x7684;&#x4e30;&#x5bcc;&#x4fe1;&#x606f;&#x5e76;&#x884c;&#x89e3;&#x7801;&#x4e3a;&#x591a;&#x4e2a;&#x72ec;&#x7acb;&#x7684;&#x201c;&#x5355;&#x5143;&#x4e8b;&#x5b9e;&#x201d;&#xff08;unit facts&#xff09;&#x3002;DBD&#x901a;&#x8fc7;&#x8bbe;&#x8ba1;&#x7684;&#x5dee;&#x5f02;&#x5316;&#x8bc4;&#x5206;&#x6307;&#x5bfc;&#x5e76;&#x884c;&#x641c;&#x7d22;&#x548c;&#x5019;&#x9009;&#x7b5b;&#x9009;&#xff0c;&#x6700;&#x540e;&#x805a;&#x5408;&#x9009;&#x4e2d;&#x7684;&#x5355;&#x5143;&#x4e8b;&#x5b9e;&#x751f;&#x6210;&#x6700;&#x7ec8;&#x63cf;&#x8ff0;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x7ec4;&#x57fa;&#x4e8e;CLIP&#x7684;&#x65b0;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#xff1a;CLIP-Precision&#x3001;CLIP-Recall&#x548c;CLIP-F1&#xff0c;&#x901a;&#x8fc7;&#x6bd4;&#x8f83;&#x771f;&#x5b9e;&#x56fe;&#x50cf;&#x533a;&#x57df;&#x548c;&#x751f;&#x6210;&#x6587;&#x672c;&#x5206;&#x7ec4;&#x7684;&#x5d4c;&#x5165;&#x5411;&#x91cf;&#x6765;&#x8bc4;&#x4f30;&#x63cf;&#x8ff0;&#x7684;&#x5168;&#x9762;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1667,1668"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x5bf9;MSCOCO&#x6570;&#x636e;&#x96c6;&#x7684;&#x624b;&#x52a8;&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;&#x8d85;&#x8fc7;55%&#x7531;CHAIR&#x5224;&#x5b9a;&#x7684;&#x5e7b;&#x89c9;&#x662f;&#x4e0d;&#x5408;&#x7406;&#x7684;&#x8bef;&#x5224;&#xff1b;2&#xff09;&#x5728;Visual Genome&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;DBD&#x65b9;&#x6cd5;&#x5728;&#x751f;&#x6210;&#x66f4;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x7684;&#x540c;&#x65f6;&#xff0c;&#x4fdd;&#x6301;&#x4e86;&#x8f83;&#x4f4e;&#x7684;&#x5e7b;&#x89c9;&#x6c34;&#x5e73;&#xff1b;3&#xff09;&#x65b0;&#x63d0;&#x51fa;&#x7684;CLIP&#x7cfb;&#x5217;&#x6307;&#x6807;&#x80fd;&#x66f4;&#x53ef;&#x9760;&#x5730;&#x8bc4;&#x4f30;&#x63cf;&#x8ff0;&#x7684;&#x5168;&#x9762;&#x6027;&#x548c;&#x5e7b;&#x89c9;&#x7a0b;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1668,1669"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x66f4;&#x591a;&#x7684;&#x7ec6;&#x8282;&#x5e76;&#x4e0d;&#x5fc5;&#x7136;&#x5f15;&#x5165;&#x66f4;&#x591a;&#x5e7b;&#x89c9;&#xff0c;&#x901a;&#x8fc7;&#x9002;&#x5f53;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff08;&#x5982;DBD&#xff09;&#xff0c;LVLM&#x80fd;&#x591f;&#x751f;&#x6210;&#x65e2;&#x8be6;&#x7ec6;&#x53c8;&#x51c6;&#x786e;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x6311;&#x6218;&#x4e86;&#x73b0;&#x6709;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;LVLM&#x5728;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#xff0c;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x8bc4;&#x4f30;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"1669,1671"}}],"payload":{"tag":"li","lines":"1665,1671","fold":1}}],"payload":{"tag":"h4","lines":"1663,1664"}},{"content":"NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;NoiseBoost&#xff0c;&#x4e00;&#x79cd;&#x901a;&#x8fc7;&#x5411;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x6ce8;&#x5165;&#x566a;&#x58f0;&#x6270;&#x52a8;&#x6765;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x7b80;&#x5355;&#x901a;&#x7528;&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x5e73;&#x8861;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x5728;&#x76d1;&#x7763;&#x5fae;&#x8c03;&#x3001;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x548c;&#x534a;&#x76d1;&#x7763;&#x5b66;&#x4e60;&#x7b49;&#x591a;&#x79cd;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#x4e0b;&#x5747;&#x6709;&#x6548;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6570;&#x636e;&#x6216;&#x663e;&#x8457;&#x589e;&#x52a0;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1672,1673"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x4fe1;&#x606f;&#xff09;&#xff0c;&#x8fd9;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8bba;&#x6587;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x53d1;&#x73b0;&#xff0c;&#x5e7b;&#x89c9;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff08;language priors&#xff09;&#x800c;&#x5ffd;&#x89c6;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x8fd9;&#x662f;&#x7531;&#x4e8e;MLLMs&#x4e2d;&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x548c;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5206;&#x522b;&#x9884;&#x8bad;&#x7ec3;&#xff0c;&#x7279;&#x5f81;&#x5b58;&#x5728;&#x5dee;&#x5f02;&#xff0c;&#x4e14;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x56fa;&#x6709;&#x7684;&#x6458;&#x8981;&#x673a;&#x5236;&#x4f1a;&#x951a;&#x5b9a;&#x67d0;&#x4e9b;&#x8bed;&#x8a00;&#x6807;&#x8bb0;&#xff0c;&#x5bfc;&#x81f4;&#x540e;&#x7eed;&#x751f;&#x6210;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8fd9;&#x4e9b;&#x951a;&#x70b9;&#x800c;&#x975e;&#x6574;&#x4f53;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;MLLMs&#x7684;&#x51c6;&#x786e;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1674,1675"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;NoiseBoost&#x65b9;&#x6cd5;&#xff0c;&#x6838;&#x5fc3;&#x662f;&#x5728;MLLMs&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x5411;&#x6295;&#x5f71;&#x540e;&#x7684;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#xff08;visual tokens&#xff09;&#x6ce8;&#x5165;&#x9ad8;&#x65af;&#x566a;&#x58f0;&#x6270;&#x52a8;&#xff08;Gaussian noise perturbation&#xff09;&#x3002;&#x566a;&#x58f0;&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x6b63;&#x5219;&#x5316;&#x5668;&#xff0c;&#x589e;&#x52a0;&#x4e86;&#x6a21;&#x578b;&#x7406;&#x89e3;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x96be;&#x5ea6;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x66f4;&#x5747;&#x8861;&#x5730;&#x5206;&#x914d;&#x6ce8;&#x610f;&#x529b;&#x6743;&#x91cd;&#x5230;&#x89c6;&#x89c9;&#x548c;&#x8bed;&#x8a00;&#x6807;&#x8bb0;&#x4e0a;&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x88ab;&#x96c6;&#x6210;&#x5230;&#x4e09;&#x79cd;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#x4e2d;&#xff1a;1) &#x76d1;&#x7763;&#x5fae;&#x8c03;&#xff08;SFT&#xff09;&#xff1a;&#x76f4;&#x63a5;&#x5728;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x4e0a;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#xff1b;2) &#x5f3a;&#x5316;&#x5b66;&#x4e60;&#xff08;&#x5982;DPO&#x7b97;&#x6cd5;&#xff09;&#xff1a;&#x5728;&#x504f;&#x597d;&#x54cd;&#x5e94;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x4e2d;&#x6dfb;&#x52a0;&#x566a;&#x58f0;&#xff0c;&#x589e;&#x5f3a;&#x8bad;&#x7ec3;&#x7a33;&#x5b9a;&#x6027;&#xff1b;3) &#x534a;&#x76d1;&#x7763;&#x5b66;&#x4e60;&#xff08;SSL&#xff09;&#xff1a;&#x5229;&#x7528;&#x672a;&#x6807;&#x6ce8;&#x6570;&#x636e;&#xff0c;&#x901a;&#x8fc7;&#x51bb;&#x7ed3;&#x7684;&#x6559;&#x5e08;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4f2a;&#x6807;&#x7b7e;&#xff0c;&#x5e76;&#x4f7f;&#x7528;NoiseBoost&#x4f5c;&#x4e3a;&#x566a;&#x58f0;&#x5b66;&#x751f;&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x4e00;&#x81f4;&#x6027;&#x6b63;&#x5219;&#x5316;&#xff0c;&#x4ee5;&#x6316;&#x6398;&#x672a;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x7684;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1675,1676"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1) NoiseBoost&#x5728;&#x5e7b;&#x89c9;&#x548c;&#x95ee;&#x7b54;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5747;&#x80fd;&#x63d0;&#x5347;&#x6027;&#x80fd;&#xff0c;&#x5728;&#x4eba;&#x7c7b;&#x8bc4;&#x4f30;&#x7684;&#x5bc6;&#x96c6;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x9ad8;&#x4e86;8.1%&#xff1b;2) &#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x76d1;&#x7763;&#x5fae;&#x8c03;&#x548c;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x4e24;&#x79cd;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#x4e0b;&#x5747;&#x6709;&#x6548;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x7279;&#x5f81;&#x6270;&#x52a8;&#x7684;&#x901a;&#x7528;&#x6027;&#xff1b;3) &#x5728;&#x534a;&#x76d1;&#x7763;&#x5b66;&#x4e60;&#x8bbe;&#x7f6e;&#x4e0b;&#xff0c;&#x4ec5;&#x4f7f;&#x7528;50%&#x7684;&#x6807;&#x6ce8;&#x6570;&#x636e;&#xff0c;&#x5c31;&#x80fd;&#x8fbe;&#x5230;&#x4e0e;&#x4f7f;&#x7528;&#x5168;&#x91cf;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x76f8;&#x5f53;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x6210;&#x529f;&#x5229;&#x7528;&#x4e86;&#x672a;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1676,1677"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: NoiseBoost&#x662f;&#x4e00;&#x79cd;&#x7b80;&#x5355;&#x3001;&#x901a;&#x7528;&#x4e14;&#x4f4e;&#x6210;&#x672c;&#x7684;&#x65b9;&#x6cd5;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x901a;&#x8fc7;&#x566a;&#x58f0;&#x6270;&#x52a8;&#x91cd;&#x65b0;&#x5e73;&#x8861;&#x6ce8;&#x610f;&#x529b;&#x673a;&#x5236;&#xff0c;&#x51cf;&#x5c11;&#x5bf9;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x589e;&#x5f3a;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5229;&#x7528;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x63d0;&#x51fa;&#x4e0d;&#x4ec5;&#x63d0;&#x5347;&#x4e86;&#x73b0;&#x6709;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#x7684;&#x6027;&#x80fd;&#xff0c;&#x8fd8;&#x9996;&#x6b21;&#x5b9e;&#x73b0;&#x4e86;MLLMs&#x7684;&#x534a;&#x76d1;&#x7763;&#x5b66;&#x4e60;&#xff0c;&#x964d;&#x4f4e;&#x4e86;&#x6570;&#x636e;&#x6807;&#x6ce8;&#x6210;&#x672c;&#xff0c;&#x4e3a;MLLMs&#x7684;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x548c;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x652f;&#x6301;&#x3002;","children":[],"payload":{"tag":"li","lines":"1677,1679"}}],"payload":{"tag":"li","lines":"1673,1679","fold":1}}],"payload":{"tag":"h4","lines":"1671,1672"}},{"content":"PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;MMGrounded-PostAlign&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x591a;&#x6a21;&#x6001; grounding &#x6a21;&#x5757;&#xff08;&#x89c6;&#x89c9; grounding &#x548c;&#x6587;&#x672c; grounding&#xff09;&#x4ee5;&#x53ca;&#x8d1f;&#x6837;&#x672c;&#x62d2;&#x7edd;&#x673a;&#x5236;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x548c;&#x5bf9;&#x865a;&#x5047;&#x76f8;&#x5173;&#x6027;&#x7684;&#x4f9d;&#x8d56;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x548c;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1680,1681"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff09;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x4f9d;&#x8d56;&#x865a;&#x5047;&#x76f8;&#x5173;&#x6027;&#x548c;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5185;&#x5bb9;&#xff09;&#x3001;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x9ad8;&#x7ea7;&#x89c6;&#x89c9;&#x7ebf;&#x7d22;&#xff08;&#x5982;&#x80cc;&#x666f;&#x8272;&#x800c;&#x975e;&#x7269;&#x4f53;&#x7ec6;&#x8282;&#xff09;&#xff0c;&#x4ee5;&#x53ca;&#x5728;&#x590d;&#x6742;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x4e2d;&#x7f3a;&#x4e4f;&#x903b;&#x8f91;&#x4e00;&#x81f4;&#x6027;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1682,1683"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x540e;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x6846;&#x67b6;MMGrounded-PostAlign&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x4e00;&#x4e2a;&#x591a;&#x6a21;&#x6001; grounding &#x6a21;&#x5757;&#xff0c;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x5173;&#x952e;&#x673a;&#x5236;&#xff1a;1. <strong>&#x89c6;&#x89c9; grounding</strong>&#xff1a;&#x7528;&#x4e8e;&#x8bc6;&#x522b;&#x56fe;&#x50cf;&#x4e2d;&#x88ab;&#x6307;&#x4ee3;&#x7684;&#x7269;&#x4f53;&#xff0c;&#x5e76;&#x5f15;&#x5165;<strong>&#x8d1f;&#x6837;&#x672c;&#x62d2;&#x7edd;&#x673a;&#x5236;&#xff08;Negative Rejection Mechanism&#xff09;</strong>&#x3002;&#x5f53;&#x6a21;&#x578b;&#x5224;&#x65ad;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x6240;&#x95ee;&#x7269;&#x4f53;&#x65f6;&#xff0c;&#x4f1a;&#x9884;&#x6d4b;&#x4e00;&#x4e2a;&#x7279;&#x6b8a;&#x7684;<rej>&#x6807;&#x8bb0;&#xff0c;&#x4ece;&#x800c;&#x76f4;&#x63a5;&#x8f93;&#x51fa;&#x7a7a;&#x63a9;&#x7801;&#x548c;&#x8fb9;&#x754c;&#x6846;&#xff0c;&#x907f;&#x514d;&#x5e7b;&#x89c9;&#x3002;2. <strong>&#x6587;&#x672c; grounding</strong>&#xff1a;&#x7528;&#x4e8e;&#x4e3a;&#x6700;&#x7ec8;&#x7b54;&#x6848;&#x751f;&#x6210;&#x63a8;&#x7406;&#x4f9d;&#x636e;&#xff08;rationale&#xff09;&#xff0c;&#x5e76;&#x5f15;&#x5165;<strong>&#x9009;&#x62e9;&#x6027;&#x63a8;&#x7406;&#x673a;&#x5236;&#xff08;Selective Reasoning Mechanism&#xff09;</strong>&#x3002;&#x8be5;&#x673a;&#x5236;&#x80fd;&#x6839;&#x636e;&#x67e5;&#x8be2;&#x7684;&#x590d;&#x6742;&#x5ea6;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x6a21;&#x578b;&#x7684;&#x63a8;&#x7406;&#x7b56;&#x7565;&#xff0c;&#x5bf9;&#x4e8e;&#x590d;&#x6742;&#x95ee;&#x9898;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63a8;&#x7406;&#xff0c;&#x5bf9;&#x4e8e;&#x7b80;&#x5355;&#x95ee;&#x9898;&#x5219;&#x76f4;&#x63a5;&#x7ed9;&#x51fa;&#x7b54;&#x6848;&#x3002;&#x6574;&#x4e2a;&#x6846;&#x67b6;&#x901a;&#x8fc7;&#x5c06;&#x89c6;&#x89c9; grounding &#x7684;&#x8f93;&#x51fa;&#xff08;&#x5982;&#x5206;&#x5272;&#x63a9;&#x7801;&#xff09;&#x548c;&#x6587;&#x672c; grounding &#x7684;&#x63a8;&#x7406;&#x4f5c;&#x4e3a;&#x9690;&#x5f0f;&#x7ea6;&#x675f;&#xff0c;&#x6765;&#x6307;&#x5bfc; MLLM &#x751f;&#x6210;&#x6700;&#x7ec8;&#x7b54;&#x6848;&#xff0c;&#x786e;&#x4fdd;&#x8f93;&#x51fa;&#x540c;&#x65f6;&#x951a;&#x5b9a;&#x5728;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x8bc1;&#x636e;&#x4e0a;&#x3002;</rej>","children":[],"payload":{"tag":"li","lines":"1683,1684"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x8bba;&#x6587;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x8fdb;&#x884c;&#x4e86;&#x5e7f;&#x6cdb;&#x8bc4;&#x4f30;&#xff0c;&#x5305;&#x62ec;POPE&#xff08;&#x76ee;&#x6807;&#x5b58;&#x5728;&#x6027;&#x5224;&#x65ad;&#xff09;&#x3001;HaloQuest&#xff08;&#x5e7b;&#x89c9;&#x8bc4;&#x4f30;&#xff09;&#x3001;VQAv2&#xff08;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff09;&#x3001;MME&#xff08;&#x591a;&#x6a21;&#x6001;&#x8bc4;&#x6d4b;&#xff09;&#x548c;MMBench&#xff08;&#x591a;&#x6a21;&#x6001;&#x57fa;&#x51c6;&#xff09;&#x3002;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;&#x65b9;&#x6cd5;&#x5728;<strong>&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;</strong>&#x548c;<strong>&#x5e7b;&#x89c9;&#x6291;&#x5236;</strong>&#x65b9;&#x9762;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x6539;&#x8fdb;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86; MLLMs &#x7684;&#x901a;&#x7528;&#x63a8;&#x7406;&#x80fd;&#x529b;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x7531;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x5f15;&#x8d77;&#x7684;&#x5171;&#x73b0;&#x5e7b;&#x89c9;&#xff08;&#x4f8b;&#x5982;&#xff0c;&#x4e0d;&#x4f1a;&#x56e0;&#x4e3a;&#x6c99;&#x53d1;&#x4e0a;&#x5e38;&#x6709;&#x732b;&#x5c31;&#x9519;&#x8bef;&#x5730;&#x62a5;&#x544a;&#x6709;&#x732b;&#xff09;&#xff0c;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x590d;&#x6742;&#x95ee;&#x9898;&#x7684;&#x63a8;&#x7406;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1684,1685"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x5c06;&#x591a;&#x6a21;&#x6001; grounding &#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x201c;&#x6821;&#x6b63;&#x955c;&#x5934;&#x201d;&#x96c6;&#x6210;&#x5230; MLLMs &#x4e2d;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x589e;&#x5f3a;&#x89c6;&#x89c9;-&#x6587;&#x672c;&#x6a21;&#x6001;&#x4e4b;&#x95f4;&#x7684;&#x5bf9;&#x9f50;&#xff0c;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5bf9;&#x865a;&#x5047;&#x76f8;&#x5173;&#x6027;&#x548c;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x7684;&#x4f9d;&#x8d56;&#x3002;MMGrounded-PostAlign &#x6846;&#x67b6;&#x901a;&#x8fc7;&#x5176;&#x8d1f;&#x6837;&#x672c;&#x62d2;&#x7edd;&#x548c;&#x9009;&#x62e9;&#x6027;&#x63a8;&#x7406;&#x673a;&#x5236;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x9c81;&#x68d2;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x6761;&#x6709;&#x6548;&#x8def;&#x5f84;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x63a8;&#x52a8;&#x4e86;&#x66f4;&#x53ef;&#x4fe1;&#x7684; MLLMs &#x7684;&#x53d1;&#x5c55;&#xff0c;&#x4f7f;&#x5176;&#x5728;&#x771f;&#x5b9e;&#x4e16;&#x754c;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x80fd;&#x505a;&#x51fa;&#x66f4;&#x57fa;&#x4e8e;&#x5b9e;&#x9645;&#x8bc1;&#x636e;&#x7684;&#x51b3;&#x7b56;&#xff0c;&#x51cf;&#x5c11;&#x9519;&#x8bef;&#x548c;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1685,1690"}}],"payload":{"tag":"li","lines":"1681,1690","fold":1}}],"payload":{"tag":"h4","lines":"1679,1680"}},{"content":"Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;REVERSE&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5e7b;&#x89c9;&#x611f;&#x77e5;&#x8bad;&#x7ec3;&#x548c;&#x63a8;&#x7406;&#x65f6;&#x56de;&#x6eaf;&#x91cd;&#x91c7;&#x6837;&#x6280;&#x672f;&#xff0c;&#x7edf;&#x4e00;&#x4e86;&#x751f;&#x6210;&#x8c03;&#x6574;&#x4e0e;&#x4e8b;&#x540e;&#x9a8c;&#x8bc1;&#x4e24;&#x79cd;&#x65b9;&#x6cd5;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x8fbe;&#x5230;&#x6700;&#x5148;&#x8fdb;&#x6c34;&#x5e73;&#x3002;","children":[],"payload":{"tag":"li","lines":"1691,1692"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x5bf9;&#x8c61;&#x3001;&#x52a8;&#x4f5c;&#x6216;&#x6982;&#x5ff5;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x8f85;&#x52a9;&#x6280;&#x672f;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x5e26;&#x6765;&#x5de8;&#x5927;&#x98ce;&#x9669;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x751f;&#x6210;&#x8c03;&#x6574;&#xff08;&#x4fee;&#x6539;&#x89e3;&#x7801;&#x884c;&#x4e3a;&#xff09;&#x548c;&#x4e8b;&#x540e;&#x9a8c;&#x8bc1;&#xff08;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x8bc4;&#x4f30;&#xff09;&#x4e24;&#x7c7b;&#xff0c;&#x4f46;&#x524d;&#x8005;&#x7f3a;&#x4e4f;&#x7ea0;&#x6b63;&#x673a;&#x5236;&#xff0c;&#x540e;&#x8005;&#x590d;&#x6742;&#x4e14;&#x591a;&#x4f9d;&#x8d56;&#x62d2;&#x7edd;&#x7b56;&#x7565;&#x800c;&#x975e;&#x4fee;&#x6b63;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x7edf;&#x4e00;&#x6846;&#x67b6;&#xff0c;&#x65e2;&#x80fd;&#x68c0;&#x6d4b;&#x5e7b;&#x89c9;&#x53c8;&#x80fd;&#x52a8;&#x6001;&#x4fee;&#x6b63;&#x3002;","children":[],"payload":{"tag":"li","lines":"1693,1694"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: REVERSE&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x7ec4;&#x4ef6;&#xff1a;1) &#x5e7b;&#x89c9;&#x611f;&#x77e5;&#x8bad;&#x7ec3;&#xff1a;&#x4f7f;&#x7528;&#x5305;&#x542b;130&#x4e07;&#x534a;&#x5408;&#x6210;&#x6837;&#x672c;&#x7684;&#x6570;&#x636e;&#x96c6;&#xff08;&#x6e90;&#x81ea;LLaVA-v1.5-665k&#xff0c;&#x901a;&#x8fc7;&#x89c4;&#x5219;&#x548c;GPT-4o-mini&#x589e;&#x5f3a;&#x751f;&#x6210;&#x8d1f;&#x6837;&#x672c;&#xff09;&#xff0c;&#x5728;&#x7b54;&#x6848;&#x4e2d;&#x5f15;&#x5165;&#x7279;&#x6b8a;&#x6807;&#x8bb0;&#xff08;<span>&#x8868;&#x793a;&#x77ed;&#x8bed;&#x5f00;&#x59cb;&#xff0c;&#x8868;&#x793a;&#x53ef;&#x4fe1;&#x77ed;&#x8bed;&#x7ed3;&#x675f;&#xff0c;&#x8868;&#x793a;&#x5e7b;&#x89c9;&#x77ed;&#x8bed;&#x7ed3;&#x675f;&#xff09;&#xff0c;&#x8bad;&#x7ec3;VLM&#x8bc6;&#x522b;&#x548c;&#x6807;&#x8bb0;&#x5e7b;&#x89c9;&#xff1b;2) &#x56de;&#x6eaf;&#x91cd;&#x91c7;&#x6837;&#xff1a;&#x5728;&#x63a8;&#x7406;&#x65f6;&#xff0c;&#x5f53;&#x6a21;&#x578b;&#x751f;&#x6210;&#x6807;&#x8bb0;&#x7684;&#x6982;&#x7387;&#x8d85;&#x8fc7;&#x9608;&#x503c;&#xff0c;&#x89e6;&#x53d1;&#x56de;&#x6eaf;&#x81f3;&#x6700;&#x8fd1;&#x7684;&#x53ef;&#x4fe1;&#x68c0;&#x67e5;&#x70b9;&#xff08;&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x62d2;&#x7edd;&#x91c7;&#x6837;&#x548c;&#x67e5;&#x8be2;&#x91cd;&#x5199;&#xff08;&#x5728;&#x67e5;&#x8be2;&#x4e2d;&#x6ce8;&#x5165;&#x5e7b;&#x89c9;&#x5173;&#x952e;&#x8bcd;&#x4f5c;&#x4e3a;&#x63d0;&#x793a;&#xff09;&#x8fdb;&#x884c;&#x8fed;&#x4ee3;&#x81ea;&#x6211;&#x4fee;&#x6b63;&#x3002;</span>","children":[],"payload":{"tag":"li","lines":"1694,1695"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;LLaVA-v1.5&#x3001;LLaVA-MORE&#x548c;Qwen2.5-VL&#x6a21;&#x578b;&#x4e0a;&#x8bc4;&#x4f30;&#xff0c;REVERSE&#x5728;&#x591a;&#x4e2a;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x53d6;&#x5f97;&#x6700;&#x5148;&#x8fdb;&#x6027;&#x80fd;&#xff1a;&#x5728;CHAIR-MSCOCO&#x548c;AMBER&#xff08;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#xff09;&#x4e0a;&#xff0c;&#x6bd4;&#x73b0;&#x6709;&#x6700;&#x4f73;&#x65b9;&#x6cd5;&#x964d;&#x4f4e;CHAIR&#x5206;&#x6570;&#x8fbe;12%&#xff1b;&#x5728;MMHal&#x548c;HaloQuest&#xff08;&#x5e7b;&#x89c9;&#x654f;&#x611f;&#x5f00;&#x653e;&#x4efb;&#x52a1;&#xff09;&#x4e0a;&#xff0c;&#x6027;&#x80fd;&#x5206;&#x522b;&#x63d0;&#x5347;10%&#x548c;34%&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9519;&#x8bef;&#x524d;&#x63d0;&#x548c;&#x4e0a;&#x4e0b;&#x6587;&#x4e0d;&#x8db3;&#x7684;&#x95ee;&#x9898;&#x4e2d;&#x8868;&#x73b0;&#x7a81;&#x51fa;&#x3002;","children":[],"payload":{"tag":"li","lines":"1695,1696"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: REVERSE&#x662f;&#x9996;&#x4e2a;&#x7edf;&#x4e00;&#x751f;&#x6210;&#x8c03;&#x6574;&#x4e0e;&#x4e8b;&#x540e;&#x9a8c;&#x8bc1;&#x7684;&#x6846;&#x67b6;&#xff0c;&#x4f7f;VLM&#x80fd;&#x591f;&#x540c;&#x65f6;&#x4f5c;&#x4e3a;&#x751f;&#x6210;&#x5668;&#x548c;&#x9a8c;&#x8bc1;&#x5668;&#xff0c;&#x5b9e;&#x73b0;&#x52a8;&#x6001;&#x81ea;&#x6211;&#x4fee;&#x6b63;&#x3002;&#x5176;&#x8d21;&#x732e;&#x5305;&#x62ec;&#xff1a;1) &#x63d0;&#x4f9b;&#x9ad8;&#x6548;&#x7684;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x8303;&#x5f0f;&#xff1b;2) &#x53d1;&#x5e03;130&#x4e07;&#x534a;&#x5408;&#x6210;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x96c6;&#x548c;&#x6784;&#x5efa;&#x6d41;&#x7a0b;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;VLM&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5bf9;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x591a;&#x8f6e;&#x5bf9;&#x8bdd;&#x548c;&#x5176;&#x4ed6;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x3002;","children":[],"payload":{"tag":"li","lines":"1696,1699"}}],"payload":{"tag":"li","lines":"1692,1699","fold":1}}],"payload":{"tag":"h4","lines":"1690,1691"}},{"content":"ReLoop: &quot;Seeing Twice and Thinking Backwards&quot; via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding","children":[{"content":"","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ReLoop&#xff0c;&#x4e00;&#x79cd;&#x53d7;&#x4eba;&#x7c7b;&#x8ba4;&#x77e5;&#x542f;&#x53d1;&#x7684;&#x95ed;&#x73af;&#x8bad;&#x7ec3;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x6574;&#x5408;&#x8bed;&#x4e49;&#x91cd;&#x5efa;&#x3001;&#x89c6;&#x89c9;&#x63cf;&#x8ff0;&#x548c;&#x6ce8;&#x610f;&#x529b;&#x5bf9;&#x9f50;&#x4e09;&#x79cd;&#x4e00;&#x81f4;&#x6027;&#x53cd;&#x9988;&#x673a;&#x5236;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x5f00;&#x653e;&#x57df;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x201c;&#x770b;&#x4e24;&#x6b21;&#x5e76;&#x53cd;&#x5411;&#x601d;&#x8003;&#x201d;&#xff0c;&#x4ee5;&#x9a8c;&#x8bc1;&#x5176;&#x8f93;&#x51fa;&#x4e0e;&#x8f93;&#x5165;&#x7684;&#x591a;&#x6a21;&#x6001;&#x4e00;&#x81f4;&#x6027;&#xff0c;&#x4ece;&#x800c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#x3002;","children":[],"payload":{"tag":"li","lines":"1700,1701"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x5f00;&#x653e;&#x57df;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x4e2d;&#x53d6;&#x5f97;&#x4e86;&#x663e;&#x8457;&#x8fdb;&#x5c55;&#xff0c;&#x4f46;&#x4ecd;&#x7136;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x8f93;&#x51fa;&#x4e0e;&#x8f93;&#x5165;&#x8bed;&#x4e49;&#x77db;&#x76fe;&#x6216;&#x5931;&#x5b9e;&#x7684;&#x5185;&#x5bb9;&#xff09;&#x3002;&#x8fd9;&#x4e9b;&#x5e7b;&#x89c9;&#x5728;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x3001;&#x5173;&#x7cfb;&#x548c;&#x4e8b;&#x4ef6;&#x7b49;&#x591a;&#x4e2a;&#x7ef4;&#x5ea6;&#x4e0a;&#x5747;&#x53ef;&#x80fd;&#x51fa;&#x73b0;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x51b3;&#x7b56;&#x3001;&#x673a;&#x5668;&#x4eba;&#x611f;&#x77e5;&#x7b49;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b89;&#x5168;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x4e8e;&#x5916;&#x90e8;&#x9a8c;&#x8bc1;&#x6216;&#x4e8b;&#x540e;&#x7ea0;&#x6b63;&#xff0c;&#x7f3a;&#x4e4f;&#x5728;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x76f4;&#x63a5;&#x9a8c;&#x8bc1;&#x8f93;&#x51fa;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x5185;&#x90e8;&#x673a;&#x5236;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x8feb;&#x5207;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x80fd;&#x591f;&#x4ece;&#x5185;&#x90e8;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x591a;&#x6a21;&#x6001;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x8bad;&#x7ec3;&#x6846;&#x67b6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1702,1703"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;ReLoop&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x4e00;&#x4e2a;&#x73af;&#x72b6;&#x7ed3;&#x6784;&#x7684;&#x95ed;&#x73af;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#xff0c;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x4e92;&#x8865;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x53cd;&#x9988;&#x673a;&#x5236;&#xff1a;","children":[],"payload":{"tag":"li","lines":"1703,1704"}}],"payload":{"tag":"li","lines":"1701,1704","fold":1}}],"payload":{"tag":"ul","lines":"1700,1704"}},{"content":"","children":[{"content":"1. <strong>&#x8bed;&#x4e49;&#x91cd;&#x5efa;&#xff08;CFP-Lang&#xff09;</strong>&#xff1a;&#x4f7f;&#x7528;&#x4e00;&#x4e2a;&#x51bb;&#x7ed3;&#x7684;&#x9884;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x6839;&#x636e;&#x6a21;&#x578b;&#x751f;&#x6210;&#x7684;&#x7b54;&#x6848;A&#x548c;&#x56fe;&#x50cf;I&#xff0c;&#x91cd;&#x5efa;&#x51fa;&#x95ee;&#x9898;Q&#x7684;&#x5019;&#x9009;&#x96c6;&#x5408;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x4e0e;&#x539f;&#x59cb;&#x95ee;&#x9898;Q&#x7684;&#x8bed;&#x4e49;&#x76f8;&#x4f3c;&#x5ea6;&#x6765;&#x76d1;&#x7763;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"1704,1705","listIndex":1}},{"content":"2. <strong>&#x89c6;&#x89c9;&#x63cf;&#x8ff0;&#xff08;CFP-Vis&#xff09;</strong>&#xff1a;&#x4f7f;&#x7528;&#x53e6;&#x4e00;&#x4e2a;&#x51bb;&#x7ed3;&#x7684;&#x9884;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x6839;&#x636e;&#x7b54;&#x6848;A&#x548c;&#x56fe;&#x50cf;I&#x751f;&#x6210;&#x4e00;&#x4e2a;&#x89c6;&#x89c9;&#x63cf;&#x8ff0;I*&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x8ba1;&#x7b97;&#x5176;&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;I&#x7684;&#x76f8;&#x4f3c;&#x5ea6;&#x6765;&#x8bc4;&#x4f30;&#x7b54;&#x6848;&#x7684;&#x4e8b;&#x5b9e; groundedness&#xff08;&#x4e8b;&#x5b9e;&#x4f9d;&#x636e;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1705,1706","listIndex":2}},{"content":"3. <strong>&#x6ce8;&#x610f;&#x529b;&#x5bf9;&#x9f50;</strong>&#xff1a;&#x63d0;&#x53d6;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x7b54;&#x6848;&#x65f6;&#x7684;token-to-image&#x6ce8;&#x610f;&#x529b;&#x56fe;H&#xff0c;&#x5e76;&#x5c06;&#x5176;&#x4e0e;&#x4e00;&#x4e2a;&#x57fa;&#x4e8e;&#x71b5;&#x7684;&#x4f2a;&#x771f;&#x503c;&#x6ce8;&#x610f;&#x529b;&#x56fe;H_pseudo&#x8fdb;&#x884c;&#x6bd4;&#x8f83;&#xff0c;&#x4ee5;&#x76d1;&#x7763;&#x5176;&#x5173;&#x6ce8;&#x6b63;&#x786e;&#x7684;&#x56fe;&#x50cf;&#x533a;&#x57df;&#x3002;<br>\n&#x8fd9;&#x4e9b;&#x53cd;&#x9988;&#x4fe1;&#x53f7;&#x88ab;&#x6574;&#x5408;&#x4e3a;&#x53ef;&#x5fae;&#x5206;&#x7684;&#x635f;&#x5931;&#x51fd;&#x6570;&#xff08;L_align, L_vis, L_attn&#xff09;&#xff0c;&#x7528;&#x4e8e;&#x5728;&#x8bad;&#x7ec3;&#x8fc7;&#x7a0b;&#x4e2d;&#x901a;&#x8fc7;&#x53cd;&#x5411;&#x4f20;&#x64ad;&#x66f4;&#x65b0;&#x4e3b;&#x6a21;&#x578b;M&#x3002;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x6a21;&#x62df;&#x4e86;&#x4eba;&#x7c7b;&#x7684;&#x201c;&#x53cd;&#x5411;&#x601d;&#x8003;&#x201d;&#x8ba4;&#x77e5;&#x8fc7;&#x7a0b;&#xff1a;&#x5148;&#x6839;&#x636e;(Q, I)&#x751f;&#x6210;&#x7b54;&#x6848;A&#xff0c;&#x518d;&#x6839;&#x636e;(A, I)&#x53cd;&#x5411;&#x9a8c;&#x8bc1;&#x751f;&#x6210;&#x7684;Q*&#x3001;I*&#x548c;&#x6ce8;&#x610f;&#x529b;&#x56fe;&#x662f;&#x5426;&#x4e0e;&#x539f;&#x59cb;&#x8f93;&#x5165;Q&#x3001;I&#x548c;&#x4f2a;&#x771f;&#x503c;&#x6ce8;&#x610f;&#x529b;&#x4e00;&#x81f4;&#x3002;","children":[{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5e7f;&#x6cdb;&#x7684;&#x8bc4;&#x4f30;&#x548c;&#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;ReLoop&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x6709;&#x6548;&#x964d;&#x4f4e;&#x4e86;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x7387;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x5185;&#x90e8;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x76d1;&#x7763;&#x673a;&#x5236;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x5bf9;&#x8c61;&#x3001;&#x5c5e;&#x6027;&#x3001;&#x5173;&#x7cfb;&#x548c;&#x4e8b;&#x4ef6;&#x7b49;&#x5404;&#x7c7b;&#x522b;&#x4e0a;&#x7684;&#x5e7b;&#x89c9;&#x8f93;&#x51fa;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x53ef;&#x9760;&#x6027;&#x548c;&#x4e8b;&#x5b9e;&#x4e00;&#x81f4;&#x6027;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1708,1709"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ReLoop&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x4e00;&#x4e2a;&#x95ed;&#x73af;&#x8bad;&#x7ec3;&#x6846;&#x67b6;&#xff0c;&#x6210;&#x529f;&#x5730;&#x5c06;&#x4eba;&#x7c7b;&#x7684;&#x53cd;&#x5411;&#x9a8c;&#x8bc1;&#x8ba4;&#x77e5;&#x8fc7;&#x7a0b;&#x878d;&#x5165;&#x4e86;MLLMs&#x7684;&#x8bad;&#x7ec3;&#x4e2d;&#xff0c;&#x4e3a;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x5f3a;&#x5927;&#x800c;&#x65b0;&#x9896;&#x7684;&#x5185;&#x90e8;&#x76d1;&#x7763;&#x673a;&#x5236;&#x3002;&#x5176;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x8bad;&#x7ec3;&#x4e2d;&#x5f3a;&#x5236;&#x8fdb;&#x884c;&#x591a;&#x6a21;&#x6001;&#x4e00;&#x81f4;&#x6027;&#x68c0;&#x67e5;&#xff0c;&#x53ef;&#x4ee5;&#x663e;&#x8457;&#x63d0;&#x9ad8;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x6df1;&#x8fdc;&#xff0c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x5b89;&#x5168;&#x3001;&#x66f4;&#x53ef;&#x4fe1;&#x7684;&#x591a;&#x6a21;&#x6001;AI&#x7cfb;&#x7edf;&#x5960;&#x5b9a;&#x4e86;&#x57fa;&#x7840;&#xff0c;&#x5e76;&#x53ef;&#x80fd;&#x63a8;&#x52a8;&#x5176;&#x5728;&#x533b;&#x7597;&#x3001;&#x673a;&#x5668;&#x4eba;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x66f4;&#x5e7f;&#x6cdb;&#x5e94;&#x7528;&#x3002;&#x8bba;&#x6587;&#x4ee3;&#x7801;&#x5df2;&#x5f00;&#x6e90;&#x3002;","children":[],"payload":{"tag":"li","lines":"1709,1711"}}],"payload":{"tag":"li","lines":"1706,1711","listIndex":3}}],"payload":{"tag":"ol","lines":"1704,1711"}}],"payload":{"tag":"h4","lines":"1699,1700"}},{"content":"HallE-Switch: Controlling Object Hallucination in Large Vision Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x9488;&#x5bf9;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5728;&#x8be6;&#x7ec6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4e2d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x65b0;&#x7684;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;CCEval&#xff0c;&#x5206;&#x6790;&#x4e86;&#x5e7b;&#x89c9;&#x6210;&#x56e0;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x4e86;&#x53ef;&#x63a7;&#x6a21;&#x578b;HallE-Control&#xff0c;&#x80fd;&#x901a;&#x8fc7;&#x5355;&#x4e00;&#x53c2;&#x6570;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x63a7;&#x5236;&#x5e7b;&#x89c9;&#x7a0b;&#x5ea6;&#xff0c;&#x5c06;&#x5e7b;&#x89c9;&#x964d;&#x4f4e;&#x4e86;44%&#x3002;","children":[],"payload":{"tag":"li","lines":"1712,1713"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5728;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x65f6;&#x4ea7;&#x751f;&#x7684;&#x2018;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x63cf;&#x8ff0;&#x4e86;&#x56fe;&#x50cf;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x975e;&#x5e38;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5b83;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LMMs&#x5728;&#x673a;&#x5668;&#x4eba;&#x3001;&#x89c6;&#x89c9;&#x641c;&#x7d22;&#x7b49;&#x4e0b;&#x6e38;&#x5e94;&#x7528;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x4e8e;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x7684;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#xff08;&#x5982;POPE&#xff09;&#x65e0;&#x6cd5;&#x5168;&#x9762;&#x8bc4;&#x4f30;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x80fd;&#x529b;&#x88ab;&#x9ad8;&#x4f30;&#x3002;","children":[],"payload":{"tag":"li","lines":"1714,1715"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x591a;&#x65b9;&#x9762;&#x7684;&#x5206;&#x6790;&#x6765;&#x63a2;&#x7a76;&#x5e7b;&#x89c9;&#x6210;&#x56e0;&#xff0c;&#x5305;&#x62ec;&#x56fe;&#x50cf;&#x5206;&#x8fa8;&#x7387;&#x3001;&#x8bed;&#x8a00;&#x89e3;&#x7801;&#x5668;&#x5927;&#x5c0f;&#x3001;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x7684;&#x6570;&#x91cf;&#x3001;&#x8d28;&#x91cf;&#x548c;&#x7c92;&#x5ea6;&#x3002;&#x5173;&#x952e;&#x53d1;&#x73b0;&#x662f;&#xff0c;&#x5f53;&#x8bed;&#x8a00;&#x63cf;&#x8ff0;&#x7684;&#x7269;&#x4f53;&#x7c92;&#x5ea6;&#x6bd4;&#x89c6;&#x89c9;&#x6a21;&#x5757;&#x80fd;&#x8bc6;&#x522b;&#x6216;&#x9a8c;&#x8bc1;&#x7684;&#x66f4;&#x7cbe;&#x7ec6;&#x65f6;&#xff0c;&#x5c31;&#x4f1a;&#x8bf1;&#x53d1;&#x5e7b;&#x89c9;&#x3002;&#x4e3a;&#x89e3;&#x51b3;&#x6b64;&#x95ee;&#x9898;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;HallE-Control&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5c06;&#x63cf;&#x8ff0;&#x53ef;&#x9760;&#x6027;&#x5f52;&#x56e0;&#x4e8e;&#x4e24;&#x79cd;&#x77e5;&#x8bc6;&#xff1a;1. &#x4e0a;&#x4e0b;&#x6587;&#x77e5;&#x8bc6;&#xff08;&#x4ec5;&#x5305;&#x542b;&#x89c6;&#x89c9;&#x6a21;&#x5757;&#x5df2;&#x63a5;&#x5730;&#x7684;&#x7269;&#x4f53;&#xff09;&#xff1b;2. &#x53c2;&#x6570;&#x77e5;&#x8bc6;&#xff08;&#x5305;&#x542b;&#x6a21;&#x578b;&#x63a8;&#x65ad;&#x51fa;&#x7684;&#x7269;&#x4f53;&#xff09;&#x3002;&#x4f5c;&#x8005;&#x7b56;&#x5212;&#x4e86;&#x4e00;&#x4e2a;&#x5305;&#x542b;3.3&#x4e07;&#x4e2a;&#x6837;&#x672c;&#x7684;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5176;&#x4e2d;&#x65e2;&#x5305;&#x542b;&#x7eaf;&#x4e0a;&#x4e0b;&#x6587;&#x77e5;&#x8bc6;&#xff0c;&#x4e5f;&#x5305;&#x542b;&#x6807;&#x8bb0;&#x4e86;&#x53c2;&#x6570;&#x77e5;&#x8bc6;&#x7684;&#x6df7;&#x5408;&#x77e5;&#x8bc6;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#xff0c;&#x4ed6;&#x4eec;&#x5728;&#x51bb;&#x7ed3;&#x7684;LMM&#x4e0a;&#x8bad;&#x7ec3;&#x4e86;&#x4e00;&#x4e2a;&#x8f7b;&#x91cf;&#x7ea7;&#x7684;&#x5355;&#x7ebf;&#x6027;&#x5c42;&#x4f5c;&#x4e3a;&#x63a7;&#x5236;&#x5668;&#x3002;&#x5728;&#x63a8;&#x7406;&#x65f6;&#xff0c;&#x901a;&#x8fc7;&#x4e00;&#x4e2a;&#x8fde;&#x7eed;&#x7684;&#x63a7;&#x5236;&#x5668;&#x53c2;&#x6570;&#xff08;&#x5982;&#x4ece;-1&#x5230;+1&#xff09;&#x6765;&#x63a7;&#x5236;&#x8f93;&#x51fa;&#xff1a;-1&#x65f6;&#x53ea;&#x8f93;&#x51fa;&#x63a5;&#x5730;&#x7684;&#x4e0a;&#x4e0b;&#x6587;&#x77e5;&#x8bc6;&#xff0c;+1&#x65f6;&#x5219;&#x6df7;&#x5408;&#x53c2;&#x6570;&#x77e5;&#x8bc6;&#xff0c;&#x5e76;&#x7528;&#x7279;&#x6b8a;&#x6807;&#x8bb0;&#xff08;&#x5982;[object]&#xff09;&#x9ad8;&#x4eae;&#x63a8;&#x65ad;&#x51fa;&#x7684;&#x7269;&#x4f53;&#x3002;","children":[],"payload":{"tag":"li","lines":"1715,1716"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7684;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1. &#x65b0;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;CCEval&#x63ed;&#x793a;&#x51fa;&#xff0c;&#x5373;&#x4f7f;&#x5728;VQA&#x57fa;&#x51c6;&#xff08;&#x5982;POPE&#xff09;&#x4e0a;&#x8868;&#x73b0;&#x826f;&#x597d;&#x7684;&#x6a21;&#x578b;&#xff0c;&#x5728;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#x4ecd;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#x3002;2. &#x5206;&#x6790;&#x8868;&#x660e;&#xff0c;&#x5e7b;&#x89c9;&#x7684;&#x4e3b;&#x8981;&#x6210;&#x56e0;&#x662f;&#x8bad;&#x7ec3;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x4e0e;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x80fd;&#x611f;&#x77e5;&#x7684;&#x7269;&#x4f53;&#x4e4b;&#x95f4;&#x7684;&#x9519;&#x4f4d;&#xff0c;&#x800c;&#x975e;&#x8bed;&#x8a00;&#x89e3;&#x7801;&#x5668;&#x7684;&#x5927;&#x5c0f;&#x3002;3. &#x4e0e;&#x57fa;&#x7ebf;&#x6a21;&#x578b;LLaVA-7B&#x76f8;&#x6bd4;&#xff0c;HallE-Control&#x5c06;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#x964d;&#x4f4e;&#x4e86;44%&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x7269;&#x4f53;&#x7684;&#x8986;&#x76d6;&#x8303;&#x56f4;&#xff08;coverage&#xff09;&#x548c;&#x53e5;&#x5b50;&#x957f;&#x5ea6;&#xff0c;&#x5b9e;&#x73b0;&#x4e86;&#x5bf9;&#x5e7b;&#x89c9;&#x7a0b;&#x5ea6;&#x7684;&#x53ef;&#x63a7;&#x751f;&#x6210;&#x3002;","children":[],"payload":{"tag":"li","lines":"1716,1717"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#x6e90;&#x4e8e;&#x89c6;&#x89c9;&#x4e0e;&#x8bed;&#x8a00;&#x6a21;&#x6001;&#x7684;&#x9519;&#x4f4d;&#xff0c;&#x800c;&#x5e76;&#x975e;&#x6240;&#x6709;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#xff08;&#x63a8;&#x65ad;&#xff09;&#x90fd;&#x662f;&#x6709;&#x5bb3;&#x7684;&#xff0c;&#x66f4;&#x53ef;&#x53d6;&#x7684;&#x65b9;&#x6cd5;&#x662f;&#x63a7;&#x5236;&#x800c;&#x975e;&#x5b8c;&#x5168;&#x6d88;&#x9664;&#x3002;HallE-Control&#x9996;&#x6b21;&#x5b9e;&#x73b0;&#x4e86;&#x5bf9;LMM&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x4e2d;&#x5e7b;&#x89c9;&#x7a0b;&#x5ea6;&#x7684;&#x53ef;&#x63a7;&#x8c03;&#x8282;&#xff0c;&#x5728;&#x51cf;&#x5c11;&#x6709;&#x5bb3;&#x5e7b;&#x89c9;&#x7684;&#x540c;&#x65f6;&#x4fdd;&#x7559;&#x4e86;&#x6709;&#x7528;&#x7684;&#x7ec6;&#x8282;&#x63a8;&#x65ad;&#x80fd;&#x529b;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x53ef;&#x63a7;&#x7684;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x7684;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#x548c;&#x6846;&#x67b6;&#xff0c;&#x5bf9;&#x63a8;&#x52a8;LMM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1717,1719"}}],"payload":{"tag":"li","lines":"1713,1719","fold":1}}],"payload":{"tag":"h4","lines":"1711,1712"}},{"content":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: ViGoR&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x5956;&#x52b1;&#x5efa;&#x6a21;&#x6765;&#x589e;&#x5f3a;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x89c6;&#x89c9; grounding &#x80fd;&#x529b;&#x7684;&#x65b0;&#x6846;&#x67b6;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x548c;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#xff0c;&#x4ec5;&#x9700;&#x5c11;&#x91cf;&#x4eba;&#x7c7b;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x5373;&#x53ef;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1720,1721"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x7684;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x89c6;&#x89c9; grounding &#x4e0d;&#x51c6;&#x786e;&#x7684;&#x95ee;&#x9898;&#xff0c;&#x4f8b;&#x5982;&#x5e7b;&#x89c9;&#xff08;&#x63cf;&#x8ff0;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#xff09;&#x3001;&#x9057;&#x6f0f;&#x91cd;&#x8981;&#x573a;&#x666f;&#x5143;&#x7d20;&#x3001;&#x4ee5;&#x53ca;&#x9519;&#x8bef;&#x63a8;&#x65ad;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x548c;&#x5173;&#x7cfb;&#x3002;&#x8fd9;&#x4e9b;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x591a;&#x6a21;&#x6001;&#x8bad;&#x7ec3;&#x6570;&#x636e;&#x7684;&#x7a00;&#x7f3a;&#x6027;&#x548c;&#x590d;&#x6742;&#x6027;&#xff0c;&#x4ee5;&#x53ca;&#x73b0;&#x6709;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x6570;&#x636e;&#x65b9;&#x6cd5;&#x7684;&#x5c40;&#x9650;&#x6027;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x4e8e;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1722,1723"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;ViGoR&#x6846;&#x67b6;&#xff0c;&#x91c7;&#x7528;&#x7ec6;&#x7c92;&#x5ea6;&#x5956;&#x52b1;&#x5efa;&#x6a21;&#x6765;&#x6539;&#x8fdb;LVLM&#x3002;&#x65b9;&#x6cd5;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x4ece;&#x9884;&#x8bad;&#x7ec3;LVLM&#xff08;&#x5982;LLaVA&#xff09;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x56fe;&#x50cf;-&#x6587;&#x672c;&#x8f93;&#x51fa;&#xff1b;2&#xff09;&#x8ba9;&#x4eba;&#x7c7b;&#x6807;&#x6ce8;&#x8005;&#x5bf9;&#x6bcf;&#x4e2a;&#x53e5;&#x5b50;&#x8fdb;&#x884c;&#x7ec6;&#x7c92;&#x5ea6;&#x8bc4;&#x4f30;&#xff0c;&#x6807;&#x6ce8;&#x9519;&#x8bef;&#x7c7b;&#x578b;&#xff08;&#x5982;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3001;&#x5c5e;&#x6027;&#x9519;&#x8bef;&#x7b49;&#xff09;&#x548c;&#x521b;&#x9020;&#x6027;&#xff0c;&#x6784;&#x5efa;&#x5305;&#x542b;15.4K&#x6837;&#x672c;&#x7684;&#x6570;&#x636e;&#x96c6;&#xff1b;3&#xff09;&#x8bad;&#x7ec3;&#x4e00;&#x4e2a;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x6765;&#x9884;&#x6d4b;&#x5bc6;&#x96c6;&#x5956;&#x52b1;&#x5206;&#x6570;&#xff1b;4&#xff09;&#x7ed3;&#x5408;&#x81ea;&#x52a8;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5f00;&#x653e;&#x96c6;&#x76ee;&#x6807;&#x68c0;&#x6d4b;&#x5668;&#xff09;&#x9a8c;&#x8bc1;&#x751f;&#x6210;&#x6587;&#x672c;&#x4e2d;&#x540d;&#x8bcd;&#x5b9e;&#x4f53;&#x7684;&#x5b58;&#x5728;&#x6027;&#xff0c;&#x751f;&#x6210;&#x81ea;&#x52a8;&#x5316;&#x5956;&#x52b1;&#x4fe1;&#x53f7;&#xff1b;5&#xff09;&#x5c06;&#x4eba;&#x7c7b;&#x548c;&#x81ea;&#x52a8;&#x5316;&#x5956;&#x52b1;&#x4fe1;&#x53f7;&#x7ed3;&#x5408;&#xff0c;&#x901a;&#x8fc7;&#x62d2;&#x7edd;&#x91c7;&#x6837;&#x9009;&#x62e9;&#x6700;&#x4f73;&#x63cf;&#x8ff0;&#xff0c;&#x7528;&#x4e8e;&#x76d1;&#x7763;&#x5fae;&#x8c03;LVLM&#x3002;","children":[],"payload":{"tag":"li","lines":"1723,1724"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: ViGoR&#x5728;POPE&#x548c;MME&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA&#xff09;&#xff0c;&#x5c55;&#x793a;&#x4e86;&#x66f4;&#x597d;&#x7684;&#x89c6;&#x89c9; grounding &#x80fd;&#x529b;&#x3002;&#x4ec5;&#x4f7f;&#x7528;16K&#x4eba;&#x7c7b;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x5c31;&#x5b9e;&#x73b0;&#x4e86;&#x5927;&#x5e45;&#x63d0;&#x5347;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x65b9;&#x6cd5;&#x7684;&#x9ad8;&#x6548;&#x6027;&#x3002;&#x81ea;&#x52a8;&#x5316;&#x5956;&#x52b1;&#x6a21;&#x578b;&#x4e5f;&#x5728;&#x65e0;&#x9700;&#x989d;&#x5916;&#x4eba;&#x7c7b;&#x52aa;&#x529b;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1724,1725"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ViGoR&#x901a;&#x8fc7;&#x7ec6;&#x7c92;&#x5ea6;&#x5956;&#x52b1;&#x5efa;&#x6a21;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;LVLM&#x7684;&#x89c6;&#x89c9; grounding &#x80fd;&#x529b;&#xff0c;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x548c;&#x9519;&#x8bef;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x63a8;&#x7406;&#x548c;&#x521b;&#x9020;&#x6027;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x901a;&#x7528;&#x6027;&#x5f3a;&#xff0c;&#x53ef;&#x7528;&#x4e8e;&#x4efb;&#x4f55;LVLM&#xff0c;&#x4e14;&#x9ad8;&#x6548;&#x5229;&#x7528;&#x4eba;&#x7c7b;&#x8bc4;&#x4f30;&#x548c;&#x81ea;&#x52a8;&#x5316;&#x65b9;&#x6cd5;&#x3002;&#x53d1;&#x5e03;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x8bc4;&#x4f30;&#x6570;&#x636e;&#x96c6;&#xff08;15.4K&#x6837;&#x672c;&#xff09;&#x5c06;&#x4fc3;&#x8fdb;&#x76f8;&#x5173;&#x7814;&#x7a76;&#x3002;&#x5de5;&#x4f5c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x8bbe;&#x7acb;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1725,1740"}}],"payload":{"tag":"li","lines":"1721,1740","fold":1}}],"payload":{"tag":"h4","lines":"1719,1720"}},{"content":"DVP: What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x2018;&#x53cd;&#x4e8b;&#x5b9e;&#x690d;&#x5165;&#x2019;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x8ba9;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x81ea;&#x6211;&#x751f;&#x6210;&#x5e76;&#x601d;&#x8003;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x76f8;&#x53cd;&#x7684;&#x53cd;&#x4e8b;&#x5b9e;&#x5173;&#x952e;&#x8bcd;&#xff0c;&#x6765;&#x51cf;&#x5c11;&#x5176;&#x56de;&#x7b54;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff08;&#x5373;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x63cf;&#x8ff0;&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x4e86;&#x57fa;&#x4e8e;CLIP&#x5206;&#x6570;&#x7684;&#x5408;&#x7406;&#x6027;&#x9a8c;&#x8bc1;&#x8fc7;&#x7a0b;&#xff08;PVP&#xff09;&#x6765;&#x7b5b;&#x9009;&#x6700;&#x4f18;&#x5173;&#x952e;&#x8bcd;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x5f00;&#x6e90;&#x548c;&#x4e13;&#x6709;&#x6a21;&#x578b;&#x4e0a;&#x9a8c;&#x8bc1;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1741,1742"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x8bd5;&#x56fe;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#xff08;LMMs&#xff09;&#x5728;&#x751f;&#x6210;&#x56de;&#x7b54;&#x65f6;&#x51fa;&#x73b0;&#x7684;&#x2018;&#x5e7b;&#x89c9;&#x2019;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x4f1a;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x3001;&#x9519;&#x8bef;&#x6216;&#x65e0;&#x610f;&#x4e49;&#x7684;&#x63cf;&#x8ff0;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x5f88;&#x91cd;&#x8981;&#xff0c;&#x56e0;&#x4e3a;&#x5e7b;&#x89c9;&#x4f1a;&#x4e25;&#x91cd;&#x635f;&#x5bb3;LMMs&#x5728;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x3001;&#x63a8;&#x7406;&#x7b49;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x5176;&#x4f5c;&#x4e3a;&#x901a;&#x7528;AI&#x7cfb;&#x7edf;&#x7684;&#x5b9e;&#x7528;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x901a;&#x5e38;&#x9700;&#x8981;&#x989d;&#x5916;&#x7684;&#x8bad;&#x7ec3;&#x6216;&#x5927;&#x91cf;&#x4eba;&#x5de5;&#x8d44;&#x6e90;&#xff0c;&#x800c;&#x672c;&#x5de5;&#x4f5c;&#x65e8;&#x5728;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x7f13;&#x89e3;&#x8be5;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1743,1744"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x2018;&#x53cd;&#x4e8b;&#x5b9e;&#x601d;&#x7ef4;&#x2019;&#x8fd9;&#x4e00;&#x4eba;&#x7c7b;&#x8ba4;&#x77e5;&#x673a;&#x5236;&#x6765;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x3002;&#x5177;&#x4f53;&#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x4e24;&#x6b65;&#xff1a;1. <strong>&#x53cd;&#x4e8b;&#x5b9e;&#x5173;&#x952e;&#x8bcd;&#x751f;&#x6210;</strong>&#xff1a;&#x8bbe;&#x8ba1;&#x63d0;&#x793a;&#x8bcd;&#xff0c;&#x5f15;&#x5bfc;LMM&#x9488;&#x5bf9;&#x7ed9;&#x5b9a;&#x56fe;&#x50cf;&#xff0c;&#x5728;&#x7269;&#x4f53;&#xff08;Object&#xff09;&#x3001;&#x5c5e;&#x6027;&#xff08;Attribute&#xff09;&#x548c;&#x5173;&#x7cfb;&#xff08;Relation&#xff09;&#x4e09;&#x4e2a;&#x5c42;&#x6b21;&#x4e0a;&#xff0c;&#x81ea;&#x6211;&#x751f;&#x6210;&#x4e00;&#x7cfb;&#x5217;&#x4e0e;&#x56fe;&#x50cf;&#x771f;&#x5b9e;&#x5185;&#x5bb9;&#x76f8;&#x53cd;&#x4f46;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x7684;&#x53cd;&#x4e8b;&#x5b9e;&#x5173;&#x952e;&#x8bcd;&#xff08;&#x4f8b;&#x5982;&#xff0c;&#x5c06;&#x2018;&#x732b;&#x2019;&#x66ff;&#x6362;&#x4e3a;&#x2018;&#x72d7;&#x2019;&#xff0c;&#x5c06;&#x2018;&#x7eff;&#x8272;&#x9999;&#x8549;&#x2019;&#x6539;&#x4e3a;&#x2018;&#x9ec4;&#x8272;&#x9999;&#x8549;&#x2019;&#xff09;&#x3002;2. <strong>&#x53cd;&#x4e8b;&#x5b9e;&#x690d;&#x5165;&#x4e0e;&#x5408;&#x7406;&#x6027;&#x9a8c;&#x8bc1;&#xff08;PVP&#xff09;</strong>&#xff1a;&#x5c06;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x8bcd;&#x4e0e;&#x7528;&#x6237;&#x67e5;&#x8be2;&#x4e00;&#x8d77;&#x8f93;&#x5165;&#x6a21;&#x578b;&#xff0c;&#x5e76;&#x9644;&#x52a0;&#x4e00;&#x4e2a;&#x53cd;&#x4e8b;&#x5b9e;&#x63d0;&#x793a;&#xff0c;&#x8981;&#x6c42;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x56de;&#x7b54;&#x65f6;&#x2018;&#x8c28;&#x614e;&#x907f;&#x514d;&#x2019;&#x4f7f;&#x7528;&#x8fd9;&#x4e9b;&#x5173;&#x952e;&#x8bcd;&#x3002;&#x4e3a;&#x786e;&#x4fdd;&#x5173;&#x952e;&#x8bcd;&#x8d28;&#x91cf;&#xff0c;&#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;<strong>&#x5408;&#x7406;&#x6027;&#x9a8c;&#x8bc1;&#x8fc7;&#x7a0b;&#xff08;PVP&#xff09;</strong>&#xff0c;&#x5229;&#x7528;CLIP&#x6a21;&#x578b;&#x8ba1;&#x7b97;&#x56fe;&#x50cf;&#x4e0e;&#x5173;&#x952e;&#x8bcd;&#x7684;&#x76f8;&#x4f3c;&#x5ea6;&#xff08;CLIPscore&#xff09;&#xff0c;&#x5e76;&#x8bbe;&#x7f6e;&#x7ea6;&#x675f;&#x533a;&#x95f4;&#xff08;&#x5982;0.11-0.18&#xff09;&#x6765;&#x8fc7;&#x6ee4;&#x6389;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x8fc7;&#x4e8e;&#x76f8;&#x4f3c;&#x6216;&#x5b8c;&#x5168;&#x65e0;&#x5173;&#x7684;&#x6b21;&#x4f18;&#x5173;&#x952e;&#x8bcd;&#xff0c;&#x4ece;&#x800c;&#x786e;&#x4fdd;&#x53cd;&#x4e8b;&#x5b9e;&#x601d;&#x7ef4;&#x80fd;&#x88ab;&#x6709;&#x6548;&#x89e6;&#x53d1;&#x3002;","children":[],"payload":{"tag":"li","lines":"1744,1745"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1. &#x5728;&#x5305;&#x62ec;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA&#x7cfb;&#x5217;&#xff09;&#x548c;&#x4e13;&#x6709;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4o&#x3001;Gemini&#xff09;&#x5728;&#x5185;&#x7684;&#x591a;&#x79cd;LMMs&#x4e0a;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;2. &#x901a;&#x8fc7;&#x53cd;&#x4e8b;&#x5b9e;&#x601d;&#x7ef4;&#xff0c;&#x6a21;&#x578b;&#x80fd;&#x591f;&#x57fa;&#x4e8e;&#x771f;&#x5b9e;&#x7684;&#x89c6;&#x89c9;&#x7ebf;&#x7d22;&#xff0c;&#x8fdb;&#x884c;&#x66f4;&#x5e7f;&#x6cdb;&#x7684;&#x4e0a;&#x4e0b;&#x6587;&#x63a2;&#x7d22;&#xff0c;&#x751f;&#x6210;&#x66f4;&#x51c6;&#x786e;&#x548c;&#x53ef;&#x9760;&#x7684;&#x56de;&#x7b54;&#x3002;3. &#x6240;&#x63d0;&#x51fa;&#x7684;PVP&#x7ea6;&#x675f;&#x6709;&#x6548;&#x5730;&#x7b5b;&#x9009;&#x4e86;&#x5173;&#x952e;&#x8bcd;&#xff0c;&#x786e;&#x4fdd;&#x4e86;&#x65b9;&#x6cd5;&#x7684;&#x4e00;&#x81f4;&#x6027;&#x548c;&#x9c81;&#x68d2;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1745,1746"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x690d;&#x5165;&#x53cd;&#x4e8b;&#x5b9e;&#x601d;&#x7ef4;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x5730;&#x3001;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x5730;&#x51cf;&#x8f7b;LMMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5e76;&#x589e;&#x5f3a;&#x5176;&#x4e0a;&#x4e0b;&#x6587;&#x7406;&#x89e3;&#x80fd;&#x529b;&#x3002;&#x5176;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#xff1a;&#x4e3a;&#x63d0;&#x5347;LMM&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x65b0;&#x9896;&#x3001;&#x4f4e;&#x6210;&#x672c;&#x4e14;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#x7684;&#x63a8;&#x7406;&#x9636;&#x6bb5;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x53c2;&#x6570;&#x6216;&#x8fdb;&#x884c;&#x6570;&#x636e;&#x6807;&#x6ce8;&#xff0c;&#x6709;&#x671b;&#x63a8;&#x52a8;LMMs&#x5728;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x7406;&#x89e3;&#x7684;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#xff09;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1746,1748"}}],"payload":{"tag":"li","lines":"1742,1748","fold":1}}],"payload":{"tag":"h4","lines":"1740,1741"}},{"content":"M3ID: Multi-Modal Hallucination Control by Visual Information Grounding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;M3ID&#x7684;&#x65b0;&#x91c7;&#x6837;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x4fe1;&#x606f; grounding &#x6765;&#x63a7;&#x5236;&#x591a;&#x6a21;&#x6001;&#x5927;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x5373;&#x53ef;&#x5e94;&#x7528;&#xff0c;&#x80fd;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x6587;&#x672c;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x8bed;&#x8a00;&#x6d41;&#x7545;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1749,1750"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x751f;&#x6210;&#x5f0f;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x672a;&#x57fa;&#x4e8e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x7684;&#x6587;&#x672c;&#xff0c;&#x5373;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#x73b0;&#x8c61;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x800c;&#x5ffd;&#x89c6;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#xff0c;&#x5bfc;&#x81f4;&#x751f;&#x6210;&#x4e0d;&#x63a5;&#x5730;&#x6c14;&#x7684;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x548c;&#x5b9e;&#x7528;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1751,1752"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x591a;&#x6a21;&#x6001;&#x4e92;&#x4fe1;&#x606f;&#x89e3;&#x7801;&#xff08;M3ID&#xff09;&#xff0c;&#x4e00;&#x79cd;&#x63a8;&#x7406;&#x65f6;&#x7684;&#x91c7;&#x6837;&#x65b9;&#x6cd5;&#x3002;M3ID&#x901a;&#x8fc7;&#x653e;&#x5927;&#x56fe;&#x50cf;&#x6761;&#x4ef6;&#x5206;&#x5e03;&#x4e0e;&#x65e0;&#x6761;&#x4ef6;&#x5206;&#x5e03;&#x4e4b;&#x95f4;&#x7684;&#x5dee;&#x5f02;&#xff0c;&#x9009;&#x62e9;&#x4e0e;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x4e92;&#x4fe1;&#x606f;&#x66f4;&#x9ad8;&#x7684; token&#x3002;&#x5177;&#x4f53;&#x64cd;&#x4f5c;&#x662f;&#x52a8;&#x6001;&#x8c03;&#x6574;&#x6743;&#x91cd;&#x7cfb;&#x6570;&#x3b3;_t&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x5728;&#x751f;&#x6210;&#x8fc7;&#x7a0b;&#x4e2d;&#x66f4;&#x4f9d;&#x8d56;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x8fd8;&#x53ef;&#x7ed3;&#x5408;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x8fdb;&#x884c;&#x8bad;&#x7ec3;&#xff0c;&#x8fdb;&#x4e00;&#x6b65;&#x63d0;&#x5347;&#x89c6;&#x89c9; grounding &#x6548;&#x679c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1752,1753"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;M3ID&#x548c;M3ID+DPO&#x80fd;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff1a;&#x5728;LLaVA 13B&#x6a21;&#x578b;&#x4e0a;&#xff0c; captioning&#x4efb;&#x52a1;&#x4e2d;&#x5e7b;&#x89c9;&#x7269;&#x4f53;&#x6570;&#x91cf;&#x5206;&#x522b;&#x51cf;&#x5c11;25%&#x548c;28%&#xff1b;&#x5728;VQA&#x57fa;&#x51c6;POPE&#x4e0a;&#x51c6;&#x786e;&#x7387;&#x63d0;&#x5347;21%&#x548c;24%&#x3002;&#x540c;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x7684;&#x8bed;&#x8a00;&#x6d41;&#x7545;&#x6027;&#x672a;&#x53d7;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1753,1754"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x8bc1;&#x660e;&#x4e86;&#x901a;&#x8fc7;&#x4e92;&#x4fe1;&#x606f;&#x6700;&#x5927;&#x5316;&#x53ef;&#x6709;&#x6548;&#x63a7;&#x5236;VLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;M3ID&#x4f5c;&#x4e3a;&#x4e00;&#x79cd;&#x8f7b;&#x91cf;&#x7ea7;&#x63a8;&#x7406;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x9002;&#x7528;&#x6027;&#x3002;&#x7ed3;&#x5408;DPO&#x8bad;&#x7ec3;&#x80fd;&#x8fdb;&#x4e00;&#x6b65;&#x4f18;&#x5316;&#x6548;&#x679c;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x4e3a;&#x63d0;&#x5347;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1754,1758"}}],"payload":{"tag":"li","lines":"1750,1758","fold":1}}],"payload":{"tag":"h4","lines":"1748,1749"}},{"content":"ESREAL: Exploiting Semantic Reconstruction to Mitigate Hallucinations in Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ESREAL&#xff0c;&#x4e00;&#x79cd;&#x65e0;&#x76d1;&#x7763;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x8bed;&#x4e49;&#x91cd;&#x5efa;&#x6765;&#x8bc6;&#x522b;&#x548c;&#x60e9;&#x7f5a;&#x5e7b;&#x89c9;token&#xff0c;&#x4ece;&#x800c;&#x51cf;&#x5c11;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x957f;&#x63cf;&#x8ff0;&#x65f6;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;VLMs&#x4e0a;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1759,1760"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x957f;&#x63cf;&#x8ff0;&#x65f6;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x6216;&#x9519;&#x8bef;&#x7684;&#x63cf;&#x8ff0;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x6807;&#x6ce8;&#x6570;&#x636e;&#xff0c;&#x4e14;&#x4ec5;&#x5728;&#x53e5;&#x5b50;&#x6216;&#x6bb5;&#x843d;&#x7ea7;&#x522b;&#x68c0;&#x6d4b;&#x5e7b;&#x89c9;&#xff0c;&#x5ffd;&#x7565;&#x4e86;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;token&#x7ea7;&#x5e7b;&#x89c9;&#x5b9a;&#x4f4d;&#x548c;&#x7c7b;&#x578b;&#x533a;&#x5206;&#x3002;&#x7531;&#x4e8e;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#x6210;&#x672c;&#x9ad8;&#x6602;&#xff0c;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x6807;&#x6ce8;&#x7684;&#x65e0;&#x76d1;&#x7763;&#x65b9;&#x6cd5;&#x6765;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1761,1762"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: ESREAL&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e09;&#x4e2a;&#x6838;&#x5fc3;&#x6a21;&#x5757;&#xff1a;1&#xff09;&#x8bed;&#x4e49;&#x91cd;&#x5efa;&#x6a21;&#x5757;&#xff1a;&#x4f7f;&#x7528;&#x6587;&#x672c;&#x5230;&#x56fe;&#x50cf;&#x6a21;&#x578b;&#xff08;&#x5982;SDXL Turbo&#xff09;&#x6839;&#x636e;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x91cd;&#x5efa;&#x56fe;&#x50cf;&#xff1b;2&#xff09;&#x5bf9;&#x9f50;&#x6a21;&#x5757;&#xff1a;&#x5229;&#x7528;Grounding DINO&#x5c06;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5bf9;&#x8c61;&#x77ed;&#x8bed;&#x4e0e;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x548c;&#x91cd;&#x5efa;&#x56fe;&#x50cf;&#x7684;&#x5bf9;&#x5e94;&#x533a;&#x57df;&#x5bf9;&#x9f50;&#xff1b;3&#xff09;&#x8bc4;&#x5206;&#x6a21;&#x5757;&#xff1a;&#x6839;&#x636e;&#x5bf9;&#x9f50;&#x533a;&#x57df;&#x7684;&#x8bed;&#x4e49;&#x76f8;&#x4f3c;&#x6027;&#x8ba1;&#x7b97;&#x5e7b;&#x89c9;token&#x7684;&#x60e9;&#x7f5a;&#x5206;&#x6570;&#xff0c;&#x533a;&#x5206;&#x4e09;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff08;&#x4e0d;&#x5b58;&#x5728;&#x5bf9;&#x8c61;&#x3001;&#x4e0d;&#x5fe0;&#x5b9e;&#x5c5e;&#x6027;&#x3001;&#x4e0d;&#x51c6;&#x786e;&#x5173;&#x7cfb;&#xff09;&#x3002;&#x6700;&#x540e;&#xff0c;&#x4f7f;&#x7528;&#x7ec6;&#x7c92;&#x5ea6;&#x8fd1;&#x7aef;&#x7b56;&#x7565;&#x4f18;&#x5316;&#xff08;PPO&#xff09;&#x7b97;&#x6cd5;&#xff0c;&#x6839;&#x636e;token&#x7ea7;&#x5e7b;&#x89c9;&#x5206;&#x6570;&#x5bf9;&#x6a21;&#x578b;&#x8fdb;&#x884c;&#x60e9;&#x7f5a;&#x6027;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1762,1763"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: ESREAL&#x5728;LLaVA&#x3001;InstructBLIP&#x548c;mPLUG-Owl2&#x4e09;&#x4e2a;VLMs&#x4e0a;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5728;CHAIR&#x6307;&#x6807;&#x4e0a;&#x5206;&#x522b;&#x63d0;&#x5347;&#x4e86;32.81%&#x3001;27.08%&#x548c;7.46%&#x3002;&#x540c;&#x65f6;&#xff0c;&#x5728;FaithScore&#x548c;GPT-4V&#x8f85;&#x52a9;&#x8bc4;&#x4f30;&#x7b49;&#x66f4;&#x5168;&#x9762;&#x7684;&#x8bc4;&#x4f30;&#x65b9;&#x6cd5;&#x4e0a;&#x4e5f;&#x8868;&#x73b0;&#x4e00;&#x81f4;&#x63d0;&#x5347;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x5176;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1763,1764"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: ESREAL&#x662f;&#x4e00;&#x79cd;&#x5b8c;&#x5168;&#x65e0;&#x76d1;&#x7763;&#x7684;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x6846;&#x67b6;&#xff0c;&#x65e0;&#x9700;&#x6807;&#x6ce8;&#x6570;&#x636e;&#x5373;&#x53ef;&#x5b9e;&#x73b0;token&#x7ea7;&#x5e7b;&#x89c9;&#x5b9a;&#x4f4d;&#x548c;&#x6291;&#x5236;&#x3002;&#x5176;&#x901a;&#x8fc7;&#x8bed;&#x4e49;&#x91cd;&#x5efa;&#x548c;&#x5bf9;&#x9f50;&#x6280;&#x672f;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff0c;&#x5e76;&#x7ed3;&#x5408;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x8fdb;&#x884c;&#x9488;&#x5bf9;&#x6027;&#x4f18;&#x5316;&#x3002;&#x8be5;&#x6846;&#x67b6;&#x5177;&#x6709;&#x53ef;&#x6269;&#x5c55;&#x6027;&#xff0c;&#x9002;&#x7528;&#x4e8e;&#x591a;&#x79cd;VLMs&#xff0c;&#x4e3a;&#x63d0;&#x9ad8;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x91cd;&#x8981;&#x652f;&#x6301;&#x3002;","children":[],"payload":{"tag":"li","lines":"1764,1766"}}],"payload":{"tag":"li","lines":"1760,1766","fold":1}}],"payload":{"tag":"h4","lines":"1758,1759"}},{"content":"CSR: Calibrated Self-Rewarding Vision Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x6821;&#x51c6;&#x81ea;&#x5956;&#x52b1;&#xff08;CSR&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x7528;&#x4e8e;&#x89e3;&#x51b3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x751f;&#x6210;&#x5019;&#x9009;&#x54cd;&#x5e94;&#x3001;&#x7ed3;&#x5408;&#x89c6;&#x89c9;&#x7ea6;&#x675f;&#x8fdb;&#x884c;&#x5956;&#x52b1;&#x6821;&#x51c6;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x504f;&#x597d;&#x6570;&#x636e;&#x5fae;&#x8c03;&#x6a21;&#x578b;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x5e76;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x5e73;&#x5747;&#x6027;&#x80fd;&#x63d0;&#x5347;7.62%&#x3002;","children":[],"payload":{"tag":"li","lines":"1767,1768"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x51fa;&#x73b0;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x5728;&#x8bed;&#x8a00;&#x4e0a;&#x5408;&#x7406;&#x4f46;&#x4e0e;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x77db;&#x76fe;&#x3002;&#x8fd9;&#x79cd;&#x95ee;&#x9898;&#x6e90;&#x4e8e;&#x6a21;&#x578b;&#x5728;&#x56fe;&#x50cf;&#x548c;&#x6587;&#x672c;&#x6a21;&#x6001;&#x4e4b;&#x95f4;&#x7684;&#x4e0d;&#x5bf9;&#x9f50;&#xff0c;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x800c;&#x5ffd;&#x7565;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x989d;&#x5916;&#x6a21;&#x578b;&#x6216;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x6765;&#x4f18;&#x5316;&#x504f;&#x597d;&#x6570;&#x636e;&#xff0c;&#x4f46;&#x6210;&#x672c;&#x9ad8;&#x6602;&#x4e14;&#x65e0;&#x6cd5;&#x6709;&#x6548;&#x53cd;&#x6620;&#x76ee;&#x6807;LVLM&#x7684;&#x504f;&#x597d;&#x3002;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1769,1770"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x6821;&#x51c6;&#x81ea;&#x5956;&#x52b1;&#xff08;CSR&#xff09;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x8fc7;&#x7a0b;&#x89e3;&#x51b3;&#x6a21;&#x6001;&#x4e0d;&#x5bf9;&#x9f50;&#x95ee;&#x9898;&#x3002;&#x5177;&#x4f53;&#x65b9;&#x6cd5;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x4f7f;&#x7528;&#x53e5;&#x5b50;&#x7ea7;&#x675f;&#x641c;&#x7d22;&#x751f;&#x6210;&#x5019;&#x9009;&#x54cd;&#x5e94;&#xff1b;2&#xff09;&#x4e3a;&#x6bcf;&#x4e2a;&#x53e5;&#x5b50;&#x8ba1;&#x7b97;&#x81ea;&#x751f;&#x6210;&#x7684;&#x6307;&#x4ee4;&#x8ddf;&#x968f;&#x5956;&#x52b1;&#xff08;RT(s)&#xff09;&#x548c;&#x56fe;&#x50cf;-&#x54cd;&#x5e94;&#x76f8;&#x5173;&#x6027;&#x5956;&#x52b1;&#xff08;RI(s)&#xff09;&#xff0c;&#x5e76;&#x901a;&#x8fc7;&#x52a0;&#x6743;&#x6c42;&#x548c;&#x5f97;&#x5230;&#x6821;&#x51c6;&#x5956;&#x52b1;&#xff08;R(s)&#xff09;&#xff1b;3&#xff09;&#x6839;&#x636e;&#x7d2f;&#x79ef;&#x5956;&#x52b1;&#x9009;&#x62e9;&#x504f;&#x597d;&#x548c;&#x975e;&#x504f;&#x597d;&#x54cd;&#x5e94;&#x5bf9;&#xff1b;4&#xff09;&#x5229;&#x7528;&#x76f4;&#x63a5;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff08;DPO&#xff09;&#x8fdb;&#x884c;&#x8fed;&#x4ee3;&#x5fae;&#x8c03;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x5916;&#x90e8;&#x6a21;&#x578b;&#x6216;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#xff0c;&#x901a;&#x8fc7;&#x89c6;&#x89c9;&#x7ea6;&#x675f;&#x5f3a;&#x5236;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x8f93;&#x5165;&#x3002;","children":[],"payload":{"tag":"li","lines":"1770,1771"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff0c;CSR&#x5728;&#x5341;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x548c;&#x4efb;&#x52a1;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x5e76;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x5e73;&#x5747;&#x6027;&#x80fd;&#x6539;&#x8fdb;&#x8fbe;7.62%&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x8fd8;&#x5c55;&#x793a;&#x4e86;&#x4e0e;&#x4e0d;&#x540c;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x517c;&#x5bb9;&#x6027;&#xff0c;&#x4ee5;&#x53ca;&#x901a;&#x8fc7;&#x8fed;&#x4ee3;&#x5fae;&#x8c03;&#x6301;&#x7eed;&#x63d0;&#x5347;&#x6027;&#x80fd;&#x7684;&#x80fd;&#x529b;&#x3002;&#x7406;&#x8bba;&#x5206;&#x6790;&#x8fdb;&#x4e00;&#x6b65;&#x9a8c;&#x8bc1;&#x4e86;&#x5f15;&#x5165;&#x89c6;&#x89c9;&#x7ea6;&#x675f;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1771,1772"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: CSR&#x901a;&#x8fc7;&#x81ea;&#x5956;&#x52b1;&#x8303;&#x5f0f;&#x7ed3;&#x5408;&#x89c6;&#x89c9;&#x7ea6;&#x675f;&#xff0c;&#x6709;&#x6548;&#x89e3;&#x51b3;&#x4e86;LVLM&#x7684;&#x6a21;&#x6001;&#x4e0d;&#x5bf9;&#x9f50;&#x548c;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4ec5;&#x6027;&#x80fd;&#x4f18;&#x8d8a;&#xff0c;&#x4e14;&#x5177;&#x6709;&#x53ef;&#x6269;&#x5c55;&#x6027;&#x548c;&#x901a;&#x7528;&#x6027;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x5bf9;&#x9f50;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5e94;&#x7528;&#x8303;&#x56f4;&#x3002;","children":[],"payload":{"tag":"li","lines":"1772,1774"}}],"payload":{"tag":"li","lines":"1768,1774","fold":1}}],"payload":{"tag":"h4","lines":"1766,1767"}},{"content":"Look, Compare, Decide: Alleviating Hallucination in Large Vision-Language Models via Multi-View Multi-Path Reasoning","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x7684;&#x8bad;&#x7ec3;&#x514d;&#x8d39;&#x6846;&#x67b6;MVP&#xff0c;&#x901a;&#x8fc7;&#x591a;&#x89c6;&#x89d2;&#x4fe1;&#x606f;&#x83b7;&#x53d6;&#x548c;&#x591a;&#x8def;&#x5f84;&#x786e;&#x5b9a;&#x6027;&#x63a8;&#x7406;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5145;&#x5206;&#x5229;&#x7528;&#x6a21;&#x578b;&#x81ea;&#x8eab;&#x80fd;&#x529b;&#xff0c;&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1775,1776"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x7684;&#x8f93;&#x51fa;&#x65f6;&#x4f1a;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x8fd9;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5bf9;&#x51c6;&#x786e;&#x6027;&#x8981;&#x6c42;&#x9ad8;&#x7684;&#x5e94;&#x7528;&#x4e2d;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4f9d;&#x8d56;&#x91cd;&#x65b0;&#x8bad;&#x7ec3;&#x6216;&#x5916;&#x90e8;&#x5de5;&#x5177;&#xff0c;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x590d;&#x6742;&#x3002;&#x672c;&#x7814;&#x7a76;&#x65e8;&#x5728;&#x5728;&#x4e0d;&#x5f15;&#x5165;&#x989d;&#x5916;&#x6210;&#x672c;&#x7684;&#x524d;&#x63d0;&#x4e0b;&#xff0c;&#x901a;&#x8fc7;&#x6316;&#x6398;&#x6a21;&#x578b;&#x5185;&#x5728;&#x80fd;&#x529b;&#x89e3;&#x51b3;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1777,1778"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: MVP&#x6846;&#x67b6;&#x5305;&#x542b;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#xff1a;1&#xff09;&#x591a;&#x89c6;&#x89d2;&#x4fe1;&#x606f;&#x83b7;&#x53d6;&#xff1a;&#x901a;&#x8fc7;&#x8bbe;&#x8ba1;&#x7279;&#x5b9a;&#x63d0;&#x793a;&#x8bcd;&#xff0c;&#x8ba9;LVLM&#x4ece;&#x2018;&#x81ea;&#x4e0a;&#x800c;&#x4e0b;&#x2019;&#xff08;&#x6574;&#x4f53;&#x573a;&#x666f;&#xff09;&#x3001;&#x2018;&#x5e38;&#x89c4;&#x2019;&#xff08;&#x57fa;&#x7840;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff09;&#x548c;&#x2018;&#x81ea;&#x4e0b;&#x800c;&#x4e0a;&#x2019;&#xff08;&#x7ec6;&#x8282;&#x7279;&#x5f81;&#xff09;&#x4e09;&#x4e2a;&#x89d2;&#x5ea6;&#x751f;&#x6210;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#xff0c;&#x4e30;&#x5bcc;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x5168;&#x5c40;&#x4fe1;&#x606f;&#xff1b;2&#xff09;&#x591a;&#x8def;&#x5f84;&#x786e;&#x5b9a;&#x6027;&#x63a8;&#x7406;&#xff1a;&#x5728;&#x89e3;&#x7801;&#x9636;&#x6bb5;&#xff0c;&#x5bf9;&#x6bcf;&#x4e2a;&#x89c6;&#x89d2;&#x751f;&#x6210;&#x7684;&#x591a;&#x6761;&#x89e3;&#x7801;&#x8def;&#x5f84;&#x8ba1;&#x7b97;&#x5019;&#x9009;&#x7b54;&#x6848;&#x7684;&#x786e;&#x5b9a;&#x6027;&#x5206;&#x6570;&#xff08;&#x901a;&#x8fc7;&#x6bd4;&#x8f83;top-2 token&#x6982;&#x7387;&#x5dee;&#xff09;&#xff0c;&#x6700;&#x7ec8;&#x9009;&#x62e9;&#x805a;&#x5408;&#x786e;&#x5b9a;&#x6027;&#x6700;&#x9ad8;&#x7684;&#x7b54;&#x6848;&#x4f5c;&#x4e3a;&#x8f93;&#x51fa;&#x3002;","children":[],"payload":{"tag":"li","lines":"1778,1779"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x5728;&#x56db;&#x4e2a;&#x4e3b;&#x6d41;LVLM&#xff08;LLaVA&#x3001;Qwen-VL&#x3001;InstructBLIP&#x3001;mPLUG-Owl2&#xff09;&#x4e0a;&#x8fdb;&#x884c;&#xff0c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x591a;&#x89c6;&#x89d2;&#x63cf;&#x8ff0;&#x4f7f;&#x56fe;&#x50cf;&#x5bf9;&#x8c61;&#x8bc6;&#x522b;&#x6570;&#x91cf;&#x4ece;&#x5e73;&#x5747;16.43&#x4e2a;&#x63d0;&#x5347;&#x81f3;36.66&#x4e2a;&#xff1b;2&#xff09;MVP&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x5728;POPE&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4f18;&#x4e8e;&#x6240;&#x6709;&#x65e0;&#x9700;&#x8bad;&#x7ec3;&#x7684;&#x57fa;&#x7ebf;&#x65b9;&#x6cd5;&#xff1b;3&#xff09;&#x6846;&#x67b6;&#x5373;&#x63d2;&#x5373;&#x7528;&#xff0c;&#x53ef;&#x4e0e;&#x5176;&#x5b83;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x8fdb;&#x4e00;&#x6b65;&#x6539;&#x8fdb;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1779,1780"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MVP&#x901a;&#x8fc7;&#x5145;&#x5206;&#x6316;&#x6398;LVLM&#x7684;&#x5185;&#x5728;&#x80fd;&#x529b;&#xff0c;&#x5728;&#x4e0d;&#x589e;&#x52a0;&#x8bad;&#x7ec3;&#x6210;&#x672c;&#x6216;&#x4f9d;&#x8d56;&#x5916;&#x90e8;&#x5de5;&#x5177;&#x7684;&#x60c5;&#x51b5;&#x4e0b;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x5176;&#x591a;&#x89c6;&#x89d2;&#x591a;&#x8def;&#x5f84;&#x7684;&#x63a8;&#x7406;&#x673a;&#x5236;&#x4e3a;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5bf9;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x5b89;&#x5168;&#x6027;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x66f4;&#x591a;&#x6a21;&#x6001;&#x548c;&#x590d;&#x6742;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x3002;","children":[],"payload":{"tag":"li","lines":"1780,1782"}}],"payload":{"tag":"li","lines":"1776,1782","fold":1}}],"payload":{"tag":"h4","lines":"1774,1775"}},{"content":"LOOK TWICE BEFORE YOU ANSWER: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;MemVR&#xff0c;&#x4e00;&#x79cd;&#x65b0;&#x9896;&#x7684;&#x89e3;&#x7801;&#x8303;&#x5f0f;&#xff0c;&#x901a;&#x8fc7;&#x2018;&#x518d;&#x770b;&#x4e00;&#x773c;&#x2019;&#x673a;&#x5236;&#x5728;&#x6a21;&#x578b;&#x63a8;&#x7406;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x9ad8;&#x65f6;&#x91cd;&#x65b0;&#x6ce8;&#x5165;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x4f5c;&#x4e3a;&#x8865;&#x5145;&#x8bc1;&#x636e;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x4e0d;&#x589e;&#x52a0;&#x989d;&#x5916;&#x65f6;&#x95f4;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"1783,1784"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x5904;&#x7406;&#x89c6;&#x89c9;&#x548c;&#x6587;&#x672c;&#x4fe1;&#x606f;&#x65f6;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#xff08;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x8f93;&#x5165;&#x6e90;&#x4e0d;&#x7b26;&#x6216;&#x65e0;&#x610f;&#x4e49;&#x7684;&#x5185;&#x5bb9;&#xff09;&#xff0c;&#x8fd9;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x5176;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x5b89;&#x5168;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x95ee;&#x9898;&#x7684;&#x91cd;&#x8981;&#x6027;&#x5728;&#x4e8e;&#xff0c;&#x5e7b;&#x89c9;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x9519;&#x8bef;&#x4e8b;&#x5b9e;&#xff0c;&#x964d;&#x4f4e;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x5982;&#x5bf9;&#x6bd4;&#x89e3;&#x7801;&#xff08;CD&#xff09;&#x867d;&#x6709;&#x6548;&#x4f46;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x5927;&#x4e14;&#x53ef;&#x80fd;&#x5f15;&#x5165;&#x566a;&#x58f0;&#xff0c;&#x800c;MemVR&#x65e8;&#x5728;&#x4ee5;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x65b9;&#x5f0f;&#x89e3;&#x51b3;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1785,1786"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x53d7;&#x4eba;&#x7c7b;&#x8ba4;&#x77e5;&#x8fc7;&#x7a0b;&#xff08;&#x9057;&#x5fd8;&#x65f6;&#x56de;&#x770b;&#x56fe;&#x50cf;&#xff09;&#x542f;&#x53d1;&#xff0c;&#x63d0;&#x51fa;MemVR&#x65b9;&#x6cd5;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x662f;&#x5728;&#x6a21;&#x578b;&#x63a8;&#x7406;&#x8fc7;&#x7a0b;&#x4e2d;&#xff0c;&#x5f53;&#x68c0;&#x6d4b;&#x5230;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x65f6;&#xff08;&#x901a;&#x8fc7;&#x4e2d;&#x95f4;&#x89e6;&#x53d1;&#x5c42;&#xff09;&#xff0c;&#x5c06;&#x89c6;&#x89c9;&#x6807;&#x8bb0;&#x4f5c;&#x4e3a;&#x2018;&#x952e;&#x503c;&#x8bb0;&#x5fc6;&#x2019;&#x91cd;&#x65b0;&#x6ce8;&#x5165;&#x5230;&#x524d;&#x9988;&#x7f51;&#x7edc;&#xff08;FFN&#xff09;&#x4e2d;&#xff0c;&#x5f62;&#x6210;&#x2018;&#x518d;&#x770b;&#x4e00;&#x773c;&#x2019;&#x673a;&#x5236;&#x3002;&#x5177;&#x4f53;&#x5305;&#x62ec;&#x9759;&#x6001;&#x548c;&#x52a8;&#x6001;&#x4e24;&#x79cd;&#x89c6;&#x89c9;&#x56de;&#x6eaf;&#xff08;VR&#xff09;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x8c03;&#x6574;&#x4e2d;&#x95f4;&#x5c42;&#x7684;&#x9690;&#x85cf;&#x72b6;&#x6001;&#x6765;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x5bf9;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x7684;&#x5173;&#x6ce8;&#xff0c;&#x800c;&#x975e;&#x76f4;&#x63a5;&#x8c03;&#x5236;&#x903b;&#x8f91;&#x503c;&#xff0c;&#x4ece;&#x800c;&#x907f;&#x514d;&#x591a;&#x8f6e;&#x89e3;&#x7801;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"1786,1787"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;MemVR&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff1a;&#x5728;POPE&#x57fa;&#x51c6;&#x4e0a;&#x63d0;&#x5347;7.0%&#xff0c;CHAIRi&#x6307;&#x6807;&#x6539;&#x8fdb;15.6%&#xff0c;MME&#x603b;&#x5206;&#x63d0;&#x9ad8;32.2&#x5206;&#x3002;&#x540c;&#x65f6;&#xff0c;MemVR&#x5728;&#x5ef6;&#x8fdf;&#xff08;68.32 ms/token&#xff09;&#x3001;&#x541e;&#x5410;&#x91cf;&#xff08;0.015 token/ms&#xff09;&#x548c;&#x5185;&#x5b58;&#x4f7f;&#x7528;&#xff08;14345 MB&#xff09;&#x65b9;&#x9762;&#x63a5;&#x8fd1;&#x539f;&#x59cb;&#x8d2a;&#x5a6a;&#x89e3;&#x7801;&#x6548;&#x7387;&#xff0c;&#x8fdc;&#x4f18;&#x4e8e;&#x5bf9;&#x6bd4;&#x65b9;&#x6cd5;&#xff08;&#x5982;VCD&#x5ef6;&#x8fdf;&#x4e3a;144.62 ms/token&#xff09;&#x3002;GPT-4o&#x8bc4;&#x4f30;&#x4e5f;&#x8bc1;&#x5b9e;&#x4e86;&#x5176;&#x7efc;&#x5408;&#x6027;&#x80fd;&#x63d0;&#x5347;&#x3002;","children":[],"payload":{"tag":"li","lines":"1787,1788"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: MemVR&#x662f;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x7684;&#x89e3;&#x7801;&#x8303;&#x5f0f;&#xff0c;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;MLLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x751a;&#x81f3;&#x63d0;&#x5347;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;&#x5176;&#x5173;&#x952e;&#x4f18;&#x52bf;&#x5728;&#x4e8e;&#x4e0d;&#x5f15;&#x5165;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6570;&#x636e;&#xff0c;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x4f4e;&#xff0c;&#x4e14;&#x53ef;&#x6269;&#x5c55;&#x5230;&#x66f4;&#x591a;&#x6a21;&#x6001;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x4e3a;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x5b89;&#x5168;&#x5173;&#x952e;&#x5e94;&#x7528;&#x5177;&#x6709;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1788,1790"}}],"payload":{"tag":"li","lines":"1784,1790","fold":1}}],"payload":{"tag":"h4","lines":"1782,1783"}},{"content":"Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;&#x89c6;&#x89c9;&#x5bf9;&#x6297;&#x6270;&#x52a8;&#xff08;VAP&#xff09;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5411;&#x8f93;&#x5165;&#x56fe;&#x50cf;&#x4e2d;&#x6dfb;&#x52a0;&#x7cbe;&#x5fc3;&#x4f18;&#x5316;&#x7684;&#x3001;&#x4eba;&#x773c;&#x96be;&#x4ee5;&#x5bdf;&#x89c9;&#x7684;&#x566a;&#x58f0;&#xff0c;&#x6765;&#x51cf;&#x5c11;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVMs&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x800c;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x672c;&#x8eab;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5728;&#x591a;&#x4e2a;&#x5148;&#x8fdb;&#x6a21;&#x578b;&#x4e0a;&#x9a8c;&#x8bc1;&#x6709;&#x6548;&#x3002;","children":[],"payload":{"tag":"li","lines":"1791,1792"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVMs&#xff09;&#x5728;&#x751f;&#x6210;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x65f6;&#xff0c;&#x5e38;&#x5e38;&#x4f1a;&#x4ea7;&#x751f;&#x770b;&#x4f3c;&#x5408;&#x7406;&#x4f46;&#x4e0e;&#x56fe;&#x50cf;&#x5b9e;&#x9645;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x2018;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x2019;&#xff0c;&#x4f8b;&#x5982;&#x63cf;&#x8ff0;&#x56fe;&#x4e2d;&#x4e0d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x3002;&#x8fd9;&#x4e2a;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x73b0;&#x5b9e;&#x4e16;&#x754c;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5e94;&#x7528;&#xff0c;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x4fe1;&#x606f;&#x6216;&#x504f;&#x89c1;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x901a;&#x8fc7;&#x5fae;&#x8c03;&#x6a21;&#x578b;&#x6216;&#x4f18;&#x5316;&#x89e3;&#x7801;&#x8fc7;&#x7a0b;&#x6765;&#x89e3;&#x51b3;&#xff0c;&#x5c5e;&#x4e8e;&#x2018;&#x4ee5;&#x6a21;&#x578b;&#x4e3a;&#x4e2d;&#x5fc3;&#x2019;&#x7684;&#x5e72;&#x9884;&#xff0c;&#x6210;&#x672c;&#x9ad8;&#x4e14;&#x590d;&#x6742;&#x3002;&#x672c;&#x6587;&#x65e8;&#x5728;&#x4ece;&#x2018;&#x4ee5;&#x6570;&#x636e;&#x4e3a;&#x4e2d;&#x5fc3;&#x2019;&#x7684;&#x89d2;&#x5ea6;&#xff0c;&#x901a;&#x8fc7;&#x6539;&#x53d8;&#x8f93;&#x5165;&#x800c;&#x975e;&#x6a21;&#x578b;&#x672c;&#x8eab;&#x6765;&#x66f4;&#x9ad8;&#x6548;&#x5730;&#x89e3;&#x51b3;&#x6b64;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1793,1794"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x89c6;&#x89c9;&#x5bf9;&#x6297;&#x6270;&#x52a8;&#xff08;VAP&#xff09;&#x6846;&#x67b6;&#x3002;&#x5176;&#x6838;&#x5fc3;&#x601d;&#x60f3;&#x662f;&#x5c06;&#x5bf9;&#x6297;&#x6027;&#x653b;&#x51fb;&#x4e2d;&#x7528;&#x4e8e;&#x8bef;&#x5bfc;&#x6a21;&#x578b;&#x7684;&#x2018;&#x6bd2;&#x836f;&#x2019;&#x566a;&#x58f0;&#xff0c;&#x8f6c;&#x5316;&#x4e3a;&#x4e00;&#x79cd;&#x2018;&#x89e3;&#x836f;&#x2019;&#x3002;&#x5177;&#x4f53;&#x65b9;&#x6cd5;&#x5982;&#x4e0b;&#xff1a;1. &#x5b9a;&#x4e49;&#x76ee;&#x6807;&#xff1a;&#x4f18;&#x5316;&#x4e24;&#x4e2a;&#x5bf9;&#x6297;&#x76ee;&#x6807;&#xff0c;&#x4e00;&#x662f;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x54cd;&#x5e94;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e4b;&#x95f4;&#x7684;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#xff0c;&#x4e8c;&#x662f;&#x51cf;&#x5c11;&#x6a21;&#x578b;&#x5185;&#x90e8;&#x53c2;&#x6570;&#x5316;&#x77e5;&#x8bc6;&#x504f;&#x89c1;&#x7684;&#x5f71;&#x54cd;&#x3002;2. &#x751f;&#x6210;&#x566a;&#x58f0;&#xff1a;&#x91c7;&#x7528;&#x2018;&#x96f6;&#x68af;&#x5ea6;&#x2019;&#x4f18;&#x5316;&#x6280;&#x672f;&#xff0c;&#x8ba1;&#x7b97;&#x4e00;&#x4e2a;&#x5fae;&#x5c0f;&#x7684;&#x6270;&#x52a8;&#x5411;&#x91cf;&#x3b4;&#xff0c;&#x5c06;&#x5176;&#x6dfb;&#x52a0;&#x5230;&#x539f;&#x59cb;&#x56fe;&#x50cf;x&#x4e0a;&#xff0c;&#x5f97;&#x5230;&#x6270;&#x52a8;&#x540e;&#x7684;&#x56fe;&#x50cf;x&#x302; = x + &#x3b4;&#x3002;&#x8fd9;&#x4e2a;&#x566a;&#x58f0;&#x7ecf;&#x8fc7;&#x4f18;&#x5316;&#xff0c;&#x80fd;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x66f4;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x7684;&#x771f;&#x5b9e;&#x5185;&#x5bb9;&#xff0c;&#x800c;&#x4e0d;&#x662f;&#x4f9d;&#x8d56;&#x5176;&#x8bad;&#x7ec3;&#x4e2d;&#x5b66;&#x5230;&#x7684;&#x6709;&#x504f;&#x89c1;&#x7684;&#x5148;&#x9a8c;&#x77e5;&#x8bc6;&#x3002;3. &#x9ed1;&#x76d2;&#x64cd;&#x4f5c;&#xff1a;&#x6574;&#x4e2a;&#x8fc7;&#x7a0b;&#x4e0d;&#x9700;&#x8981;&#x8bbf;&#x95ee;&#x6216;&#x4fee;&#x6539;LVMs&#x7684;&#x5185;&#x90e8;&#x53c2;&#x6570;&#xff0c;&#x662f;&#x4e00;&#x79cd;&#x5b9e;&#x7528;&#x4e14;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;","children":[],"payload":{"tag":"li","lines":"1794,1795"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x8bba;&#x6587;&#x5728;POPE&#x3001;BEAF&#xff08;&#x7528;&#x4e8e;&#x5c01;&#x95ed;&#x5f0f;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#x8bc4;&#x4f30;&#xff09;&#x548c;CHAIR&#xff08;&#x7528;&#x4e8e;&#x5f00;&#x653e;&#x5f0f;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x751f;&#x6210;&#xff09;&#x7b49;&#x591a;&#x4e2a;&#x8bc4;&#x4f30;&#x6846;&#x67b6;&#x4e0a;&#x8fdb;&#x884c;&#x4e86;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#xff0c;&#x6db5;&#x76d6;&#x4e86;8&#x4e2a;&#x6700;&#x5148;&#x8fdb;&#x7684;LVMs&#xff08;&#x5982;Intern-VL2-MPO, DeepSeek-VL2, LLaVA-v1.5&#x7b49;&#xff09;&#x3002;&#x5173;&#x952e;&#x7ed3;&#x679c;&#x663e;&#x793a;&#xff0c;VAP&#x65b9;&#x6cd5;&#x5728;&#x6240;&#x6709;&#x6d4b;&#x8bd5;&#x6a21;&#x578b;&#x4e0a;&#x90fd;&#x4e00;&#x81f4;&#x5730;&#x51cf;&#x5c11;&#x4e86;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x3002;&#x4f8b;&#x5982;&#xff0c;&#x5728;POPE&#x8bc4;&#x4f30;&#x4e2d;&#xff0c;&#x5404;&#x4e2a;&#x6a21;&#x578b;&#x7684;F1&#x5206;&#x6570;&#x5747;&#x6709;&#x63d0;&#x5347;&#xff08;&#x63d0;&#x5347;&#x5e45;&#x5ea6;&#x5728;+0.42%&#x5230;+2.09%&#x4e4b;&#x95f4;&#xff09;&#xff0c;&#x8bc1;&#x660e;&#x4e86;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6709;&#x6548;&#x6027;&#x548c;&#x666e;&#x9002;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1795,1796"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x901a;&#x8fc7;&#x5411;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#x6dfb;&#x52a0;&#x7ecf;&#x8fc7;&#x7b56;&#x7565;&#x6027;&#x4f18;&#x5316;&#x7684;&#x5bf9;&#x6297;&#x6027;&#x566a;&#x58f0;&#xff0c;&#x53ef;&#x4ee5;&#x6709;&#x6548;&#x6291;&#x5236;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#xff0c;&#x4ece;&#x800c;&#x63d0;&#x9ad8;&#x5176;&#x4e8b;&#x5b9e;&#x51c6;&#x786e;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;&#x8fd9;&#x79cd;&#x2018;&#x4ee5;&#x6570;&#x636e;&#x4e3a;&#x4e2d;&#x5fc3;&#x2019;&#x7684;&#x65b9;&#x6cd5;&#x4e3a;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x7684;&#x8303;&#x5f0f;&#xff0c;&#x4e0e;&#x4f20;&#x7edf;&#x2018;&#x4ee5;&#x6a21;&#x578b;&#x4e3a;&#x4e2d;&#x5fc3;&#x2019;&#x7684;&#x65b9;&#x6cd5;&#xff08;&#x5982;&#x5fae;&#x8c03;&#xff09;&#x76f8;&#x6bd4;&#xff0c;&#x5177;&#x6709;&#x65e0;&#x9700;&#x6539;&#x52a8;&#x6a21;&#x578b;&#x3001;&#x8ba1;&#x7b97;&#x6210;&#x672c;&#x4f4e;&#x3001;&#x6613;&#x4e8e;&#x90e8;&#x7f72;&#x7684;&#x4f18;&#x52bf;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7684;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5728;&#x4e8e;&#x4e3a;&#x6784;&#x5efa;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x66f4;&#x5b89;&#x5168;&#x7684; multimodal AI &#x7cfb;&#x7edf;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x79cd;&#x5b9e;&#x7528;&#x5de5;&#x5177;&#xff0c;&#x6709;&#x671b;&#x5e94;&#x7528;&#x4e8e;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x5b9e;&#x9645;&#x573a;&#x666f;&#xff0c;&#x5982;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#x7b49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1796,1798"}}],"payload":{"tag":"li","lines":"1792,1798","fold":1}}],"payload":{"tag":"h4","lines":"1790,1791"}},{"content":"From Uncertainty to Trust: Enhancing Reliability in Vision-Language Models with Uncertainty-Guided Dropout Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;DROPOUT DECODING&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x91cf;&#x5316;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x7684;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x5e76;&#x9009;&#x62e9;&#x6027;&#x5c4f;&#x853d;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x4ee4;&#x724c;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x5347;&#x8f93;&#x51fa;&#x53ef;&#x9760;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1799,1800"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x591a;&#x6a21;&#x6001;&#x4efb;&#x52a1;&#x4e2d;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x5bb9;&#x6613;&#x9519;&#x8bef;&#x89e3;&#x8bfb;&#x89c6;&#x89c9;&#x8f93;&#x5165;&#xff0c;&#x5bfc;&#x81f4;&#x5e7b;&#x89c9;&#x548c;&#x4e0d;&#x53ef;&#x9760;&#x8f93;&#x51fa;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5728;&#x9700;&#x8981;&#x7cbe;&#x786e;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x9700;&#x8981;&#x5927;&#x91cf;&#x8d44;&#x6e90;&#x8fdb;&#x884c;&#x5fae;&#x8c03;&#xff0c;&#x8981;&#x4e48;&#x4f9d;&#x8d56;&#x542f;&#x53d1;&#x5f0f;&#x8bbe;&#x8ba1;&#x4e14;&#x63a8;&#x7406;&#x6210;&#x672c;&#x9ad8;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x66f4;&#x539f;&#x5219;&#x6027;&#x7684;&#x65b9;&#x6cd5;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x63d0;&#x5347;&#x6a21;&#x578b;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1801,1802"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;DROPOUT DECODING&#x65b9;&#x6cd5;&#xff1a;1&#xff09;&#x5c06;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#x6295;&#x5f71;&#x5230;&#x6587;&#x672c;&#x7a7a;&#x95f4;&#xff0c;&#x5206;&#x89e3;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x4e3a;&#x5076;&#x7136;&#x6027;&#xff08;&#x6570;&#x636e;&#x76f8;&#x5173;&#xff09;&#x548c;&#x8ba4;&#x77e5;&#x6027;&#xff08;&#x6a21;&#x578b;&#x76f8;&#x5173;&#xff09;&#x4e24;&#x90e8;&#x5206;&#xff1b;2&#xff09;&#x91cd;&#x70b9;&#x5173;&#x6ce8;&#x8ba4;&#x77e5;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#xff08;&#x53cd;&#x6620;&#x6a21;&#x578b;&#x77e5;&#x8bc6;&#x4e0d;&#x8db3;&#xff09;&#xff0c;&#x8bc6;&#x522b;&#x9ad8;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff1b;3&#xff09;&#x53d7;dropout&#x542f;&#x53d1;&#xff0c;&#x5728;&#x63a8;&#x7406;&#x65f6;&#x5bf9;&#x8f93;&#x5165;&#x89c6;&#x89c9;&#x4ee4;&#x724c;&#xff08;&#x800c;&#x975e;&#x6a21;&#x578b;&#x53c2;&#x6570;&#xff09;&#x5e94;&#x7528;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x5f15;&#x5bfc;&#x7684;&#x4ee4;&#x724c;&#x4e22;&#x5f03;&#xff0c;&#x751f;&#x6210;&#x591a;&#x4e2a;&#x63a9;&#x7801;&#x540e;&#x7684;&#x89e3;&#x7801;&#x4e0a;&#x4e0b;&#x6587;&#x96c6;&#x5408;&#xff1b;4&#xff09;&#x901a;&#x8fc7;&#x591a;&#x6570;&#x6295;&#x7968;&#x805a;&#x5408;&#x591a;&#x4e2a;&#x9884;&#x6d4b;&#x7ed3;&#x679c;&#xff0c;&#x5f97;&#x5230;&#x6700;&#x7ec8;&#x8f93;&#x51fa;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x53c2;&#x6570;&#x6216;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1802,1803"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;CHAIR&#x3001;THRONE&#x548c;MMBench&#x7b49;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#xff0c;DROPOUT DECODING&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x76ee;&#x6807;&#x5e7b;&#x89c9;&#xff08;OH&#xff09;&#xff0c;&#x5e76; consistently &#x63d0;&#x5347;&#x4e86;LVLM&#x8f93;&#x51fa;&#x5728;&#x5404;&#x79cd;&#x89c6;&#x89c9;&#x4e0a;&#x4e0b;&#x6587;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1803,1804"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: DROPOUT DECODING&#x662f;&#x4e00;&#x79cd;&#x6709;&#x6548;&#x7684;&#x63a8;&#x7406;&#x65f6;&#x4f18;&#x5316;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x91cf;&#x5316;&#x548c;&#x4ee4;&#x724c;&#x4e22;&#x5f03;&#x673a;&#x5236;&#xff0c;&#x663e;&#x8457;&#x63d0;&#x5347;LVLM&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x8f93;&#x51fa;&#x8d28;&#x91cf;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x989d;&#x5916;&#x8bad;&#x7ec3;&#x6216;&#x6a21;&#x578b;&#x4fee;&#x6539;&#xff0c;&#x5177;&#x6709;&#x5e7f;&#x6cdb;&#x7684;&#x9002;&#x7528;&#x6027;&#x548c;&#x5b9e;&#x9645;&#x90e8;&#x7f72;&#x6f5c;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1804,1806"}}],"payload":{"tag":"li","lines":"1800,1806","fold":1}}],"payload":{"tag":"h4","lines":"1798,1799"}},{"content":"SGD: Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x540d;&#x4e3a;SumGD&#x7684;&#x65b0;&#x89e3;&#x7801;&#x65b9;&#x6cd5;&#xff0c;&#x901a;&#x8fc7;&#x6458;&#x8981;&#x538b;&#x7f29;&#x5386;&#x53f2;&#x6587;&#x672c;&#xff0c;&#x4ec5;&#x5728;&#x9884;&#x6d4b;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x8bcd;&#x6027;&#x65f6;&#x5f15;&#x5bfc;&#x6a21;&#x578b;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6587;&#x672c;&#x8d28;&#x91cf;&#x3002;","children":[],"payload":{"tag":"li","lines":"1807,1808"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x8fc7;&#x5ea6;&#x4f9d;&#x8d56;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff08;&#x6587;&#x672c;&#x6a21;&#x5f0f;&#xff09;&#xff0c;&#x5bfc;&#x81f4;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x77db;&#x76fe;&#x7684;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x865a;&#x6784;&#x7269;&#x4f53;&#xff09;&#x3002;&#x968f;&#x7740;&#x751f;&#x6210;&#x6587;&#x672c;&#x957f;&#x5ea6;&#x589e;&#x52a0;&#xff0c;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x52a0;&#x5267;&#xff0c;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#x3002;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#x5bf9;&#x63d0;&#x5347;LVLMs&#x5728;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x3001;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff09;&#x4e2d;&#x7684;&#x51c6;&#x786e;&#x6027;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1809,1810"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x6458;&#x8981;&#x5f15;&#x5bfc;&#x89e3;&#x7801;&#xff08;SumGD&#xff09;&#xff1a;1. &#x5206;&#x6790;&#x53d1;&#x73b0;&#x5e7b;&#x89c9;&#x4e3b;&#x8981;&#x53d1;&#x751f;&#x5728;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x8bcd;&#x6027;&#xff08;&#x5982;&#x540d;&#x8bcd;&#x3001;&#x5f62;&#x5bb9;&#x8bcd;&#xff09;&#x4e14;&#x968f;&#x6587;&#x672c;&#x957f;&#x5ea6;&#x589e;&#x52a0;&#x800c;&#x6076;&#x5316;&#xff1b;2. &#x4f7f;&#x7528;&#x6458;&#x8981;&#x6a21;&#x578b;&#x538b;&#x7f29;&#x5df2;&#x751f;&#x6210;&#x6587;&#x672c;&#xff0c;&#x7f29;&#x77ed;&#x4e0a;&#x4e0b;&#x6587;&#x957f;&#x5ea6;&#x4ee5;&#x51cf;&#x5c11;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#x4f9d;&#x8d56;&#xff1b;3. &#x4ec5;&#x5728;&#x9884;&#x6d4b;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x8bcd;&#x6027;&#x65f6;&#x5f15;&#x5165;&#x6458;&#x8981;&#x5f15;&#x5bfc;&#xff0c;&#x5176;&#x4ed6;&#x8bcd;&#x6027;&#x4fdd;&#x6301;&#x539f;&#x59cb;&#x5206;&#x5e03;&#x4ee5;&#x7ef4;&#x62a4;&#x6587;&#x672c;&#x6d41;&#x7545;&#x6027;&#xff1b;4. &#x901a;&#x8fc7;&#x8c03;&#x6574;&#x4e0b;&#x4e00;&#x8bcd;&#x6982;&#x7387;&#x5206;&#x5e03;&#xff0c;&#x4f7f;&#x6a21;&#x578b;&#x66f4;&#x5173;&#x6ce8;&#x56fe;&#x50cf;&#x4fe1;&#x606f;&#x800c;&#x975e;&#x6587;&#x672c;&#x6a21;&#x5f0f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1810,1811"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x5728;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x57fa;&#x51c6;&#xff08;CHAIRS/CHAIRI&#xff09;&#x4e0a;&#x663e;&#x8457;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#xff08;&#x63d0;&#x5347;16.5%-19%&#xff09;&#xff1b;2. &#x5728;&#x7cbe;&#x5ea6;-&#x53ec;&#x56de;&#x6743;&#x8861;&#x4e2d;&#x8fbe;&#x5230;&#x5e15;&#x7d2f;&#x6258;&#x6700;&#x4f18;&#xff0c;&#x5e73;&#x8861;&#x4e86;&#x5e7b;&#x89c9;&#x51cf;&#x5c11;&#x4e0e;&#x76ee;&#x6807;&#x53ec;&#x56de;&#x7387;&#xff1b;3. &#x751f;&#x6210;&#x957f;&#x6587;&#x672c;&#x65f6;&#x9c81;&#x68d2;&#x6027;&#x66f4;&#x5f3a;&#xff0c;&#x51e0;&#x4e4e;&#x5b8c;&#x5168;&#x4fdd;&#x6301;&#x6587;&#x672c;&#x8d28;&#x91cf;&#xff1b;4. &#x5728;&#x4e0d;&#x540c;&#x6a21;&#x578b;&#x89c4;&#x6a21;&#x548c;&#x67b6;&#x6784;&#x4e0a;&#x5747;&#x8868;&#x73b0;&#x6700;&#x4f18;&#x3002;","children":[],"payload":{"tag":"li","lines":"1811,1812"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: SumGD&#x901a;&#x8fc7;&#x9488;&#x5bf9;&#x6027;&#x63a7;&#x5236;&#x56fe;&#x50cf;&#x76f8;&#x5173;&#x8bcd;&#x6027;&#x7684;&#x89e3;&#x7801;&#x7b56;&#x7565;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x4e86;LVLMs&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x4e14;&#x4e0d;&#x5f71;&#x54cd;&#x6587;&#x672c;&#x751f;&#x6210;&#x8d28;&#x91cf;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e3a;&#x6539;&#x8fdb;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x4fe1;&#x5ea6;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5bf9;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x5f71;&#x50cf;&#x5206;&#x6790;&#x7b49;&#x9700;&#x8981;&#x9ad8;&#x7cbe;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x5e94;&#x7528;&#x5177;&#x6709;&#x6f5c;&#x5728;&#x4ef7;&#x503c;&#x3002;","children":[],"payload":{"tag":"li","lines":"1812,1814"}}],"payload":{"tag":"li","lines":"1808,1814","fold":1}}],"payload":{"tag":"h4","lines":"1806,1807"}}],"payload":{"tag":"h3","lines":"1619,1620","fold":1}}],"payload":{"tag":"h2","lines":"1547,1548"}},{"content":"Benchmark","children":[{"content":"HalLoc: Token-level Localization of Hallucinations for Vision Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;HalLoc&#xff0c;&#x4e00;&#x4e2a;&#x7528;&#x4e8e;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x7684;&#x5927;&#x89c4;&#x6a21;&#x6570;&#x636e;&#x96c6;&#x548c;&#x57fa;&#x7ebf;&#x6a21;&#x578b;&#x3002;&#x8be5;&#x6570;&#x636e;&#x96c6;&#x5305;&#x542b;15.5&#x4e07;&#x4e2a;token&#x7ea7;&#x522b;&#x7684;&#x6807;&#x6ce8;&#x6837;&#x672c;&#xff0c;&#x8986;&#x76d6;&#x591a;&#x79cd;&#x4efb;&#x52a1;&#x548c;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x3002;&#x57fa;&#x4e8e;&#x6b64;&#x8bad;&#x7ec3;&#x7684;&#x8f7b;&#x91cf;&#x7ea7;&#x6a21;&#x578b;&#x80fd;&#x4ee5;&#x6982;&#x7387;&#x5f62;&#x5f0f;&#x5b9e;&#x65f6;&#x5b9a;&#x4f4d;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x53ef;&#x5373;&#x63d2;&#x5373;&#x7528;&#x5730;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;VLM&#x4e2d;&#xff0c;&#x63d0;&#x5347;&#x53ef;&#x9760;&#x6027;&#x4e14;&#x51e0;&#x4e4e;&#x4e0d;&#x589e;&#x52a0;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x3002;","children":[],"payload":{"tag":"li","lines":"1816,1817"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;VLM&#xff09;&#x5bb9;&#x6613;&#x4ea7;&#x751f;&#x4e0e;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x201c;&#x5e7b;&#x89c9;&#x201d;&#xff0c;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x5176;&#x5728;&#x5173;&#x952e;&#x5e94;&#x7528;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x3002;&#x73b0;&#x6709;&#x7684;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x65b9;&#x6cd5;&#x5b58;&#x5728;&#x4e24;&#x5927;&#x95ee;&#x9898;&#xff1a;&#x4e00;&#x662f;&#x4f9d;&#x8d56;&#x8ba1;&#x7b97;&#x5bc6;&#x96c6;&#x578b;&#x6a21;&#x578b;&#xff08;&#x5982;GPT-4&#xff09;&#xff0c;&#x5bfc;&#x81f4;&#x9ad8;&#x5ef6;&#x8fdf;&#x548c;&#x9ad8;&#x8d44;&#x6e90;&#x6d88;&#x8017;&#xff0c;&#x96be;&#x4ee5;&#x5b9e;&#x65f6;&#x5e94;&#x7528;&#xff1b;&#x4e8c;&#x662f;&#x901a;&#x5e38;&#x53ea;&#x7ed9;&#x51fa;&#x201c;&#x662f;/&#x5426;&#x201d;&#x7684;&#x4e8c;&#x503c;&#x5224;&#x65ad;&#xff0c;&#x65e0;&#x6cd5;&#x5904;&#x7406;&#x73b0;&#x5b9e;&#x4e16;&#x754c;&#x4e2d;&#x5927;&#x91cf;&#x5b58;&#x5728;&#x7684;&#x6a21;&#x7cca;&#x6848;&#x4f8b;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x8feb;&#x5207;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x9ad8;&#x6548;&#x3001;&#x80fd;&#x63d0;&#x4f9b;&#x6982;&#x7387;&#x5316;&#x7f6e;&#x4fe1;&#x5ea6;&#x8bc4;&#x4f30;&#x7684;&#x8f7b;&#x91cf;&#x7ea7;&#x68c0;&#x6d4b;&#x673a;&#x5236;&#x3002;","children":[],"payload":{"tag":"li","lines":"1818,1819"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x90e8;&#x5206;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#xff1a;","children":[{"content":"1. <strong>&#x6784;&#x5efa;HalLoc&#x6570;&#x636e;&#x96c6;</strong>&#xff1a;&#x8fd9;&#x662f;&#x4e00;&#x4e2a;&#x5927;&#x89c4;&#x6a21;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x5305;&#x542b;155,000&#x4e2a;&#x6837;&#x672c;&#xff0c;&#x6db5;&#x76d6;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#x3001;&#x6307;&#x4ee4;&#x8ddf;&#x968f;&#x548c;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4e09;&#x5927;&#x4efb;&#x52a1;&#x3002;&#x5176;&#x5173;&#x952e;&#x521b;&#x65b0;&#x5728;&#x4e8e;&#x63d0;&#x4f9b;&#x4e86;<strong>token&#x7ea7;&#x522b;</strong>&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#xff0c;&#x5e76;&#x6807;&#x6ce8;&#x4e86;&#x56db;&#x79cd;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff1a;&#x7269;&#x4f53;&#xff08;Object&#xff09;&#x3001;&#x5c5e;&#x6027;&#xff08;Attribute&#xff09;&#x3001;&#x5173;&#x7cfb;&#xff08;Relation&#xff09;&#x548c;&#x573a;&#x666f;&#xff08;Scene&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1820,1821","listIndex":1}},{"content":"2. <strong>&#x5f00;&#x53d1;HalLocalizer&#x57fa;&#x7ebf;&#x6a21;&#x578b;</strong>&#xff1a;&#x4e00;&#x4e2a;&#x57fa;&#x4e8e;HalLoc&#x6570;&#x636e;&#x96c6;&#x8bad;&#x7ec3;&#x7684;&#x8f7b;&#x91cf;&#x7ea7;&#x6a21;&#x578b;&#x3002;&#x8be5;&#x6a21;&#x578b;&#x7684;&#x8bbe;&#x8ba1;&#x76ee;&#x6807;&#x662f;&#x4f4e;&#x5f00;&#x9500;&#xff0c;&#x80fd;&#x591f;&#x4e0e;VLM&#x7684;&#x751f;&#x6210;&#x8fc7;&#x7a0b;<strong>&#x5e76;&#x53d1;&#x8fd0;&#x884c;</strong>&#xff0c;&#x5b9e;&#x65f6;&#x4e3a;&#x6bcf;&#x4e2a;&#x8f93;&#x51fa;&#x7684;token&#x8ba1;&#x7b97;&#x5176;&#x5c5e;&#x4e8e;&#x5e7b;&#x89c9;&#x7684;<strong>&#x6982;&#x7387;</strong>&#xff0c;&#x800c;&#x975e;&#x7b80;&#x5355;&#x7684;&#x4e8c;&#x503c;&#x5224;&#x65ad;&#x3002;&#x8be5;&#x6a21;&#x578b;&#x88ab;&#x8bbe;&#x8ba1;&#x4e3a;&#x53ef;<strong>&#x5373;&#x63d2;&#x5373;&#x7528;&#xff08;Plug-and-Play&#xff09;</strong>&#xff0c;&#x80fd;&#x65e0;&#x7f1d;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;&#x7684;&#x5404;&#x79cd;VLM&#x4e2d;&#x3002;","children":[],"payload":{"tag":"li","lines":"1821,1822","listIndex":2}}],"payload":{"tag":"li","lines":"1819,1822"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x8bba;&#x6587;&#x901a;&#x8fc7;&#x4e0e;&#x5176;&#x4ed6;&#x6570;&#x636e;&#x96c6;&#xff08;HaELM, MHalDetect, MHaluBench&#xff09;&#x548c;&#x68c0;&#x6d4b;&#x65b9;&#x6cd5;&#xff08;CHAIR, GAVIE, FAITHScore, UNIHD&#xff09;&#x7684;&#x5bf9;&#x6bd4;&#xff0c;&#x7a81;&#x663e;&#x4e86;HalLoc&#x548c;HalLocalizer&#x7684;&#x4f18;&#x8d8a;&#x6027;&#xff1a;","children":[{"content":"1. <strong>&#x6570;&#x636e;&#x96c6;&#x65b9;&#x9762;</strong>&#xff1a;HalLoc&#x5728;&#x89c4;&#x6a21;&#xff08;155K&#xff09;&#x3001;&#x6807;&#x6ce8;&#x7c92;&#x5ea6;&#xff08;token&#x7ea7;&#xff09;&#x3001;&#x4efb;&#x52a1;&#x8986;&#x76d6;&#x5ea6;&#xff08;3&#x79cd;&#xff09;&#x548c;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#x6807;&#x6ce8;&#xff08;4&#x79cd;&#xff09;&#x4e0a;&#x5747;&#x4f18;&#x4e8e;&#x73b0;&#x6709;&#x6570;&#x636e;&#x96c6;&#x3002;","children":[],"payload":{"tag":"li","lines":"1823,1824","listIndex":1}},{"content":"2. <strong>&#x6a21;&#x578b;&#x65b9;&#x9762;</strong>&#xff1a;HalLocalizer&#x80fd;&#x591f;&#x5b9e;&#x73b0;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;token&#x7ea7;&#x6982;&#x7387;&#x68c0;&#x6d4b;&#xff0c;&#x5e76;&#x5177;&#x5907;&#x4f4e;&#x5ef6;&#x8fdf;&#x548c;&#x6613;&#x96c6;&#x6210;&#x7684;&#x7279;&#x70b9;&#xff0c;&#x5f25;&#x8865;&#x4e86;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x8981;&#x4e48;&#x8ba1;&#x7b97;&#x5f00;&#x9500;&#x5927;&#x3001;&#x8981;&#x4e48;&#x8f93;&#x51fa;&#x7ed3;&#x679c;&#x8fc7;&#x4e8e;&#x7edd;&#x5bf9;&#x7684;&#x7f3a;&#x9677;&#x3002;","children":[],"payload":{"tag":"li","lines":"1824,1825","listIndex":2}},{"content":"3. <strong>&#x5206;&#x6790;&#x53d1;&#x73b0;</strong>&#xff1a;&#x8bba;&#x6587;&#x8fd8;&#x901a;&#x8fc7;&#x5b9e;&#x9a8c;&#x9a8c;&#x8bc1;&#x4e86;VLM&#x4ea7;&#x751f;&#x5e7b;&#x89c9;&#x7684;&#x7edf;&#x8ba1;&#x504f;&#x5dee;&#x6a21;&#x5f0f;&#xff0c;&#x5982;&#x8bed;&#x8a00;&#x5148;&#x9a8c;&#xff08;Language Prior&#xff09;&#x548c;&#x56fe;&#x50cf;&#x5148;&#x9a8c;&#xff08;Image Prior&#xff09;&#xff0c;&#x4e3a;&#x7406;&#x89e3;&#x5e7b;&#x89c9;&#x6210;&#x56e0;&#x63d0;&#x4f9b;&#x4e86; insights&#x3002;","children":[],"payload":{"tag":"li","lines":"1825,1826","listIndex":3}}],"payload":{"tag":"li","lines":"1822,1826"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;HalLoc&#x6570;&#x636e;&#x96c6;&#x548c;HalLocalizer&#x6a21;&#x578b;&#x4e3a;VLM&#x7684;&#x5e7b;&#x89c9;&#x68c0;&#x6d4b;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x66f4;&#x5b9e;&#x7528;&#x3001;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#x3002;&#x5176;&#x6700;&#x7ec8;&#x5f71;&#x54cd;&#x548c;&#x8d21;&#x732e;&#x5728;&#x4e8e;&#xff1a;","children":[{"content":"1. <strong>&#x63a8;&#x52a8;&#x53ef;&#x9760;&#x4e14;&#x5b9e;&#x7528;&#x7684;VLM&#x53d1;&#x5c55;</strong>&#xff1a;&#x901a;&#x8fc7;&#x63d0;&#x4f9b;&#x4e00;&#x79cd;&#x4f4e;&#x5f00;&#x9500;&#x3001;&#x6982;&#x7387;&#x5316;&#x7684;&#x5b9e;&#x65f6;&#x68c0;&#x6d4b;&#x65b9;&#x6cd5;&#xff0c;&#x4f7f;VLM&#x5728;&#x4fdd;&#x6301;&#x9ad8;&#x6548;&#x7684;&#x540c;&#x65f6;&#x66f4;&#x5177;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x66f4;&#x9002;&#x5408;&#x5728;&#x8ba1;&#x7b97;&#x8d44;&#x6e90;&#x53d7;&#x9650;&#x7684;&#x771f;&#x5b9e;&#x573a;&#x666f;&#xff08;&#x5982;&#x8fb9;&#x7f18;&#x8bbe;&#x5907;&#x3001;&#x5b9e;&#x65f6;&#x5e94;&#x7528;&#xff09;&#x4e2d;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1827,1828","listIndex":1}},{"content":"2. <strong>&#x63d0;&#x4f9b;&#x7814;&#x7a76;&#x57fa;&#x7840;</strong>&#xff1a;&#x5927;&#x89c4;&#x6a21;&#x3001;&#x7ec6;&#x7c92;&#x5ea6;&#x6807;&#x6ce8;&#x7684;HalLoc&#x6570;&#x636e;&#x96c6;&#x5c06;&#x4e3a;&#x672a;&#x6765;&#x66f4;&#x6df1;&#x5165;&#x7684;&#x5e7b;&#x89c9;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x575a;&#x5b9e;&#x7684;&#x57fa;&#x7840;&#x3002;","children":[],"payload":{"tag":"li","lines":"1828,1829","listIndex":2}},{"content":"3. <strong>&#x6539;&#x53d8;&#x4eba;&#x673a;&#x4ea4;&#x4e92;&#x6a21;&#x5f0f;</strong>&#xff1a;&#x6982;&#x7387;&#x5316;&#x7684;&#x8f93;&#x51fa;&#x5141;&#x8bb8;&#x7528;&#x6237;&#x6216;&#x4e0b;&#x6e38;&#x7cfb;&#x7edf;&#x5bf9;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x91c7;&#x53d6;&#x66f4;&#x7ec6;&#x81f4;&#x7684;&#x5e94;&#x5bf9;&#x7b56;&#x7565;&#xff08;&#x5982;&#x5bf9;&#x9ad8;&#x6982;&#x7387;&#x5e7b;&#x89c9;&#x5185;&#x5bb9;&#x8981;&#x6c42;&#x989d;&#x5916;&#x9a8c;&#x8bc1;&#xff09;&#xff0c;&#x800c;&#x4e0d;&#x662f;&#x5168;&#x76d8;&#x63a5;&#x53d7;&#x6216;&#x5426;&#x5b9a;&#xff0c;&#x4ece;&#x800c;&#x589e;&#x5f3a;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#x5e76;&#x4fc3;&#x8fdb;&#x66f4;&#x5b89;&#x5168;&#x7684;&#x90e8;&#x7f72;&#x3002;","children":[],"payload":{"tag":"li","lines":"1829,1831","listIndex":3}}],"payload":{"tag":"li","lines":"1826,1831"}}],"payload":{"tag":"li","lines":"1817,1831","fold":1}}],"payload":{"tag":"h4","lines":"1815,1816"}},{"content":"Detect-then-Calibrate: A Comprehensive Benchmark for Relation Hallucination Evaluation, Analysis and Mitigation in Multimodal Large Language Models","children":[{"content":"","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: Reefknot&#x662f;&#x4e00;&#x4e2a;&#x9488;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x4e2d;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x7684;&#x7efc;&#x5408;&#x57fa;&#x51c6;&#xff0c;&#x5305;&#x542b;&#x8d85;2&#x4e07;&#x4e2a;&#x771f;&#x5b9e;&#x6837;&#x672c;&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x5f53;&#x524d;MLLMs&#x5728;&#x5173;&#x7cfb;&#x63a8;&#x7406;&#x4e0a;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x5e7b;&#x89c9;&#xff0c;&#x5e76;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x79cd;&#x57fa;&#x4e8e;&#x7f6e;&#x4fe1;&#x5ea6;&#x7684;&#x68c0;&#x6d4b;-&#x6821;&#x51c6;&#x65b9;&#x6cd5;&#xff0c;&#x5e73;&#x5747;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;9.75%&#x3002;","children":[],"payload":{"tag":"li","lines":"1832,1833"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x73b0;&#x6709;&#x7814;&#x7a76;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x5bf9;&#x8c61;&#x7ea7;&#x6216;&#x5c5e;&#x6027;&#x7ea7;&#x5e7b;&#x89c9;&#xff0c;&#x4f46;&#x5ffd;&#x7565;&#x4e86;&#x9700;&#x8981;&#x9ad8;&#x7ea7;&#x63a8;&#x7406;&#x7684;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#xff08;&#x5982;&#x7269;&#x4f53;&#x95f4;&#x7684;&#x7a7a;&#x95f4;&#x6216;&#x52a8;&#x4f5c;&#x5173;&#x7cfb;&#xff09;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x51c6;&#x7f3a;&#x4e4f;&#x8be6;&#x7ec6;&#x8bc4;&#x4f30;&#x548c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x7b56;&#x7565;&#xff0c;&#x4e14;&#x6570;&#x636e;&#x96c6;&#x5b58;&#x5728;&#x7cfb;&#x7edf;&#x6807;&#x6ce8;&#x504f;&#x5dee;&#x3002;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x5f71;&#x54cd;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5728;&#x771f;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x9519;&#x8bef;&#x51b3;&#x7b56;&#xff0c;&#x56e0;&#x6b64;&#x4e9f;&#x9700;&#x7cfb;&#x7edf;&#x7814;&#x7a76;&#x3002;","children":[],"payload":{"tag":"li","lines":"1834,1835"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: 1. &#x6570;&#x636e;&#x6784;&#x5efa;&#xff1a;&#x57fa;&#x4e8e;Visual Genome&#x573a;&#x666f;&#x56fe;&#x6570;&#x636e;&#x96c6;&#xff0c;&#x63d0;&#x53d6;&#x8bed;&#x4e49;&#x4e09;&#x5143;&#x7ec4;&#xff08;&#x4e3b;&#x4f53;-&#x5173;&#x7cfb;-&#x5ba2;&#x4f53;&#xff09;&#xff0c;&#x65e0;&#x9700;&#x540e;&#x5904;&#x7406;&#x6216;&#x5408;&#x6210;&#x6570;&#x636e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1835,1836"}}],"payload":{"tag":"li","lines":"1833,1836","fold":1}}],"payload":{"tag":"ul","lines":"1832,1836"}},{"content":"","children":[{"content":"2. &#x5206;&#x7c7b;&#x5b9a;&#x4e49;&#xff1a;&#x5c06;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x5206;&#x4e3a;&#x611f;&#x77e5;&#x578b;&#xff08;&#x5177;&#x4f53;&#x4f4d;&#x7f6e;&#x5173;&#x7cfb;&#xff0c;&#x5982;&#x201c;&#x5728;...&#x4e0a;&#x201d;&#xff09;&#x548c;&#x8ba4;&#x77e5;&#x578b;&#xff08;&#x62bd;&#x8c61;&#x52a8;&#x4f5c;&#x5173;&#x7cfb;&#xff0c;&#x5982;&#x201c;&#x5439;&#x201d;&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1836,1837","listIndex":2}},{"content":"3. &#x8bc4;&#x4f30;&#x4efb;&#x52a1;&#xff1a;&#x8bbe;&#x8ba1;&#x4e09;&#x7c7b;&#x4efb;&#x52a1;&#x2014;&#x2014;&#x4e8c;&#x9009;&#x4e00;&#xff08;Y/N&#xff09;&#x3001;&#x591a;&#x9009;&#x9898;&#xff08;MCQ&#xff09;&#x548c;&#x89c6;&#x89c9;&#x95ee;&#x7b54;&#xff08;VQA&#xff09;&#xff0c;&#x5168;&#x9762;&#x6d4b;&#x8bd5;&#x6a21;&#x578b;&#x80fd;&#x529b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1837,1838","listIndex":3}},{"content":"4. &#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#xff1a;&#x63d0;&#x51fa;&#x201c;&#x68c0;&#x6d4b;-&#x6821;&#x51c6;&#x201d;&#x7b56;&#x7565;&#xff0c;&#x901a;&#x8fc7;&#x5206;&#x6790;&#x6a21;&#x578b;&#x8f93;&#x51fa;token&#x7684;&#x7f6e;&#x4fe1;&#x5ea6;&#xff08;&#x6982;&#x7387;&#x4e0b;&#x964d;&#x81f3;50%&#x5de6;&#x53f3;&#x65f6;&#x6807;&#x8bc6;&#x5e7b;&#x89c9;&#xff09;&#xff0c;&#x5e76;&#x5229;&#x7528;&#x6821;&#x51c6;&#x6280;&#x672f;&#x964d;&#x4f4e;&#x4e2d;&#x7b49;&#x7f6e;&#x4fe1;&#x5ea6;&#x4e0b;&#x7684;&#x5e7b;&#x89c9;&#x3002;","children":[{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: 1. &#x4e3b;&#x6d41;MLLMs&#xff08;&#x5982;LLaVA&#x3001;GPT-4o&#x7b49;&#xff09;&#x5728;Reefknot&#x4e0a;&#x5e73;&#x5747;&#x5e7b;&#x89c9;&#x7387;&#x8f83;&#x9ad8;&#xff08;&#x611f;&#x77e5;&#x578b;&#x6700;&#x9ad8;78%&#xff0c;&#x8ba4;&#x77e5;&#x578b;&#x6700;&#x9ad8;68.5%&#xff09;&#xff0c;&#x4e14;&#x611f;&#x77e5;&#x578b;&#x5e7b;&#x89c9;&#x66f4;&#x4e25;&#x91cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1839,1840"}}],"payload":{"tag":"li","lines":"1838,1840","listIndex":4}},{"content":"5. &#x5e7b;&#x89c9;&#x53d1;&#x751f;&#x65f6;&#xff0c;&#x6a21;&#x578b;&#x8f93;&#x51fa;&#x7f6e;&#x4fe1;&#x5ea6;&#x663e;&#x8457;&#x4e0b;&#x964d;&#xff08;&#x4ece;&#x8fd1;90%&#x964d;&#x81f3;50%&#x5de6;&#x53f3;&#xff09;&#xff0c;&#x8868;&#x660e;&#x4e0d;&#x786e;&#x5b9a;&#x6027;&#x589e;&#x52a0;&#x3002;","children":[],"payload":{"tag":"li","lines":"1840,1841","listIndex":5}},{"content":"6. &#x63d0;&#x51fa;&#x7684;&#x7f13;&#x89e3;&#x65b9;&#x6cd5;&#x5728;&#x4e09;&#x4e2a;&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x5e73;&#x5747;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x7387;9.75%&#xff0c;&#x9a8c;&#x8bc1;&#x4e86;&#x7f6e;&#x4fe1;&#x5ea6;&#x5206;&#x6790;&#x7684;&#x6709;&#x6548;&#x6027;&#x3002;","children":[{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#x662f;MLLMs&#x7684;&#x6838;&#x5fc3;&#x7f3a;&#x9677;&#xff0c;&#x9700;&#x4e13;&#x95e8;&#x57fa;&#x51c6;&#x548c;&#x9488;&#x5bf9;&#x6027;&#x65b9;&#x6cd5;&#x89e3;&#x51b3;&#x3002;Reefknot&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x9760;&#x8bc4;&#x4f30;&#x5de5;&#x5177;&#xff0c;&#x7f6e;&#x4fe1;&#x5ea6;&#x68c0;&#x6d4b;&#x4e3a;&#x672a;&#x6765;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x8be5;&#x5de5;&#x4f5c;&#x63a8;&#x52a8;&#x591a;&#x6a21;&#x6001;AI&#x5411;&#x66f4;&#x53ef;&#x9760;&#x3001;&#x53ef;&#x89e3;&#x91ca;&#x7684;&#x65b9;&#x5411;&#x53d1;&#x5c55;&#xff0c;&#x5bf9;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#x3002;","children":[],"payload":{"tag":"li","lines":"1842,1844"}}],"payload":{"tag":"li","lines":"1841,1844","listIndex":6}}],"payload":{"tag":"ol","lines":"1836,1844"}}],"payload":{"tag":"h4","lines":"1831,1832"}},{"content":"VHExpansion: Automatically Generating Visual Hallucination Test Cases for Multimodal Large Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;&#x4e86;VHExpansion&#xff0c;&#x9996;&#x4e2a;&#x81ea;&#x52a8;&#x5316;&#x751f;&#x6210;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff08;VH&#xff09;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#x7684;&#x6846;&#x67b6;&#x3002;&#x5b83;&#x901a;&#x8fc7;&#x5426;&#x5b9a;&#x95ee;&#x9898;/&#x7b54;&#x6848;&#x3001;&#x5e38;&#x89c1;&#x53ca;&#x5bf9;&#x6297;&#x6027;&#x56fe;&#x50cf;&#x6270;&#x52a8;&#x6269;&#x5c55;&#x521d;&#x59cb;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x5bf9;&#x79f0;&#x51c6;&#x786e;&#x7387;&#x8fd9;&#x4e00;&#x65e0;&#x504f;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x80fd;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x66f4;&#x591a;VH&#x6848;&#x4f8b;&#xff0c;&#x4e14;&#x7528;&#x5176;&#x6269;&#x5c55;&#x6570;&#x636e;&#x5fae;&#x8c03;&#x6a21;&#x578b;&#x80fd;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1845,1846"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x5bb9;&#x6613;&#x51fa;&#x73b0;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff08;VH&#xff09;&#xff0c;&#x5373;&#x4ea7;&#x751f;&#x9519;&#x8bef;&#x7684;&#x89c6;&#x89c9;&#x7ec6;&#x8282;&#x63cf;&#x8ff0;&#xff0c;&#x8fd9;&#x5728;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x3001;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x5e94;&#x7528;&#x4e2d;&#x53ef;&#x80fd;&#x5bfc;&#x81f4;&#x4e25;&#x91cd;&#x540e;&#x679c;&#x3002;&#x73b0;&#x6709;VH&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#x751f;&#x6210;&#x65b9;&#x6cd5;&#x4f9d;&#x8d56;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#xff0c;&#x96be;&#x4ee5;&#x89c4;&#x6a21;&#x5316;&#xff0c;&#x4e14;&#x7f3a;&#x4e4f;&#x9488;&#x5bf9;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#x7684;&#x767d;&#x76d2;&#x5bf9;&#x6297;&#x6d4b;&#x8bd5;&#x3002;&#x56e0;&#x6b64;&#xff0c;&#x9700;&#x8981;&#x81ea;&#x52a8;&#x5316;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#x751f;&#x6210;&#x65b9;&#x6cd5;&#x4ee5;&#x5168;&#x9762;&#x8bc4;&#x4f30;&#x548c;&#x7f13;&#x89e3;VH&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1847,1848"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;VHExpansion&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x4e09;&#x79cd;&#x65b9;&#x5f0f;&#x81ea;&#x52a8;&#x6269;&#x5c55;VH&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#xff08;&#x56fe;&#x50cf;&#x3001;&#x95ee;&#x9898;&#x3001;&#x7b54;&#x6848;&#x4e09;&#x5143;&#x7ec4;&#xff09;&#xff1a;1&#xff09;&#x5426;&#x5b9a;&#xff1a;&#x4f7f;&#x7528;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LLM&#xff09;&#x81ea;&#x52a8;&#x751f;&#x6210;&#x539f;&#x95ee;&#x9898;&#x7684;&#x5426;&#x5b9a;&#x5f62;&#x5f0f;&#x53ca;&#x5bf9;&#x5e94;&#x5426;&#x5b9a;&#x7b54;&#x6848;&#xff1b;2&#xff09;&#x5e38;&#x89c1;&#x56fe;&#x50cf;&#x6270;&#x52a8;&#xff1a;&#x5bf9;&#x56fe;&#x50cf;&#x5e94;&#x7528;JPEG&#x538b;&#x7f29;&#x3001;&#x9ad8;&#x65af;&#x566a;&#x58f0;&#x7b49;&#x5e38;&#x89c1;&#x64cd;&#x4f5c;&#xff1b;3&#xff09;&#x5bf9;&#x6297;&#x6027;&#x56fe;&#x50cf;&#x6270;&#x52a8;&#xff1a;&#x901a;&#x8fc7;&#x6295;&#x5f71;&#x68af;&#x5ea6;&#x4e0b;&#x964d;&#xff08;PGD&#xff09;&#x6216;&#x8fed;&#x4ee3;&#x5feb;&#x901f;&#x68af;&#x5ea6;&#x7b26;&#x53f7;&#x6cd5;&#xff08;FGSM&#xff09;&#x6dfb;&#x52a0;&#x4eba;&#x773c;&#x4e0d;&#x53ef;&#x89c1;&#x7684;&#x6270;&#x52a8;&#xff0c;&#x4f7f;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x8f93;&#x51fa;&#x663e;&#x8457;&#x53d8;&#x5316;&#x7684;&#x5d4c;&#x5165;&#x5411;&#x91cf;&#x3002;&#x6b64;&#x5916;&#xff0c;&#x63d0;&#x51fa;&#x5bf9;&#x79f0;&#x51c6;&#x786e;&#x7387;&#x6307;&#x6807;&#xff0c;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x5bf9;&#x539f;&#x59cb;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#x53ca;&#x5176;&#x5426;&#x5b9a;&#x7248;&#x672c;&#x540c;&#x65f6;&#x56de;&#x7b54;&#x6b63;&#x786e;&#x7684;&#x6bd4;&#x4f8b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1848,1849"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5173;&#x952e;&#x7ed3;&#x679c;&#x5305;&#x62ec;&#xff1a;1&#xff09;VHExpansion&#x6210;&#x529f;&#x6269;&#x5c55;&#x4e86;&#x4e09;&#x4e2a;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x7684;VH&#x6570;&#x636e;&#x96c6;&#xff0c;&#x4e3a;7&#x4e2a;MLLM&#x751f;&#x6210;&#x66f4;&#x591a;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#xff0c;&#x6709;&#x6548;&#x8bc6;&#x522b;&#x51fa;&#x66f4;&#x591a;VH&#x95ee;&#x9898;&#xff1b;2&#xff09;&#x5bf9;&#x79f0;&#x51c6;&#x786e;&#x7387;&#x88ab;&#x8bc1;&#x660e;&#x5728;&#x6a21;&#x578b;&#x968f;&#x673a;&#x731c;&#x6d4b;&#x65f6;&#x4e0d;&#x53d7;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#x7b54;&#x6848;&#x5206;&#x5e03;&#x4e0d;&#x5e73;&#x8861;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x800c;&#x4f20;&#x7edf;&#x51c6;&#x786e;&#x7387;&#x5219;&#x6613;&#x53d7;&#x5e72;&#x6270;&#xff08;&#x4f8b;&#x5982;Cambrian-1&#x5728;POPE&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x4f20;&#x7edf;&#x51c6;&#x786e;&#x7387;&#x9ad8;&#x4e8e;LLaVA-NeXT&#xff0c;&#x4f46;&#x5bf9;&#x79f0;&#x51c6;&#x786e;&#x7387;&#x66f4;&#x4f4e;&#xff09;&#xff1b;3&#xff09;&#x4f7f;&#x7528;VHExpansion&#x6269;&#x5c55;&#x7684;&#x6570;&#x636e;&#x5fae;&#x8c03;MLLM&#xff08;&#x5982;LLaVA-1.5&#xff09;&#x80fd;&#x663e;&#x8457;&#x63d0;&#x5347;&#x5bf9;&#x79f0;&#x51c6;&#x786e;&#x7387;&#xff08;&#x4ece;0.180&#x5347;&#x81f3;0.711&#xff09;&#xff0c;&#x4e14;&#x4e0d;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x5728;&#x5176;&#x4ed6;&#x901a;&#x7528;VQA&#x6570;&#x636e;&#x96c6;&#x4e0a;&#x7684;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1849,1850"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x662f;&#xff1a;VHExpansion&#x4e3a;MLLM&#x7684;VH&#x6d4b;&#x8bd5;&#x63d0;&#x4f9b;&#x4e86;&#x9996;&#x4e2a;&#x81ea;&#x52a8;&#x5316;&#x3001;&#x53ef;&#x6269;&#x5c55;&#x7684;&#x89e3;&#x51b3;&#x65b9;&#x6848;&#xff0c;&#x80fd;&#x9ad8;&#x6548;&#x751f;&#x6210;&#x591a;&#x6837;&#x5316;&#x6d4b;&#x8bd5;&#x7528;&#x4f8b;&#xff1b;&#x5bf9;&#x79f0;&#x51c6;&#x786e;&#x7387;&#x4f5c;&#x4e3a;&#x65e0;&#x504f;&#x6307;&#x6807;&#x80fd;&#x66f4;&#x53ef;&#x9760;&#x5730;&#x8bc4;&#x4f30;&#x6a21;&#x578b;&#x5bf9;VH&#x7684;&#x8106;&#x5f31;&#x6027;&#xff1b;&#x57fa;&#x4e8e;&#x6269;&#x5c55;&#x6570;&#x636e;&#x7684;&#x5fae;&#x8c03;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x89c6;&#x89c9;&#x5e7b;&#x89c9;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x6a21;&#x578b;&#x901a;&#x7528;&#x80fd;&#x529b;&#x3002;&#x8fd9;&#x9879;&#x5de5;&#x4f5c;&#x5bf9;&#x63d0;&#x5347;MLLM&#x7684;&#x5b89;&#x5168;&#x90e8;&#x7f72;&#x5177;&#x6709;&#x91cd;&#x8981;&#x610f;&#x4e49;&#xff0c;&#x5e76;&#x4e3a;&#x672a;&#x6765;&#x81ea;&#x52a8;&#x5316;&#x6d4b;&#x8bd5;&#x548c;&#x6a21;&#x578b;&#x6539;&#x8fdb;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;","children":[],"payload":{"tag":"li","lines":"1850,1852"}}],"payload":{"tag":"li","lines":"1846,1852","fold":1}}],"payload":{"tag":"h4","lines":"1844,1845"}},{"content":"HLPU: Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x9488;&#x5bf9;&#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5728;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x8d28;&#x91cf;&#x8bc4;&#x4f30;&#xff09;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x9996;&#x4e2a;&#x4e13;&#x6ce8;&#x4e8e;&#x8be5;&#x95ee;&#x9898;&#x7684;HLPU&#x6307;&#x4ee4;&#x6570;&#x636e;&#x5e93;&#xff08;&#x542b;20&#x4e07;&#x95ee;&#x7b54;&#x5bf9;&#xff09;&#x3001;SAFEQA&#x6a21;&#x578b;&#xff08;&#x6574;&#x5408;&#x56fe;&#x50cf;&#x3001;&#x663e;&#x8457;&#x533a;&#x57df;&#x548c;&#x8d28;&#x91cf;&#x7279;&#x5f81;&#xff09;&#x4ee5;&#x53ca;ESA-PO&#x8bad;&#x7ec3;&#x6846;&#x67b6;&#xff08;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x81ea;&#x77e5;&#x6027;&#xff09;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#x8be5;&#x65b9;&#x6cd5;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x51c6;&#x786e;&#x6027;&#x548c;&#x81ea;&#x77e5;&#x6027;&#xff0c;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x3002;","children":[],"payload":{"tag":"li","lines":"1853,1854"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MLLMs&#xff09;&#x5728;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x4e0e;&#x7406;&#x89e3;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x56fe;&#x50cf;&#x8d28;&#x91cf;&#x8bc4;&#x4f30;&#x3001;&#x7f8e;&#x5b66;&#x8bc4;&#x4ef7;&#xff09;&#x4e2d;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x9519;&#x8bef;&#x6216;&#x8352;&#x8c2c;&#x56de;&#x7b54;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5c1a;&#x672a;&#x5728;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x9886;&#x57df;&#x5f97;&#x5230;&#x5145;&#x5206;&#x7814;&#x7a76;&#xff0c;&#x4f46;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x6a21;&#x578b;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x963b;&#x788d;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#xff08;&#x5982;&#x63a8;&#x8350;&#x7cfb;&#x7edf;&#x3001;&#x76f8;&#x673a;&#x8f85;&#x52a9;&#xff09;&#x3002;&#x6839;&#x672c;&#x539f;&#x56e0;&#x5728;&#x4e8e;&#x6a21;&#x578b;&#x7f3a;&#x4e4f;&#x5bf9;&#x81ea;&#x8eab;&#x77e5;&#x8bc6;&#x8fb9;&#x754c;&#x7684;&#x6e05;&#x6670;&#x8ba4;&#x77e5;&#xff08;&#x81ea;&#x77e5;&#x6027;&#xff09;&#xff0c;&#x65e0;&#x6cd5;&#x533a;&#x5206;&#x5df2;&#x77e5;&#x4e0e;&#x672a;&#x77e5;&#x4fe1;&#x606f;&#x3002;","children":[],"payload":{"tag":"li","lines":"1855,1856"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x901a;&#x8fc7;&#x4e09;&#x90e8;&#x5206;&#x89e3;&#x51b3;&#x8be5;&#x95ee;&#x9898;&#xff1a;1) &#x6784;&#x5efa;HLPU&#x6307;&#x4ee4;&#x6570;&#x636e;&#x5e93;&#xff08;&#x7ea6;20&#x4e07;&#x95ee;&#x7b54;&#x5bf9;&#xff09;&#xff0c;&#x5305;&#x542b;&#x56db;&#x79cd;&#x6307;&#x4ee4;&#x7c7b;&#x578b;&#xff0c;&#x4e13;&#x95e8;&#x9488;&#x5bf9;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff1b;2) &#x63d0;&#x51fa;SAFEQA&#x6a21;&#x578b;&#xff0c;&#x5229;&#x7528;&#x56fe;&#x50cf;&#x7279;&#x5f81;&#x3001;&#x663e;&#x8457;&#x533a;&#x57df;&#x7279;&#x5f81;&#x548c;&#x8d28;&#x91cf;&#x7279;&#x5f81;&#x589e;&#x5f3a;&#x5bf9;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x7684;&#x611f;&#x77e5;&#x80fd;&#x529b;&#xff1b;3) &#x8bbe;&#x8ba1;ESA-PO&#xff08;&#x589e;&#x5f3a;&#x81ea;&#x77e5;&#x6027;&#x504f;&#x597d;&#x4f18;&#x5316;&#xff09;&#x8bad;&#x7ec3;&#x6846;&#x67b6;&#xff0c;&#x901a;&#x8fc7;&#x5f15;&#x5165;&#x201c;&#x6211;&#x4e0d;&#x77e5;&#x9053;&#x201d;&#x4f5c;&#x4e3a;&#x6b21;&#x4f18;&#x504f;&#x597d;&#x9009;&#x9879;&#xff0c;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#x8bc6;&#x522b;&#x77e5;&#x8bc6;&#x8fb9;&#x754c;&#xff0c;&#x62d2;&#x7edd;&#x56de;&#x7b54;&#x8d85;&#x51fa;&#x80fd;&#x529b;&#x7684;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1856,1857"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x7ed3;&#x679c;&#x8868;&#x660e;&#xff1a;1) HLPU&#x6570;&#x636e;&#x5e93;&#x6709;&#x6548;&#x63ed;&#x793a;&#x4e86;&#x6a21;&#x578b;&#x5728;&#x4e0d;&#x540c;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#xff08;&#x5982;&#x5931;&#x771f;&#x7c7b;&#x578b;&#x3001;&#x989c;&#x8272;&#x3001;&#x4eae;&#x5ea6;&#xff09;&#x548c;&#x95ee;&#x9898;&#x7c7b;&#x578b;&#x4e0a;&#x7684;&#x81ea;&#x77e5;&#x6027;&#x5dee;&#x5f02;&#xff1b;2) SAFEQA&#x6a21;&#x578b;&#x5728;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x611f;&#x77e5;&#x4e0a;&#x4f18;&#x4e8e;&#x57fa;&#x51c6;&#x65b9;&#x6cd5;&#xff1b;3) ESA-PO&#x6846;&#x67b6;&#x663e;&#x8457;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x81ea;&#x77e5;&#x6027;&#xff0c;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#xff0c;&#x540c;&#x65f6;&#x5728;&#x591a;&#x9879;&#x8bc4;&#x4f30;&#x6307;&#x6807;&#x4e0a;&#x8d85;&#x8d8a;&#x95ed;&#x6e90;&#x6a21;&#x578b;&#xff08;&#x5982;GPT&#x7cfb;&#x5217;&#xff09;&#x548c;&#x5f00;&#x6e90;&#x6a21;&#x578b;&#xff08;&#x5982;LLaVA&#xff09;&#x3002;","children":[],"payload":{"tag":"li","lines":"1857,1858"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8be5;&#x7814;&#x7a76;&#x9996;&#x6b21;&#x7cfb;&#x7edf;&#x6027;&#x5730;&#x89e3;&#x51b3;&#x4e86;MLLMs&#x5728;&#x4f4e;&#x7ea7;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5f3a;&#x8c03;&#x6a21;&#x578b;&#x81ea;&#x77e5;&#x6027;&#x662f;&#x5173;&#x952e;&#x3002;&#x6240;&#x63d0;&#x51fa;&#x7684;&#x6570;&#x636e;&#x5e93;&#x3001;&#x6a21;&#x578b;&#x548c;&#x8bad;&#x7ec3;&#x6846;&#x67b6;&#x4e3a;&#x63d0;&#x5347;MLLMs&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x56fe;&#x50cf;&#x8d28;&#x91cf;&#x8bc4;&#x4f30;&#x3001;&#x7f8e;&#x5b66;&#x5206;&#x6790;&#x7b49;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x4e2d;&#x5177;&#x6709;&#x91cd;&#x8981;&#x4ef7;&#x503c;&#x3002;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x66f4;&#x5e7f;&#x6cdb;&#x7684;&#x89c6;&#x89c9;&#x4efb;&#x52a1;&#xff0c;&#x63a8;&#x52a8;&#x53ef;&#x4fe1;AI&#x7cfb;&#x7edf;&#x7684;&#x53d1;&#x5c55;&#x3002;","children":[],"payload":{"tag":"li","lines":"1858,1861"}}],"payload":{"tag":"li","lines":"1854,1861","fold":1}}],"payload":{"tag":"h4","lines":"1852,1853"}},{"content":"VisionWeaver: Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x9488;&#x5bf9;&#x5927;&#x578b;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x63d0;&#x51fa;&#x4e86;&#x4e00;&#x4e2a;&#x7ec6;&#x7c92;&#x5ea6;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;VHBench-10&#x548c;&#x4e00;&#x79cd;&#x52a8;&#x6001;&#x7279;&#x5f81;&#x878d;&#x5408;&#x65b9;&#x6cd5;VisionWeaver&#x3002;&#x7814;&#x7a76;&#x53d1;&#x73b0;&#x4e0d;&#x540c;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x5b58;&#x5728;&#x72ec;&#x7279;&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#xff0c;&#x800c;&#x7b80;&#x5355;&#x7279;&#x5f81;&#x878d;&#x5408;&#x6548;&#x679c;&#x4e0d;&#x4f73;&#x3002;VisionWeaver&#x901a;&#x8fc7;&#x4e0a;&#x4e0b;&#x6587;&#x611f;&#x77e5;&#x8def;&#x7531;&#x52a8;&#x6001;&#x805a;&#x5408;&#x591a;&#x4e2a;&#x4e13;&#x5bb6;&#x7f16;&#x7801;&#x5668;&#x7684;&#x7279;&#x5f81;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1862,1863"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x8bba;&#x6587;&#x65e8;&#x5728;&#x89e3;&#x51b3;LVLM&#x4e2d;&#x666e;&#x904d;&#x5b58;&#x5728;&#x7684;&#x7269;&#x4f53;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x6a21;&#x578b;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x865a;&#x5047;&#x63cf;&#x8ff0;&#x3002;&#x8be5;&#x95ee;&#x9898;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;LVLM&#x5728;&#x73b0;&#x5b9e;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x5e94;&#x7528;&#x4ef7;&#x503c;&#x3002;&#x7814;&#x7a76;&#x91cd;&#x70b9;&#x5173;&#x6ce8;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x9009;&#x62e9;&#x5bf9;&#x5e7b;&#x89c9;&#x884c;&#x4e3a;&#x7684;&#x5f71;&#x54cd;&#xff0c;&#x8ba4;&#x4e3a;&#x4e0d;&#x540c;&#x7f16;&#x7801;&#x5668;&#x56e0;&#x8bad;&#x7ec3;&#x8303;&#x5f0f;&#x548c;&#x67b6;&#x6784;&#x5dee;&#x5f02;&#x4f1a;&#x5f15;&#x5165;&#x4e0d;&#x540c;&#x7684;&#x5f52;&#x7eb3;&#x504f;&#x5dee;&#xff0c;&#x5bfc;&#x81f4;&#x72ec;&#x7279;&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#x3002;&#x73b0;&#x6709;&#x57fa;&#x51c6;&#xff08;&#x5982;POPE&#xff09;&#x4ec5;&#x5173;&#x6ce8;&#x7c97;&#x7c92;&#x5ea6;&#x7684;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x68c0;&#x6d4b;&#xff0c;&#x65e0;&#x6cd5;&#x8bca;&#x65ad;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7f3a;&#x9677;&#x3002;","children":[],"payload":{"tag":"li","lines":"1864,1865"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x9996;&#x5148;&#x6784;&#x5efa;&#x4e86;VHBench-10&#x57fa;&#x51c6;&#xff0c;&#x5305;&#x542b;10&#x4e2a;&#x7ec6;&#x7c92;&#x5ea6;&#x5e7b;&#x89c9;&#x7c7b;&#x522b;&#xff08;&#x5982;&#x989c;&#x8272;&#x3001;&#x5f62;&#x72b6;&#x3001;&#x6587;&#x672c;&#x8bc6;&#x522b;&#x3001;&#x7a7a;&#x95f4;&#x5173;&#x7cfb;&#x7b49;&#xff09;&#xff0c;&#x6db5;&#x76d6;&#x68c0;&#x6d4b;&#x3001;&#x5206;&#x5272;&#x3001;&#x5b9a;&#x4f4d;&#x548c;&#x5206;&#x7c7b;&#x56db;&#x5927;&#x89c6;&#x89c9;&#x80fd;&#x529b;&#x3002;&#x8be5;&#x57fa;&#x51c6;&#x5305;&#x542b;&#x7ea6;10,000&#x4e2a;&#x6837;&#x672c;&#xff0c;&#x6bcf;&#x4e2a;&#x6837;&#x672c;&#x5305;&#x542b;&#x56fe;&#x50cf;&#x3001;&#x771f;&#x5b9e;&#x63cf;&#x8ff0;&#x548c;&#x7279;&#x5b9a;&#x7c7b;&#x578b;&#x7684;&#x5e7b;&#x89c9;&#x63cf;&#x8ff0;&#x3002;&#x57fa;&#x4e8e;&#x8be5;&#x57fa;&#x51c6;&#x7684;&#x5206;&#x6790;&#x53d1;&#x73b0;&#x4e0d;&#x540c;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff08;&#x5982;CLIP&#x3001;DINOv2&#x3001;Vary&#xff09;&#x5728;&#x7279;&#x5b9a;&#x4efb;&#x52a1;&#x4e0a;&#x8868;&#x73b0;&#x5dee;&#x5f02;&#x663e;&#x8457;&#x3002;&#x9488;&#x5bf9;&#x7b80;&#x5355;&#x7279;&#x5f81;&#x878d;&#x5408;&#x7684;&#x4e0d;&#x8db3;&#xff0c;&#x63d0;&#x51fa;&#x4e86;VisionWeaver&#x2014;&#x2014;&#x4e00;&#x79cd;&#x4e0a;&#x4e0b;&#x6587;&#x611f;&#x77e5;&#x8def;&#x7531;&#x7f51;&#x7edc;&#xff1a;&#x4f7f;&#x7528;CLIP&#x7684;[CLS]&#x4ee4;&#x724c;&#x7279;&#x5f81;&#x4f5c;&#x4e3a;&#x5168;&#x5c40;&#x4e0a;&#x4e0b;&#x6587;&#x4fe1;&#x53f7;&#xff0c;&#x52a8;&#x6001;&#x751f;&#x6210;&#x8def;&#x7531;&#x6743;&#x91cd;&#xff0c;&#x805a;&#x5408;&#x591a;&#x4e2a;&#x4e13;&#x7528;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff08;&#x4e13;&#x5bb6;&#xff09;&#x7684;&#x7279;&#x5f81;&#xff0c;&#x4ee5;&#x9002;&#x5e94;&#x4e0d;&#x540c;&#x4efb;&#x52a1;&#x9700;&#x6c42;&#x3002;","children":[],"payload":{"tag":"li","lines":"1865,1866"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1&#xff09;&#x4e0d;&#x540c;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x5728;VHBench-10&#x4e0a;&#x8868;&#x73b0;&#x51fa;&#x72ec;&#x7279;&#x7684;&#x5e7b;&#x89c9;&#x6a21;&#x5f0f;&#xff08;&#x5982;Vary&#x5728;&#x6587;&#x672c;&#x4efb;&#x52a1;&#x4e0a;&#x5e7b;&#x89c9;&#x7387;&#x4f4e;&#xff0c;&#x4f46;&#x5728;&#x5176;&#x4ed6;&#x4efb;&#x52a1;&#x4e0a;&#x8f83;&#x5dee;&#xff09;&#xff1b;2&#xff09;&#x7b80;&#x5355;&#x7279;&#x5f81;&#x878d;&#x5408;&#xff08;&#x5982;&#x62fc;&#x63a5;&#x6216;&#x76f8;&#x52a0;&#xff09;&#x6548;&#x679c;&#x4e0d;&#x5982;&#x5355;&#x72ec;&#x4f7f;&#x7528;&#x6700;&#x4f73;&#x7f16;&#x7801;&#x5668;&#xff1b;3&#xff09;VisionWeaver&#x5728;VHBench-10&#x7684;&#x6240;&#x6709;10&#x4e2a;&#x5b50;&#x4efb;&#x52a1;&#x4e0a;&#x5747;&#x53d6;&#x5f97;&#x6700;&#x4f4e;&#x9519;&#x8bef;&#x7387;&#xff0c;&#x540c;&#x65f6;&#x5728;POPE&#x3001;AutoHallusion&#x7b49;&#x73b0;&#x6709;&#x57fa;&#x51c6;&#x548c;&#x901a;&#x7528;LVLM&#x57fa;&#x51c6;&#x4e0a;&#x5747;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x5e76;&#x63d0;&#x5347;&#x6574;&#x4f53;&#x6027;&#x80fd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1866,1867"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x9009;&#x62e9;&#x76f4;&#x63a5;&#x5f71;&#x54cd;LVLM&#x7684;&#x5e7b;&#x89c9;&#x884c;&#x4e3a;&#xff0c;&#x5176;&#x5f52;&#x7eb3;&#x504f;&#x5dee;&#x5bfc;&#x81f4;&#x4efb;&#x52a1;&#x7279;&#x5f02;&#x6027;&#x8868;&#x73b0;&#xff1b;2&#xff09;VHBench-10&#x80fd;&#x591f;&#x7cbe;&#x7ec6;&#x8bca;&#x65ad;&#x89c6;&#x89c9;&#x611f;&#x77e5;&#x7f3a;&#x9677;&#xff0c;&#x4e3a;&#x6a21;&#x578b;&#x6539;&#x8fdb;&#x63d0;&#x4f9b;&#x9488;&#x5bf9;&#x6027;&#x6307;&#x5bfc;&#xff1b;3&#xff09;VisionWeaver&#x901a;&#x8fc7;&#x52a8;&#x6001;&#x8def;&#x7531;&#x673a;&#x5236;&#x6709;&#x6548;&#x6574;&#x5408;&#x591a;&#x7f16;&#x7801;&#x5668;&#x4f18;&#x52bf;&#xff0c;&#x4e3a;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63a8;&#x52a8;&#x66f4;&#x53ef;&#x9760;&#x7684;LVLM&#x8bbe;&#x8ba1;&#xff0c;&#x4fc3;&#x8fdb;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x5e94;&#x7528;&#x3002;","children":[],"payload":{"tag":"li","lines":"1867,1869"}}],"payload":{"tag":"li","lines":"1863,1869","fold":1}}],"payload":{"tag":"h4","lines":"1861,1862"}}],"payload":{"tag":"h2","lines":"1814,1815","fold":1}},{"content":"&#x65e0;&#x6cd5;&#x5206;&#x7c7b;","children":[{"content":"MOCHa: Mitigating Open-Vocabulary Caption Hallucinations","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x8be5;&#x8bba;&#x6587;&#x63d0;&#x51fa;&#x4e86;OpenCHAIR&#x57fa;&#x51c6;&#x548c;MOCHa&#x6846;&#x67b6;&#xff0c;&#x7528;&#x4e8e;&#x5728;&#x5f00;&#x653e;&#x8bcd;&#x6c47;&#x8bbe;&#x7f6e;&#x4e0b;&#x91cf;&#x5316;&#x548c;&#x7f13;&#x89e3;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;OpenCHAIR&#x5229;&#x7528;&#x751f;&#x6210;&#x6a21;&#x578b;&#x521b;&#x5efa;&#x591a;&#x6837;&#x5316;&#x6570;&#x636e;&#x96c6;&#xff0c;MOCHa&#x5219;&#x901a;&#x8fc7;&#x591a;&#x76ee;&#x6807;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x4f18;&#x5316;&#x63cf;&#x8ff0;&#x7684;&#x5fe0;&#x5b9e;&#x6027;&#x548c;&#x5145;&#x5206;&#x6027;&#xff0c;&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1872,1873"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x4efb;&#x52a1;&#x4e2d;&#xff0c;&#x5f53;&#x524d;&#x6700;&#x5148;&#x8fdb;&#x7684;&#x6a21;&#x578b;&#x5b58;&#x5728;&#x4e25;&#x91cd;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x4e0e;&#x56fe;&#x50cf;&#x65e0;&#x5173;&#x7684;&#x865a;&#x5047;&#x7ec6;&#x8282;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x57fa;&#x4e8e;&#x5c01;&#x95ed;&#x8bcd;&#x6c47;&#x8868;&#xff0c;&#x65e0;&#x6cd5;&#x8986;&#x76d6;&#x5b9e;&#x8df5;&#x4e2d;&#x957f;&#x5c3e;&#x7684;&#x5e7b;&#x89c9;&#x7c7b;&#x578b;&#xff0c;&#x9650;&#x5236;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x5b9e;&#x7528;&#x6027;&#x548c;&#x53ef;&#x9760;&#x6027;&#x3002;&#x89e3;&#x51b3;&#x5f00;&#x653e;&#x8bcd;&#x6c47;&#x4e0b;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x5bf9;&#x4e8e;&#x63d0;&#x5347;&#x751f;&#x6210;&#x5185;&#x5bb9;&#x7684;&#x771f;&#x5b9e;&#x6027;&#x548c;&#x7528;&#x6237;&#x4fe1;&#x4efb;&#x81f3;&#x5173;&#x91cd;&#x8981;&#x3002;","children":[],"payload":{"tag":"li","lines":"1874,1875"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;&#x4e24;&#x4e2a;&#x6838;&#x5fc3;&#x8d21;&#x732e;&#xff1a;1) OpenCHAIR&#x57fa;&#x51c6;&#xff1a;&#x4f7f;&#x7528;Llama-2&#x751f;&#x6210;&#x591a;&#x6837;&#x5316;&#x63cf;&#x8ff0;&#xff0c;&#x901a;&#x8fc7;&#x53e5;&#x6cd5;&#x89e3;&#x6790;&#x7b5b;&#x9009;&#x5177;&#x4f53;&#x540d;&#x8bcd;&#xff0c;&#x5229;&#x7528;Stable Diffusion XL&#x751f;&#x6210;&#x5bf9;&#x5e94;&#x56fe;&#x50cf;&#xff0c;&#x6784;&#x5efa;&#x5305;&#x542b;&#x7ea6;5000&#x4e2a;&#x56fe;&#x50cf;-&#x63cf;&#x8ff0;&#x5bf9;&#x7684;&#x6570;&#x636e;&#x96c6;&#xff1b;&#x8bc4;&#x4f30;&#x65f6;&#x4f7f;&#x7528;LLM&#x5224;&#x65ad;&#x751f;&#x6210;&#x63cf;&#x8ff0;&#x4e2d;&#x7684;&#x5bf9;&#x8c61;&#x662f;&#x5426;&#x5b58;&#x5728;&#x4e8e;&#x771f;&#x5b9e;&#x63cf;&#x8ff0;&#x4e2d;&#xff0c;&#x907f;&#x514d;&#x5c01;&#x95ed;&#x8bcd;&#x6c47;&#x8868;&#x7684;&#x9650;&#x5236;&#x3002;2) MOCHa&#x6846;&#x67b6;&#xff1a;&#x57fa;&#x4e8e;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#xff08;PPO&#x7b97;&#x6cd5;&#xff09;&#xff0c;&#x8bbe;&#x8ba1;&#x591a;&#x76ee;&#x6807;&#x5956;&#x52b1;&#x51fd;&#x6570;&#xff0c;&#x8054;&#x5408;&#x4f18;&#x5316;&#x5fe0;&#x5b9e;&#x6027;&#xff08;&#x901a;&#x8fc7;&#x81ea;&#x7136;&#x8bed;&#x8a00;&#x63a8;&#x7406;&#x6a21;&#x578b;&#x8bc4;&#x4f30;&#xff09;&#x548c;&#x5145;&#x5206;&#x6027;&#xff08;&#x901a;&#x8fc7;BERTScore&#x8bc4;&#x4f30;&#xff09;&#xff0c;&#x540c;&#x65f6;&#x4f7f;&#x7528;KL&#x6563;&#x5ea6;&#x60e9;&#x7f5a;&#x4fdd;&#x6301;&#x751f;&#x6210;&#x591a;&#x6837;&#x6027;&#xff0c;&#x65e0;&#x9700;&#x4eba;&#x5de5;&#x6807;&#x6ce8;&#x5373;&#x53ef;&#x81ea;&#x52a8;&#x4f18;&#x5316;&#x591a;&#x79cd;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x3002;","children":[],"payload":{"tag":"li","lines":"1875,1876"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;1) OpenCHAIR&#x5728;&#x591a;&#x6837;&#x6027;&#x548c;&#x51c6;&#x786e;&#x6027;&#x4e0a;&#x5747;&#x4f18;&#x4e8e;&#x5c01;&#x95ed;&#x8bcd;&#x6c47;&#x57fa;&#x51c6;CHAIR&#xff0c;&#x80fd;&#x66f4;&#x5168;&#x9762;&#x6355;&#x83b7;&#x5f00;&#x653e;&#x8bcd;&#x6c47;&#x5e7b;&#x89c9;&#xff1b;2) MOCHa&#x663e;&#x8457;&#x964d;&#x4f4e;&#x4e86;&#x591a;&#x79cd;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x6a21;&#x578b;&#xff08;&#x5982;BLIP-2&#xff09;&#x7684;&#x5e7b;&#x89c9;&#x7387;&#xff0c;&#x5728;OpenCHAIR&#x4e0a;&#x5e73;&#x5747;&#x63d0;&#x5347;&#x7ea6;15%&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x63cf;&#x8ff0;&#x7684;&#x5145;&#x5206;&#x6027;&#x548c;&#x6d41;&#x7545;&#x6027;&#xff1b;3) MOCHa&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x7ed3;&#x6784;&#x5373;&#x53ef;&#x7075;&#x6d3b;&#x5e94;&#x7528;&#x4e8e;&#x4e0d;&#x540c;&#x89c4;&#x6a21;&#x7684;&#x6a21;&#x578b;&#xff0c;&#x4e14;&#x65e0;&#x9700;&#x5f3a;&#x76d1;&#x7763;&#x4fe1;&#x53f7;&#x3002;","children":[],"payload":{"tag":"li","lines":"1876,1877"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x8bc1;&#x660e;&#x4e86;&#x5f00;&#x653e;&#x8bcd;&#x6c47;&#x8bbe;&#x5b9a;&#x5bf9;&#x5168;&#x9762;&#x8bc4;&#x4f30;&#x548c;&#x7f13;&#x89e3;&#x5e7b;&#x89c9;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;OpenCHAIR&#x63d0;&#x4f9b;&#x4e86;&#x66f4;&#x53ef;&#x9760;&#x7684;&#x8bc4;&#x4f30;&#x57fa;&#x51c6;&#xff0c;&#x800c;MOCHa&#x901a;&#x8fc7;&#x591a;&#x76ee;&#x6807;&#x5f3a;&#x5316;&#x5b66;&#x4e60;&#x6709;&#x6548;&#x5e73;&#x8861;&#x4e86;&#x5fe0;&#x5b9e;&#x6027;&#x4e0e;&#x63cf;&#x8ff0;&#x6027;&#xff0c;&#x4e3a;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x63d0;&#x4f9b;&#x4e86;&#x53ef;&#x4fe1;&#x4efb;&#x7684;&#x56fe;&#x50cf;&#x63cf;&#x8ff0;&#x751f;&#x6210;&#x65b9;&#x6848;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5bf9;&#x63a8;&#x52a8;&#x89c6;&#x89c9;-&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x90e8;&#x7f72;&#x5177;&#x6709;&#x91cd;&#x8981;&#x5f71;&#x54cd;&#xff0c;&#x672a;&#x6765;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x89c6;&#x9891;&#x63cf;&#x8ff0;&#x6216;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x4efb;&#x52a1;&#x3002;","children":[],"payload":{"tag":"li","lines":"1877,1889"}}],"payload":{"tag":"li","lines":"1873,1889","fold":1}}],"payload":{"tag":"h4","lines":"1871,1872"}},{"content":"EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: EAGLE&#x662f;&#x4e00;&#x79cd;&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x63a5;&#x5730;&#x80fd;&#x529b;&#x6765;&#x51cf;&#x5c11;&#x591a;&#x6a21;&#x6001;&#x6307;&#x4ee4;&#x6a21;&#x578b;&#xff08;IT-VLM&#xff09;&#x5e7b;&#x89c9;&#x7684;&#x540e;&#x9884;&#x8bad;&#x7ec3;&#x65b9;&#x6cd5;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x901a;&#x8fc7;&#x6539;&#x8fdb;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#xff0c;&#x4f7f;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x80fd;&#x66f4;&#x597d;&#x5730;&#x6355;&#x6349;&#x5bf9;&#x8c61;&#x7ec6;&#x8282;&#xff0c;&#x65e0;&#x9700;&#x4fee;&#x6539;&#x6a21;&#x578b;&#x67b6;&#x6784;&#x5373;&#x53ef;&#x76f4;&#x63a5;&#x96c6;&#x6210;&#x5230;&#x73b0;&#x6709;IT-VLM&#x4e2d;&#xff0c;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1890,1891"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x591a;&#x6a21;&#x6001;&#x6307;&#x4ee4;&#x6a21;&#x578b;&#xff08;IT-VLM&#xff09;&#x5728;&#x751f;&#x6210;&#x54cd;&#x5e94;&#x65f6;&#x7ecf;&#x5e38;&#x4ea7;&#x751f;&#x4e0e;&#x56fe;&#x50cf;&#x771f;&#x5b9e;&#x5185;&#x5bb9;&#x4e0d;&#x7b26;&#x7684;&#x5e7b;&#x89c9;&#xff08;hallucinations&#xff09;&#xff0c;&#x8fd9;&#x964d;&#x4f4e;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x548c;&#x53ef;&#x4fe1;&#x5ea6;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x4e3b;&#x8981;&#x5173;&#x6ce8;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x6216;&#x878d;&#x5408;&#x6a21;&#x5757;&#x7684;&#x4f18;&#x5316;&#xff0c;&#x4f46;&#x5ffd;&#x7565;&#x4e86;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x5c40;&#x9650;&#x6027;&#x3002;EAGLE&#x65e8;&#x5728;&#x901a;&#x8fc7;&#x76f4;&#x63a5;&#x63d0;&#x5347;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x8868;&#x5f81;&#x80fd;&#x529b;&#xff0c;&#x4ece;&#x6839;&#x6e90;&#x4e0a;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#xff0c;&#x89e3;&#x51b3;&#x591a;&#x6a21;&#x6001;&#x5bf9;&#x9f50;&#x4e0d;&#x8db3;&#x7684;&#x95ee;&#x9898;&#x3002;","children":[],"payload":{"tag":"li","lines":"1892,1893"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: EAGLE&#x57fa;&#x4e8e;&#x5bf9;&#x6bd4;&#x5b66;&#x4e60;&#x6846;&#x67b6;&#x8fdb;&#x884c;&#x540e;&#x9884;&#x8bad;&#x7ec3;&#xff0c;&#x6539;&#x8fdb;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#xff08;ViT&#xff09;&#x7684;&#x5c40;&#x90e8;&#x7279;&#x5f81;&#x5bf9;&#x9f50;&#x3002;&#x5177;&#x4f53;&#x6b65;&#x9aa4;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x4f7f;&#x7528;&#x5b9e;&#x4f8b;&#x7ea7;&#x5206;&#x5272;&#x63a9;&#x7801;&#x5bf9;&#x56fe;&#x50cf;&#x5bf9;&#x8c61;&#x8fdb;&#x884c;&#x5b9a;&#x4f4d;&#xff1b;2&#xff09;&#x901a;&#x8fc7;&#x63a9;&#x7801;&#x5e73;&#x5747;&#x6c60;&#x5316;&#xff08;masked average pooling&#xff09;&#x63d0;&#x53d6;&#x5bf9;&#x8c61;&#x5bf9;&#x5e94;&#x7684;&#x89c6;&#x89c9;token&#x7279;&#x5f81;&#xff1b;3&#xff09;&#x5f3a;&#x5236;&#x8fd9;&#x4e9b;&#x5c40;&#x90e8;&#x7279;&#x5f81;&#x4e0e;&#x8bed;&#x8a00;&#x7f16;&#x7801;&#x5668;&#x4e2d;&#x7684;&#x5bf9;&#x8c61;&#x6587;&#x672c;&#x8868;&#x5f81;&#x5bf9;&#x9f50;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x4e0d;&#x4f9d;&#x8d56;&#x7279;&#x5b9a;IT-VLM&#x67b6;&#x6784;&#xff0c;&#x6539;&#x8fdb;&#x540e;&#x7684;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x53ef;&#x76f4;&#x63a5;&#x66ff;&#x6362;&#x539f;&#x6709;&#x6a21;&#x5757;&#xff0c;&#x65e0;&#x9700;&#x989d;&#x5916;&#x5fae;&#x8c03;&#x3002;","children":[],"payload":{"tag":"li","lines":"1893,1894"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;EAGLE&#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x663e;&#x8457;&#x964d;&#x4f4e;&#x5e7b;&#x89c9;&#xff1a;1&#xff09;&#x5728;MMVP&#x57fa;&#x51c6;&#x4e0a;&#x76f8;&#x5bf9;&#x63d0;&#x5347;11.2%&#xff1b;2&#xff09;&#x5728;MERLIM&#x57fa;&#x51c6;&#x4e0a;&#x76f8;&#x5bf9;&#x63d0;&#x5347;6.3%&#xff1b;3&#xff09;&#x5728;6&#x79cd;&#x4e0d;&#x540c;IT-VLM&#x6a21;&#x578b;&#xff08;&#x5982;InstructBLIP&#x7cfb;&#x5217;&#xff09;&#x4e0a;&#x5747;&#x53d6;&#x5f97;&#x4e00;&#x81f4;&#x6027;&#x6539;&#x8fdb;&#x3002;&#x5b9a;&#x6027;&#x5206;&#x6790;&#x663e;&#x793a;&#xff0c;&#x6a21;&#x578b;&#x80fd;&#x66f4;&#x51c6;&#x786e;&#x8bc6;&#x522b;&#x7ec6;&#x7c92;&#x5ea6;&#x5bf9;&#x8c61;&#xff08;&#x5982;&#x6805;&#x680f;&#x3001;&#x6811;&#x6728;&#xff09;&#x548c;&#x590d;&#x6742;&#x573a;&#x666f;&#x4e2d;&#x7684;&#x7269;&#x4f53;&#x4f4d;&#x7f6e;&#x3002;","children":[],"payload":{"tag":"li","lines":"1894,1895"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: EAGLE&#x901a;&#x8fc7;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x7f16;&#x7801;&#x5668;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x548c;&#x8bed;&#x8a00;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x4e86;IT-VLM&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x5177;&#x6709;&#x67b6;&#x6784;&#x65e0;&#x5173;&#x6027;&#x3001;&#x5373;&#x63d2;&#x5373;&#x7528;&#x6027;&#x548c;&#x53ef;&#x6269;&#x5c55;&#x6027;&#xff0c;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#x63d0;&#x5347;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x65b9;&#x5411;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x4e86;&#x539f;&#x59cb;&#x6a21;&#x578b;&#x7684;&#x96f6;&#x6837;&#x672c;&#x80fd;&#x529b;&#x3002;&#x672a;&#x6765;&#x53ef;&#x8fdb;&#x4e00;&#x6b65;&#x63a2;&#x7d22;&#x66f4;&#x9ad8;&#x6548;&#x7684;&#x89c6;&#x89c9;&#x63a5;&#x5730;&#x8bad;&#x7ec3;&#x7b56;&#x7565;&#x3002;","children":[],"payload":{"tag":"li","lines":"1895,1897"}}],"payload":{"tag":"li","lines":"1891,1897","fold":1}}],"payload":{"tag":"h4","lines":"1889,1890"}},{"content":"MIAVLM: Mitigating Hallucinations on Object Attributes using Multiview Images and Negative Instructions","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;MIAVLM&#x6a21;&#x578b;&#xff0c;&#x901a;&#x8fc7;&#x5229;&#x7528;&#x591a;&#x89c6;&#x89d2;&#x56fe;&#x50cf;&#x548c;&#x8d1f;&#x6307;&#x4ee4;&#x6765;&#x7f13;&#x89e3;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x63cf;&#x8ff0;&#x4e0a;&#x7684;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff08;HoOA&#xff09;&#x3002;&#x8be5;&#x65b9;&#x6cd5;&#x7ed3;&#x5408;&#x4e86;&#x591a;&#x89c6;&#x56fe;&#x5c5e;&#x6027;&#x611f;&#x77e5;&#x5668;&#xff08;MAP&#xff09;&#x6765;&#x6d88;&#x9664;&#x8f93;&#x5165;&#x987a;&#x5e8f;&#x5f71;&#x54cd;&#xff0c;&#x5e76;&#x5f15;&#x5165;&#x8d1f;&#x6307;&#x4ee4;&#x7ea0;&#x6b63;&#x6a21;&#x578b;&#x504f;&#x5411;&#x80af;&#x5b9a;&#x56de;&#x7b54;&#x7684;&#x504f;&#x5dee;&#xff0c;&#x5728;&#x4e13;&#x95e8;&#x6784;&#x5efa;&#x7684;HoOA&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x9a8c;&#x8bc1;&#x4e86;&#x6709;&#x6548;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1898,1899"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5f53;&#x524d;&#x5927;&#x578b;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLM&#xff09;&#x5728;&#x63cf;&#x8ff0;&#x56fe;&#x50cf;&#x65f6;&#x5b58;&#x5728;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#xff08;HoOA&#xff09;&#xff0c;&#x5373;&#x9519;&#x8bef;&#x63cf;&#x8ff0;&#x7269;&#x4f53;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x5c5e;&#x6027;&#xff08;&#x5982;&#x989c;&#x8272;&#x3001;&#x5f62;&#x72b6;&#xff09;&#x3002;&#x8fd9;&#x4e00;&#x95ee;&#x9898;&#x5728;&#x73b0;&#x6709;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e2d;&#x4e0e;&#x7269;&#x4f53;&#x5b58;&#x5728;&#x6027;&#x5e7b;&#x89c9;&#xff08;HoOE&#xff09;&#x548c;&#x5173;&#x7cfb;&#x5e7b;&#x89c9;&#xff08;HoOR&#xff09;&#x8026;&#x5408;&#xff0c;&#x96be;&#x4ee5;&#x5355;&#x72ec;&#x5206;&#x6790;&#x548c;&#x89e3;&#x51b3;&#x3002;HoOA&#x4e25;&#x91cd;&#x5f71;&#x54cd;LVLM&#x5728;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x4efb;&#x52a1;&#xff08;&#x5982;&#x4eba;&#x8138;&#x5c5e;&#x6027;&#x63cf;&#x8ff0;&#xff09;&#x4e2d;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x72ec;&#x7acb;&#x57fa;&#x51c6;&#x548c;&#x4e13;&#x95e8;&#x65b9;&#x6cd5;&#x4e88;&#x4ee5;&#x89e3;&#x51b3;&#x3002;","children":[],"payload":{"tag":"li","lines":"1900,1901"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x591a;&#x89c6;&#x89d2;&#x56fe;&#x50cf;&#x589e;&#x5f3a;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;MIAVLM&#xff09;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x5305;&#x62ec;&#xff1a;1&#xff09;&#x4f7f;&#x7528;&#x591a;&#x89c6;&#x89d2;&#x751f;&#x6210;&#x5668;&#xff08;HFGI3D&#xff09;&#x4ece;&#x5355;&#x56fe;&#x50cf;&#x751f;&#x6210;3D&#x8868;&#x793a;&#x5e76;&#x91c7;&#x6837;&#x591a;&#x89c6;&#x89d2;&#x56fe;&#x50cf;&#xff0c;&#x4f5c;&#x4e3a;&#x89c6;&#x89c9;&#x63d0;&#x793a;&#x8f93;&#x5165;&#xff1b;2&#xff09;&#x8bbe;&#x8ba1;&#x591a;&#x89c6;&#x56fe;&#x5c5e;&#x6027;&#x611f;&#x77e5;&#x5668;&#xff08;MAP&#xff09;&#x5b50;&#x6a21;&#x5757;&#xff0c;&#x5305;&#x542b;&#x89c6;&#x89c9;&#x63d0;&#x53d6;&#x5668;&#xff08;Visual Extractor&#xff09;&#x548c;&#x591a;&#x5934;&#x91c7;&#x6837;&#x5668;&#xff08;Multihead Sampler&#xff09;&#xff0c;&#x901a;&#x8fc7;&#x4ea4;&#x53c9;&#x6ce8;&#x610f;&#x529b;&#x548c;&#x52a0;&#x6743;&#x805a;&#x5408;&#x5bf9;&#x9f50;&#x591a;&#x89c6;&#x89d2;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#x4e0e;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;Flan-T5-large&#xff09;&#xff0c;&#x6d88;&#x9664;&#x8f93;&#x5165;&#x987a;&#x5e8f;&#x5f71;&#x54cd;&#xff1b;3&#xff09;&#x5f15;&#x5165;&#x8d1f;&#x6307;&#x4ee4;&#xff08;&#x8d1f;&#x6837;&#x672c;&#x95ee;&#x9898;&#xff09;&#x8bad;&#x7ec3;&#x6a21;&#x578b;&#xff0c;&#x7ea0;&#x6b63;&#x6a21;&#x578b;&#x5bf9;&#x201c;&#x662f;&#x201d;&#x56de;&#x7b54;&#x7684;&#x504f;&#x5dee;&#x3002;","children":[],"payload":{"tag":"li","lines":"1901,1902"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x4e13;&#x95e8;&#x6784;&#x5efa;&#x7684;HoOA&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#xff08;&#x57fa;&#x4e8e;CelebAText-HQ&#x4eba;&#x8138;&#x5c5e;&#x6027;&#x6570;&#x636e;&#x96c6;&#xff09;&#x4e0a;&#xff0c;MIAVLM&#x663e;&#x8457;&#x51cf;&#x5c11;&#x4e86;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#x3002;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff1a;&#x591a;&#x89c6;&#x89d2;&#x56fe;&#x50cf;&#x63d0;&#x4f9b;&#x4e86;&#x66f4;&#x5b8c;&#x6574;&#x7684;&#x89c6;&#x89c9;&#x4fe1;&#x606f;&#xff0c;&#x6709;&#x6548;&#x7f13;&#x89e3;&#x5c5e;&#x6027;&#x4e0d;&#x5b8c;&#x6574;&#x5bfc;&#x81f4;&#x7684;&#x5e7b;&#x89c9;&#xff1b;MAP&#x6a21;&#x5757;&#x6210;&#x529f;&#x6d88;&#x9664;&#x4e86;&#x591a;&#x89c6;&#x89d2;&#x8f93;&#x5165;&#x987a;&#x5e8f;&#x5bf9;&#x6a21;&#x578b;&#x6027;&#x80fd;&#x7684;&#x5f71;&#x54cd;&#xff1b;&#x8d1f;&#x6307;&#x4ee4;&#x8bad;&#x7ec3;&#x6709;&#x6548;&#x5e73;&#x8861;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x6b63;&#x8d1f;&#x95ee;&#x9898;&#x7684;&#x56de;&#x7b54;&#x504f;&#x5dee;&#xff0c;&#x63d0;&#x9ad8;&#x4e86;&#x6574;&#x4f53;&#x51c6;&#x786e;&#x6027;&#x3002;","children":[],"payload":{"tag":"li","lines":"1902,1903"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7ed3;&#x8bba;&#x6307;&#x51fa;&#xff0c;&#x591a;&#x89c6;&#x89d2;&#x56fe;&#x50cf;&#x548c;&#x8d1f;&#x6307;&#x4ee4;&#x80fd;&#x6709;&#x6548;&#x7f13;&#x89e3;LVLM&#x7684;&#x7269;&#x4f53;&#x5c5e;&#x6027;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;MIAVLM&#x6846;&#x67b6;&#x4e3a;&#x591a;&#x6a21;&#x6001;&#x6a21;&#x578b;&#x5904;&#x7406;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x5c5e;&#x6027;&#x63d0;&#x4f9b;&#x4e86;&#x65b0;&#x601d;&#x8def;&#xff0c;&#x5176;&#x6a21;&#x5757;&#x5316;&#x8bbe;&#x8ba1;&#xff08;&#x5982;MAP&#xff09;&#x53ef;&#x6269;&#x5c55;&#x81f3;&#x5176;&#x4ed6;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x4efb;&#x52a1;&#x3002;&#x6f5c;&#x5728;&#x5f71;&#x54cd;&#x5305;&#x62ec;&#x63d0;&#x5347;LVLM&#x5728;&#x533b;&#x7597;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#x7b49;&#x9ad8;&#x98ce;&#x9669;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5e76;&#x4e3a;&#x89e3;&#x8026;&#x4e0d;&#x540c;&#x7c7b;&#x578b;&#x5e7b;&#x89c9;&#x63d0;&#x4f9b;&#x4e86;&#x65b9;&#x6cd5;&#x8bba;&#x53c2;&#x8003;&#x3002;","children":[],"payload":{"tag":"li","lines":"1903,1942"}}],"payload":{"tag":"li","lines":"1899,1942","fold":1}}],"payload":{"tag":"h4","lines":"1897,1898"}},{"content":"ViHallu: See Different, Think Better: Visual Variations Mitigating Hallucinations in LVLMs","children":[{"content":"<strong>&#x6458;&#x8981;</strong>: &#x672c;&#x6587;&#x63d0;&#x51fa;ViHallu&#xff0c;&#x4e00;&#x79cd;&#x89c6;&#x89c9;&#x4e2d;&#x5fc3;&#x7684;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x6846;&#x67b6;&#x3002;&#x901a;&#x8fc7;&#x751f;&#x6210;&#x4fdd;&#x6301;&#x6574;&#x4f53;&#x7ed3;&#x6784;&#x4f46;&#x5305;&#x542b;&#x53ef;&#x63a7;&#x5c40;&#x90e8;&#x53d8;&#x5316;&#x7684;&#x89c6;&#x89c9;&#x53d8;&#x4f53;&#x56fe;&#x50cf;&#xff0c;&#x5e76;&#x6784;&#x5efa;&#x76f8;&#x5e94;&#x7684;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#xff0c;&#x6765;&#x589e;&#x5f3a;&#x6a21;&#x578b;&#x7684;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x80fd;&#x529b;&#xff0c;&#x6709;&#x6548;&#x51cf;&#x5c11;&#x5e7b;&#x89c9;&#x73b0;&#x8c61;&#x3002;","children":[],"payload":{"tag":"li","lines":"1943,1944"}},{"content":"<strong>&#x8be6;&#x7ec6;&#x4fe1;&#x606f;</strong>","children":[{"content":"<strong>&#x7814;&#x7a76;&#x52a8;&#x673a;</strong>: &#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;LVLMs&#xff09;&#x5728;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x548c;&#x591a;&#x6a21;&#x6001;&#x63a8;&#x7406;&#x65b9;&#x9762;&#x8868;&#x73b0;&#x51fa;&#x8272;&#xff0c;&#x4f46;&#x666e;&#x904d;&#x5b58;&#x5728;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#xff0c;&#x5373;&#x751f;&#x6210;&#x7684;&#x6587;&#x672c;&#x54cd;&#x5e94;&#x4e0e;&#x63d0;&#x4f9b;&#x7684;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0d;&#x4e00;&#x81f4;&#x3002;&#x8fd9;&#x79cd;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x7684;&#x9519;&#x4f4d;&#x4e25;&#x91cd;&#x5f71;&#x54cd;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x53ef;&#x9760;&#x6027;&#xff0c;&#x5c24;&#x5176;&#x5728;&#x9700;&#x8981;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x7406;&#x89e3;&#x7684;&#x573a;&#x666f;&#xff08;&#x5982;&#x533b;&#x7597;&#x8bca;&#x65ad;&#x3001;&#x81ea;&#x52a8;&#x9a7e;&#x9a76;&#xff09;&#x4e2d;&#x9650;&#x5236;&#x4e86;&#x5176;&#x5b9e;&#x9645;&#x5e94;&#x7528;&#x3002;&#x73b0;&#x6709;&#x65b9;&#x6cd5;&#x591a;&#x4ee5;&#x6587;&#x672c;&#x4e3a;&#x4e2d;&#x5fc3;&#xff0c;&#x5728;&#x89e3;&#x51b3;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x5dee;&#x5f02;&#x95ee;&#x9898;&#x4e0a;&#x6548;&#x679c;&#x6709;&#x9650;&#xff0c;&#x56e0;&#x6b64;&#x9700;&#x8981;&#x4e00;&#x79cd;&#x4ee5;&#x89c6;&#x89c9;&#x4e3a;&#x4e2d;&#x5fc3;&#x7684;&#x65b0;&#x65b9;&#x6cd5;&#x6765;&#x589e;&#x5f3a;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x3002;","children":[],"payload":{"tag":"li","lines":"1945,1946"}},{"content":"<strong>&#x7814;&#x7a76;&#x65b9;&#x6cd5;</strong>: &#x4f5c;&#x8005;&#x63d0;&#x51fa;&#x4e86;ViHallu&#x6846;&#x67b6;&#xff0c;&#x5176;&#x6838;&#x5fc3;&#x65b9;&#x6cd5;&#x5206;&#x4e3a;&#x4e24;&#x4e2a;&#x9636;&#x6bb5;&#xff1a;1. <strong>&#x89c6;&#x89c9;&#x53d8;&#x4f53;&#x56fe;&#x50cf;&#x751f;&#x6210;</strong>&#xff1a;&#x9996;&#x5148;&#x4f7f;&#x7528;Tag2Text&#x548c;MobileSAM&#x6a21;&#x578b;&#x4ece;&#x539f;&#x59cb;&#x56fe;&#x50cf;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x548c;&#x5206;&#x5272;&#x63a9;&#x7801;&#xff1b;&#x7136;&#x540e;&#x5229;&#x7528;&#x5927;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#xff08;&#x5982;DeepSeek-chat&#xff09;&#x5bf9;&#x63cf;&#x8ff0;&#x8fdb;&#x884c;&#x7f16;&#x8f91;&#xff0c;&#x901a;&#x8fc7;&#x6982;&#x5ff5;&#x66ff;&#x6362;&#x751f;&#x6210;&#x53d8;&#x4f53;&#x63cf;&#x8ff0;&#xff0c;&#x6307;&#x5b9a;&#x8981;&#x6539;&#x53d8;&#x7684;&#x7269;&#x4f53;&#x7c7b;&#x522b;&#x6216;&#x5c5e;&#x6027;&#xff0c;&#x540c;&#x65f6;&#x4fdd;&#x6301;&#x5176;&#x4ed6;&#x4e0a;&#x4e0b;&#x6587;&#x4e0d;&#x53d8;&#xff1b;&#x6700;&#x540e;&#xff0c;&#x4f7f;&#x7528;&#x6587;&#x672c;&#x5f15;&#x5bfc;&#x548c;&#x5206;&#x5272;&#x63a9;&#x7801;&#x63a7;&#x5236;&#x7684;&#x6587;&#x751f;&#x56fe;&#x6a21;&#x578b;&#xff08;T2I&#xff09;&#x751f;&#x6210;&#x89c6;&#x89c9;&#x53d8;&#x4f53;&#x56fe;&#x50cf;&#x3002;&#x8fd9;&#x4e9b;&#x56fe;&#x50cf;&#x4ec5;&#x5728;&#x76ee;&#x6807;&#x533a;&#x57df;&#x6709;&#x53d8;&#x5316;&#xff0c;&#x6574;&#x4f53;&#x7ed3;&#x6784;&#x4e0e;&#x539f;&#x56fe;&#x4fdd;&#x6301;&#x4e00;&#x81f4;&#xff0c;&#x5e76;&#x80fd;&#x5c06;&#x7269;&#x4f53;&#x7f6e;&#x4e8e;&#x4e0d;&#x5e38;&#x89c1;&#x7684;&#x80cc;&#x666f;&#x4e2d;&#xff0c;&#x521b;&#x5efa;&#x53cd;&#x4e8b;&#x5b9e;&#x5e72;&#x9884;&#xff0c;&#x8feb;&#x4f7f;&#x6a21;&#x578b;&#x4f9d;&#x8d56;&#x89c6;&#x89c9;&#x7279;&#x5f81;&#x800c;&#x975e;&#x7edf;&#x8ba1;&#x5148;&#x9a8c;&#x3002;2. <strong>&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6784;&#x5efa;</strong>&#xff1a;&#x4e3a;&#x914d;&#x5bf9;&#x7684;&#x539f;&#x56fe;&#x548c;&#x53d8;&#x4f53;&#x56fe;&#x50cf;&#x6784;&#x5efa;&#x9ad8;&#x8d28;&#x91cf;&#x7684;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#xff0c;&#x5305;&#x62ec;&#x751f;&#x6210;&#x8be6;&#x7ec6;&#x63cf;&#x8ff0;&#x3001;&#x5bf9;&#x8c61;&#x6807;&#x7b7e;&#x3001;&#x95ee;&#x7b54;&#x5bf9;&#x5e76;&#x8fdb;&#x884c;&#x8d28;&#x91cf;&#x8bc4;&#x4f30;&#xff0c;&#x4ece;&#x800c;&#x8ba9;&#x6a21;&#x578b;&#x901a;&#x8fc7;&#x5fae;&#x8c03;&#x5b66;&#x4e60;&#x7ec6;&#x7c92;&#x5ea6;&#x7684;&#x5224;&#x522b;&#x7279;&#x5f81;&#x3002;","children":[],"payload":{"tag":"li","lines":"1946,1947"}},{"content":"<strong>&#x5b9e;&#x9a8c;&#x7ed3;&#x679c;</strong>: &#x5728;&#x591a;&#x4e2a;&#x57fa;&#x51c6;&#x6d4b;&#x8bd5;&#x4e0a;&#x7684;&#x5e7f;&#x6cdb;&#x5b9e;&#x9a8c;&#x8868;&#x660e;&#xff0c;ViHallu&#x663e;&#x8457;&#x51cf;&#x8f7b;&#x4e86;LVLMs&#x7684;&#x5e7b;&#x89c9;&#x503e;&#x5411;&#xff0c;&#x5e76;&#x589e;&#x5f3a;&#x4e86;&#x6a21;&#x578b;&#x7684;&#x9c81;&#x68d2;&#x6027;&#x3002;&#x5177;&#x4f53;&#x800c;&#x8a00;&#xff0c;&#x8be5;&#x65b9;&#x6cd5;&#x6709;&#x6548;&#x63d0;&#x5347;&#x4e86;&#x6a21;&#x578b;&#x5bf9;&#x7ec6;&#x7c92;&#x5ea6;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x7684;&#x7406;&#x89e3;&#x80fd;&#x529b;&#xff0c;&#x4f7f;&#x5176;&#x80fd;&#x66f4;&#x7cbe;&#x786e;&#x5730;&#x6355;&#x6349;&#x89c6;&#x89c9;&#x5185;&#x5bb9;&#x4e0e;&#x6587;&#x672c;&#x4e4b;&#x95f4;&#x7684;&#x5bf9;&#x5e94;&#x5173;&#x7cfb;&#x3002;&#x4f5c;&#x8005;&#x8fd8;&#x53d1;&#x5e03;&#x4e86;&#x4e13;&#x95e8;&#x4e3a;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x548c;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#x8bbe;&#x8ba1;&#x7684;&#x89c6;&#x89c9;&#x6307;&#x4ee4;&#x6570;&#x636e;&#x96c6;ViHallu-Instruction&#x3002;","children":[],"payload":{"tag":"li","lines":"1947,1948"}},{"content":"<strong>&#x8bba;&#x6587;&#x7ed3;&#x8bba;</strong>: &#x8bba;&#x6587;&#x7684;&#x7ed3;&#x8bba;&#x662f;&#xff0c;&#x6240;&#x63d0;&#x51fa;&#x7684;&#x89c6;&#x89c9;&#x4e2d;&#x5fc3;&#x65b9;&#x6cd5;ViHallu&#x901a;&#x8fc7;&#x751f;&#x6210;&#x89c6;&#x89c9;&#x53d8;&#x4f53;&#x56fe;&#x50cf;&#x548c;&#x6784;&#x5efa;&#x76f8;&#x5e94;&#x7684;&#x6307;&#x4ee4;&#x6570;&#x636e;&#xff0c;&#x6709;&#x6548;&#x5730;&#x589e;&#x5f3a;&#x4e86;&#x5927;&#x89c6;&#x89c9;&#x8bed;&#x8a00;&#x6a21;&#x578b;&#x7684;&#x89c6;&#x89c9;-&#x8bed;&#x4e49;&#x5bf9;&#x9f50;&#xff0c;&#x663e;&#x8457;&#x7f13;&#x89e3;&#x4e86;&#x5e7b;&#x89c9;&#x95ee;&#x9898;&#x3002;&#x8fd9;&#x4e00;&#x5de5;&#x4f5c;&#x4e3a;&#x5e7b;&#x89c9;&#x7f13;&#x89e3;&#x7814;&#x7a76;&#x63d0;&#x4f9b;&#x4e86;&#x4e00;&#x4e2a;&#x65b0;&#x7684;&#x8303;&#x5f0f;&#xff0c;&#x5f3a;&#x8c03;&#x4e86;&#x89c6;&#x89c9;&#x6a21;&#x6001;&#x7684;&#x91cd;&#x8981;&#x6027;&#x3002;&#x5176;&#x53d1;&#x5e03;&#x7684;&#x6570;&#x636e;&#x96c6;&#x548c;&#x4ee3;&#x7801;&#x5c06;&#x4fc3;&#x8fdb;&#x540e;&#x7eed;&#x76f8;&#x5173;&#x7814;&#x7a76;&#xff0c;&#x5e76;&#x5bf9;LVLMs&#x5728;&#x5173;&#x952e;&#x9886;&#x57df;&#x7684;&#x53ef;&#x9760;&#x5e94;&#x7528;&#x5177;&#x6709;&#x6f5c;&#x5728;&#x7684;&#x91cd;&#x5927;&#x5f71;&#x54cd;&#x3002;","children":[],"payload":{"tag":"li","lines":"1948,1956"}}],"payload":{"tag":"li","lines":"1944,1956","fold":1}}],"payload":{"tag":"h4","lines":"1942,1943"}}],"payload":{"tag":"h2","lines":"1869,1870","fold":1}},{"content":"&#x6587;&#x732e;&#x9519;&#x8bef;","children":[{"content":"Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model","children":[{"content":"&#x4e0b;&#x8f7d;&#x8bba;&#x6587;&#x5931;&#x8d25;&#x3002;","children":[],"payload":{"tag":"li","lines":"1958,1960"}}],"payload":{"tag":"h4","lines":"1957,1958"}},{"content":"SEED: Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation","children":[{"content":"&#x4e0b;&#x8f7d;&#x8bba;&#x6587;&#x5931;&#x8d25;&#x3002;","children":[],"payload":{"tag":"li","lines":"1961,1962"}}],"payload":{"tag":"h4","lines":"1960,1961"}}],"payload":{"tag":"h2","lines":"1956,1957","fold":1}}]},{"colorFreezeLevel":3})</script></body></html>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kaggle Grandmasters playbook：7 种经过实战考验的表格数据建模技术</title>
    <link href="/2025/20251014/"/>
    <url>/2025/20251014/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>转自<a href="https://developer.nvidia.com/blog/the-kaggle-grandmasters-playbook-7-battle-tested-modeling-techniques-for-tabular-data/">nvidia的blog</a></p><span id="more"></span><p><img src="/2025/20251014/1.webp"></p><p>在数百场 Kaggle 比赛中，我们改进了一本剧本（playbook），使我们始终接近排行榜的榜首——无论我们处理的是数百万行、缺失值还是行为与训练数据完全不同的测试集。这不仅仅是建模技巧的集合，它还是一个可重复的系统，用于快速解决现实世界的表格问题。</p><p>以下是我们经过实战考验的七种技术，每一种技术都通过 GPU 加速变得实用。无论您是攀登排行榜还是在生产中部署模型，这些策略都可以为您带来优势。</p><p>我们提供了指向每种技术过去比赛的示例文章或笔记本的链接。<br>**注意：**Kaggle 和 Google Colab 笔记本电脑附带免费的 GPU 和加速插入式，就像您将在下面预装的那样。</p><h2 id="核心原则：制胜工作流程的基础"><a href="#核心原则：制胜工作流程的基础" class="headerlink" title="核心原则：制胜工作流程的基础"></a>核心原则：制胜工作流程的基础</h2><p>在深入研究技术之前，值得暂停一下，介绍一下为本手册中的所有内容提供动力的两个原则：快速实验和仔细验证。这些不是可选的最佳实践，它们是我们处理每个表格建模问题的基础。</p><h3 id="快速实验"><a href="#快速实验" class="headerlink" title="快速实验"></a>快速实验</h3><p>在任何比赛或实际项目中，我们最大的杠杆是我们可以运行的高质量实验的数量。迭代次数越多，发现的模式就越多，当模型失败、漂移或过度拟合时，我们就能更快地发现，因此我们可以及早纠正方向并更快地改进。</p><p>在实践中，这意味着我们优化了整个管道的速度，而不仅仅是我们的模型训练步骤。</p><p><strong>以下是我们如何使其发挥作用：</strong></p><ul><li>使用 pandas 或 Polars 的 <a href="https://developer.nvidia.com/blog/7-drop-in-replacements-to-instantly-speed-up-your-python-data-science-workflows/?linkId=100000376196126">GPU 插入式替换</a>来加速数据帧作，以大规模转换和设计功能。</li><li>使用 <a href="https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml">NVIDIA cuML</a> 或 XGBoost、LightGBM 和 CatBoost 的 GPU 后端训练模型。</li></ul><p>GPU 加速不仅适用于深度学习，而且通常是使高级表格技术大规模实用的唯一方法。</p><h3 id="本地验证"><a href="#本地验证" class="headerlink" title="本地验证"></a>本地验证</h3><p>如果你不能相信你的验证分数，你就是盲目飞行。这就是为什么交叉验证 （CV） 是我们工作流程的基石。</p><p><strong>我们的方法：</strong></p><ul><li>使用 k 折交叉验证，其中模型对大部分数据进行训练，并在保留的部分上进行测试。</li><li>旋转折叠，以便数据的每个部分都经过一次测试。</li></ul><p>这提供了比单个训练&#x2F;验证拆分更可靠的性能衡量标准。</p><p><strong>专业提示：</strong> 将您的简历策略与测试数据的结构相匹配。 </p><p><strong>例如：</strong></p><ul><li>将 TimeSeriesSplit 用于与时间相关的数据</li><li>将 GroupKFold 用于分组数据（如用户或患者）</li></ul><p>有了这些基础——快速行动并仔细验证——我们现在可以深入研究技术本身。每一个都建立在这些原则之上，并展示了我们如何将原始数据转化为世界一流的模型。</p><h2 id="1-从更智能的-EDA-开始，而不仅仅是基础知识"><a href="#1-从更智能的-EDA-开始，而不仅仅是基础知识" class="headerlink" title="1. 从更智能的 EDA 开始，而不仅仅是基础知识"></a>1. 从更智能的 EDA 开始，而不仅仅是基础知识</h2><p>大多数从业者都知道基础知识：检查缺失值、异常值、相关性和特征范围。这些步骤很重要，但它们是赌注。要构建在现实世界中经得起考验的模型，您需要更深入地探索数据 - 我们发现一些有用的快速检查，但许多人忽略了：</p><p><strong>训练与测试分布检查：</strong> 发现评估数据与训练何时不同，因为分布偏移可能会导致模型验证良好，但在部署中失败。</p><p><img src="/2025/20251014/2.webp"></p><p>上图比较训练（蓝色）和测试（红色）之间的特征分布可以发现明显的转变——测试数据集中在更高的范围内，重叠最小。这种分布转移可能会导致模型验证良好，但在部署中失败。</p><p><strong>分析目标变量的时间模式：</strong> 检查趋势或季节性，因为忽略时间模式可能会导致模型在训练中看起来准确，但在生产中中断。</p><p><img src="/2025/20251014/3.webp"></p><p>随着时间的推移分析目标变量，可以发现季节性波动和加速增长的强劲上升趋势。忽略此类时间模式可能会误导模型，除非使用时间感知验证。</p><p>这些技术并不新鲜，但它们经常被忽视，忽视它们可能会使项目沉沦。</p><p><strong>为什么重要：</strong> 跳过这些检查可能会破坏原本可靠的工作流程。</p><p><strong>在行动中：</strong> 在 Amazon KDD Cup ‘23 的获胜解决方案中，该团队发现了测试分布偏移和目标时间模式的列车——这些见解塑造了最终方法。<a href="https://openreview.net/forum?id=J3wj55kK5t"> 阅读全文 &gt;</a></p><p>使用 <strong>GPU 变得实用：</strong> 现实世界的数据集通常是数百万行，这在 pandas 中可能会变慢。通过使用 <a href="https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cudf">NVIDIA cuDF</a> 添加 GPU 加速，您可以在几秒钟内大规模运行分布比较和相关性。<a href="https://developer.nvidia.com/blog/accelerated-data-analytics-speed-up-data-exploration-with-rapids-cudf/"> 阅读技术博客 &gt;</a></p><h2 id="2-快速构建多样化的基线"><a href="#2-快速构建多样化的基线" class="headerlink" title="2. 快速构建多样化的基线"></a>2. 快速构建多样化的基线</h2><p>大多数人构建一些简单的基线（可能是均值预测、逻辑回归或快速 XGBoost），然后继续。问题在于，单个基线并不能告诉您太多有关数据格局的信息。</p><p><strong>我们的方法不同：</strong> 我们立即跨模型类型启动一组不同的基线。了解线性模型、GBDT 甚至小型神经网络如何并排执行，为我们提供了更多的背景信息来指导实验。 </p><p><strong>为什么重要：</strong> 基线是您的直觉检查——它们确认您的模型比猜测做得更好，设置最低性能标准，并充当快速反馈循环。在数据更改后重新运行基线可以揭示您是否取得了进展，或者发现泄漏等问题。</p><p>多样化的基线还可以尽早显示哪些模型族最适合您的数据，因此您可以加倍努力，而不是在错误的路径上浪费周期。</p><p><strong>在行动中：</strong> 在降<em>雨数据集二元预测</em>竞赛中，我们的任务是根据天气数据预测降雨量。我们的基线让我们走得很远——梯度增强树、神经网络和支持向量回归 （SVR） 模型的集合，没有任何特征工程，足以为我们赢得第二名。在探索其他基线时，我们发现即使是单个支持向量分类器 （SVC） 基线也会位于排行榜顶部附近。<a href="https://www.kaggle.com/competitions/playground-series-s5e3/writeups/chris-deotte-2nd-place-gbdt-nn-svr-original-data"> 阅读全文 &gt;</a></p><p>使用 <strong>GPU 变得实用：</strong> 在 CPU 上训练各种模型可能会非常慢。借助 GPU 加速，您可以尝试所有这些方法（用于快速统计的 cuDF、用于线性&#x2F;逻辑回归的 cuML，以及 GPU 加速的 XGBoost、LightGBM、CatBoost 和神经网络），因此您可以在几分钟内获得更好的洞察力，而不是几小时。</p><h2 id="3-生成更多特征，发现更多模式"><a href="#3-生成更多特征，发现更多模式" class="headerlink" title="3. 生成更多特征，发现更多模式"></a>3. 生成更多特征，发现更多模式</h2><p>特征工程仍然是提高表格数据准确性的最有效方法之一。挑战：在 CPU 上使用 pandas 生成和验证数千个特征太慢，不切实际。</p><p><strong>为什么重要：</strong> 从少数手动转换扩展到数百或数千个工程特征，通常会揭示仅靠模型无法捕获的隐藏信号。 </p><p><strong>例：</strong> 组合分类列</p><p>在一次 Kaggle 竞赛中，数据集有八个分类列。通过组合它们对，我们创建了 28 个新的分类特征，这些特征捕获了原始数据未显示的交互。以下是该方法的简化片段：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> i,c1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(CATS[:-<span class="hljs-number">1</span>]):<br>     <span class="hljs-keyword">for</span> j,c2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(CATS[i+<span class="hljs-number">1</span>:]):<br>        n = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;c1&#125;</span>_<span class="hljs-subst">&#123;c2&#125;</span>&quot;</span><br>        train[n] = train[c1].astype(<span class="hljs-string">&#x27;str&#x27;</span>)+<span class="hljs-string">&quot;_&quot;</span>+train[c2].astype(<span class="hljs-string">&#x27;str&#x27;</span>)<br></code></pre></td></tr></table></figure><p><strong>在行动中：</strong> 大规模功能工程推动了 Kaggle Backpack 和 Insurance 竞赛的第一名，数以千计的新功能发挥了作用。 </p><p>使用 <strong>GPU 变得实用：</strong> 借助 cuDF，panda 作（如分组、聚合和编码）的运行速度提高了几个数量级，从而可以在几天而不是几个月内生成和测试数千个新功能。</p><p>查看下面的技术博客和培训课程，了解实践示例：</p><ul><li>博客：<a href="https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-kaggle-competition-with-feature-engineering-using-nvidia-cudf-pandas/">Grandmaster Pro 提示：使用 cuDF pandas 在特征工程的 Kaggle 竞赛中赢得第一名</a></li><li>课程：<a href="https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-DS-06+V1"> 表格数据特征工程的最佳实践</a></li></ul><h2 id="组合不同的模型（集成）可提高性能"><a href="#组合不同的模型（集成）可提高性能" class="headerlink" title="组合不同的模型（集成）可提高性能"></a>组合不同的模型（集成）可提高性能</h2><p>我们发现，结合不同模型的优势通常会使性能超出任何一种模型所能达到的范围。两种特别有用的技术是爬山和模型堆叠。</p><h3 id="4-爬山"><a href="#4-爬山" class="headerlink" title="4. 爬山"></a>4. 爬山</h3><p>爬山是一种简单但强大的模特组合方式。从最强的单个模型开始，然后系统地添加具有不同权重的其他模型，仅保留可改善验证的组合。重复直到没有进一步的收益。</p><p><strong>为什么重要：</strong> 集成可以捕捉模型之间的互补优势，但很难找到正确的组合。爬山可以自动执行搜索，通常会降低准确性并优于单一模型解决方案。 </p><p><strong>在行动中：</strong> 在<em>预测卡路里消耗</em>竞赛中，我们使用了 XGBoost、CatBoost、神经网络和线性模型的爬山集成来获得第一名。<a href="https://www.kaggle.com/code/cdeotte/gpu-hill-climbing-cv-0-05930"> 阅读写入 &gt;</a></p><p>使用 <strong>GPU 变得实用：</strong> 爬山本身并不新鲜——它是比赛中常见的合奏技术——但它通常会变得太慢而无法大规模应用。借助 GPU 上的 Cupy，我们可以矢量化度量计算（如 RMSE 或 AUC）并并行评估数千种权重组合。这种加速使得测试比在 CPU 上可行的更多的集成变得实用，通常会发现更强的混合。</p><p>以下是用于在 GPU 上评估爬山合奏的代码的简化版本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> cupy <span class="hljs-keyword">as</span> cp<br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">multiple_rmse_scores</span>(<span class="hljs-params">actual, predicted</span>):<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(actual.shape)==<span class="hljs-number">1</span>: <br>        actual = actual[:,cp.newaxis]<br>    rmses = cp.sqrt(cp.mean((actual-predicted)**<span class="hljs-number">2.0</span>,axis=<span class="hljs-number">0</span>))<br>    <span class="hljs-keyword">return</span> rmses<br> <br><span class="hljs-keyword">def</span> <span class="hljs-title function_">multiple_roc_auc_scores</span>(<span class="hljs-params">actual, predicted</span>):<br>    n_pos = cp.<span class="hljs-built_in">sum</span>(actual)  <br>    n_neg = <span class="hljs-built_in">len</span>(actual) - n_pos  <br>    ranked = cp.argsort(cp.argsort(predicted, axis=<span class="hljs-number">0</span>), axis=<span class="hljs-number">0</span>)+<span class="hljs-number">1</span> <br>    aucs = (cp.<span class="hljs-built_in">sum</span>(ranked[actual == <span class="hljs-number">1</span>, :], axis=<span class="hljs-number">0</span>)- n_pos\<br>    *(n_pos + <span class="hljs-number">1</span>)/<span class="hljs-number">2</span>) / (n_pos*n_neg)     <br>    <span class="hljs-keyword">return</span> aucs<br></code></pre></td></tr></table></figure><h3 id="5-堆叠"><a href="#5-堆叠" class="headerlink" title="5. 堆叠"></a>5. 堆叠</h3><p>堆叠通过在另一个模型的输出上训练一个模型，使集成更进一步。堆叠不是用权重（如爬山）对预测进行平均，而是构建一个二级模型，该模型学习如何最好地组合其他模型的输出。</p><p><strong>为什么重要：</strong> 当数据集具有不同模型以不同方式捕获的复杂模式时，堆叠尤其有效，例如线性趋势与非线互。 </p><p><strong>专业提示：</strong> 两种堆叠方式：</p><ul><li>残差：根据第 1 阶段出错的地方（残差）训练第 2 阶段模型。</li><li>OOF 功能：使用阶段 1 预测作为阶段 2 的新输入功能。</li></ul><p>这两种方法都有助于通过捕获基础模型遗漏的模式从数据中挤出更多信号。</p><p><strong>在行动中：</strong> 堆叠被用于在播客收听时间竞赛中赢得第一名，该竞赛使用了不同模型（线性、GBDT、神经网络和 AutoML）的三级堆栈。<a href="https://developer.nvidia.com/blog/grandmaster-pro-tip-winning-first-place-in-a-kaggle-competition-with-stacking-using-cuml/"> 阅读技术博客 &gt;</a></p><p><img src="/2025/20251014/4.webp"></p><h2 id="6-将未标记的数据转换为具有伪标记的训练信号"><a href="#6-将未标记的数据转换为具有伪标记的训练信号" class="headerlink" title="6. 将未标记的数据转换为具有伪标记的训练信号"></a>6. 将未标记的数据转换为具有伪标记的训练信号</h2><p>伪标记将未标记的数据转换为训练信号。您可以使用最佳模型在缺少标签的数据（例如，测试数据或外部数据集）上推断标签，然后将这些“伪标签”折叠回训练中以提高模型性能。</p><p><img src="/2025/20251014/5.webp"></p><p>伪标记将未标记的数据转换为训练信号。您可以使用最佳模型在缺少标签的数据（例如，测试数据或外部数据集）上推断标签，然后将这些“伪标签”折叠回训练中以提高模型性能。</p><p><strong>为什么重要：</strong> 更多数据 &#x3D; 更多信号。伪标记提高了鲁棒性，起到了知识蒸馏的作用（学生模型从强有力的教师的预测中学习），甚至可以通过过滤掉模型不同意的样本来帮助去噪标记的数据。使用<em>软标签</em> （概率而不是硬 0&#x2F;1）可以增加正则化并减少噪声。</p><p><strong>有效伪标签的专业技巧：</strong></p><ul><li>模型越强，伪标签越好。集成或多轮伪标记通常优于单程方法</li><li>伪标签也可用于预训练。作为最后一步，对初始数据进行微调，以减少之前引入的噪声。</li><li>使用软伪标签。它们可以增加更多信号，降低噪声，并让您滤除低置信度样本。</li><li>伪标记可用于标记数据，可用于去除有噪声的样品。</li><li>避免信息泄露。使用 k 折时，必须计算 k 组伪标签，以便验证数据永远不会看到来自自身训练的模型的标签。</li></ul><p><strong>在行动中：</strong> 在 <em>BirdCLEF 2024</em> 比赛中，任务是根据鸟类录音对物种进行分类。伪标记在未标记的剪辑上使用软标签扩展了训练集，这有助于我们的模型更好地泛化到新物种和记录条件。<a href="https://www.kaggle.com/competitions/birdclef-2024/discussion/511905"> 阅读全文 &gt;</a></p><p>使用 <strong>GPU 变得实用：</strong> 伪标记通常需要多次重新训练管道（基线 &gt; 伪标记 &gt; 改进的伪标签）。这在 CPU 上可能需要数天时间，使得迭代不切实际。通过 GPU 加速（通过 cuML、XGBoost 或 CatBoost GPU 后端），您可以在数小时内运行多个伪标记周期。</p><h2 id="7-通过额外培训加强您的最终模型"><a href="#7-通过额外培训加强您的最终模型" class="headerlink" title="7. 通过额外培训加强您的最终模型"></a>7. 通过额外培训加强您的最终模型</h2><p>即使在优化了我们的模型和整体之后，我们也发现了两个可以挤出额外性能的最终调整：</p><ul><li><strong>使用不同的随机种子进行训练</strong> 。更改初始化和训练路径，然后对预测进行平均，通常可以提高性能。</li><li><strong>对 100% 的数据进行重新训练</strong> 。找到最佳超参数后，将最终模型拟合到所有训练数据上会挤出额外的准确性。</li></ul><p><strong>为什么重要：</strong> 这些步骤不需要新的架构，只需对您已经信任的模型进行更多运行即可。它们共同提高了稳健性并确保您充分利用数据。</p><p><strong>在行动中：</strong> 在<em>预测最佳肥料</em>挑战中，对 100 种不同种子进行集成 XGBoost 模型明显优于单种子训练。对完整数据集进行重新训练提供了另一个排行榜提升。<a href="https://www.kaggle.com/competitions/playground-series-s5e6/discussion/587393"> 阅读全文 &gt;</a></p><p><img src="/2025/20251014/6.webp"></p><p>使用 <strong>GPU 变得实用：</strong> 在 GPU 上更快的训练和推理使得多次重新运行模型成为可能。在 CPU 上可能需要数天的时间在 GPU 上变成数小时——将“额外”训练变成每个项目的现实步骤。 </p><h2 id="总结：特级大师赛的战术手册"><a href="#总结：特级大师赛的战术手册" class="headerlink" title="总结：特级大师赛的战术手册"></a>总结：特级大师赛的战术手册</h2><p>这本剧本经过实战考验，是经过多年的比赛和无数实验锻造而成的。它基于两个原则——快速实验和仔细验证——我们把这些原则应用于每个项目。借助 GPU 加速，这些先进技术变得大规模实用，使其对于现实世界的表格问题和攀登排行榜同样有效。</p><p>如果您想将这些想法付诸实践，这里有一些资源可以帮助您在已经使用的工具中开始使用 GPU 加速：</p><ul><li><a href="https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cudf_pandas_colab_demo.ipynb"><strong>Intro Notebook：使用 cuDF 加速 pandas</strong></a>（pandas 工作流程的零代码更改加速）</li><li><a href="https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/cuml_sklearn_colab_demo.ipynb"><strong>Intro Notebook：使用 cuML 加速 scikit-learn</strong></a>（常见 ML 模型的直接加速）</li><li>入<a href="https://colab.research.google.com/drive/1vlzvB981pej2RlKmXBUF1CNzyxl8YpJg?usp=sharing#scrollTo=p8MdG_7hthDD"><strong>门笔记本：GPU 加速的 XGBoost</strong></a>（在几分钟内训练数百万行上的梯度提升树）</li><li><a href="https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cudf"><strong>NVIDIA cuDF 概述</strong> </a>：了解有关加速 pandas 和 Polars 的更多信息。</li><li><a href="https://developer.nvidia.com/topics/ai/data-science/cuda-x-data-science-libraries/cuml"><strong>NVIDIA cuML 概述</strong> </a>：具有 GPU 加速的 Scikit 学习式机器学习。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五篇幻觉相关论文速览（二）</title>
    <link href="/2025/20251011/"/>
    <url>/2025/20251011/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>师兄组会所提到。 </p><p>2506.04039</p><p>2509.21997</p><p><a href="https://aclanthology.org/2025.naacl-long.75.pdf">https://aclanthology.org/2025.naacl-long.75.pdf</a></p><p>2505.24007</p><p>2503.13107</p><span id="more"></span><h2 id="通过以实体为中心的多模态偏好最优化缓解大型视觉-语言模型中的幻觉问题"><a href="#通过以实体为中心的多模态偏好最优化缓解大型视觉-语言模型中的幻觉问题" class="headerlink" title="通过以实体为中心的多模态偏好最优化缓解大型视觉-语言模型中的幻觉问题"></a>通过以实体为中心的多模态偏好最优化缓解大型视觉-语言模型中的幻觉问题</h2><p>（开源仓库为空） EMNLP2025 Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization</p><p><strong>模态错位（Modality Misalignment）</strong>：视觉编码器与 LLM 语义不对齐（如把“禁止机动车”错识为“禁止停车”）。 </p><p><strong>语言内生幻觉（LLM Inherent Hallucination）</strong>：LLM 根据训练共现（如“road”常伴“car”）臆造不存在实体。</p><p><img src="/2025/20251011/1.png"></p><p>为了构建被拒绝的图像，作者首先使用 GPT 4o-mini识别指令和响应中的实体，确保编辑后的图<br>像与文本紧密对齐。</p><p>随后使用目标检测模型定位这些实体。接着，应用 Stable-diffusion-2 来移除 30% 的实体或用视觉上合理的替代品替换它们，从而生成一个作为被拒绝样本的编辑图像 $v_l$。</p><p>最后，使用 CLIP计算编辑后图像区域与实体标签之间的相似度，以确保图像已正确编辑。</p><p>对齐人类偏好包含三个层面：图像、指令和响应。</p><p><img src="/2025/20251011/2.png"></p><h2 id="暴露幻觉以抑制幻觉：基于生成锚点的-VLMs-表示编辑"><a href="#暴露幻觉以抑制幻觉：基于生成锚点的-VLMs-表示编辑" class="headerlink" title="暴露幻觉以抑制幻觉：基于生成锚点的 VLMs 表示编辑"></a>暴露幻觉以抑制幻觉：基于生成锚点的 VLMs 表示编辑</h2><p>EXPOSING HALLUCINATIONS TO SUPPRESS THEM: VLMSREPRESENTATION EDITING WITH GENERATIVE ANCHORS</p><p><strong>贡献</strong>：</p><p>1)提出了一种无需训练的自监督方法，用于缓解多模态大模型中的幻觉问题。通过直接从模型自身输出中获取监督信号，以完全端到端且即插即用的方式运行。</p><p>2)引入了一种新颖的幻觉放大机制，利用 T2I 模型将字幕语义投影到视觉空间中。这使得原本隐式的幻觉变得可观测，并提供了一种轻量级的方法来构建可靠的监督信号。</p><p>3)同时锚定原始图像中的语义，并抑制重构图像中的幻觉方向。这种双重引导仅去除幻觉成分，同时保留真实语义，实现了忠实性与信息丰富性之间的平衡。</p><p>4)实验结果表明，显著优于现有方法。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>给定一张输入图像，使用视觉语言模型（VLM）生成一个初始描述，该描述可能包含幻觉对象或关系。为了揭示描述中潜在的幻觉内容，基于描述使用文本到图像（T2I）模型合成一张重构图像。<br>这种重构能够自然地将幻觉内容放大并外化到视觉空间中。</p><p>因此，原本在文本语义空间中隐含且难以检测的幻觉，在投影到图像后变得可感知。原始图像和重构图像均通过图像编码器和投影头，获得嵌入表示，分别记为 f (I) 和 f (I′)。</p><p>其中，f (I) 作为干净的语义锚点，引导表示向忠实的视觉语义方向发展；而 f (I′) 则显式捕捉通过重构放大的幻觉方向。通过同时将图像 token 嵌入拉向 f (I) 并推离 f (I′)，的方法建立了一种对抗性修正机制，无需手工设计的度量指标或外部监督。</p><p>因此，该设计将幻觉抑制转化为完全自监督的过程，实现了无需人工干预的端到端修正。</p><h4 id="潜在表示编辑"><a href="#潜在表示编辑" class="headerlink" title="潜在表示编辑"></a>潜在表示编辑</h4><p>$$<br>K’ _ {h,l}&#x3D;K _ {h,l}+\alpha f(I)-\beta f(I’),h\in \mathcal{H} _ {img},l\in[1,L]<br>$$</p><p>K表示嵌入。</p><p>f (·) 表示图像编码器与投影器的联合变换。</p><p><img src="/2025/20251011/3.png"></p><p>没有代码，不知道联合变换和$\alpha$是啥</p><h2 id="通过图像标记注意力引导解码减轻多模态大型语言模型的幻觉"><a href="#通过图像标记注意力引导解码减轻多模态大型语言模型的幻觉" class="headerlink" title="通过图像标记注意力引导解码减轻多模态大型语言模型的幻觉"></a>通过图像标记注意力引导解码减轻多模态大型语言模型的幻觉</h2><p>ACL</p><p>Mitigating Hallucinations in Multi-modal Large Language Models via Image Token Attention-Guided Decoding</p><h2 id="抢占式幻觉减少：一种用于多模态语言-模型的输入级方法"><a href="#抢占式幻觉减少：一种用于多模态语言-模型的输入级方法" class="headerlink" title="抢占式幻觉减少：一种用于多模态语言 模型的输入级方法"></a>抢占式幻觉减少：一种用于多模态语言 模型的输入级方法</h2><p>An Input-Level Approach for Multimodal Language Model</p><p><strong>准备三种图像变体</strong>：</p><p>原始图像 (org)：未作任何处理的图片 。</p><p>降噪图像 (NR)：使用中值滤波技术去除图像中的噪点，同时保留边缘清晰度 。</p><p>边缘增强图像 (EE)：使用拉普拉斯算子增强图像的边缘和细节 。</p><p><strong>模型生成答案</strong>：<br>将同一个问题分别与这三种图像变体配对，输入给大语言模型（本文使用的是 GPT-3.5），从而得到三个不同的答案 。</p><p><strong>评估与选择</strong>：<br>使用一个名为 SelfCheckGPT 的评估工具，通过计算 自然语言推理 (NLI) 分数 来判断哪个答案与“基准答案 (Ground Truth)”最一致 。NLI 分数越低，表示幻觉程度越低，答案越可靠 。最终，选择NLI分数最低的那个答案作为最终输出 。</p><p><strong>NLI分数</strong>：</p><p>NLI 的任务是判断“前提”是否能推导出“假设”。在幻觉检测的场景下，“前提”是<strong>模型生成的多个参考回答中的一个样本</strong> ($S^n$)，而“假设”是<strong>当前正在被评估的句子</strong> ($r_i$)。</p><p>计算过程主要关注两个逻辑类别：“蕴含 (entailment)”和“矛盾 (contradiction)”。</p><ol><li><p><strong>计算单个句子与单个样本的矛盾概率</strong>：<br>这个概率的计算公式如下：</p><p>$$P(contradict | r_i, S^n) &#x3D; \frac{\exp(z_c)}{\exp(z_e) + \exp(z_c)}$$</p><ul><li>$P(contradict | r_i, S^n)$：代表在给定一个参考样本 $S^n$ 的情况下，句子 $r_i$ 被判定为“矛盾”的概率。</li><li>$z_c$ 和 $z_e$：分别代表 NLI 模型输出的“矛盾”和“蕴含”这两个类别的原始得分（logits）。</li><li>$\exp()$：是指数函数，通常在多分类问题中与 Softmax 函数结合使用，用于将原始得分转换为概率。</li><li>这个公式的特点是它忽略了“中性”类别，只在“蕴含”和“矛盾”之间进行归一化，确保概率值在 0.0 到 1.0 之间。</li></ul></li><li><p><strong>计算最终的 NLI 分数 (SelfCheckGPT Score)</strong>：<br>为了得到一个更可靠的分数，模型会生成 N 个不同的参考样本 ($S^n$)，然后计算待评估句子 $r_i$ 与所有这些样本的平均矛盾概率。最终的 NLI 分数由以下公式得出：</p><p>$$S_{NLI}(i) &#x3D; \frac{1}{N} \sum_{n&#x3D;1}^{N} P(contradict | r_i, S^n)$$</p><ul><li>$S_{NLI}(i)$：就是句子 $r_i$ 的最终 NLI 分数。</li><li>$N$：是生成的参考样本的总数。</li><li>$\sum_{n&#x3D;1}^{N}$：表示将句子 $r_i$ 与从 1 到 N 的每一个参考样本计算出的矛盾概率全部加起来。</li><li>$\frac{1}{N}$：表示取平均值。这种对多个样本取平均的做法是为了确保分数的稳定性与可靠性。</li></ul></li></ol><h2 id="ClearSight：面向多模态大语言模型物体幻觉缓解的视觉信号增强技术"><a href="#ClearSight：面向多模态大语言模型物体幻觉缓解的视觉信号增强技术" class="headerlink" title="ClearSight：面向多模态大语言模型物体幻觉缓解的视觉信号增强技术"></a>ClearSight：面向多模态大语言模型物体幻觉缓解的视觉信号增强技术</h2><p>CVPR 2025</p><h3 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h3><p>ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models</p><p>对比解码方法无需训练或依赖外部工具，具备高计算效率与广泛适用性，在学术界引起了极大关注。然而，此类方法仍存在两大不足：生成内容质量下降及推理速度较慢。</p><p>利用泰勒展开式计算注意力矩阵中每个元素的显著性得分：<br>$$<br>I_l&#x3D;\left|\sum_h A_{h,l}\odot \frac{\partial \mathcal{L}(x)}{\partial A_{h.l}} \right|<br>$$<br>为了更清晰地描绘 MLLMs 中的视觉信息流，基于 $I_l(i, j) $引入了两个定量指标，特别关注涉及图像 token 的信息交互。<br>$$<br>\begin{align}<br>S_{vv}&#x3D;\frac{\sum_{(i,j)\in C_{vv}}I_l(i,j)}{C_{vv}},C_{vv}&#x3D;((i,j):i,j\in\mathcal{V},i\ge j)<br>\\<br>S_{vt}&#x3D;\frac{\sum_{(i,j)\in C_{vt}}I_l(i,j)}{C_{vv}},C_{vt}&#x3D;((i,j):i\in\mathcal{T},j\in\mathcal{V})<br>\end{align}<br>$$<br><img src="/2025/20251011/4.png"></p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p>基于前面提出的见解，引入了一种称为视觉增强融合（VAF）的幻觉缓解方法。</p><p>在中间层（即 8 &lt; l &lt; 15）中修改注意力得分矩阵如下：</p><p><img src="/2025/20251011/5.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>So You Want to Be an Academic?</title>
    <link href="/2025/20251006/"/>
    <url>/2025/20251006/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>转自<a href="https://anandbhattad.github.io/blogs/jr_grads.html">《所以你想成为一名学者吗？我希望我在研究生院早点就知道什么》l</a>。作者是是约翰霍普金斯大学计算机科学系的助理教授。担任过ICCV 2025、CVPR 2025、WACV 2025 的AC。</p><span id="more"></span><hr><p>你攻读博士学位已经几年了，一个大问题开始悄悄出现： <strong>我做对了吗？</strong> 大多数建议都是针对处于终点线的学生——即将毕业的学生——关于工作包、面试脚本和谈判策略。但对于仍在决定学术道路是否适合他们的低年级学生来说，情况要少得多。这是给你的。这是我学到的关于工作、知名度、社区和理智的知识（通常是艰难的）——早在就业市场即将到来之前。</p><h2 id="机构品牌（Institutional-Branding）的现实"><a href="#机构品牌（Institutional-Branding）的现实" class="headerlink" title="机构品牌（Institutional Branding）的现实"></a>机构品牌（Institutional Branding）的现实</h2><p>让我们从残酷的事实开始：您机构的品牌和导师的声誉很重要。</p><p>我很幸运能在一个强大的项目中与一位著名的导师合作，这种结合绝对为我打开了大门。“老男孩俱乐部”（”old boys’ club”）效应是真实存在的。</p><blockquote><p>Old boys’ club”（老男孩俱乐部）是一个源自英国的说法，如今在国际上广泛使用。它指的是一个非正式的社交和职业网络，其成员通常是来自相似社会背景、教育经历（尤其是精英学校）的男性。他们在这个封闭的圈子里互相提携，共享资源和机会，从而巩固彼此的社会和经济地位。</p><p>这个词最初用来形容英国那些曾就读于著名男子公学的“老男孩”（old boys）们所形成的紧密人脉网。如今，它的含义已经扩大，泛指任何以男性为主导、具有排外性的权势小圈子，尤其是在政界、商界和某些特定行业中。</p><p><strong>“老男孩俱乐部”的主要特征包括：</strong></p><ul><li><strong>排他性强：</strong> 这种网络通常将女性和少数族裔等“圈外人”排斥在外，使得他们难以进入权力的核心圈层。</li><li><strong>非正式运作：</strong> 许多重要的决策和信息交流并非在正式的会议中进行，而是在高尔夫球场、私人会所、豪华晚宴或体育赛事等非正式社交场合完成。</li><li><strong>心照不宣的互惠：</strong> 圈子内的成员会优先录用、提拔或与自己背景相似的人合作，形成一种“自己人帮自己人”的文化。</li><li><strong>固化不平等：</strong> 这种文化会阻碍组织内部的多元化发展，导致晋升渠道对女性和少数族裔不公，加剧职场中的“玻璃天花板”现象。</li></ul></blockquote><p>网络是粘性的：精英学校相互招聘更多，这加强了他们的主导地位。这不仅仅是一种感觉;数据支持它。不成比例的教职工作流向了少数大学（众所周知的“四大”：麻省理工学院、斯坦福大学、伯克利大学、卡内基梅隆大学）的毕业生。</p><p><img src="/2025/20251006/1.png"></p><p>即使是像我攻读博士学位的伊利诺伊州这样的项目，尽管培养了优秀的研究人员，但也没有在这些招聘网络中显眼。这表明学术招聘的集中程度。</p><p>看到这一点很容易，感觉游戏在开始之前就被操纵了。那么，如果你不在那个系统中，你会怎么做？ <strong>你不会按照他们的条件竞争;你改变了游戏规则。</strong> 声望的反策略是成为自己利基市场中不可或缺的专家。找到一个新问题、一种新方法或一个独特的数据集，并成为人们与之联系在一起的人。如果你能创造一个拥有明显优势的知识空间，你的声望就会跟随你的工作，而不是相反。</p><h2 id="了解学术指标：它们很重要"><a href="#了解学术指标：它们很重要" class="headerlink" title="了解学术指标：它们很重要"></a>了解学术指标：它们很重要</h2><p>现在，另一个残酷的事实：引用数和你的 h 指数很重要。你会听到不同的观点，但根据我的经验，他们确实会听到。只是不是你想的那样。</p><p>当我向导师询问进入市场的问题时，他们中的一些人立即打开了我的 Google 学术页面。他们没有看原始数字;他们正在寻找我的生产力和一种趋势——非正式的“曲棍球棒”（缓慢的开始，然后是陡峭的上升曲线）。这条曲线的轨迹远比任何单个数字都重要。是平坦的，还是开始攀升？</p><p><img src="/2025/20251006/2.png"></p><p>对于大多数应届博士来说，绝对数字仍然相对较低，因此“曲棍球棒”模式可能还没有戏剧性。在我看来，委员会真正寻找的是影响力不断增强的证据——无论是通过一些开始受到关注的论文，还是为其他人探索新问题开辟的工作。</p><p>在我采访过的几家机构中，他们希望认真考虑 ~2,000 次引用（如果您从事计算机视觉等应用机器学习工作）;否则，推荐信需要解释低引用次数以及您如何脱颖而出。您可以查看目标机构最近招聘的人员，以了解他们实际使用的基准。</p><p>最重要的是<strong>要了解的是，您不会直接控制数字;您控制的是赢得关注的工作。</strong> 这是许多人出错的地方。如果你的工作始终是增量的，那么成为一台“造纸机”可能会适得其反。渐进式步骤是研究的必要组成部分，但您不能仅凭这些步骤建立职业生涯。我听说过几位资深同事的来信，他们审查过具有出色数字（论文数量和指标）的候选人，但由于工作本身缺乏实质内容而通过了。</p><p>旨在制作出为他人打开一扇新门的作品。因此，当您计划下一个项目时，请问一个不同的问题：不仅仅是“这会被发布吗？”而是 <strong>“这将允许其他人提出什么新问题？</strong></p><h2 id="以“灌篮高手”（Slam-Dunk）论文为目标"><a href="#以“灌篮高手”（Slam-Dunk）论文为目标" class="headerlink" title="以“灌篮高手”（Slam Dunk）论文为目标"></a>以“灌篮高手”（Slam Dunk）论文为目标</h2><p>在研究生院，人们可能会觉得目标是积累尽可能多的出版物。但影响并不总是累积的。有时，它是指数级的。</p><p>一篇重要的“灌篮高手”论文可以改变你的整个轨迹。我亲眼目睹了这一点。在我攻读博士学位后期，一篇论文突然让我成为竞争激烈的博士后职位的关注点。在 TTIC，随后的几篇论文增加了连贯性和动力，围绕我的研究创造了一个清晰的故事。人们对这三个“锚定”项目的记忆远远超过他们拥有更长的较小、渐进式胜利的清单。</p><p><img src="/2025/20251006/3.png"></p><p>这通常意味着通过“将项目切成薄片”来抵制快速发布的冲动。这需要耐心，有时甚至是一个艰难的决定。我的博士生导师看着我的时间表，已经为我毕业做好了准备。但我有一种直觉，一个项目需要更多时间才能充分发挥其潜力。我努力再待一年，这个决定导致了可以说是我最重要的工作。</p><p>当然，时机很重要。你不能总是预测哪些论文会产生重大影响——研究本质上是不确定的。有些论文需要时间才能产生明显的影响。有些人很早就找到了他们的定义项目，而对于其他人来说，这需要更长的时间。两条路都很好。关键是要保持雄心勃勃的心态。以让你兴奋并提出新问题为目标的工作，或者想一想你钦佩的研究人员会说“我为什么没有想到这一点”或“哇，这是一个巧妙的想法，而且非常有用”的问题。为了培养这种雄心勃勃的思维，我一直很喜欢我的博士生导师的挑衅口号：“我需要解决什么问题才能成名？</p><p>虽然“出名”听起来可能很宏大，但潜在的问题是一个强大的战略工具： <strong>如果你解决了这个问题，会产生不可否认的影响并开辟一个新的研究领域？</strong> 这就是你的灌篮高手。</p><h2 id="害怕被抢走？好！这是一个信号。"><a href="#害怕被抢走？好！这是一个信号。" class="headerlink" title="害怕被抢走？好！这是一个信号。"></a>害怕被抢走？好！这是一个信号。</h2><p>“如果我的想法被挖了怎么办？”恐惧是自然的。我们都感受到了。但我已经学会了停止将这种恐惧视为焦虑的根源，并开始将其用作战略信号。</p><p>如果一个想法感觉很容易被挖出来，那是因为你身处一个拥挤的领域，每个人都在玩同一个游戏。这个信号告诉你做出选择：要么成为正面交锋中最快的执行者，要么彻底改变游戏规则。</p><p>我不得不做出这个选择。我知道我不是最快的编码员。我知道我不会赢得出版竞赛。所以我决定停止按照这些条件竞争，而是在自己的地盘上打球。</p><p>对我来说，这意味着专注于利基但重要的问题，随着时间的推移，我可以建立特定的、复合的优势。你的地盘是你拥有独特优势的山丘：它可以是你收集的数据集、来自你独特背景的数学框架、更高的评估标准，或者只是你对问题的品味。</p><p><img src="/2025/20251006/4.png"></p><p><strong>一旦你找到了自己的地盘，就拥有它。</strong> 坚定你的想法。将实验的标准提高到如此之高，以至于休闲竞争对手不会打扰。让你的工作变得明确。不要让害怕被偷窃将你推入疯狂、肤浅的工作中。让它引导您解决深刻的、可防御的问题，在那里您可以构建持久的东西。</p><p>这里有一个权衡：利基（意为小众领域）问题减少了竞争并帮助您建立独特的专业知识，但它们也可能限制您的总影响和引用次数。对于大多数不在精英院校的学生来说，这种权衡是值得的——最好在较小的领域成为领先的专家，而不是在拥挤的领域中成为次要参与者。但这并不意味着您应该自动避开拥挤的场地。如果你有独特的视角，能够明显脱颖而出，那么值得一试。在我看来，探索和开发之间的这种权衡来自经验，以及与你的顾问就你的战略定位进行诚实的对话。这让我想到了下一点。</p><h2 id="与您的导师交谈——直接且经常"><a href="#与您的导师交谈——直接且经常" class="headerlink" title="与您的导师交谈——直接且经常"></a>与您的导师交谈——直接且经常</h2><p>你的博士生导师是你学术旅程中最重要的向导，但他们不是读心术者。</p><p>如果你想要他们的帮助，你需要直接、具体和有策略。就您的目标进行诚实的对话。不要等他们问。我发现明确是有用的：“ <strong>我的目标是毕业后获得一份顶尖的学术工作。看看我目前的发展轨迹，在接下来的六个月里，我应该关注哪一两件事来提高我的机会？</strong></p><p>通常，我从我的导师大卫那里得到的答案很简单：“好研究，好研究，好研究。虽然这听起来很明显，但这种重复中有一个强大的信号。我学会了“读懂字里行间”，并将这些建议转化为具体行动：提出好的研究问题，提高我的评估标准，写得更清晰，并准备好捍卫每一个选择。</p><p><img src="/2025/20251006/5.png"></p><p>这是一个对我有用的策略：我在特定里程碑之后安排了专门的反馈会议，比如提交论文。背景很新鲜，作品是一个完整的包，我们可以就它的优点和缺点进行集中对话。这比在随机的每周会议上试图获得模糊的反馈要有效得多。 <strong>创建强制建设性反馈的事件。</strong></p><h2 id="寻求建议（即使您认为自己知道）"><a href="#寻求建议（即使您认为自己知道）" class="headerlink" title="寻求建议（即使您认为自己知道）"></a>寻求建议（即使您认为自己知道）</h2><p>你在研究生院可能犯的最大错误之一就是一旦你感到自信就停止寻求建议。</p><p>人们慷慨地投入时间并乐于提供帮助。征求他们的意见是一种强大的策略，但它需要正确的方法。这不是要问，“我应该怎么做？相反，带着完整的计划去找导师、委员会成员或高级同事并寻求批评。</p><p>试试这个框架：“ <strong>这是我下一个项目的计划。我很想让你读一读。你看到我可能错过的任何盲点或潜在陷阱吗？</strong> 这表明了信心，同时对输入保持开放态度。</p><p>通常，您会得到一个新的角度或重要的健全性检查，以加强您的工作。但真正的好处是关系性的。当您寻求建议时，您不仅仅是在获取信息，而是在获取信息。你正在邀请某人成为你旅程的一部分。人们开始投资于他们帮助指导的人的成功。</p><p><img src="/2025/20251006/6.png"></p><p>随着时间的推移，这些导师不仅会给你建议，还会给你建议。他们成为您的拥护者，将您与机会联系起来，并在您不在房间时为您辩护。</p><h2 id="建立学术知名度"><a href="#建立学术知名度" class="headerlink" title="建立学术知名度"></a>建立学术知名度</h2><p>做好工作是必要的，但这还不够。在学术界，你必须确保你的工作被看到，这需要深思熟虑的、持续的行动。</p><p>不要等待邀请或外部验证;创造自己的机会。参加讲座。成为提出深思熟虑的问题的人。成为跟进演讲者的人。做一个作品与众不同的人，而不仅仅是当前趋势的又一个后续。联系不是偶然发生的，而是偶然发生的。你建造它们。他们是跨群体传递您的想法和声誉的原因。</p><h3 id="掌握问题的艺术"><a href="#掌握问题的艺术" class="headerlink" title="掌握问题的艺术"></a>掌握问题的艺术</h3><p>您参加的每一次演讲都是一个机会。将问答环节视为您的训练场。我们的目标不是炫耀或问一个冗长、漫无边际的问题，而这些问题实际上是一个评论。目标是仔细倾听并提出一个清晰、诚实的问题，表明您很投入。</p><p>始终如一地这样做可以实现两件事。首先，它建立了你作为敏锐、建设性思考者的知名度和声誉。其次，它提高了你自己看到问题结构的能力——这项技能将在你自己的研究中带来好处。</p><p><img src="/2025/20251006/7.png"></p><h3 id="演讲是一门艺术"><a href="#演讲是一门艺术" class="headerlink" title="演讲是一门艺术"></a>演讲是一门艺术</h3><p>我仍然记得在 2021 年的 CMU MISC 阅读小组中发表了可能是我有史以来最糟糕的演讲。它很仓促，很混乱，事后我感觉很糟糕。但我非常高兴失败发生在当时，而不是后来在高风险的工作谈话中。</p><p>演讲是一门艺术，就像任何手艺一样，只有通过练习才能提高。邀请自己在大学阅读小组或实验室会议上发表演讲。那些早期的、风险较低的会谈会给你带来观点，迫使你简化你的信息而不削弱它，并在还有时间修复它们时暴露你的弱点。你不希望你的工作演讲是你第一次在严肃的外部听众面前。</p><h2 id="建立您想成为其中一员的社区"><a href="#建立您想成为其中一员的社区" class="headerlink" title="建立您想成为其中一员的社区"></a>建立您想成为其中一员的社区</h2><p>服务通常被视为一件苦差事，一个需要检查的方框。但如果你做得好的话，这可能是你学术生活中最有价值的部分之一。我经常看到人们担任服务角色只是因为他们觉得自己“应该”，或者因为它在简历上看起来不错。这是一个错误。服务应该是关于建立你想成为其中一部分的社区。</p><blockquote><p>If the community doesn’t exist, build it. —— Sara Beery</p></blockquote><p>萨拉·比里 （Sara Beery） 的这句话引起了我的深刻共鸣。在 COVID 隔离期间，我感到沮丧的是，每次虚拟研讨会都邀请了同样少数知名人士，而初级研究人员却在努力提高知名度。因此，我在 UIUC 开始了一个早期职业演讲者系列。我们的第一个阵容包括十几位该领域的后起之秀，其中许多人现在正在领导自己的实验室。</p><p>该系列持续了多年，最终成为学分研讨会（for-credit seminar）。它帮助了其他人，并教会了我如何为新想法创造一个表面区域。后来，“学者和大模型”研讨会源于我在阅读了一篇 DeepMind 论文后提出的一个真实问题，该论文展示了其巨大的 GPU 预算：如果没有这种规模，学者们怎么可能进行有影响力的研究？</p><p>我首先给几位高级教师发了电子邮件。“这个问题变成了一个多年的研讨会系列。今天，我经常遇到那些从这些事件中认识我的人。虽然我仍然想以我的科学而闻名，但我了解到社区建设是工作的一部分——尤其是当它是由真正的好奇心驱动的，而不是由简历驱动时。</p><p><img src="/2025/20251006/8.png"></p><h2 id="心理健康和冒名顶替的感觉"><a href="#心理健康和冒名顶替的感觉" class="headerlink" title="心理健康和冒名顶替的感觉"></a>心理健康和冒名顶替的感觉</h2><p>博士学位是一场可以测试您心理健康的马拉松。你很容易感到自己渺小，尤其是当你看到其他实验室的华而不实的结果时。</p><p>你没有归属感——你是一个冒名顶替者——几乎是普遍的。认识到这些感觉的本质：不是证明你是一个冒名顶替者的证据，而是在有才华的人的包围下解决雄心勃勃的问题的自然副产品。不要让这种感觉让你脱轨。相反，重新构建游戏。</p><p>选择规模不是唯一杠杆的问题;专注于您的独特品味、精心设计或严格测量可以成为您优势的领域。当你发现这些问题时，你的信心就会增强，因为你正在发挥自己的优势，而不是试图超越他人。</p><p>最重要的是，保护您的健康。留出休息时间。学会在必须的时候说“不”。在艰难的日子里，阅读你的小胜利的运行清单，这样你就不会忘记你正在取得进步。</p><h2 id="必要的警告：幸存者偏差"><a href="#必要的警告：幸存者偏差" class="headerlink" title="必要的警告：幸存者偏差"></a>必要的警告：幸存者偏差</h2><p>这是我的道路，而不是一个普遍的公式。时机、运气和特权发挥了重要作用。我有特定的合作者、背景和限制，而你不会这样做。请将这些想法视为启发式方法，而不是严格的规则。 <strong>你的道路将而且应该看起来不同。</strong></p><h2 id="最后的思考"><a href="#最后的思考" class="headerlink" title="最后的思考"></a>最后的思考</h2><p>在所有这些建议之后，最重要的是<strong>相信</strong>自己并<strong>专注于</strong>你觉得真正令人兴奋的事情。你控制输入：你的想法的清晰度、你执行时的谨慎、你向社区展示的慷慨，以及你在困难部分的坚持。</p><p>如果你正确地输入了这些输入，指标和认可度往往会随之而来。但更重要的是，您将建立一系列工作、同事网络和一个您自豪地支持的故事。归根结底，这才是学术生涯中最重要的事情。</p><h2 id="相关阅读"><a href="#相关阅读" class="headerlink" title="相关阅读"></a>相关阅读</h2><ul><li><a href="https://matt.might.net/articles/phd-school-in-pictures/">Matt Might — The Illustrated Guide to a Ph.D.</a></li><li><a href="https://www.cs.jhu.edu/~jason/advice/">Jason Eisner — Advice for Research Students</a></li><li><a href="https://michaelnielsen.org/blog/principles-of-effective-research/">Michael Nielsen — Principles of Effective Research</a></li><li><a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">Richard Hamming — You and Your Research</a></li><li><a href="https://homes.cs.washington.edu/~mernst/advice/">Michael Ernst — Advice for Researchers and Students</a></li><li><a href="https://github.com/pliang279/awesome-phd-advice">Paul Liang — Awesome PhD Advice (curated list)</a></li><li><a href="http://linyun.info/phd-grinding.pdf">Philip Guo — The Ph.D. Grind</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>演讲</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一篇顶会论文的诞生：从零到一的实战经验分享</title>
    <link href="/2025/20251005/"/>
    <url>/2025/20251005/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>作者<strong>吴家隆</strong>，东南大学硕士生。之前曾在西湖大学和 AIWaves 实习，并且是通义实验室的DeepResearch团队的创始成员。</p><span id="more"></span><h2 id="个人介绍"><a href="#个人介绍" class="headerlink" title="个人介绍"></a>个人介绍</h2><p>本科三年级第一篇一座顶会COLING2022 CCF-B</p><p>2024年2月投稿两篇论文（共）一作ACL2025。</p><p>研究方向：</p><p>情感分类</p><p>生成式的parsing和图结构任务</p><p>Agent</p><p>高效LLM</p><p><img src="/2025/20251005/1.png"></p><p><img src="/2025/20251005/2.png"></p><h2 id="Idea阶段"><a href="#Idea阶段" class="headerlink" title="Idea阶段"></a>Idea阶段</h2><p><strong>科研信息获取：</strong></p><ul><li>arxiv→作者在社交媒体宣传→公众号宣传</li><li>X&#x2F;twitter：关注一个研究方向的学术大V，顺着评论区和关注列表一路点进去，然后相信推荐系统。用户AK会转发热门的paper。</li><li>小红书&#x2F;公众号&#x2F;知乎</li><li>机器之心、量子位、新智元</li></ul><p><strong>研究方向：</strong></p><ul><li><p>follow组里研究方向</p></li><li><p>自己找感兴趣的</p></li></ul><p>热点：竞争力大，资源消耗大。</p><p>冷门：竞争力小，影响力小，找工作&#x2F;读博时机会相对小。</p><ul><li><p>看看别人在做什么</p><ul><li><p>工业届</p><p>腾讯犀牛鸟</p></li><li><p>学术界</p></li></ul><p><a href="https://aclanthology.org/">https://aclanthology.org</a></p><p>看每年会议不同track的投稿量和接受率。</p></li></ul><h2 id="idea怎么来"><a href="#idea怎么来" class="headerlink" title="idea怎么来"></a>idea怎么来</h2><ul><li>看看别人在做什么</li></ul><p>工业届，犀牛鸟有动机和反思。</p><p>学术界，关注会议的workshop。</p><ul><li><p>LLM时代下可以定义新的任务</p></li><li><p>经典的idea构造想法</p></li></ul><p>A+B</p><p>自己熟悉的场景+热点话题。</p><ul><li><strong>最重要的是motivation</strong></li></ul><p>看survey</p><p>看论文原文</p><p>看博士论文，（有代表作，刚毕业。美国毕业论文都会公开）</p><p>解决什么问题（limitation）！怎么解决（methods）？</p><h2 id="写作阶段"><a href="#写作阶段" class="headerlink" title="写作阶段"></a>写作阶段</h2><p><strong>论文类型：</strong></p><ul><li>benchmark</li><li>强分析弱方法</li><li>纯方法</li><li>纯分析</li><li>survey</li></ul><p><strong>实验部分</strong><br>看高star的开源项目</p><p>一定要使用ai coding的工具，copilot&#x2F;cursor&#x2F;claude code</p><p><img src="/2025/20251005/3.png"></p><p><strong>LLM时代下的特别之处</strong></p><p>标题可以带emoji</p><p>图表一定要fancy</p><p><strong>画图</strong></p><p>审美：</p><p>配色：科研配色&#x2F;糖果色，colorhunt.co</p><p>图标：flatucon&#x2F;iconfont</p><p>画图：PPT&#x2F;draw.io</p><p><strong>学会从arxiv中下载latex源码</strong></p><h3 id="Rebuttal："><a href="#Rebuttal：" class="headerlink" title="Rebuttal："></a><strong>Rebuttal：</strong></h3><p><img src="/2025/20251005/4.png"></p><p>大模型时代下的新问题：</p><p>1.AI审稿</p><p>2.hack ai审稿（ICLR已禁止）</p><p>3.是否向AC举报，flag问题</p><h2 id="PR宣传阶段"><a href="#PR宣传阶段" class="headerlink" title="PR宣传阶段"></a>PR宣传阶段</h2><p>一定要花大量时间做PR</p><p>提前挂arxiv。</p><p>利用github page</p><p><img src="/2025/20251005/5.png"></p><h2 id="展示阶段"><a href="#展示阶段" class="headerlink" title="展示阶段"></a>展示阶段</h2><p><strong>好的工作永远不会埋没。</strong></p><p>不要相信小红书的投票，有严重的幸存者偏差。</p><p>投稿结束后去放松，避免燃尽。</p><p>可以利用各种bug提前知道结果。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>演讲</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过外推一个巨大且假设的语言模型的概率来解释和改进对比解码</title>
    <link href="/2025/20250928/"/>
    <url>/2025/20250928/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（EMNLP 2024 Oral）</p><span id="more"></span><p>arxiv 2411.01610</p><h2 id="对比解码相当于外推"><a href="#对比解码相当于外推" class="headerlink" title="对比解码相当于外推"></a>对比解码相当于外推</h2><p>ELM专家，ALM业余。</p><p>tokenw的logit为：<br>$$<br>L_c^{CD}(w)&#x3D;L_c^{ELM}(w)-\frac{1}{T}L_c^{ALM}(w)<br>$$<br>假设ALM的温度T&gt;1，以及logit与模型大小存在线性关系。</p><blockquote><p>有一点合理性，比如模型更大，准确率可能越高，可能越自信。</p></blockquote><p>$$<br>\begin{align}<br>L_c^{CD}(w)&amp;&#x3D;L_c^{ELM}(w)-\frac{1}{T}L_c^{ALM}(w)<br>\\&amp;&#x3D;(1-\frac{1}{T})(L_c^{ELM}(w)+\frac{1}{T}(L_c^{ELM(w)}-L_c^{ALM}(w)))<br>\\&amp;&#x3D;(1-\frac{1}{T})(L_c^{ELM}(w)-\frac{\frac{1}{T}}{1-\frac{1}{T}}L_c^d(w))<br>\end{align}<br>$$</p><p><img src="/2025/20250928/1.png"></p><p>HLM和ELM的大小差异应为$\frac{\frac{1}{T}}{1-\frac{1}{T}}S^d$。<br>$$<br>\begin{align}<br>\log s^{HLM}<br>&amp;&#x3D; \log s^{ELM} + \frac{\frac{1}{T}}{1-\frac{1}{T}} S^d \\<br>&amp;&#x3D; \log s^{ELM} + \frac{1}{T-1}\left( \log s^{ELM} - \log s^{ALM} \right) \\<br>&amp;&#x3D; \frac{T}{T-1}\log s^{ELM} - \frac{1}{T-1}\log s^{ALM} \\<br>&amp;&#x3D; \frac{1}{T-1} \log \left( \frac{\left(s^{ELM}\right)^T}{s^{ALM}} \right)<br>\end{align}<br>$$</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><strong>渐进概率解码 (Asymptotic Probability Decoding, APD)</strong> ——的实现可以分为两个阶段：<strong>训练阶段</strong>和<strong>推理（测试）阶段</strong>。其核心在于通过一个创新的训练过程来微调（fine-tune）业余模型（ALM） 。</p><h3 id="1-训练阶段：微调业余模型（ALM）"><a href="#1-训练阶段：微调业余模型（ALM）" class="headerlink" title="1. 训练阶段：微调业余模型（ALM）"></a>1. 训练阶段：微调业余模型（ALM）</h3><p>训练阶段是整个方法最关键和最复杂的部分。它的目标不是从头训练一个新模型，而是<strong>对一个现有的、小的“业余模型”（ALM）进行微调，得到一个新版本 ALM’</strong> 。这个新的 ALM’ 经过特殊训练，能够帮助专家模型（ELM）更好地预测出那个“无限大”的假设模型的概率。</p><p>具体实现步骤如下，整个过程在 <strong>Algorithm 1</strong> 中有总结：</p><p><strong>第一步：数据准备和概率收集</strong></p><ol><li><strong>选择模型家族</strong>：首先，需要一个包含多个不同尺寸模型的语言模型（LLM）家族，例如论文中使用的Pythia（包含70M, 160M, …, 6.9B等多个尺寸）。</li><li><strong>收集概率</strong>：在训练语料库（例如维基百科的一个子集）上运行这个家族中的 N 个模型 。对于每一个上下文 <code>c</code>，记录下这些模型为下一个候选词元 <code>w</code> 输出的概率，得到一组经验概率点 $\{p(w|c, \theta _ {s_i})\} _ {i&#x3D;1}^{N}$ 。</li><li><strong>选择候选词元</strong>：由于词汇表太大，不可能为所有词元都建模。因此，对于每个上下文，只选择一部分候选词元 <code>A_c</code> ，主要包括专家模型（ELM）认为最可能的Top-20个词元，以及一些随机采样的低概率词元 。</li></ol><p><strong>第二步：概率曲线参数化</strong><br>为了能够对外推进行建模，APD需要拟合一条穿过上述经验概率点的曲线。</p><ol><li><strong>统一曲线方向</strong>：因为概率曲线可能随模型增大而上升或下降，为了简化建模，论文首先使用一个预处理步骤 <code>R(·)</code>，将所有上升的概率曲线“翻转”（通过 <code>1-p</code> 的方式）为下降曲线。</li><li><strong>选择曲线函数</strong>：使用一个简单的<strong>指数衰减函数</strong>来参数化这条（翻转后的）下降曲线 。这个函数的形式如下（见公式(7)） ：<br>$$<br>\hat{p} _ {w,c}(s) &#x3D; \hat{P’} _ {c}^{AP}(w) + a _ {w,c}e^{-max(0, b _ {w,c}(s-d _ {w,c}))}<br>$$<ul><li><code>s</code> 是模型大小的对数。</li><li>$\hat{P’} _ {c}^{AP}(w)$ 是模型尺寸趋于无穷大时的<strong>渐进概率</strong>，这也是我们最终想要预测的目标。</li><li><code>a, b, d</code> 是控制曲线形状的三个正参数。</li></ul></li></ol><p><strong>第三步：使用MLP作为能量网络进行拟合</strong></p><ol><li><strong>MLP的角色</strong>：论文使用一个4层的多层感知机（MLP）来确定上述曲线的参数 <code>a, b, d</code> 。这个MLP不直接预测概率，而是扮演一个“能量网络”或“评估器”的角色 。</li><li><strong>MLP的输入和输出</strong>：<ul><li><strong>输入</strong>：MLP接收两组数据：(1) 从不同大小模型收集到的经验概率点 $\{p’(w|c, \theta _ {s_i})\} _ {i&#x3D;1}^{N}$ ；(2) 由当前微调中的 ALM’ 和 ELM 计算出的<strong>预测渐进概率</strong> $\hat{P’} _ {c}^{AP}(w)$。</li><li><strong>输出</strong>：MLP输出曲线参数 <code>a, b, d</code> 。</li></ul></li><li><strong>核心机制</strong>：如果 ALM’ 预测出的渐进概率是“好的”，那么MLP就能很容易地找到一组 <code>a, b, d</code> 参数，使得生成的曲线能够很好地拟合所有经验概率点 。反之，如果预测的渐进概率“不好”，MLP将无法生成一条能同时穿过所有已知点的平滑曲线，从而导致很高的损失 。</li></ol><p><strong>第四步：定义损失函数并进行反向传播</strong><br>为了训练 ALM’ 和 MLP，论文设计了一个包含三个部分的组合损失函数：</p><ul><li><strong>损失1 ($L_1$)</strong>：拟合损失。计算MLP生成的曲线与真实经验概率点之间的均方根误差（RMSE），确保曲线能很好地拟合已知数据。</li></ul><p>$$<br>L _ {1} &#x3D;<br>\sqrt{<br>\frac{1}{Z \cdot (N-1)}<br>\sum _ {c \in B}<br>\sum _ {w \in A _ {c}}<br>\sum _ {i&#x3D;1}^{N-1}<br>\left(<br>p’(w \mid c, \theta _ {s _ {i}}) - \hat{p} _ {w,c}(s _ {i})<br>\right)^{2}<br>}<br>$$</p><p>其中$A_c$为ELM的top token候选，归一化项$Z&#x3D;|B||A_c|$。</p><ul><li><strong>损失2 ($L_2$)</strong>：过高惩罚损失。为了防止模型对下降曲线的渐进概率预测过高，这个损失项专门惩罚那些在ELM尺寸上预测概率超过真实观测概率的情况。</li></ul><p>$$<br>L_2 &#x3D; \sqrt{\frac{1}{Z} \sum _ {c \in B} \sum _ {w \in A_c} \max(0, \hat{p} _ {w,c}(s_N) - p’(w|c, \theta _ {s_N}))}<br>$$</p><ul><li><strong>损失3 ($L_3$)</strong>：正则化损失。为了防止微调后的 ALM’ 与原始的 ALM 偏离太远，这个损失项计算它们输出对数（logit）之间的差异，起到了稳定训练的作用。</li></ul><p>$$<br>L_3 &#x3D; \sqrt{\frac{1}{Z} \sum _ {c \in B} \sum _ {w \in A_c} (L_c^{ALM’}(w) - L_c^{ALM}(w))^2}<br>$$</p><p>将这三个损失加权求和 <code>Loss = L_1 + \lambda_2 L_2 + \lambda_3 L_3</code> ，然后通过<strong>反向传播</strong>，梯度会从损失函数流经MLP，再流回到 ALM’。这样，ALM’ 的参数就会被更新，使其能够生成一个更好的渐进概率预测，从而让MLP更容易拟合曲线，最终使总损失降低。</p><h3 id="2-推理（测试）阶段：高效生成文本"><a href="#2-推理（测试）阶段：高效生成文本" class="headerlink" title="2. 推理（测试）阶段：高效生成文本"></a>2. 推理（测试）阶段：高效生成文本</h3><p>一旦训练完成，我们就得到了一个优化后的业余模型 ALM’ 。推理阶段就变得非常简单和高效了：</p><ol><li><strong>抛弃复杂组件</strong>：在推理时，不再需要MLP和那些用于训练的中间尺寸模型。</li><li><strong>使用新公式</strong>：直接使用微调好的 ALM’ 替换掉原始CD公式中的 ALM。最终的概率计算公式（见公式(5)）如下：<br>$$<br>\hat{P} _ {c}^{AP}(w) &#x3D; \text{Softmax}(L _ {c}^{ELM}(w) - \frac{1}{T}L _ {c}^{ALM’}(w))<br>$$</li><li><strong>效率</strong>：这个计算过程和原始的对比解码（CD）完全一样，都只需要一次专家模型（ELM）和一次业余模型（ALM’）的前向传播。因此，<strong>APD在实现更复杂的外推的同时，没有增加任何额外的推理成本</strong> 。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>五篇幻觉相关论文速览</title>
    <link href="/2025/20250927/"/>
    <url>/2025/20250927/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>师兄组会所提到。</p><p>2508.20181,2410.15778,2311.07362,2407.06192,2406.01920</p><span id="more"></span><h2 id="《通过目标感知的偏好优化来缓解多模态大语言模型中的幻觉》"><a href="#《通过目标感知的偏好优化来缓解多模态大语言模型中的幻觉》" class="headerlink" title="《通过目标感知的偏好优化来缓解多模态大语言模型中的幻觉》"></a>《通过目标感知的偏好优化来缓解多模态大语言模型中的幻觉》</h2><p>《Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization》</p><p>BMVC 2025（CCF C，17年知乎上都称是略低于CV顶会的第二档会议）</p><p>但感觉这篇论文的含金量并不足。</p><p>CHAIR-DPO 基于 CHAIR。<br>$$<br>CHAIR&#x3D;\frac{|回答y中提到的、但图像中不存在的物体|}{|回答y中所提到的所有物体|}<br>$$<br>本文将缓解幻觉问题视为一个对齐问题，通过训练让模型更“偏爱”那些不含幻觉的回答 。（<strong>这不是废话吗</strong>）</p><p><img src="/2025/20250927/1.png"></p><p>首先，对于数据集中的每个三元组，丢弃 y ，并从参考的指令微调多模态大模型$\pi_{ref}$中采样一组新的可能答案 {y1, y2} 。然后，我们将一个现成的检测器应用于图像$x_I$，该检测器经过训练以识别预定义的一组对象，将检测到的项目类别名称视为图像中实际存在的真实物体集合$x_I$。此时，可以计算生成答案的$CHAIR_i$得分，任何未出现在真实物体集合中的提及对象均被视为幻觉。</p><p>然后就是DPO了。</p><p>还有作者发现，一个关键挑战源于存在多个完成对的$CHAIR_i$得分无法区分，导致无法分配胜者和败者标签。将此类配对纳入最优化过程会引入噪声监督，因为模型被迫从随机选择的偏好标签中学习。为解决此问题，我们提出一种简单而有效的过滤策略：我们丢弃所有完成对之间$CHAIR_i$得分差为零的训练实例。</p><h2 id="通过潜在空间引导减少视觉-语言模型中的幻觉"><a href="#通过潜在空间引导减少视觉-语言模型中的幻觉" class="headerlink" title="通过潜在空间引导减少视觉-语言模型中的幻觉"></a>通过潜在空间引导减少视觉-语言模型中的幻觉</h2><p>Reducing Hallucinations in Vision-Language Models via Latent Space Steering</p><p>ICLR 2025 Spotlight</p><p>代码：<a href="https://github.com/shengliu66/VTI">shengliu66&#x2F;VTI: Code for Reducing Hallucinations in Vision-Language Models via Latent Space Steering</a></p><p><strong>V</strong>isual and <strong>T</strong>extual <strong>I</strong>ntervention &#x3D; VTI</p><h3 id="LVLMs-中的幻觉机制"><a href="#LVLMs-中的幻觉机制" class="headerlink" title="LVLMs 中的幻觉机制"></a>LVLMs 中的幻觉机制</h3><p>幻觉可能源于对视觉编码器不稳定性的敏感性。</p><p>与计算机视觉中的传统神经网络类似，理想的视觉编码器应在图像语义保持不变时输出稳定的特征，例如在对图像进行轻微扰动后。这种情况在 LVLMs 中尤为突出，其中图像编码器和文本解码器是预训练分离的，并且仅进行了最小程度的联合微调。文本解码器对图像特征的敏感性可能会加剧两种模态之间的错位，从而导致幻觉。</p><p><img src="/2025/20250927/4.png"></p><p>然而，如上图所示，虽然大多数视觉特征保持稳定，但约 15% 的特征表现出显著的方差，形成了长尾分布。</p><p>尽管特征平均能够缓解幻觉现象，但它也带来了显著的弊端。由于视觉编码器是在干净数据上训练的，对图像进行扰动会损害信息提取，导致细节丢失。此外，对视觉特征进行平均需要多次通过模型进行前向传播，大幅增加了计算成本。</p><p>因此，作者的目标是开发一种方法，既能有效增强视觉特征的稳定性并减少幻觉，又不牺牲信息或产生过高的计算开销。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>具体而言，预先计算了更稳定特征的“方向”，并在推理过程中一致地应用于所有查询示例以减少幻觉，而无需引入额外的训练或推理成本。由于有时幻觉源于文本解码器（即 LLM），进一步获取了文本方向并将其应用于文本解码器以最大化性能。</p><p><img src="/2025/20250927/5.png"></p><p>具体来说，给定视觉驶入v，其潜在状态为$h_{l,t}^v$，其中l表示层，t表示视觉token的索引。</p><p><strong>视觉token偏移向量：</strong></p><p>类似于特征平均，使用m种不同的随机掩码$C_i$，随机mask输入图像v。直观上，它们的平均嵌入$\bar h_{l,t}^v&#x3D;\sum h_{l,t}^{C_i(v)}$是鲁棒嵌入。</p><p>故视觉方向如此计算：$\Delta_{l,t}^v&#x3D;\bar h_{l,t}^v-h_{l,t}^v$</p><p>由于期望的视觉方向应能轻松应用于新的图像查询。为了去除方向向量中的图像特定信息，仅保留特征平均带来的变化。</p><p>获取N个样本，并计算$[\Delta_{l,t}^{v_1},\Delta_{l,t}^{v_2},…,\Delta_{l,t}^{v_N}]$主方向$d_{l,t}^{vision}$。</p><p>不同于其他论文，本论文使用的是归一化的SVD，也就是PCA。</p><p><strong>文本偏移向量：</strong></p><p>精心挑选N张无幻觉的图像描述，记为$x$，并采用GPT模型生成带有幻觉的版本$\tilde x$。</p><p>简单地使用原始对应的图像v作为视觉输入，文本方向为：$\Delta_{l,t}^{x_i,v_i}&#x3D;h_{l,t}^{x_i,v_i}-h_{l,t}^{\tilde x_iv_i}$</p><p>其中$h_{l,t}$代表在生成输出最后一个 token 时，第 t 个文本 token 在第 l 层的隐状态。特别地，因为文本解码器是因果建模的，我们仅使用最后一个 token 的潜在状态，t&#x3D;last token。同样地只取主方向$d_{l,t}^{text}$。</p><blockquote><p>作者有点符号混用。</p></blockquote><p>由于视觉编码器并非因果建模，在前向传播过程中对所有层的潜在状态在所有 token 位置进行了位移调整：<br>$$<br>h_{l,t}^v:&#x3D;h_{l,t}^v+\alpha\cdot d_{l,t}^{vision}<br>$$<br>对于文本，我们通过文本方向来调整文本解码器的潜在状态：<br>$$<br>h_{l,t}^{x,v}:&#x3D;h_{l,t}^{x,v}+\beta\cdot d_{l,t&#x3D;last token}^{text}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VTILayer</span>(nn.Module):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vti_direction, lam</span>):<br>        <span class="hljs-built_in">super</span>(VTILayer, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.vti_direction = vti_direction<br>        <span class="hljs-variable language_">self</span>.lam = lam<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.vti_direction <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            norm = torch.norm(x.<span class="hljs-built_in">float</span>(),dim=-<span class="hljs-number">1</span>).unsqueeze(-<span class="hljs-number">1</span>)            <br>            y = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.vti_direction)):<br>                <span class="hljs-keyword">if</span> x.size(<span class="hljs-number">1</span>) &lt; <span class="hljs-number">2</span>:<br>                    lambda_sim = <span class="hljs-number">1.0</span> <span class="hljs-comment">#+ torch.max(torch.tensor([0.]).to(x.device), F.cosine_similarity(x.float(), -self.vti_direction[i][None,None,:], dim=-1)).unsqueeze(-1)</span><br>                    y += <span class="hljs-variable language_">self</span>.lam[i] * lambda_sim * F.normalize(<span class="hljs-variable language_">self</span>.vti_direction[i], dim=-<span class="hljs-number">1</span>).repeat(<span class="hljs-number">1</span>,x.shape[<span class="hljs-number">1</span>],<span class="hljs-number">1</span>)<br>                <span class="hljs-keyword">else</span>:<br>                    lambda_sim = <span class="hljs-number">1.0</span><br>                    y += <span class="hljs-variable language_">self</span>.lam[i] * lambda_sim * F.normalize(<span class="hljs-variable language_">self</span>.vti_direction[i], dim=-<span class="hljs-number">1</span>)<br>            y = y/<span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.vti_direction)<br>            x = F.normalize(F.normalize(x.<span class="hljs-built_in">float</span>(),dim=-<span class="hljs-number">1</span>) +  <span class="hljs-number">0.1</span> * y, dim=-<span class="hljs-number">1</span>) * norm<br>                <br>            <span class="hljs-keyword">return</span> x.half()<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="Volcano-通过自我反馈引导修正减轻多模态幻觉"><a href="#Volcano-通过自我反馈引导修正减轻多模态幻觉" class="headerlink" title="Volcano: 通过自我反馈引导修正减轻多模态幻觉"></a>Volcano: 通过自我反馈引导修正减轻多模态幻觉</h2><p>Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision</p><h3 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h3><p><img src="/2025/20250927/9.png"></p><p><img src="/2025/20250927/8.png"></p><h4 id="迭代式自我修订"><a href="#迭代式自我修订" class="headerlink" title="迭代式自我修订"></a>迭代式自我修订</h4><p>Volcano 是一个通过四阶段顺序过程生成改进响应的单一模型。</p><p>首先，类似于其他 LLM，它为图像 $R_{initial}$ 和问题 $I$ 生成初始响应 Q ，并将最佳响应 $R_{best}$ 初始化为 $R_{initial}$ 。此阶段在整个创建最终响应的过程中只执行一次。</p><p>其次，它根据$R_{best}$ 生成反馈F (<strong>阶段 1</strong>) 。使用此反馈，它自我修订 $R_{best}$ (<strong>阶段 2</strong>)。由于无法保证修订后的响应 $R_{revised}$ 会比现有的 $R_{best}$更好，因此需要确定哪一响应对于给定的 Q 和I 更好。此时，Volcano 被提供 Q 、I 以及两个响应，并经历决定哪个响应更好的过程 (<strong>阶段 3</strong>)。在阶段 3 中，$R_{revised}$ 和 $R_{best}$ 的顺序是随机的，以防止位置影响结果。 （作者对此引用了《Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models》，但并未找到具体文字，只找到“Li et al. (2022c) 发现大型语言模型有时会 错误地将偶然相关性，例如位置接近或高度共 现的关联，当作事实知识。”）</p><p>如果模型认为 $R_{revised}$ 比 $R_{best}$ 更好，则 $R_{best}$ 会被更新为 $R_{revised}$ ，并重复从阶段1 到阶段 3 的过程，直到达到预定的最大迭代次数。否则，循环将提前停止，$R_{best}$ 将被选为最终输出。</p><h4 id="数据采集"><a href="#数据采集" class="headerlink" title="数据采集"></a>数据采集</h4><p><img src="/2025/20250927/10.png"></p><p>从开源的 LMM 中收集视觉问题的初始回答，并使用专有 LLM 生成反馈和修改。</p><p>专有的大语言模型可能会利用黄金答案（Gold Answer，预先准备好的、完全正确的回答）来生成反馈，这可能导致在推理时由于未提供黄金答案而产生潜在的不准确性。为了避免这种情况，作者给大语言模型明确的提示，使其在生成反馈时关注文本格式的图像细节。</p><p>在构建修订数据时，作者建立了一个系统，将现有的黄金答案作为输出进行预测，使用从上一步骤中获得的反馈数据、图像、问题和初始回答作为输入，而不涉及任何分离的模型生成过程。</p><h3 id="定性分析"><a href="#定性分析" class="headerlink" title="定性分析"></a>定性分析</h3><p>作者还定性分析反馈如何有效减少多模态幻觉。</p><h4 id="视觉信息量"><a href="#视觉信息量" class="headerlink" title="视觉信息量"></a>视觉信息量</h4><p>对于每个实例，执行top-k 平均汇聚（作者尝试了最小值、最大值、平均值，但是还是top-k提供了最清晰的可视化效果）来聚合初始响应或反馈词元在每个图像特征上的注意力权重。具体来说，在隐藏层上对前 3 个注意力权重求平均，在自注意力头上的前 3 个权重求平均，然后在输出词元上对前-l 个权重求平均，其中 l 是初始响应和反馈中较短输出的长度。</p><p><img src="/2025/20250927/6.png"></p><h4 id="视觉信息的覆盖"><a href="#视觉信息的覆盖" class="headerlink" title="视觉信息的覆盖"></a>视觉信息的覆盖</h4><p>作者进一步进一步通过实验研究单个词元的注意力如何有助于关键视觉信息的覆盖。</p><p>将输出中所有词元的注意力权重与输出中一部分词元的注意力权重进行比较。对于后一种过程，生成过程中最强烈关注图像特征的词元被视为显著词元并被选择。</p><p><img src="/2025/20250927/7.png"></p><p>此示例中的任务是识别图像中锅的颜色，初始响应错误地回答为（“红色”），然后反馈更正了答案（“银色”）。</p><p>这种修正可以通过生成每个词元时对图像特征的注意力分布差异来解释。</p><p>基于所有关注图像特征的词元的热力图，当 Volcano 生成初始响应时，它主要关注外边缘的特征，对应于锅的边缘；在生成反馈时，它关注整个图像，包括对应于银色锅的外部区域和包含红色浆果的内部区域。</p><p>关注图像特征的特定词元的热力图显示，在改进初始响应的过程中，Volcano 在生成这些词时确实关注了对应于关键颜色描述符“银色”和“红色”的确切图像区域。</p><h3 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h3><p>Volcano 需要多次调用模型，因此比直接生成响应的效率更低。平均而言，Volcano 的运行速度比基础模型慢约2 到 3 倍，对于给定的图像和指令生成响应需要 5.8 秒，而 LLaVA-1.5 只需要 2.7 秒。作者用来减少总体执行时间的一个策略是将迭代次数限制为 3 次。</p><h2 id="视觉语言模型中的多对象幻觉"><a href="#视觉语言模型中的多对象幻觉" class="headerlink" title="视觉语言模型中的多对象幻觉"></a>视觉语言模型中的多对象幻觉</h2><p>Multi-Object Hallucination in Vision Language Models</p><p>NeurIPS 2024</p><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>系统地研究了一个重要问题：当大型视觉语言模型（LVLMs）被要求同时关注和识别图像中的多个物体时，它们会如何产生误解（例如，凭空捏造不存在的物体或分散注意力） 。</p><p>系统地研究多目标幻觉，论文提出了一个名为<strong>ROPE (Recognition-based Object Probing Evaluation，基于识别的物体探测评估)</strong> 的自动化评估协议。</p><p>为了避免使用文字描述物体时可能产生的歧义（比如一张图里有多个苹果，说“苹果”就不知道指哪个），ROPE 在图像上使用红色的边界框直接框出要识别的物体，从而精确指向特定对象 。</p><p>ROPE 设计了标准化的问答格式（例如，要求模型以 <code>obj1: &lt;class1&gt;, obj2: &lt;class2&gt;, ...</code> 的格式输出），这样就可以通过简单的文本解析来自动评估模型的准确性，无需依赖人类或其他“黑箱”模型进行评判 。</p><p>ROPE 在测试时考虑图像中对象类别的分布，将 ROPE 分为四个子集：</p><ul><li><p>In-the-Wild（真实场景）：随机选择和排序图像中的5个物体，类别分布混合 。</p></li><li><p>Homogeneous：所有被框出的5个物体都属于同一个类别（例如，5个都是苹果） </p></li><li><p>Heterogeneous：5个被框出的物体分别属于5个不同的类别 。</p></li><li><p>Adversarial：前4个物体属于同一类别，最后一个物体属于不同类别（例如，4个苹果，1个橙子），用来测试模型是否会因为惯性而出错 。</p></li></ul><p>主要发现简介如下：</p><p>(1)与仅关注单一对象相比，LVLM 在被要求关注多个对象时会产生更多幻觉；</p><p>(2) 测试对象类别分布影响幻觉行为，揭示出 LVLM 可能依赖捷径和虚假相关系数；</p><p>(3) LVLM 的幻觉行为受到数据特定因子、显著性与频率以及模型内在行为的影响。</p><h3 id="主要发现"><a href="#主要发现" class="headerlink" title="主要发现"></a>主要发现</h3><p><img src="/2025/20250927/2.png"></p><p>在多种主流的视觉语言模型（如 LLaVA, GPT-4V, GPT-4O, CogVLM ，Yi-VL等）上进行了实验，得出了三个关键结论：</p><p><strong>多目标任务比单目标任务更容易产生幻觉：</strong> 实验结果清晰地表明，与一次只识别一个物体相比，当模型被要求同时识别多个物体时，其准确率显著下降，幻觉现象更加严重 。</p><p><strong>被测物体的类别分布会影响幻觉行为：</strong> 模型在处理<strong>同质</strong>物体（都是同一类）时表现最好，而在处理<strong>异质</strong>物体（每个都不同类）时表现最差，幻觉最多 。这表明模型可能依赖于某种“捷径”或虚假的关联性来做出判断，而不是真正地逐一识别每个物体 。</p><p><strong>对抗性测试揭示了模型的“思维捷径”：</strong> 在“AAAAB”这样的对抗性测试中，模型在识别前几个相同的物体（A）时准确率很高，甚至越来越高。但当识别到最后一个不同的物体（B）时，准确率会断崖式下跌，模型极大概率会把它也错误地识别成A 。这有力地证明了模型并没有真正理解任务，而只是在利用语言偏见和重复先前答案这样的“捷径” 。</p><h3 id="影响幻觉的因素分析"><a href="#影响幻觉的因素分析" class="headerlink" title="影响幻觉的因素分析"></a>影响幻觉的因素分析</h3><p>深入分析了可能导致多目标幻觉的多种因素：</p><h4 id="数据特定因素"><a href="#数据特定因素" class="headerlink" title="数据特定因素"></a>数据特定因素</h4><p><img src="/2025/20250927/3.png"></p><p><strong>查询同质性（Query Homogeneity）：</strong> 当任务中需要识别的物体类别越单一时，模型幻觉越少 。</p><p><strong>物体在图像中的位置（Object Centrality）：</strong> 位于图像边缘的物体比位于中心的物体更容易被错误识别 。</p><h4 id="显著性和频率因素"><a href="#显著性和频率因素" class="headerlink" title="显著性和频率因素"></a>显著性和频率因素</h4><p><strong>物体显著性</strong>：先前的研究表明，较小的物体更难被检测和定位到。本文将物体显著性定义为物体实例分割掩码占据的像素数量与图像总像素数量的比值。</p><p><strong>语义显著性（Semantic Salience）：</strong> 如果某一类别的物体在图像中多次出现（例如，桌子上有很多罐子），那么模型在识别这个类别的单个物体时更不容易出错 。</p><p><strong>训练数据频率（Training Salience）：</strong> 模型对于在训练数据中频繁出现的物体类别，更不容易产生幻觉 。</p><h4 id="模型内在行为"><a href="#模型内在行为" class="headerlink" title="模型内在行为"></a>模型内在行为</h4><p><strong>模型不确定性（Object Token Entropy）：</strong> 当模型对自己的预测感到不确定时（表现为输出词元的熵更高），更容易产生幻觉 。</p><p><strong>对视觉信息的关注度（Visual Modality Contribution）：</strong> 分析发现，模型在做判断时，对文本信息的依赖远大于视觉信息。当模型对视觉信息的关注度降低时，产生幻觉的可能性会略微增加 。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p><strong>简单提升模型参数可能不够：</strong> 仅仅增加语言模型的参数量可以减少单目标幻觉，但对更复杂的多目标幻觉问题效果不明显 。</p><p><strong>训练数据是关键：</strong> 需要构建更多样化、更平衡的训练数据，包含更多需要同时理解多个物体的指令，特别是那些不常见的、位于图像边缘的物体 。</p><p><strong>应用上的建议：</strong> 在要求高准确率的场景下，可以考虑将复杂的多目标识别任务分解成多个单目标识别任务，即一次只问一个物体，以减少幻觉的风险 。</p><h2 id="CODE：对比自生成描述以应对大型多模态模型中的幻觉问题"><a href="#CODE：对比自生成描述以应对大型多模态模型中的幻觉问题" class="headerlink" title="CODE：对比自生成描述以应对大型多模态模型中的幻觉问题"></a>CODE：对比自生成描述以应对大型多模态模型中的幻觉问题</h2><p>CODE: Contrasting Self-generated Description to Combat Hallucination in Large Multi-modal Models</p><p>NeurIPS 2024</p><p>CODE (<strong>CO</strong>untering <strong>DE</strong>scription Contrastive Decoding)</p><p><strong>贡献</strong>：</p><ul><li>CODE是一种无需训练的解码策略，利用自生成的描述来减少大型多模态模型中的幻觉现象。通过对比描述所包含的 Logit 信息与实际视觉内容，CODE 提升了模型响应的视觉一致性和连贯性。</li><li>在对比解码阶段引入了动态限制策略。通过根据 token 在词表中的分布来调整其层级预测，有选择性地调控信息流，从而确保生成更具上下文相关性的响应。</li><li>在多种基准上验证了有效性。</li></ul><h2 id="方法-2"><a href="#方法-2" class="headerlink" title="方法"></a>方法</h2><p><img src="/2025/20250927/11.png"></p><p><img src="/2025/20250927/12.png"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250927/13.png"></p><p><img src="/2025/20250927/14.png"></p><p>代码流程已经写得还明白了。</p><blockquote><p>直接把有界散度代入$\alpha$很能work有点太奇了</p></blockquote><h2 id="失败案例"><a href="#失败案例" class="headerlink" title="失败案例"></a>失败案例</h2><p>作者推断，仅依赖自生成的描述并不总是足够，尤其是在描述本身存在偏见或不准确的情况下。</p><p><img src="/2025/20250927/15.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Machine Unlearning</title>
    <link href="/2025/20250926/"/>
    <url>/2025/20250926/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Lucas Bourtoule 等人于 2019年在 arXiv 上发表，并最终在 2021年的 IEEE Symposium on Security and Privacy (S&amp;P) 上正式发表的论文 《Machine Unlearning》。</p><p>比较早期的模型，方法比较朴素。</p><span id="more"></span><p><img src="/2025/20250926/2.png"></p><p><strong>训练阶段 (Training Phase)：</strong></p><ol><li><p><strong>数据分片 (Sharding)</strong>：将完整的原始训练数据集 <code>D</code> 分割成 <code>S</code> 个互不相交的数据子集，称为“分片” (Shards)。</p></li><li><p><strong>数据切片 (Slicing)</strong>：将每个分片 <code>Dk</code> 内部的数据再进一步分割成 <code>R</code> 个互不相-交的“切片” (Slices)。</p></li><li><p><strong>隔离与增量训练 (Isolated &amp; Incremental Training)</strong>：为每一个分片 <code>Dk</code> 单独训练一个模型 <code>Mk</code>，过程如下：</p><ul><li><strong>步骤 1</strong>: 使用随机初始化的参数，仅在第一个切片 <code>Dk,1</code> 上训练模型。训练完成后，保存当前模型的参数状态。</li><li><strong>步骤 2</strong>: 加载上一步保存的模型参数，然后在 <code>Dk,1</code> ∪ <code>Dk,2</code> 的数据上继续训练。完成后保存新的模型状态。</li><li><strong>重复此过程</strong>: 直到模型在所有 <code>R</code> 个切片上都训练完毕。每次加入一个新的切片进行训练后，都要保存一次模型状态。</li></ul></li><li><p><strong>聚合 (Aggregation)</strong>：在需要进行预测时，将输入数据分别送入所有 <code>S</code> 个独立训练好的模型。然后通过“多数投票”等策略将它们的输出结果聚合起来，得到最终的预测结果。</p></li></ol><p><strong>反学习阶段 (Unlearning Phase)：</strong></p><p>当收到一个删除数据点 <code>du</code> 的请求时：</p><ol><li><p><strong>定位 (Locate)</strong>：首先，确定这个数据点 <code>du</code> 属于哪个分片 <code>Dk</code> 和哪个切片 <code>Dk,r</code>。</p></li><li><p><strong>回滚 (Rollback)</strong>：加载在训练阶段保存的模型状态，具体来说，是加载在第 <code>r</code> 个切片被引入<em>之前</em>保存的状态。</p></li><li><p><strong>重新训练 (Retrain)</strong>：从加载的状态开始，继续执行增量训练过程。但在处理第 <code>r</code> 个切片时，使用剔除了数据点 <code>du</code> 的数据。</p></li><li><p><strong>完成 (Complete)</strong>：只有包含 <code>du</code> 的这一个模型 <code>Mk</code> 被重新训练，其他所有模型完全不受影响。</p></li></ol><h2 id="优化策略"><a href="#优化策略" class="headerlink" title="优化策略"></a>优化策略</h2><p>作者提出了如何更智能地进行分片以最小化未来的unlearning成本，称为分布感知的智能分片 (Distribution-Aware Sharding)</p><p>我们定义：</p><ul><li>数据集 <code>D</code></li><li>一个常数阈值 <code>C</code></li></ul><p><strong>流程</strong>:</p><ol><li><p><strong>排序</strong>: 根据每个数据点 <code>du</code> 被请求删除的概率 <code>p(u)</code>，对整个数据集进行升序排序（概率最低的在前）。</p></li><li><p><strong>初始化</strong>: 创建第一个空的分片 <code>D0</code>。</p></li><li><p><strong>循环分配</strong>: 遍历排序后的数据集：</p><ul><li>从数据集中取出当前删除概率最低的数据点 <code>du</code>。</li><li>将 <code>du</code> 添加到当前的分片 <code>Di</code> 中。</li></ul></li><li><p><strong>检查阈值</strong>: 每次添加后，计算当前分片 <code>Di</code> 的“预期被删除次数” <code>E(χi)</code>。</p><ul><li><strong>如果 <code>E(χi) ≥ C</code></strong>:<ul><li>将刚刚添加的数据点 <code>du</code> 从当前分片 <code>Di</code> 中移出。</li><li>创建一个新的空分片 <code>Di+1</code>。</li><li>将数据点 <code>du</code> 添加到这个新的分片 <code>Di+1</code> 中。</li></ul></li></ul></li><li><p><strong>结束</strong>: 重复步骤3和4，直到所有数据点都被分配到某个分片中。</p></li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>比较早期的模型，方法比较朴素。</p><p>模型参数量大，重新训练就很耗时。</p><p>在当时，作者认为遗忘有以下难度。</p><p><img src="/2025/20250926/1.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MokA：Multimodal Low-Rank Adaptation for MLLMs</title>
    <link href="/2025/20250919/"/>
    <url>/2025/20250919/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（NIPS 2025）</p><span id="more"></span><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>现在的多模态大模型的微调直接接借用大型语言模型（LLM）的微调技术，往往忽略了多模态场景的内在差异，导致非文本模态的信息未能被充分利用。该论文的核心观点是，有效的MLLM微调应同时包含**单模态自适应（unimodal adaptation）<strong>和</strong>跨模态自适应（cross-modal adaptation）**两个关键部分 。</p><p>文本 token 推理能够达到与常规全模态情景相当的性能。然而，非文本 token（如音频或视觉）推理则导致性能显著下降。</p><p>MokA 保留了广泛采用的低秩分解矩阵，但重新定义了矩阵 A 和B 的角色，以更好地适应多模态特性。具体而言，矩阵 A 被设计为模态特定的，允许每个模态独立压缩信息，从而避免其他模态的干扰。随后，引入交叉注意力机制以加强文本 token与非文本 token 之间的交互，突出任务相关特征。最后，一个共享的多模态矩阵 B 将单模态低秩表示投影到一个统一的空间中，促进跨模态的有效对齐。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2025/20250919/1.png"></p><p>我们针对<strong>音频 (audio, a)</strong>、<strong>视觉 (visual, v)</strong> 和 <strong>文本 (text, t)</strong> 三种模态的场景。</p><h3 id="单模态矩阵"><a href="#单模态矩阵" class="headerlink" title="单模态矩阵"></a>单模态矩阵</h3><p>对于 MLLM 中的任意一个预训练权重矩阵 $W_0 \in \mathbb{R}^{d \times k}$，MokA 的目标是学习一个低秩更新 $\Delta W$，使得最终的权重为 $W_0 + \Delta W$。其前向传播的计算流程如下：</p><p>输入序列 $x$ 是由不同模态的词元（tokens）拼接而成：<br>$$x &#x3D; [x^a; x^v; x^t]$$<br>其中，$x^a, x^v, x^t$ 分别代表音频、视觉和文本模态的词元序列。</p><p>为了独立地捕捉每个模态的特有信息，避免模态间的相互干扰，MokA 为每种模态都使用一个独立的、不共享的低秩矩阵 $A^i$。其中 $i \in {a, v, t}$。</p><p>每个模态的词元序列 $x^i$ 会被其对应的矩阵 $A^i \in \mathbb{R}^{r \times k}$ (其中 $r \ll \min(d, k)$ 是低秩维度) 独立地映射到一个低秩空间中。</p><p>该步骤的输出为：<br>$$Ax &#x3D; [A^a x^a; A^v x^v; A^t x^t]$$</p><h3 id="任务为中心的跨注意力机制-Task-centric-Cross-Attention"><a href="#任务为中心的跨注意力机制-Task-centric-Cross-Attention" class="headerlink" title="任务为中心的跨注意力机制 (Task-centric Cross-Attention)"></a>任务为中心的跨注意力机制 (Task-centric Cross-Attention)</h3><p>在指令微调场景下，文本词元通常描述任务，而非文本词元提供上下文信息。为了有效捕捉任务描述与上下文之间的语义关联，MokA 在低秩空间中引入了跨注意力机制，以显式地增强跨模态交互。</p><p>该机制的设计是：<strong>使用非文本模态 (音频、视觉) 的表征作为查询 (Query)，并使用文本模态的表征作为键 (Key) 和值 (Value)</strong>。这样做可以在不改变文本原有表征的前提下，将与任务相关的文本信息融入到非文本模态中。</p><ol><li><p><strong>计算音频模态的注意力增强</strong>:<br>$$Att _ {a,t,t} &#x3D; \text{softmax}\left(\frac{(A^a x^a)(A^t x^t)^T}{\sqrt{N_t}}\right) A^t x^t$$<br>其中 $N_t$ 是文本词元的数量。</p></li><li><p><strong>计算视觉模态的注意力增强</strong>:<br>$$Att _ {v,t,t} &#x3D; \text{softmax}\left(\frac{(A^v x^v)(A^t x^t)^T}{\sqrt{N_t}}\right) A^t x^t$$</p></li><li><p><strong>通过残差连接更新非文本表征</strong>:</p><ul><li>增强后的音频表征: $A^a x^a + Att _ {a,t,t}$</li><li>增强后的视觉表征: $A^v x^v + Att _ {v,t,t}$</li></ul></li></ol><p>经过此步骤，整合了跨模态信息的序列变为：<br>$$Ax _ {enhanced} &#x3D; [A^a x^a + Att _ {a,t,t}; A^v x^v + Att _ {v,t,t}; A^t x^t]$$</p><p>线性的投影（Wq 、Wk 和 Wv ）并未包含在交叉注意力模块中，因为在这种情况下，每个模态的低秩矩阵 A 实际上可以被视为注意力中的线性投影。</p><h3 id="共享多模态矩阵B"><a href="#共享多模态矩阵B" class="headerlink" title="共享多模态矩阵B"></a>共享多模态矩阵B</h3><p>在增强了跨模态交互之后，需要将所有模态的低秩表征投影回原始的高维空间，以实现最终的跨模态对齐。MokA 使用一个<strong>所有模态共享</strong>的矩阵 $B \in \mathbb{R}^{d \times r}$ 来完成这个任务。</p><p>MokA 旁路计算的最终输出为：<br>$$BAx &#x3D; [B(A^a x^a + Att _ {a,t,t}); B(A^v x^v + Att _ {v,t,t}); BA^t x^t]$$</p><h3 id="最终输出"><a href="#最终输出" class="headerlink" title="最终输出"></a>最终输出</h3><p>一个应用了 MokA 的层的最终输出 $h$ 是原始预训练权重 $W_0$ 的输出与 MokA 旁路输出的总和：<br>$$h &#x3D; W_0 x + \Delta W x$$将 $\Delta W x$ 的计算展开，可以清晰地看到单模态和跨模态自适应两个部分：$$h &#x3D; W_0 x + \underbrace{[BA^a x^a; BA^v x^v; BA^t x^t]} _ {\text{unimodal adaptation}} + \underbrace{[BAtt _ {a,t,t}; BAtt _ {v,t,t}; 0 _ {N_t}]} _ {\text{cross-modal adaptation}}$$<br>其中 $0 _ {N_t}$ 表示一个零向量，因为文本词元在跨注意力步骤中未被改变。</p><p>这个公式清晰地展示了 MokA 如何通过其结构设计，在一次前向传播中同时实现了对各个模态的独立调整以及模态间的显式交互。</p><p>为了保证训练开始时的稳定性，初始化策略与 LoRA 类似：</p><ul><li>所有单模态矩阵 $A^i$ 使用 Kaiming 均匀分布进行初始化。</li><li>共享的投影矩阵 $B$ 初始化为全零。<br>这确保了在训练开始时，整个更新量 $\Delta W &#x3D; 0$，模型从其原始预训练状态平稳启动。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250919/2.png"></p><p><img src="/2025/20250919/3.png"></p><p>在显式跨模态整合（如交叉注意力）过程中，与给定模态更相关的文本 token 可以获得更高的注意力权重。例如，token“sound”在与音频模态的关系中获得了更大的注意力。这种跨模态整合可以更好地促进文本 token 与非文本token 之间的对齐和交互。</p><p><img src="/2025/20250919/4.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations</title>
    <link href="/2025/20250917/"/>
    <url>/2025/20250917/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（EMNLP 2025 ）</p><p>Autonomous Preference Alignment via Self-Injection &#x3D;APASI</p><span id="more"></span><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>LVLM幻觉模式的三个关键观察 ：</p><p><strong>物体共现</strong>：模型倾向于幻觉出那些经常与图像中真实存在的物体一同出现的物体（例如，在有“沙发”的场景中幻觉出“椅子”或“桌子”）。</p><p><strong>语言先验</strong>：模型在生成内容时会过度依赖语言中的常见搭配，而非完全基于视觉信息 。</p><p><strong>位置因素</strong>：幻觉内容通常出现在回复的后半部分 。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>APASI 针对前面的三个观察，进行构建偏好对：</p><ul><li>preffered $y_i^+$：原始相应</li><li>dis-preffered $y_i^-$：故意注意幻觉后生成的响应</li></ul><p><img src="/2025/20250917/1.png"></p><h3 id="构建共现图（Co-Occurrence-Graph-Construction）"><a href="#构建共现图（Co-Occurrence-Graph-Construction）" class="headerlink" title="构建共现图（Co-Occurrence Graph Construction）"></a>构建共现图（Co-Occurrence Graph Construction）</h3><p>对$y_i^+$进行预处理，通过<a href="https://courses.cs.umbc.edu/graduate/691/spring13/01/papers/p39-miller.pdf">WordNET</a>工具箱和同义词集S解析成一组物体标签。</p><p>构建一个图。其中节点代表物体标签，边权重代表连接物体的共现频率。</p><h3 id="加权采样"><a href="#加权采样" class="headerlink" title="加权采样"></a>加权采样</h3><p>以概率&#x2F;注入率$\rho$对原始句子替换为幻觉对应的句子。</p><p>由于考虑幻觉通常出现在响应的后半部分，我们经验性设定最后一个句子被采样的概率为第一个句子的两倍，并线性插值，即第k个句子的权重为$w_k&#x3D;1+\frac{k-1}{L}$</p><h3 id="幻觉补全与注入"><a href="#幻觉补全与注入" class="headerlink" title="幻觉补全与注入"></a>幻觉补全与注入</h3><p>假设APASI在$y_i^+$中采样第k个句子，用一个幻觉句子$y_{i,k}^-$替代，即：<br>$$<br>y_i^-&#x3D;(y_{i,1}^+,…,y_{i,k-1}^+,y_{i,k}^-，y_{i,k+1}^+,…,y_{i,L}^+)<br>$$<br>对于一个待替换的句子，APASI会查询共现图，找到一个与回复中其他句子所描述物体共现频率高，但实际图片中不存在的物体。然后将其放到预定义的引导模板中，例如，“一个&lt;幻觉物体&gt;出现了”）来引导一个**“盲”LVLM** (即不给它看图片，只给它文本上下文) 来完成一个描述这个幻觉物体的句子  。</p><p>最后得到$y_i^-$。</p><h3 id="课程学习的迭代对齐"><a href="#课程学习的迭代对齐" class="headerlink" title="课程学习的迭代对齐"></a>课程学习的迭代对齐</h3><p>为缓解偏好对齐中的分布偏移问题，APASI采用一种迭代对其策略。在迭代t时，使用最新优化的模型$M_{\theta_{t-1}}$进行偏好数据构建，包括生成偏好响应和注入幻觉。随后，Mθt−1 通过DPO目标与偏好数据进行优化，得到$M_{\theta_t}$ ，用于下一次迭代的数据构建。</p><p>还引入了一种课程学习机制，逐步增加对齐任务的难度。</p><p>该课程学习机制具体地根据一个单调递减的课程函数 $f_c(t)$，在每次迭代 t 中降低注入率$\rho$。随着 $\rho$的减小，优选响应$y^+$与非优选响应$y^-$之间的差距逐渐缩小，使得区分偏好对中细微差异的挑战性显著增强。</p><p><img src="/2025/20250917/2.png"></p><p><strong>代码未公开</strong>。</p><p>所以我们也不知道这个$f_c(t)$选的是什么。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nullu：通过 HalluSpace 投影减轻大型视觉-语言模型中的对象幻觉</title>
    <link href="/2025/20250915/"/>
    <url>/2025/20250915/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（CVPR 2025）</p><p>在视觉语言模型中，<strong>物体幻觉</strong>（Object Hallucinations，OH）指的是模型在生成图像描述时，<strong>错误地提到了图像中并不存在的物体</strong>。</p><span id="more"></span><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>构建成对视觉-语言输入数据集。这两个输入具有相同的图像但不同的文本提示，其中一个是包含真实值$x ^ {−}_i$的正确描述，该描述准确描述了图像中的物体，另一个是包含幻觉描述$x^+_i$的文本。</p><p><img src="/2025/20250915/1.png"></p><p>第$\ell$层的差分矩阵$E_\ell$为$E_\ell&#x3D;X^+ _ \ell-X^- _ \ell$.</p><p>然后我们可以对$E_\ell$进行SVD分解。</p><p>然后选取top k 奇异值的右奇异向量。（左代表样本维度，右代表特征维度。）</p><p>而$V _ {\ell,k}$的零空间是$(I-V _ {\ell,k}V _ {\ell,k}^T)$。</p><p><img src="/2025/20250915/5.png"></p><p>故我们将MLP的权重投影到零空间上，即$W_\ell ^ {ed}&#x3D;(I-V _ {\ell,k}V _ {\ell,k}^T)W _ {\ell} ^ {org}$。</p><h3 id="和DPO的联系"><a href="#和DPO的联系" class="headerlink" title="和DPO的联系"></a>和DPO的联系</h3><p>$\mathcal{L} _ {DPO} &#x3D; -\mathbb{E} _ {(x,y ^ {+},y ^ {-})\sim\mathcal{D}}[\log \sigma(\beta \log\frac{\pi _ {\theta}(y ^ {+}|x)}{\pi _ {ref}(y ^ {+}|x)} - \beta \log\frac{\pi _ {\theta}(y ^ {-}|x)}{\pi _ {ref}(y ^ {-}|x)})]$</p><p>为了简化，我们定义 $\hat{r} _ {\theta}(x, y) &#x3D; \beta \log\frac{\pi _ {\theta}(y|x)}{\pi _ {ref}(y|x)}$。<br>那么损失函数可以写成：<br>$\mathcal{L} _ {DPO} &#x3D; -\mathbb{E}[\log \sigma(\hat{r} _ {\theta}(x, y^+) - \hat{r} _ {\theta}(x, y^-))]$<br>令 $z &#x3D; \hat{r} _ {\theta}(x, y^+) - \hat{r} _ {\theta}(x, y^-)$，那么 $\mathcal{L} _ {DPO} &#x3D; -\log \sigma(z)$ (暂时忽略期望 $\mathbb{E}$)。</p><p>梯度可以分解为：<br>$\nabla_W \mathcal{L} _ {DPO} &#x3D; \frac{\partial \mathcal{L} _ {DPO}}{\partial z} \cdot \nabla_W z$</p><p>首先计算第一部分 $\frac{\partial \mathcal{L} _ {DPO}}{\partial z}$：</p><ul><li>$\frac{d}{dz} \sigma(z) &#x3D; \sigma(z)(1-\sigma(z))$</li><li>$\frac{d}{dz} \log(u) &#x3D; \frac{1}{u}$</li><li>因此，$\frac{d}{dz} \log(\sigma(z)) &#x3D; \frac{1}{\sigma(z)} \cdot \sigma(z)(1-\sigma(z)) &#x3D; 1 - \sigma(z)$</li><li>所以，$\frac{\partial \mathcal{L} _ {DPO}}{\partial z} &#x3D; -\frac{\partial}{\partial z} \log \sigma(z) &#x3D; -(1 - \sigma(z)) &#x3D; \sigma(z) - 1$</li></ul><p>论文中假定，梯度是在初始化状态下计算的，即策略模型与参考模型相等（$\pi_\theta &#x3D; \pi _ {ref}$）。<br>在这种情况下：</p><ul><li>$\log\frac{\pi _ {\theta}(y|x)}{\pi _ {ref}(y|x)} &#x3D; \log(1) &#x3D; 0$</li><li>所以 $z &#x3D; \hat{r} _ {\theta}(x, y^+) - \hat{r} _ {\theta}(x, y^-) &#x3D; 0 - 0 &#x3D; 0$</li><li>当 $z&#x3D;0$ 时，sigmoid函数 $\sigma(0) &#x3D; \frac{1}{1+e^0} &#x3D; \frac{1}{2}$</li><li>代入第二步的结果，我们得到 $\frac{\partial \mathcal{L} _ {DPO}}{\partial z} | _ {\pi_\theta &#x3D; \pi _ {ref}} &#x3D; \sigma(0) - 1 &#x3D; \frac{1}{2} - 1 &#x3D; -\frac{1}{2}$</li></ul><p>接着我们计算梯度的第二部分 $\nabla_W z$：<br>$\nabla_W z &#x3D; \nabla_W (\hat{r} _ {\theta}(x, y^+) - \hat{r} _ {\theta}(x, y^-))$<br>$&#x3D; \nabla_W (\beta \log\frac{\pi _ {\theta}(y ^ {+}|x)}{\pi _ {ref}(y ^ {+}|x)}) - \nabla_W (\beta \log\frac{\pi _ {\theta}(y ^ {-}|x)}{\pi _ {ref}(y ^ {-}|x)})$<br>由于 $\pi _ {ref}$ 不依赖于我们要优化的权重 $W$，它的梯度为零。<br>$&#x3D; \beta (\nabla_W \log \pi _ {\theta}(y ^ {+}|x) - \nabla_W \log \pi _ {\theta}(y ^ {-}|x))$</p><p>进一步的，论文使用了一个简化的逻辑回归模型来表示概率：<br>$\pi_W(y|x) &#x3D; Z_W ^ {-1} \exp(\sum _ {m&#x3D;1} ^ {M} o _ {y_m} ^ {\top} W x_m)$<br>取对数后得到：<br>$\log \pi_W(y|x) &#x3D; \sum _ {m&#x3D;1} ^ {M} o _ {y_m} ^ {\top} W x_m - \log Z_W$</p><p>现在对它求关于 $W$ 的梯度。</p><ul><li>$\nabla_W (\sum o _ {y_m} ^ {\top} W x_m) &#x3D; \sum \nabla_W (\text{Tr}(o _ {y_m} ^ {\top} W x_m)) &#x3D; \sum o _ {y_m} x_m ^ {\top}$ (这是矩阵微积分的标准结论)</li><li>$\nabla_W (\log Z_W)$ 这一项是存在的，但论文指出，当我们计算 $y^+$ 和 $y^-$ 之间的<strong>差值</strong>时，这个归一化项的梯度会相互抵消掉。</li></ul><p>因此，我们只关注 logits 部分的梯度。为了简化（如论文中只考虑M&#x3D;1的情况）：<br>$\nabla_W \log \pi_W(y|x) &#x3D; o_y x ^ {\top}$</p><p>代入第四步的结果：<br>$\nabla_W z &#x3D; \beta (o _ {y^+}(x^+) ^ {\top} - o _ {y^-}(x^-) ^ {\top})$</p><p>现在，我们将前面的结果组合起来：<br>$\nabla_W \mathcal{L} _ {DPO} | _ {\pi_\theta &#x3D; \pi _ {ref}} &#x3D; \frac{\partial \mathcal{L} _ {DPO}}{\partial z} \cdot \nabla_W z$<br>$&#x3D; (-\frac{1}{2}) \cdot \beta (o _ {y^+}(x^+) ^ {\top} - o _ {y^-}(x^-) ^ {\top})$</p><p>考虑到损失函数是所有样本的期望（平均值），我们引入求和与系数 $-\frac{1}{N}$。常数 $1&#x2F;2$ 可以合并到超参数 $\beta$ 中。这样，我们就得到了以下形式：<br>$\nabla_W \mathcal{L} _ {DPO} &#x3D; -\frac{\beta}{N}\sum _ {i&#x3D;1} ^ {N}(o _ {y _ {i}} ^ {+}(x _ {i} ^ {+}) ^ {\top}-o _ {y _ {i}} ^ {-}(x _ {i} ^ {-}) ^ {\top})$</p><p>最后，为了展示“特征差异”和“输出差异”，论文对这个结果做了一个简单的代数变换。通过加上和减去同一个项 $o _ {y_i^+}(x_i^-)^\top$：<br>$\nabla_W \mathcal{L} _ {DPO} \propto -(o _ {y^+}(x^+)^\top - o _ {y^-}(x^-)^\top)$<br>$&#x3D; -(o _ {y^+}(x^+)^\top \color{blue}{- o _ {y^+}(x^-)^\top + o _ {y^+}(x^-)^\top} \color{black}{- o _ {y^-}(x^-)^\top})$<br>$&#x3D; -( [o _ {y^+}(x^+)^\top - o _ {y^+}(x^-)^\top] + [o _ {y^+}(x^-)^\top - o _ {y^-}(x^-)^\top] )$<br>$&#x3D; -( o _ {y^+} (x^+ - x^-)^\top + (o _ {y^+} - o _ {y^-}) (x^-)^\top )$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>理想情况下，如果HalluSpace有效代表了这些偏差，那么将测试样本映射到 HalluSpace 时，差异向量应该具有较大的投影分量。</p><p><img src="/2025/20250915/2.png"></p><p>另外的一些实验结果：</p><p><img src="/2025/20250915/3.png"></p><p><img src="/2025/20250915/4.png"></p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>Chao Shen后续也做了一些相关的工作。2505.17812</p><p>但很像2410.15778的《Reducing Hallucinations in Vision-Language Models via Latent Space Steering》，同样都是使用$h&#x3D;h+\alpha v^T$的形式。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Vision Transformers Don&#39;t Need Trained Registers</title>
    <link href="/2025/20250914/"/>
    <url>/2025/20250914/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ICLR 2024的《VISION TRANSFORMERS NEED REGISTERS》指出了VIT中也会出现类似attention sinks的伪影。对于REGISTERS我们是否需要可训练呢？</p><span id="more"></span><hr><p>ICLR 2024的《VISION TRANSFORMERS NEED REGISTERS》指出了VIT中也会出现类似attention sinks的伪影。</p><p><img src="/2025/20250914/1.png"></p><p>应对的方法类似于streamingllm，即额外添加一些元素来承载多余的注意力。</p><p><img src="/2025/20250914/2.png"></p><h2 id="反驳"><a href="#反驳" class="headerlink" title="反驳"></a>反驳</h2><p>但最近9月的一篇arxiv（2506.08010）指出我们其实不需要在训练就加入，我们完全可以在测试时再把多余的注意力移走。</p><p>作者提出了一种如何移动的方法。</p><p><img src="/2025/20250914/3.png"></p><p>我们可以随意移动。</p><p><img src="/2025/20250914/4.png"></p><p>我们既然可以随意移动，那我们也可以把它移动到图像外的额外token上。</p><p>从其代码看不出来额外的token的位置在哪。不过根据其demo<a href="https://github.com/nickjiang2378/test-time-registers/blob/910bf45a607bf0800f20c310b6c0d54b14d6d5d0/llava_demo.ipynb#L68">llava_demo.ipynb </a>，更像是类似GPT-OSS的处理方式，即需要对模型进行修改，而非对原生模型进行处理。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLM中MOE的安全行为</title>
    <link href="/2025/20250913/"/>
    <url>/2025/20250913/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（arxiv 2025）</p><p>[<a href="https://www.arxiv.org/abs/2509.09660">2509.09660] Steering MoE LLMs via Expert (De)Activation</a></p><p>[<a href="https://arxiv.org/abs/2506.17368">2506.17368] SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification</a></p><span id="more"></span><h2 id="SAFEx"><a href="#SAFEx" class="headerlink" title="SAFEx"></a>SAFEx</h2><p>能够清晰地定位并区分专家为两个不同的功能类别：</p><p>(1) 有害内容检测组（HCDG）：专门用于识别和检测用户输入中有害内容的专家。</p><p>(2) 有害响应控制组（HRCG）：专门用于控制和强制模型行为，生成适当的对齐安全响应（例如，拒绝或拒绝响应）的专家。</p><h3 id="专家统计-Expert-Statistics-与稳定性专家选择-SES-算法"><a href="#专家统计-Expert-Statistics-与稳定性专家选择-SES-算法" class="headerlink" title="专家统计 (Expert Statistics) 与稳定性专家选择 (SES) 算法"></a>专家统计 (Expert Statistics) 与稳定性专家选择 (SES) 算法</h3><p><strong>重复采样</strong>: 首先，从一个大的基础数据分布中（例如，包含各类有害问题的 <code>D_Regular</code> 数据集），多次独立地、不放回地抽取样本，形成多个经验数据集 <code>X^(k)</code>。</p><p><strong>独立估计</strong>: 针对每一个采样出的数据集 <code>X^(k)</code>，独立计算其中每个专家 <code>e</code> 的激活概率 <code>p(e|X^(k))</code>。</p><p><strong>识别高频专家</strong>: 在每个数据集中，根据计算出的激活概率，选出排名最高的 <code>N</code> 个专家，形成一个高频专家集合 <code>E_top^(k)</code>。</p><p><strong>取交集</strong>: 最后，将所有采样数据集得到的高频专家集合 <code>E_top^(k)</code> 进行求交集操作。这个交集中的专家，就是被认为不受特定数据样本影响、稳定且频繁被激活的关键专家集合 <code>E_top(X)</code>。</p><p>通过对常规有害数据集 ($\mathcal{D} _ {Regular}$) 和“越狱”有害数据集 ($\mathcal{D} _ {Jailbreak}$) 分别执行 SES 算法，可以得到两组不同的稳定高频专家集合。</p><h3 id="专家功能分类"><a href="#专家功能分类" class="headerlink" title="专家功能分类"></a>专家功能分类</h3><p>在获得不同场景下的高频专家集合后，算法通过简单的集合运算对它们进行功能划分。</p><ul><li><p><strong>有害内容识别组 (Harmful Content Detection Group, $\mathcal{E} _ {id}$)</strong>:</p><ul><li><strong>定义</strong>: 这些专家在处理<strong>常规有害</strong>输入和<strong>越狱有害</strong>输入时都会被稳定激活。这表明它们的核心作用是识别内容本身的有害性，而不受提问方式的影响。</li><li><strong>计算方法</strong>: 通过取两个高频专家集合的<strong>交集</strong>获得：$\mathcal{E} _ {id} &#x3D; \mathcal{E} _ {top}(\mathcal{D} _ {Regular}) \cap \mathcal{E} _ {top}(\mathcal{D} _ {Jailbreak})$。</li></ul></li><li><p><strong>安全响应控制组 (Harmful Response Control Group, $\mathcal{E} _ {ctrl}$)</strong>:</p><ul><li><strong>定义</strong>: 这些专家只在模型成功拒绝<strong>常规有害</strong>输入时被激活，而在模型被<strong>越狱</strong>成功、生成不安全内容时则不会被激活。这表明它们专门负责执行模型的安全机制，生成拒绝回答。</li><li><strong>计算方法</strong>: 通过取两个高频专家集合的<strong>差集</strong>获得：$\mathcal{E} _ {ctrl} &#x3D; \mathcal{E} _ {top}(\mathcal{D} _ {Regular}) - \mathcal{E} _ {top}(\mathcal{D} _ {Jailbreak})$。</li></ul></li></ul><h3 id="专家功能验证"><a href="#专家功能验证" class="headerlink" title="专家功能验证"></a>专家功能验证</h3><p>最后，为了验证上述分类的准确性，算法设计了两种针对性的实验。</p><ul><li><p><strong>线性探测 (Linear Probing) - 验证 $\mathcal{E} _ {id}$</strong>:</p><ul><li><strong>目的</strong>: 验证“有害内容识别组”的专家是否真的具备区分有害内容的能力。</li><li><strong>方法</strong>: 提取 $\mathcal{E} _ {id}$ 中每个专家的输出特征（隐状态），然后用这些特征来训练一个简单的线性分类器，任务是判断输入是有害的还是无害的。如果基于这些专家特征训练的分类器表现（如准确率、F1分数等）远超随机选择的专家，就证明了它们的识别功能。</li></ul><p>  利用所识别专家的前馈网络（FFNs）输出的特征作为线性分类器的输入，该分类器预测一个二元标签，用以判断输入提示是否具有危害性或无害。<br>  $$<br>  \hat y&#x3D;\sigma(W\times h_{id}(x)+b),\hat y\in{0,1}<br>  $$<br>  <img src="/2025/20250913/1.png">  </p></li><li><p><strong>专家屏蔽 (Expert Masking) - 验证 $\mathcal{E} _ {ctrl}$</strong>:</p><ul><li><strong>目的</strong>: 验证“安全响应控制组”的专家是否对模型的拒绝行为至关重要。</li><li><strong>方法</strong>: 在模型进行推理时，人为地“屏蔽”（禁用）$\mathcal{E} _ {ctrl}$ 集合中的所有专家。具体做法是将这些专家的路由权重设置为一个极小值（负无穷），使其在 Softmax 后的概率接近于零，从而不会被选中。然后观察模型在处理有害请求时的拒绝率是否有显著下降。实验结果表明，屏蔽极少数这类专家就会导致模型的安全性能大幅降低。</li></ul><p>  <img src="/2025/20250913/2.png"></p></li></ul><h2 id="SteerMOE"><a href="#SteerMOE" class="headerlink" title="SteerMOE"></a>SteerMOE</h2><p>提出将 MoE 路由机制重新诠释为一种可控制且可解释的模块，而不仅仅是一个分配计算资源的工具，它实际上是一个富含信号的层，可在测试时调节模型行为。具体而言，我们假设某些专家与特定技能、特质或倾向存在行为上的纠缠，通过检测并（禁用&#x2F;启用）这些专家，可以有针对性地引导模型的输出。</p><h3 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h3><p><strong>创建成对的样本：</strong><br>首先，研究人员需要准备两组不同的输入样本，每一组代表一种相反的行为。</p><ul><li><strong>样本组 (1)</strong>：包含“安全”行为的样本（例如，一个有害提问 + 一个拒绝回答）。</li><li><strong>样本组 (2)</strong>：包含“不安全”行为的样本（例如，同一个有害提问 + 一个顺从的有害回答）。</li></ul><p><strong>统计专家的激活次数：</strong><br>    接下来，统计在处理这两组样本时每个专家被激活的总次数（即被路由到的令牌数量）。</p><ul><li><strong>$A _ {i}^{(1)}$</strong>：专家 <code>i</code> 在所有“安全”样本中被激活的总次数 。</li><li><strong>$A _ {i}^{(2)}$</strong>：专家 <code>i</code> 在所有“不安全”样本中被激活的总次数。</li><li><strong>$N^{(1)}$</strong>：所有“安全”样本中的令牌总数。</li><li><strong>$N^{(2)}$</strong>：所有“不安全”样本中的令牌总数。</li></ul><p><strong>计算激活率（Activation Rate）</strong>：</p><p>用每个专家的总激活次数除以对应样本组的总令牌数，得到该专家在特定行为下的激活概率（或频率）。</p><ul><li>专家 <code>i</code> 在“安全”行为下的激活率为： <strong>$P_i^{(1)} &#x3D; A _ {i}^{(1)} &#x2F; N^{(1)}$</strong> </li><li>专家 <code>i</code> 在“不安全”行为下的激活率为： <strong>$P_i^{(2)} &#x3D; A _ {i}^{(2)} &#x2F; N^{(2)}$</strong></li></ul><p><strong>计算风险差异（RD）</strong>：</p><p>最后，将这两种行为下的激活率相减，就得到了专家 <code>i</code> 的风险差异（RD），在论文中用符号 <strong>$\Delta _ {i}$</strong> 表示，$\Delta _ {i} &#x3D; P_i^{(1)} - P_i^{(2)}$</p><p>若$\Delta _ {i}$为正值，说明它与安全行为正相关，是安全专家。</p><p>若$\Delta _ {i}$为负值，说明它与不安全行为正相关，是不安全专家。</p><p>若$\Delta _ {i}$接近于0，说明该专家与这两种行为的关联性不强。</p><h3 id="引导"><a href="#引导" class="headerlink" title="引导"></a>引导</h3><p><strong>确定目标专家</strong>： 根据 RD 分数对专家进行排序，以确定要促进或抑制哪些专家来实现预期结果。例如，为了让模型更安全，他们会激活具有高“安全”RD 分数的专家，并抑制具有高“不安全”RD 分数的专家 。</p><p><strong>调整路由器 Logits</strong>： 在生成回答时，对于每个令牌，他们会截获路由器为每个专家生成的原始分数（logits） 。</p><p>之前会首先进行logsoftmax，以将其置于一个共享的尺度上。即s&#x3D;logsoftmax(z)</p><blockquote><p>而我们有softmax(logsoftmax(z))&#x3D;softmax(z)</p></blockquote><p>应用引导规则，简单的规则 ：</p><ul><li>要激活一个专家，就人为地将其分数设置为所有专家中的<strong>最高分</strong> 。</li></ul><p>$$<br>s_k \leftarrow s _ {min}-\epsilon<br>$$</p><ul><li>要抑制一个专家，就人为地将其分数设置为所有专家中的<strong>最低分</strong> 。</li></ul><p><strong>生成输出</strong>： 模型随后根据这些修改后的分数照常选择得分最高的 k 个专家，并继续生成过程。这种“软”引导确保了目标专家被优先选择或避免，而不会完全破坏模型的整体功能。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DoLa：通过对比层解码提高大型语言模型的事实性</title>
    <link href="/2025/20250910/"/>
    <url>/2025/20250910/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2024）</p><p> DoLa &#x3D; Decoding by Contrasting Layers</p><span id="more"></span><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p>虽然人们对 LMs 幻觉的确切原因尚不完全了解，但一个可能的原因是极大似然语言建模的目标，该目标最小化数据分布和模型分布之间的前向 KL 散度。这个目标可能会导致具有质量寻求行为的模型，从而使 LM 为与训练数据中嵌入的知识完全一致的句子分配非零概率。</p><p>从模型可解释性的角度来看，已经粗略地表明 Transformer 语言模型在较早的层中对“低级”信息（例如词性标签）进行编码，并在较晚的层中对更多“语义”信息进行编码。</p><p>本文利用了这一事实，通过对比不同层之间的差异来增强 LLM 输出事实正确的概率。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>记录模型最后一层（称为“成熟层”，Mature Layer）的输出 logits</p><p>每一步动态地选择中间的一个最合适的层 ：计算“成熟层”的输出概率分布与每一个“候选早层”的输出概率分布之间的<strong>JS散度</strong> ，选择与“成熟层”分布差异最大（即JSD值最高）的那个早层，将其命名为“不成熟层”（Premature Layer）。</p><p>最终输出概率为“成熟层”的对数概率减去“不成熟层”的对数概率 。</p><p>另外，还有两个小改进：</p><p>（1）<strong>自适应合理性约束 (Adaptive Plausibility Constraint, APC)</strong>: 为了防止一些本身概率极低的词元在对比后得分异常高，这个对比操作只应用于在“成熟层”输出中概率已经足够高的一小部分词汇上 。对于不在这个合理范围内的词元，其概率会被设为零（或一个极小值）。<br>$$<br>\mathcal{V} _ {head}(x _ {t}|x _ {&lt;t}) &#x3D; {x _ {t} \in \mathcal{X} : q _ {N}(x _ {t}) \ge \alpha \max _ {w} q _ {N}(w)}<br>$$<br>（2）<strong>重复惩罚 (Repetition Penalty)</strong>: 算法发现对比操作有时会增加生成重复内容的倾向，因此会额外施加一个重复惩罚，降低已生成词元的概率。</p><p>来自《<a href="https://arxiv.org/abs/1909.05858">CTRL: A Conditional Transformer Language Model for Controllable Generation</a>》的方法。</p><p>博客之前已经提到贪心或者beam search会导致产生重复的短语和句子。故该论文提出了一种新的采样方案，通过接近贪婪的采样来信任模型分布，但通过惩罚来防止重复。</p><p>给定一个生成的词元列表 g，下一个词元的概率分布$p_i$定义为：<br>$$<br>p _ {i} &#x3D; \frac{\exp(x _ {i} &#x2F; (T \cdot I(i \in g)))}{\sum _ {j} \exp(x _ {j} &#x2F; (T \cdot I(j \in g)))} \quad I(c) &#x3D; \theta \text{ if c is True else } 1<br>$$<br><code>If c is True</code>的意思是若词元出现过。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PerturboLLaVA：通过扰动视觉训练减少多模态幻觉</title>
    <link href="/2025/20250908/"/>
    <url>/2025/20250908/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2025）</p><span id="more"></span><h2 id="两个贡献"><a href="#两个贡献" class="headerlink" title="两个贡献"></a>两个贡献</h2><p>当前缺乏一种在概念层面精细衡量描述质量的指标，故有了：</p><p><strong>HalFscore</strong>：一种新颖的评估指标，旨在从概念层面精细地衡量图像描述的准确性和完整性。</p><p>缓解幻觉，故有了：</p><p><strong>PerturboLLaVA</strong>：一种创新的“扰动式视觉训练”方法。该方法通过在训练过程中引入与图像内容相冲突的、精心设计的误导性文本，来降低模型对其固有语言知识（即“语言先验”）的过度依赖，从而迫使其更加关注视觉输入。</p><p>像OPERA和VCD，尽管解码策略具有无需训练的优势，但它们并未解决多模态模型中幻觉的根本原因，因为这些问题源于训练阶段。此外，从实际角度来看，大模型的推理成本通常超过训练成本，因为模型训练一次但部署无数次。</p><h2 id="HalFscore"><a href="#HalFscore" class="headerlink" title="HalFscore"></a>HalFscore</h2><p><img src="/2025/20250908/1.jpg"></p><p><strong>图谱构建</strong>：通过 GPT-4o 模型，从文本中提取信息，并将其表示为<strong>三元组（triplets）</strong>，形式为 <code>&lt;实体1, 关系, 实体2&gt;</code> 。例如，“时钟在墙上”可以表示为 <code>(时钟, on, 墙)</code>，“镜子是粉色的”可以表示为 <code>(镜子, is, 粉色)</code> 。这些三元组随后被整合成一个图谱，其中实体是节点，关系是边 。<br>$$<br>\text{Precision} &#x3D; \frac{|C_{\text{gen}} \cap C_{\text{gt}}|}{|C_{\text{gen}}|}<br>&#x3D; 1 - \frac{|C_{\text{hallucinated}}|}{|C_{\text{gen}}|},<br>$$</p><p>$$<br>\text{Recall} &#x3D; \frac{|C_{\text{gen}} \cap C_{\text{gt}}|}{|C_{\text{gt}}|}<br>&#x3D; 1 - \frac{|C_{\text{omitted}}|}{|C_{\text{gt}}|}.<br>$$</p><p>最后综合起来得到$\text{HalFscore}$（其实就是F1）：</p><p>$$<br>\text{HalFscore} &#x3D; 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}.<br>$$</p><h2 id="PerturboLLaVA"><a href="#PerturboLLaVA" class="headerlink" title="PerturboLLaVA"></a>PerturboLLaVA</h2><p><img src="/2025/20250908/2.jpg"></p><p>为确保扰动的有效性和自然性，遵循以下原则。</p><ol><li>上下文相关性。扰动应与图像内容在上下文上相关，使其看似合理但具有误导性。</li><li>与预训练知识对齐。扰动设计需与常见的语言先验产生共鸣，确保其现实性并反映潜在的模型偏差。</li><li>语义多样性。通过改变$x_p$的结构和主题元素，确保扰动的多样性，使其与常见的误解或偏见保持一致。在实际操作中，使用 GPT-4o 生成扰动文本。GPT-4o 模型会查看图像、问题和答案，并根据世界知识以及某些图像细节，构建强大且多样化的扰动，而不泄露答案。GPT-4的指令提示详见附录。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对比解码之VCD</title>
    <link href="/2025/20250907/"/>
    <url>/2025/20250907/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（CVPR 2024 Highlight）</p><p>《Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive》</p><span id="more"></span><h2 id="对比解码"><a href="#对比解码" class="headerlink" title="对比解码"></a>对比解码</h2><p>对比解码（ Contrastive Decoding ，CD）提供了一种用于缓解幻觉景的方法，通过对比原始输入和失真输入之间的输出分布。具体而言，CD 方法首先生成两个输出分布：一个来自原始视觉图像 v 和文本查询 q，另一个来自扰动后的输入 v∗ 和 q∗。然后，通过检查两个分布之间的差异，可以构建一个对比响应 ℓcd，如下所示：<br>$$<br>\begin{align}<br>\ell _ {cd}&amp;&#x3D;mlogp(y_t\mid v,q,y _ {&lt;t};\theta)-nlogp(y_t\mid v^\ast,q^\ast,y _ {&lt;t};\theta)<br>\\<br>y_t&amp;\sim Softmax(\ell _ {cd})<br>\end{align}<br>$$<br>比如m可以为$1+\alpha$,n为$\alpha$。</p><p><img src="/2025/20250907/1.jpg"></p><p>（图来自《Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding》）</p><h2 id="VCD"><a href="#VCD" class="headerlink" title="VCD"></a>VCD</h2><p>VCD遵循同样的流程。</p><p>对于构建失真图像，使用的是diffusion的（高斯）加噪过程。</p><p><img src="/2025/20250907/2.jpg"></p><p>为了避免过度惩罚可能正确的常规用语，VCD还引入了一项“自适应合理性约束” 。该机制会保留原始图像输入下置信度较高的候选词，防止模型生成不合逻辑的文本 。</p><p>在决定下一个要生成的词语时，它并不直接在整个词汇表中进行对比解码。而是先做一步筛选：</p><p>1.<strong>创建候选词列表</strong>：首先，模型会<strong>仅根据原始的、清晰的图像</strong>来预测下一个最有可能的词语 。</p><p>2.<strong>设置置信度门槛</strong>：它会筛选出一个“高置信度”的候选词集合 <code>V_head</code> 。这个集合只包含那些在清晰图像下预测概率足够高的词语。这个“足够高”是由一个超参数<br>   <strong>β (beta)</strong> 来控制的 。</p><p>3.<strong>在候选范围内解码</strong>：最后，核心的VCD对比解码过程只会在这个预先筛选出的“合理”候选词列表 ( <code>V_head</code> ) 中进行，而不是在整个词汇表中进行 。</p><p>具体而言，</p><p>$$<br>\mathcal{V} _ {\text{head}}(y _ {&lt;t}) &#x3D; {y_t \in \mathcal{V} : p _ {\theta}(y_t | v, x, y _ {&lt;t}) \ge \beta \max _ {w} p _ {\theta}(w | v, x, y _ {&lt;t})}<br>$$</p><p>$$<br>p _ {\text{vcd}}(y_t | v, v’, x) &#x3D; 0, \quad \text{if } y_t \notin \mathcal{V} _ {\text{head}}(y _ {&lt;t})<br>$$</p><p>其中 $\mathcal{V}$ 是 LVLMs 的输出词表，$\beta$ 是 [0,1] 中用于控制下一个 token 分布截断的超参数。较大的 $\beta$ 表示更激进的截断，仅保留高概率的 token。</p><p><img src="/2025/20250907/figure1.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ALPHAEDIT：NULL-SPACE CONSTRAINED KNOWLEDGE EDITING FOR LANGUAGE MODELS</title>
    <link href="/2025/20250903/"/>
    <url>/2025/20250903/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2025 outstanding paper）</p><span id="more"></span><h2 id="准备知识"><a href="#准备知识" class="headerlink" title="准备知识"></a>准备知识</h2><h3 id="自回归LLM"><a href="#自回归LLM" class="headerlink" title="自回归LLM"></a>自回归LLM</h3><p>模型可以写为：</p><p>$$<br>\begin{align}<br>h^i&amp;&#x3D;h^{i-1}+a^i+m^i<br>\\<br>m^i&amp;&#x3D;W_{out}^i\sigma(W_{in}\gamma(h^{l-1}+a^l))<br>\end{align}<br>$$</p><h3 id="LLM-中的模型编辑"><a href="#LLM-中的模型编辑" class="headerlink" title="LLM 中的模型编辑"></a>LLM 中的模型编辑</h3><p>我们会用“键-值”（key-value）对来描述。 Wout 在 FFN 层中通常被解释为一个线性的联想记忆，充当信息检索的关键-值存储。（可以适当联想一下上篇博客）。</p><p>只关注更新知识时：<br>$$<br>\Delta&#x3D;argmin_{\tilde \Delta}||(W+\tilde\Delta)K_1-V_1||^2<br>$$<br>即关注新知识，又注重原知识：<br>$$<br>\Delta&#x3D;argmin_{\tilde \Delta}||(W+\tilde\Delta)K_1-V_1||^2+||(W+\tilde\Delta)K_0-V_0||^2<br>$$<br>上式的闭式解为：$\Delta&#x3D;(V_1-WK_1)K_1^T(K_0K_0^T+K_1K_1^T)^{-1}$</p><blockquote><p>由于$WK_0&#x3D;V_0$，$||(W+\tilde\Delta)K_0-V_0||^2&#x3D;||\tilde\Delta K_0||^2$</p><p>故等价于<br>$$<br>\begin{align}<br>\Delta&amp;&#x3D;argmin_{\tilde \Delta}||(W+\tilde\Delta)K_1-V_1||^2+||\tilde\Delta K_0||^2<br>\\<br>&amp;&#x3D;argmin_{\tilde \Delta}||\tilde\Delta K_1-(V_1-WK_1)||^2+||\tilde\Delta K_0||^2<br>\end{align}<br>$$<br>记$J(\tilde \Delta)&#x3D;||\tilde\Delta K_1-(V_1-WK_1)||^2+||\tilde\Delta K_0||^2$<br>$$<br>\begin{align}<br>\frac{\partial J}{\partial \Delta}&amp;&#x3D;\frac{\partial }{\partial \Delta}||\Delta K_1-(V_1-WK_1)||^2+||\Delta K_0||^2&#x3D;0<br>\\<br>&amp;\Leftrightarrow2(\Delta K_1-(V_1-WK_1))K_1^T+2(\Delta K_0)K_0^T&#x3D;0<br>\\<br>&amp;\Leftrightarrow \Delta(K_1K_1^T+K_0K_0^T)&#x3D;(V_1-WK_1)K_1^T<br>\\<br>&amp;\Leftrightarrow\Delta&#x3D;(V_1-WK_1)K_1^T(K_0K_0^T+K_1K_1^T)^{-1}<br>\end{align}<br>$$</p></blockquote><h3 id="零空间"><a href="#零空间" class="headerlink" title="零空间"></a>零空间</h3><p>BA&#x3D;0，则B属于A的（左）零空间。</p><p>故对于扰动$\Delta$，则有$(W+\Delta)K_0&#x3D;WK_0&#x3D;V_0$。这正是我们所需要的性质。</p><p>我们可以把扰动投影到K0的零空间从而将K0从损失函数中消去，这样我们可以不用去权衡新旧知识损失函数的比率。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>由于$K_0\in R^{d_0\times 100000}$具有高维度，而$K_0$和$K_0(K_0)^T$具有相同的零空间，故我们对$K_0K_0^T$进行分析。</p><p>对$K_0K_0^T$进行SVD：$U,\Lambda, U^T&#x3D;SVD(K_0K_0^T)$.</p><p>U的每一列是特征向量，移除非0（由于实际上特征值很难为0，故移除的是大于$10^{-2}$的）的特征值，得到$\hat U$。</p><p>故投影矩阵P定义为：$P&#x3D;\hat U(\hat U)^T$</p><p>故$(W+\Delta P)K_0&#x3D;WK_0&#x3D;V_0$.</p><p>最终的优化函数为：<br>$$<br>\Delta&#x3D;argmin_{\tilde \Delta}||(W+\tilde\Delta P)K_1-V_1||^2+||\tilde \Delta P||^2<br>$$<br>在顺序编辑任务中，在当前编辑过程中，我们需要在目标中添加一项以防止扰动破坏先前编辑中更新的知识。令$K_p$ 和$V_p$表示之前更新知识的关键和值矩阵。即$||(W+\tilde\Delta P)K_p-V_p||^2$最小化，而其可以简化为$||\tilde \Delta PK_p||^2$。</p><p>优化函数为：<br>$$<br>\Delta&#x3D;argmin_{\tilde \Delta}||(W+\tilde\Delta P)K_1-V_1||^2+||\tilde \Delta P||^2+||\tilde \Delta PK_p||^2<br>$$</p><p>为了方便求导，我们首先定义残差来简化：$R&#x3D;V_1-WK_1$</p><p>同样记$J(\tilde\Delta)&#x3D;||(W+\tilde\Delta P)K_1-V_1||^2+||\tilde \Delta P||^2+||\tilde \Delta PK_p||^2$.</p><p>则：<br>$$<br>\begin{align}<br>J(\tilde\Delta)&amp;&#x3D;||(W+\tilde\Delta P)K_1-V_1||^2+||\tilde \Delta P||^2+||\tilde \Delta PK_p||^2<br>\\&amp;&#x3D;||\tilde\Delta PK_1-R||^2+||\tilde \Delta P||^2+||\tilde \Delta PK_p||^2<br>\end{align}<br>$$<br>求导有，注意对称和幂等：$P&#x3D;P^T,P^2&#x3D;P$：<br>$$<br>\begin{align}<br>&amp;2(\tilde\Delta PK_1-R)(PK_1)^T+2(\tilde \Delta P)(P^T)+2(\tilde \Delta PK_p)(PK_p)^T&#x3D;0<br>\\<br>&amp;\Leftrightarrow \tilde\Delta PK_1K_1^TP-RK_1^TP+\tilde\Delta P+\tilde\Delta PK_pK_p^TP&#x3D;0<br>\\<br>&amp;\Leftrightarrow (\tilde\Delta PK_1-R)K_1^TP+\tilde\Delta P+\tilde\Delta PK_pK_p^TP&#x3D;0<br>\\<br>&amp;\Leftrightarrow \tilde\Delta PK_1K_1^TP+\tilde\Delta P+\tilde\Delta PK_pK_p^TP&#x3D;RK_1^TP<br>\\<br>&amp;\Leftrightarrow \tilde\Delta P(K_1K_1^TP+I+K_pK_p^TP)&#x3D;RK_1^TP<br>\\<br>&amp;\Leftrightarrow \tilde\Delta P&#x3D;RK_1^TP(K_1K_1^TP+I+K_pK_p^TP)^{-1}<br>\end{align}<br>$$<br>而和现有方法（MEMIT）比较可以发现只需修改一行代码就能得到更好的效果：<br>$$<br>\Delta_{MEMIT}&#x3D;RK_1^T(K_pK_p^T+K_1K_1^T+K_0K_0^T)^{-1}<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kNN-LMs：一种RAG和LLM前的记忆挂靠方法</title>
    <link href="/2025/20250902/"/>
    <url>/2025/20250902/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2020）</p><p>《Generalization through Memorization: Nearest Neighbor Language Models》</p><span id="more"></span><p>这是一种免训练的方法。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/12.jpg"></p><p>Training Contexts处或者更扩展的来说，可以是额外的数据集。</p><p><strong>对于额外数据集</strong>：</p><p>上下文词元序列$c_t&#x3D;(w_1,…,w _ {t-1})$</p><p>记f(c)为模型某一层输出，定义键值对为：<br>$$<br>(K,V)&#x3D;((f(c_i),w_i)\mid (c_i,w_i)\in D)<br>$$<br>再根据负距离的softmax计算邻居上的分布：<br>$$<br>p _ {\text{kNN}}(y|x) \propto \sum _ {\langle k_i, v_i \rangle \in \mathcal{N}} \mathbb{1} _ {y&#x3D;v_i} \exp(-d(k_i, f(x)))<br>$$<br><strong>对于原始输入</strong>：</p><p>我们把以上融合进来：<br>$$<br>p(y\mid x)&#x3D;\lambda p _ {knn}(y\mid x)+(1-\lambda)p _ {LM}(y\mid x)<br>$$<br>使用faiss进行检索。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250902/1.jpg"></p><p><img src="/2025/20250902/2.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DINO系列</title>
    <link href="/2025/20250831/"/>
    <url>/2025/20250831/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将介绍facebook&#x2F;meta出品的DINOv1~v3。</p><p>DINO &#x3D; Self-<strong>di</strong>stillation with <strong>no</strong> labels</p><span id="more"></span><p>注意与目标检测《<a href="https://arxiv.org/abs/2203.03605">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection</a>》那篇关于目标检测的文章做区别。</p><h1 id="DINO-V1"><a href="#DINO-V1" class="headerlink" title="DINO V1"></a>DINO V1</h1><p>在开始之前我们不妨回顾一下一些相关的对比学习方法。</p><p><img src="/2025/20250831/1_0brkjcnDkgEcWpFpe0bxeg.webp"></p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2025/20250831/1.jpg"></p><p><img src="/2025/20250831/2.jpg"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250831/3.jpg"></p><h1 id="iBOT"><a href="#iBOT" class="headerlink" title="iBOT"></a>iBOT</h1><p>iBOT使用了v1的思想，且后续的DINO基于iBOT，所以我们需要先介绍一下这承上启下的iBOT。</p><p>字节跳动出品。</p><p>iBOT &#x3D; image BERT 预训练结合 Online Tokenizer</p><p><img src="/2025/20250831/8.jpg"></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2025/20250831/9.jpg"></p><p>它与何恺明的MAE有些相似，不过何恺明一贯的更简单。</p><p><img src="/2025/20250831/10.jpg"></p><p>MAE发现mask 75%效果最好。</p><p>而iBOT没有这样的勇气，只选择0.5的概率mask，且这部分以均匀采样的方式从范围 [0.1 , 0.5 ] 中选取的mask比率。故iBOT通过加大模型复杂度来获取高性能。</p><p><img src="/2025/20250831/11.jpg"></p><h1 id="DINO-V2"><a href="#DINO-V2" class="headerlink" title="DINO V2"></a>DINO V2</h1><p>通过一种判别式的自监督方法学习特征，这种方法可以看作是 DINO 和 iBOT 损失的结合，并带有SwAV的中心化。还添加了一个正则化项以扩展特征，并进行一个短时的高分辨率训练阶段。</p><h2 id="数据搜集"><a href="#数据搜集" class="headerlink" title="数据搜集"></a>数据搜集</h2><p><strong>数据集来源</strong>。精选的数据集其中包含 ImageNet-22k、ImageNet-1k 的训练集、Google Landmarks 以及几个细粒度数据集。对于未经整理的数据源，从公开可用的爬取网络数据存储库中收集了一个原始的未过滤图像数据集。从存储库中的每个网页提取<code> &lt;img&gt;</code> 标签中的图像 URL 链接。丢弃那些不安全或受领域限制的 URL，并对下载的图像进行后处理（PAC 哈希去重、NSFW 过滤和模糊处理可辨认的面部）。这导致了 12 亿张独特的图像。</p><p><strong>去重</strong>。 将 Pizzi的复制检测流水线应用于未经整理的数据，并去除近似重复的图像。还去除了任何本工作中使用的基准测试或验证集中包含的图像的近似重复项。<br><strong>Self-supervised image retrieval</strong>。 通过从非精选数据源中检索与精选来源中的图像接近的图像来构建精心策划的预训练数据集。为了实现这一点，首先使用在 ImageNet-22k 上预训练的自监督 ViT-H&#x2F;16 网络计算图像嵌入，并使用余弦相似度作为图像之间的距离度量。然后，对非精选数据进行 k-均值聚类。给定一个用于检索的查询数据集，如果它足够大，为每个查询图像检索 N （通常是 4 个）最近邻图像。如果它较小，从与每个查询图像对应的簇中采样 M 张图像。</p><p><img src="/2025/20250831/4.jpg"></p><h2 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h2><p>与iBOT的比较。</p><p><img src="/2025/20250831/7.jpg"></p><p>随机掩码一些输入patches提供给学生，但不提供给教师。</p><p>加入了 Koleo 正则化项（《Spreading vectors for similarity search》）。</p><p>使用了Swav的Sinkhorn-Knopp centering 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sinkhorn</span>(<span class="hljs-params">scores, eps=<span class="hljs-number">0.05</span>, niters=<span class="hljs-number">3</span></span>):<br>Q = exp(scores / eps).T<br>Q /= <span class="hljs-built_in">sum</span>(Q)<br>K, B = Q.shape<br>u, r, c = zeros(K), ones(K) / K, ones(B) / B<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(niters):<br>u = <span class="hljs-built_in">sum</span>(Q, dim=<span class="hljs-number">1</span>)<br>Q *= (r / u).unsqueeze(<span class="hljs-number">1</span>)<br>Q *= (c / <span class="hljs-built_in">sum</span>(Q, dim=<span class="hljs-number">0</span>)).unsqueeze(<span class="hljs-number">0</span>)<br><span class="hljs-keyword">return</span> (Q / <span class="hljs-built_in">sum</span>(Q, dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>)).T<br></code></pre></td></tr></table></figure><p><strong>增加图像分辨率</strong>。增加图像分辨率对于分割或检测等像素级下游任务至关重要，因为在低分辨率下小物体可能会消失。然而，在高分辨率下训练既耗时又占用大量内存，因此在预训练的最后阶段短暂地将图像分辨率提高到 518 × 518。</p><h1 id="DINO-V3"><a href="#DINO-V3" class="headerlink" title="DINO V3"></a>DINO V3</h1><p>将自监督学习（SSL）扩展到大型前沿模型。</p><p>这篇主要是将DINOV2 scale up和使用了更多的tricks。</p><h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><p><strong>数据采集与整理</strong>。第一部分，通过应用基于层次化 k -means 的自动筛选方法构建。采用 DINOv2 作为图像嵌入，并使用 5 级聚类，从最低到最高级别的簇数分别为 200 M、8 M、800 k、100 k 和 25 k。在构建簇的层次结构后，应用了 平衡采样算法。这生成了一个包含 16.89 亿张图像的精选子集（命名为 LVD-1689M），确保了对网络上所有视觉概念的平衡覆盖。</p><p>第二部分，采用了与 Oquab 提出的程序类似的基于检索的筛选系统。从数据池中检索与选定种子数据集相似的图像，创建了一个覆盖与下游任务相关的视觉概念的数据集。</p><p>第三部分，使用了包括 ImageNet1k 、ImageNet22k 和 Mapillary 街景序列在内的原始公开计算机视觉数据集。</p><h2 id="模型-2"><a href="#模型-2" class="headerlink" title="模型"></a>模型</h2><p>加大参数。</p><p><img src="/2025/20250831/5.jpg"></p><p>加入了 Koleo 正则化项（《Spreading vectors for similarity search》）。</p><blockquote><p>引入的正则化项，旨在使点在 $\mathcal{S} _ {d _ {out}}$ 上均匀分布。</p><p>在了解点密度 $p$ 的前提下，  可以直接最大化微分熵 $- \int p(u)\log(p(u))du$。</p><p>仅给定样本 $(f(x_1),…,f(x_n))$ 时， 采用微分熵的估计量作为代理。</p><p>Kozachenko 和 Leononenko证明，  定义 $\rho _ {n,i} &#x3D; \min _ {j\neq i}||f(x_i)-f(x_j)||$ 后，该分布的微分熵可通过以下方式估计：<br>$$<br>H_n &#x3D; \alpha_n \frac{1}{n} \sum _ {i&#x3D;1}^n \log(\rho _ {n,i}) + \beta_n<br>$$</p><p>其中 $\alpha_n$ 和 $\beta_n$ 是两个依赖于样本数量 $n$ 和数据维度 $d _ {out}$ 的常数。</p><p>忽略仿射项， 将熵正则化项定义为<br>$$<br>\mathcal{L} _ {KoLeo} &#x3D; -\frac{1}{n} \sum _ {i&#x3D;1}^n \log(\rho _ {n,i})<br>$$</p><p>该损失项具有令人满意的数学解释：最近邻点被推开，其强度是非递减且凹的。  </p><p>这确保了回报递减：随着点之间的距离增大，进一步增加距离的边际影响会逐渐减小。</p></blockquote><p>采用了 RoPE-box 抖动。坐标框 [−1, 1]被随机缩放到 [−s, s]，其中 s ∈ [0.5, 2]</p><p>为了确保训练正常启动，仍然对学习率和教师温度使用线性预热。遵循常见做法，使用 AdamW，并将总批量大小设置为 4096 张图像，分布在 256 个 GPU 上。采用多裁剪策略训练模型，每张图像取 2 个全局裁剪和 8 个局部裁剪。对于全局&#x2F;局部裁剪，使用边长为256&#x2F;112 像素的方形图像。</p><h2 id="Gram-锚定"><a href="#Gram-锚定" class="headerlink" title="Gram 锚定"></a>Gram 锚定</h2><p>我们在之前的博客中已经介绍，故不再详细解释。</p><h2 id="后训练"><a href="#后训练" class="headerlink" title="后训练"></a>后训练</h2><p>包括一个高分辨率适应阶段，以实现不同输入分辨率下的有效推理，模型蒸馏生成高质量且高效的小尺寸模型，以及文本对齐为 DINOv3 添加 zero-shot 能力。</p><h3 id="分辨率缩放"><a href="#分辨率缩放" class="headerlink" title="分辨率缩放"></a>分辨率缩放</h3><p>通过高分辨率适应步骤 (Touvron et al., 2019) 扩展了训练方案。为了确保在一系列分辨率下的高性能，采用了混合分辨率，在每个小批量中采样不同大小的全局和局部裁剪。具体来说，从 {512, 768} 的全局裁剪大小和从 {112, 168, 224, 336} 的局部裁剪大小，并额外训练模型 10k 次迭代。</p><p>与主训练阶段相似，此高分辨率适应阶段的一个关键组成部分是引入 Gram 锚定，采用 7B 教师作为 Gram教师。缺少这一部分，模型在稠密预测任务上的性能会显著下降。Gram 锚定促使模型在空间位置上保持一致且稳健的特征相关系数，这对于处理高分辨率输入带来的复杂性至关重要。</p><h3 id="模型蒸馏"><a href="#模型蒸馏" class="headerlink" title="模型蒸馏"></a>模型蒸馏</h3><p>蒸馏方法采用了与初始训练阶段相同的训练目标，确保了学习信号的一致性。然而，不同于依赖模型权重的指数移动平均（EMA），作者直接使用 7B 模型作为教师模型来指导较小的学生模型，在此过程中，教师模型保持固定。由于未观察到补丁级别的一致性问题，因此未应用 Gram 锚定技术。</p><h3 id="将-DINOv3-与文本对齐"><a href="#将-DINOv3-与文本对齐" class="headerlink" title="将 DINOv3 与文本对齐"></a>将 DINOv3 与文本对齐</h3><p>有研究表明，利用预训练的自监督视觉骨干网络可以实现有效的图像-文本对齐。这使得在多模态情景中利用这些强大模型成为可能，促进了超越全局语义的更为丰富和精确的文本到图像关联，同时由于视觉编码已预先学成，还降低了计算成本。</p><p>采用先前在Jose等人中提出的训练策略（《DINOv2 meets text: A unified framework for image-and pixel-level vision-language alignmen》）将文本编码器与DINOv3 模型对齐。</p><p><img src="/2025/20250831/6.jpg"></p><p>该方法遵循 LiT 训练范式，从头开始训练文本表示，以通过对比目标将图像与其标题匹配，同时保持视觉编码器冻结。为了在视觉方面提供一定的灵活性，在冻结的视觉骨干之上引入了两个Transformer层。该方法的一个关键增强是在与文本嵌入匹配之前，将平均池化的补丁嵌入与输出的 CLS token 进行拼接。</p><h1 id="更多阅读-参考资料"><a href="#更多阅读-参考资料" class="headerlink" title="更多阅读&amp;参考资料"></a>更多阅读&amp;参考资料</h1><p>1.<a href="https://zhuanlan.zhihu.com/p/1933583851923439816">万字长文超详解读之DINO全系列—视觉表征对比学习的高峰</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ICR 探针：追踪隐藏状态动态以在 LLMs 中实现可靠的幻觉检测</title>
    <link href="/2025/20250829/"/>
    <url>/2025/20250829/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ACL 2025）</p><p>目前存在多种幻觉检测方法。主流方法通过一致性检查或参考对比分析生成结果 (对比解码等)，而基于概率的方法则关注 Logit概率的不确定性。</p><p>另一种方法是检查大语言模型各层中的隐状态（例如嵌入向量）以检测幻觉 。基于输出或 Logit 概率的方法通常需要真实值参考或多次生成以保证一致性。相比之下，基于隐状态的检测方法具有无需参考的优势，无需依赖外部资源。</p><p>当前基于隐状态的幻觉检测方法大致可分为基于训练和无训练两类。</p><p>基于训练的方法通常涉及训练单独或组合的探针，或使用语义熵探针 ，而无训练方法则直接从隐状态计算检测指标。然而，现有方法通常聚焦于静态的高维隐状态（约 4000 维度），这限制了特征抽取能力。这些方法难以捕捉隐状态的更新以及残差流的跨层演化，最终制约了幻觉检测的有效性。</p><p>为了克服这些局限性，本文将关注点从隐状态本身转移到其在各层间的更新过程。引入一种新指标——ICR 得分（Information Contribution to Residual Stream），用于量化模块对隐状态更新的贡献。</p><span id="more"></span><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>对于模型有，$x_i^{\ell} &#x3D; z_i^{\ell-1} + \underbrace{\text{MHSA}^\ell(x_i^{\ell-1})} _ {a_i^\ell} + \underbrace{\text{FFN}^\ell(z_i^{\ell-1} + a_i^\ell)} _ {m_i^\ell}$。</p><p>定义层 $\ell$ 中隐藏状态的总更新为：</p><p>$$<br>\Delta x_i^{\ell}<br>&#x3D; x_i^{\ell} - x_i^{\ell-1}<br>&#x3D; a_i^{\ell} + m_i^{\ell}<br>$$<br>一种简单的方法是$\frac{a}{a+m}$。但作者认为这是不可靠，m是通过a计算得到的，这使得区分 FFN 对新参数化知识的贡献与其对 MHSA 信息的强化作用变得复杂。</p><p>作者通过以下方式计算所谓的ICR 得分：</p><p>（1）提取注意力，并对H个注意力头的得分进行池化&#x2F;平均。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> induction_heads_this_layer:<br>stacked_heads = torch.stack(induction_heads_this_layer)<br><span class="hljs-keyword">if</span> pooling == <span class="hljs-string">&#x27;mean&#x27;</span>:<br>        pooled_layer = torch.mean(stacked_heads, dim=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">elif</span> pooling == <span class="hljs-string">&#x27;max&#x27;</span>:<br>        pooled_layer = torch.<span class="hljs-built_in">max</span>(stacked_heads, dim=<span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">elif</span> pooling == <span class="hljs-string">&#x27;min&#x27;</span>:<br>        pooled_layer = torch.<span class="hljs-built_in">min</span>(stacked_heads, dim=<span class="hljs-number">0</span>)[<span class="hljs-number">0</span>]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;pooling&#125;</span> is not a valid pooling method.&quot;</span>)<br>    pooled_attentions.append(pooled_layer)<br></code></pre></td></tr></table></figure><p>（2）识别隐状态的更新方向。$p _ {i,j}^\ell&#x3D;\frac{(\Delta x_i^{\ell})^T\cdot x_j^\ell}{||x_j^\ell||} $</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">hs_diff = (current_token_hs - previous_token_hs)<br>    <br>w_i = torch.<span class="hljs-built_in">sum</span>(hs_diff * current_token_hs_topk, dim=<span class="hljs-number">1</span>) / (torch.norm(current_token_hs_topk, dim=<span class="hljs-number">1</span>) + <span class="hljs-number">1e-8</span>)<br></code></pre></td></tr></table></figure><p>（3）隐状态更新方向与注意力得分之间的一致性计算。使用 JS 散度（JSD）。<br>$$<br>ICR_i^\ell&#x3D;JSD(Proj_i^\ell,Attn_i^\ell)<br>$$<br><img src="/2025/20250829/overview_v2.png"></p><p>完整代码：<a href="https://github.com/XavierZhang2002/ICR_Probe/blob/f84a530594208ae06ded911e587ae375fb29dbb1/src/icr_score.py#L18">https://github.com/XavierZhang2002/ICR_Probe/blob/f84a530594208ae06ded911e587ae375fb29dbb1/src/icr_score.py#L18</a></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250829/1.jpg"></p><p><img src="/2025/20250829/2.jpg"></p><p><img src="/2025/20250829/3.jpg"></p><p><img src="/2025/20250829/4.jpg"></p><p><strong>消融实验</strong>：</p><p>在 NONE 情景中，同时排除隐状态和注意力信号，导致随机性能（AUROC&#x3D; 0.5）。</p><p>在 HS ONLY 情景中，仅使用隐状态信息，而注意力得分被设置为均匀分布。此时，计算对应于 Proj的熵。</p><p>在 HS + ATTN 情景中，同时整合隐状态和注意力信号。</p><p><img src="/2025/20250829/5.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Gram矩阵的妙用</title>
    <link href="/2025/20250826/"/>
    <url>/2025/20250826/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="Gram矩阵介绍"><a href="#Gram矩阵介绍" class="headerlink" title="Gram矩阵介绍"></a>Gram矩阵介绍</h2><p>给定一个内积空间中的一组向量 v1,v2,…,vn，它们的Gram矩阵 G 是一个n×n的方阵，其元素 Gij 定义为向量 vi 和 vj 的内积。</p><span id="more"></span><p><img src="/2025/20250826/1.png"></p><p><img src="/2025/20250826/2.png"></p><p>它有以下性质：</p><p><strong>体积计算</strong>：Gram行列式的平方根等于这组向量所张成的平行多面体的体积。</p><p><strong>线性独立性判定</strong>：一组向量是线性独立的当且仅当它们的Gram矩阵是可逆的（即行列式不为零）。</p><h2 id="ICLR-2025-使用Gram来对齐多模态embedding"><a href="#ICLR-2025-使用Gram来对齐多模态embedding" class="headerlink" title="ICLR 2025-使用Gram来对齐多模态embedding"></a>ICLR 2025-使用Gram来对齐多模态embedding</h2><p><a href="https://github.com/ispamm/GRAM">https://github.com/ispamm/GRAM</a></p><p>GRAM 在高维空间中直接学习并对齐 n 种模态，通过最小化由模态向量张成的 k 维平行多面体的格拉姆体积，确保所有模态同时几何对齐。GRAM 可以在任何下游方法中替代余弦相似度，适用于 2 到 n 种模态，并提供相较于以往相似度度量更有意义的对齐。基于 GRAM 的新型对比损失函数增强了多模态模型在高维嵌入空间中的对齐。</p><p>简单来说，它使用了Gram矩阵的体积性质。</p><p><img src="/2025/20250826/1.jpg"></p><p><strong>具体算法为</strong>：</p><p><img src="/2025/20250826/3.png"></p><p>损失函数为：</p><p><img src="/2025/20250826/4.png"></p><p>除了提出的基于体积的多模态损失函数外，还采用了辅助的数据-锚点匹配损失。该损失旨在鼓励模型推断一对锚点和数据是否匹配。</p><p>通过沿序列维度连接所有编码器的未池化特征来获得数据特征。在多模态编码器的底部，一个 MLP 层返回二元预测$p_{dam}$，并计算交叉熵。</p><p><img src="/2025/20250826/2.jpg"></p><p><strong>代码</strong>：</p><p>简单版本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">simple_volume_computation</span>(<span class="hljs-params">language, video, audio</span>):<br>    A = torch.stack([language, video, audio])<br>    G = A @ A.T<br>    gramian = torch.linalg.det(G)<br>    <span class="hljs-keyword">return</span> torch.sqrt(gramian)<br></code></pre></td></tr></table></figure><p>标准版本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">volume_computation</span>(<span class="hljs-params">anchor, *inputs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    General function to compute volume for contrastive learning loss functions.</span><br><span class="hljs-string">    Compute the volume metric for each vector in anchor batch and all the other modalities listed in *inputs.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">    - anchor (torch.Tensor): Tensor of shape (batch_size1, dim)</span><br><span class="hljs-string">    - *inputs (torch.Tensor): Variable number of tensors of shape (batch_size2, dim)</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    - torch.Tensor: Tensor of shape (batch_size1, batch_size2) representing the volume for each pair.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    batch_size1 = anchor.shape[<span class="hljs-number">0</span>]<br>    batch_size2 = inputs[<span class="hljs-number">0</span>].shape[<span class="hljs-number">0</span>]<br><br>    <span class="hljs-comment"># Compute pairwise dot products for language with itself</span><br>    aa = torch.einsum(<span class="hljs-string">&#x27;bi,bi-&gt;b&#x27;</span>, anchor, anchor).unsqueeze(<span class="hljs-number">1</span>).expand(-<span class="hljs-number">1</span>, batch_size2)<br><br>    <span class="hljs-comment"># Compute pairwise dot products for language with each input</span><br>    l_inputs = [anchor @ <span class="hljs-built_in">input</span>.T <span class="hljs-keyword">for</span> <span class="hljs-built_in">input</span> <span class="hljs-keyword">in</span> inputs]<br><br>    <span class="hljs-comment"># Compute pairwise dot products for each input with themselves and with each other</span><br>    input_dot_products = []<br>    <span class="hljs-keyword">for</span> i, input1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>        row = []<br>        <span class="hljs-keyword">for</span> j, input2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(inputs):<br>            dot_product = torch.einsum(<span class="hljs-string">&#x27;bi,bi-&gt;b&#x27;</span>, input1, input2).unsqueeze(<span class="hljs-number">0</span>).expand(batch_size1, -<span class="hljs-number">1</span>)<br>            row.append(dot_product)<br>        input_dot_products.append(row)<br><br>    <span class="hljs-comment"># Stack the results to form the Gram matrix for each pair</span><br>    G = torch.stack([<br>        torch.stack([aa] + l_inputs, dim=-<span class="hljs-number">1</span>),<br>        *[torch.stack([l_inputs[i]] + input_dot_products[i], dim=-<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(inputs))]<br>    ], dim=-<span class="hljs-number">2</span>)<br><br>    <span class="hljs-comment"># Compute the determinant for each Gram matrix</span><br>    gram_det = torch.det(G.<span class="hljs-built_in">float</span>())<br><br>    <span class="hljs-comment"># Compute the square root of the absolute value of the determinants</span><br>    res = torch.sqrt(torch.<span class="hljs-built_in">abs</span>(gram_det))<br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-comment"># Hyperparameters</span><br>bs = <span class="hljs-number">32</span><br>latent_dim = <span class="hljs-number">512</span><br>contrastive_temp = <span class="hljs-number">0.07</span><br><br><span class="hljs-comment"># Output of the encoders</span><br>language = torch.randn((bs,latent_dim))<br>video = torch.randn((bs,latent_dim))<br>audio = torch.randn((bs,latent_dim))<br><br>volume = volume_computation(language,video,audio)<br>volume = volume / contrastive_temp<br><br><br>volumeT = volume_computation(language,video,audio).T<br>volumeT = volumeT / contrastive_temp<br><br>targets = torch.linspace(<span class="hljs-number">0</span>, bs - <span class="hljs-number">1</span>, bs, dtype=<span class="hljs-built_in">int</span>)<br><br>loss = (<br>        F.cross_entropy(-volume, targets, label_smoothing=<span class="hljs-number">0.1</span>) <span class="hljs-comment">#d2a</span><br>        + F.cross_entropy(-volumeT, targets, label_smoothing=<span class="hljs-number">0.1</span>) <span class="hljs-comment">#a2d</span><br>) / <span class="hljs-number">2</span><br><br><span class="hljs-built_in">print</span>(loss)<br><br></code></pre></td></tr></table></figure><h2 id="arxiv-Principled-Multimodal-Representation-Learning"><a href="#arxiv-Principled-Multimodal-Representation-Learning" class="headerlink" title="arxiv-Principled Multimodal Representation Learning"></a>arxiv-Principled Multimodal Representation Learning</h2><p>这篇是上篇的改进。National University of Singapore提出。</p><p>不过是对奇异值进行处理，极大化主奇异值并极小化其他的特征值。</p><p><img src="/2025/20250826/5.png"></p><p>ICLR 2025的行列式为0，相当于存在一个为0的特征值（行列式&#x3D;特征值之积）。</p><p>和促进不同实例间的主奇异值的分离。</p><p><img src="/2025/20250826/6.png"></p><p>同样也使用了一个 MLP 层返回二元预测$p_{dam}$，并计算交叉熵。</p><h3 id="详细代码"><a href="#详细代码" class="headerlink" title="详细代码"></a>详细代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">Integrating PMRL <span class="hljs-keyword">with</span> four steps<br><span class="hljs-comment"># 1. Singular Value Decomposition on Multimodal Representations &gt;&gt;&gt;</span><br>U, S, _ = torch.linalg.svd(<br>torch.stack ([ feat_t ,feat_v ,feat_a ,feat_s], dim =-<span class="hljs-number">1</span>)<br>)<br><span class="hljs-comment"># 2. Principled learning via maximum singular values &gt;&gt;&gt;</span><br>loss1 = F. cross_entropy (S/<span class="hljs-variable language_">self</span>.tau1 , torch.zeros(S.shape [<span class="hljs-number">0</span>]).to(S.device).long ())<br><span class="hljs-comment"># Implemented by cross -entropy , and the singular value at the first position is the</span><br>maximum one<br><span class="hljs-comment"># 3. Principled regularization via eigenvector corresponding to the maximum singular</span><br>values &gt;&gt;&gt;<br>U1 = U[:, :, <span class="hljs-number">0</span>]<br>loss2 = F. cross_entropy ((U1 @ U1.T)/<span class="hljs-variable language_">self</span>.tau2 , torch.arange(U1.shape [<span class="hljs-number">0</span>]).to(U1.device<br>).long ())<br>......<br><span class="hljs-comment"># 4. Combine the loss &gt;&gt;&gt;</span><br>loss = loss1 + <span class="hljs-variable language_">self</span>. lambda1 * loss2 + <span class="hljs-variable language_">self</span>. lambda2 * loss_IM<br></code></pre></td></tr></table></figure><p><img src="/2025/20250826/7.png"></p><h2 id="DINO-V3中的Gram矩阵"><a href="#DINO-V3中的Gram矩阵" class="headerlink" title="DINO V3中的Gram矩阵"></a>DINO V3中的Gram矩阵</h2><p>DINO V3是Meta AI在自监督学习（Self-Supervised Learning, SSL）领域推出的最新一代模型，它是DINO和DINOv2的延续和重大升级。DINO V3的目标是利用大规模的无标签图像数据，训练出通用且强大的视觉骨干网络，从而在各种下游视觉任务中取得卓越性能。</p><p>DINOV3中提出了一种Gram Anchoring的方法。</p><p>在实验过程中，作者们发现学习强判别性特征与保持局部一致性之间存在相对独立性，这一点从全局性能与稠密性能之间缺乏相关系数得以体现。尽管将全局 DINO 损失与局部 iBOT 损失相结合已开始解决这一问题，但这种平衡并不稳定，随着训练的推进，全局表示逐渐占据主导。</p><p>故作者使用Gram进一步明确利用了这种独立性。</p><p>这个新的损失函数作用于 Gram 矩阵：即图像中所有补丁特征对成对点的积的矩阵。</p><p>作者还希望将学生的 Gram 矩阵推向一个早期模型，称为 Gram 教师。通过选择教师网络的早期迭代来选择 Gram 教师，该迭代表现出优越的稠密特性。通过作用于 Gram 矩阵而非特征本身，局部特征可以自由移动，只要相似性结构保持不变。</p><p>假设有一张由 P 个补丁组成的图像，以及一个在维度 d 上运行的网络。用 $X_S$ (分别 $X_G$) 表示学生 (分别 Gram 教师) 的 $L_2$ 规范化局部特征的 $P \times d$ 矩阵。定义损失 $L_\text{Gram}$ 如下：<br>$$<br>L_\text{Gram} &#x3D; |X_S \cdot X_S^T - X_G \cdot X_G^T|_F^2<br>$$</p><p>仅在全局裁剪上计算此损失。尽管它可以在训练早期应用，但出于效率考虑，仅在 1 M 次迭代后才开始。有趣的是，作者观察到了 $L_\text{Gram}$ 的延迟应用仍能“修复”严重退化的局部特征。为了进一步提升性能，每 10k 次迭代更新一次 Gram 教师，此时 Gram 教师与主 EMA 教师完全相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GramLoss</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;Implementation of the gram loss&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        apply_norm=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        img_level=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        remove_neg=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">        remove_only_teacher_neg=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-comment"># Loss</span><br>        <span class="hljs-variable language_">self</span>.mse_loss = torch.nn.MSELoss()<br><br>        <span class="hljs-comment"># Parameters</span><br>        <span class="hljs-variable language_">self</span>.apply_norm = apply_norm<br>        <span class="hljs-variable language_">self</span>.remove_neg = remove_neg<br>        <span class="hljs-variable language_">self</span>.remove_only_teacher_neg = remove_only_teacher_neg<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.remove_neg <span class="hljs-keyword">or</span> <span class="hljs-variable language_">self</span>.remove_only_teacher_neg:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.remove_neg != <span class="hljs-variable language_">self</span>.remove_only_teacher_neg<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, output_feats, target_feats, img_level=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Compute the MSE loss between the gram matrix of the input and target features.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            output_feats: Pytorch tensor (B, N, dim) or (B*N, dim) if img_level == False</span><br><span class="hljs-string">            target_feats: Pytorch tensor (B, N, dim) or (B*N, dim) if img_level == False</span><br><span class="hljs-string">            img_level: bool, if true gram computed at the image level only else over the entire batch</span><br><span class="hljs-string">        Returns:</span><br><span class="hljs-string">            loss: scalar</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># Dimensions of the tensor should be (B, N, dim)</span><br>        <span class="hljs-keyword">if</span> img_level:<br>            <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(target_feats.shape) == <span class="hljs-number">3</span> <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(output_feats.shape) == <span class="hljs-number">3</span><br><br>        <span class="hljs-comment"># Float casting</span><br>        output_feats = output_feats.<span class="hljs-built_in">float</span>()<br>        target_feats = target_feats.<span class="hljs-built_in">float</span>()<br><br>        <span class="hljs-comment"># SSL correlation</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.apply_norm:<br>            target_feats = F.normalize(target_feats, dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> img_level <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(target_feats.shape) == <span class="hljs-number">3</span>:<br>            <span class="hljs-comment"># Flatten (B, N, D) into  (B*N, D)</span><br>            target_feats = target_feats.flatten(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Compute similarities</span><br>        target_sim = torch.matmul(target_feats, target_feats.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br><br>        <span class="hljs-comment"># Patch correlation</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.apply_norm:<br>            output_feats = F.normalize(output_feats, dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> img_level <span class="hljs-keyword">and</span> <span class="hljs-built_in">len</span>(output_feats.shape) == <span class="hljs-number">3</span>:<br>            <span class="hljs-comment"># Flatten (B, N, D) into  (B*N, D)</span><br>            output_feats = output_feats.flatten(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Compute similarities</span><br>        student_sim = torch.matmul(output_feats, output_feats.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>))<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.remove_neg:<br>            target_sim[target_sim &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0.0</span><br>            student_sim[student_sim &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.remove_only_teacher_neg:<br>            <span class="hljs-comment"># Remove only the negative sim values of the teacher</span><br>            target_sim[target_sim &lt; <span class="hljs-number">0</span>] = <span class="hljs-number">0.0</span><br>            student_sim[(student_sim &lt; <span class="hljs-number">0</span>) &amp; (target_sim &lt; <span class="hljs-number">0</span>)] = <span class="hljs-number">0.0</span><br><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.mse_loss(student_sim, target_sim)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ICL是低秩更新</title>
    <link href="/2025/20250817/"/>
    <url>/2025/20250817/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ICL (In-Context Learning)是一种在不更新模型参数（即不进行梯度下降或微调）的情况下，通过在输入（即“上下文”）中提供少量示例来指导模型完成特定任务的能力的方法。</p><span id="more"></span><p>比如</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs">这是电影《速度与激情》的影评，请判断是正面评价还是负面评价。<br><br>影评：这部电影太棒了！我看得热血沸腾！<br>情感：正面<br><br>影评：这是一部毫无新意，令人昏昏欲睡的作品。<br>情感：负面<br><br>影评：剧情太拖沓了，而且演员的表演也很僵硬。<br>情感：负面<br><br>影评：特效非常震撼，故事也出乎意料，我太喜欢了。<br>情感：<br></code></pre></td></tr></table></figure><p>Google Research团队的<a href="https://arxiv.org/pdf/2507.16003">Learning without training: The implicit dynamics of in-context learning</a>指出了ICL实际上是一种低秩更新。</p><h2 id="模型建立与证明"><a href="#模型建立与证明" class="headerlink" title="模型建立与证明"></a>模型建立与证明</h2><p>称一个上下文层为一种网络层 A(·)，它能够单独接受一个向量 x 作为输入，产生输出A(x)；或者，可选地，A 还可以额外接受一个上下文 C（例如，一个 token 序列、一张图片等）与向量 x 一起，生成输出 A([C, x])。注意，在后续描述中，我们通常会省略上下文层输入中的连接符号 [C, x]，而简单地用 A(C, x) 来表示 A([C, x])</p><p><strong>定义.</strong> 一个上下文块是由上述上下文层 $A$ 与神经网络 $M_W$ 组成的复合结构 $$T_W &#x3D; M_W \circ A;$$ 即$$M_W(z) &#x3D; f_\theta(Wz + b),$$ 其中 $W$ 和 $b$ 是第一个全连接稠密层的权重，$f_\theta(z)$ 是神经网络的其余部分。</p><p><strong>定理.</strong> 考虑一个上下文块 $T_W &#x3D; M_W \circ A$，如上所述，由一个上下文层 $A$ 与一个全连接层 $M_W$ 组成，其中全连接层具有权重矩阵 $W$。</p><p>给定一个上下文 $C$ 和一个输入 $x$，上下文 $C$ 的某部分 $Y \subseteq C$ 对上下文块输出的影响隐式地对应于 $M_W$ 的第一层的秩 1 权重更新 $W + \Delta W(Y)$。即，</p><p>$$<br>\begin{align}<br>T_W(C, x) &#x3D; T _ {W + \Delta W(Y)}(C \setminus Y, x)<br>\quad<br>\\<br>\text{where} \quad<br>\Delta W(Y) &#x3D; \frac{(W \Delta A(Y)) A(C \setminus Y, x)^T}{|A(C \setminus Y, x)|^2}, \tag{1}<br>\end{align}<br>$$</p><p>其中$\Delta A(Y) &#x3D; A(C, x) - A(C \setminus Y, x)$ 是与 $Y$ 关联的上下文向量。</p><p>注意 $\Delta W(Y)$ 是秩 1，因为 $W \Delta A(Y)$ 是一个列向量，而 $A(C \setminus Y, x)^T$ 是一个行向量。</p><p><strong>证明.</strong> 结果通过直接计算得出，其中我们使用符号 $M_W(z) &#x3D; f_\theta(Wz + b)$，其中 $W$ 和 $b$ 是 $M$ 的第一个稠密层的权重，而 $f_\theta$ 是网络的其余部分。<br>在上述符号中，根据定义有：</p><p>$$<br>T _ {W+\Delta W(Y)}(C \setminus Y, x)<br>&#x3D; M _ {W+\Delta W(Y)}\big(A(C \setminus Y, x)\big)<br>$$</p><p>$$<br>&#x3D; f_\theta\big((W + \Delta W(Y)) A(C \setminus Y, x) + b\big)<br>$$</p><p>$$<br>&#x3D; f_\theta\big(W A(C \setminus Y, x) + \Delta W(Y) A(C \setminus Y, x) + b\big).<br>$$</p><p>将 $\Delta W(Y)$ 替换为式 (1) 中给出的定义，并利用 $\frac{z^T}{|z|} z &#x3D; 1$，我们得到：</p><p>$$<br>T _ {W+\Delta W(Y)}(C \setminus Y, x)<br>&#x3D; f_\theta\left(W A(C \setminus Y, x) + \frac{(W \Delta A(Y)) A(C \setminus Y, x)^T}{|A(C \setminus Y, x)|^2} A(C \setminus Y, x) + b\right)<br>$$</p><p>$$<br>&#x3D; f_\theta\left(W A(C \setminus Y, x) + W \Delta A(Y) + b\right).<br>$$</p><p>现在根据上下文向量的定义，有  $A(C \setminus Y, x) + \Delta A(Y) &#x3D; A(C, x)$，最终得到</p><p>$$<br>\begin{align}<br>T _ {W+\Delta W(Y)}(C \setminus Y, x)<br>&amp;&#x3D; f_\theta\big(W A(C, x) + b\big)<br>\\&amp;&#x3D; M_W\big(A(C, x)\big)<br>\\&amp;&#x3D; T_W(C, x)<br>\end{align}<br>$$</p><h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>仍在分析一个简化模型，这构成了分析的局限性：</p><ul><li>推导仅对单个 Transformer 块有效，因为论文的主要定理仅量化了上下文对最后一个输入 token 输出的影响，而非整个 Transformer 块的输出。</li><li>论文的主要定理仅分析了上下文对首个生成 token 的影响，并未涵盖之后生成过程的完整机制。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RobustKV：通过 KV 驱逐机制保护大型语言模型免受越狱攻击</title>
    <link href="/2025/20250813/"/>
    <url>/2025/20250813/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2025）</p><span id="more"></span><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>基于一个关键洞察：尽管越狱提示可以采取任意、自适应的形式，但有害查询在操纵上的灵活性显著较低。这一约束源于有害查询必须明确编码攻击者试图从 LLM 中诱发的特定恶意信息。</p><p>于越狱提示要绕过 LLM 的安全防护，其 token 必须达到足够的“重要<br>性”（通过注意力得分衡量），这不可避免地会降低有害查询中 token 的重要性。利用这一独特模式，提出了RobustKV，一种新颖的越狱防御机制，它策略性地移除最低秩 token的 KVs，从而最小化有害查询在 LLM 的 KV 缓存中的存在。值得注意的是，RobustKV 并不完全阻止 LLM 对攻击者提示的响应（例如拒绝），而是阻碍其生成针对有害查询的信息性回复的能力。如图（使用 RobustKV）所示，RobustKV 有效地将有害查询的关键 token从 KV 缓存中驱逐，导致对有害查询的响应缺乏信息性。</p><p><img src="/2025/20250813/1.jpg"></p><p>本工作首次探索了将 KV最优化用作越狱防御。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="洞察"><a href="#洞察" class="headerlink" title="洞察"></a>洞察</h3><p>下图展示了在随机选择的案例<br>中，由 AutoDAN 在 Llama2 上结合 SnapKV 生成的恶意查询和越狱提示中 token 的整体重要性排名。</p><p><img src="/2025/20250813/2.jpg"></p><p>为了让越狱提示绕过 LLM 的安全防护，其 token 必须获得足够的重要性，这不可避免地<br>降低了隐藏的恶意查询中token 的重要性。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/2025/20250813/3.jpg"></p><p>为了证明这样选取并不会删除掉有用的token，作者还证明了：</p><p><img src="/2025/20250813/4.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>InLine attention：弥合Softmax与线性注意力机制的差距</title>
    <link href="/2025/20250812/"/>
    <url>/2025/20250812/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（NeurIPS 2024）</p><span id="more"></span><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>作者指出：</p><p>（1）单射性是线性注意力与 Softmax 注意力之间的关键差异。虽然 Softmax 注意力具有单射性，但线性注意力的非单射性会导致语义混淆，并严重损害模型性能。</p><p>首次将注意力概念化为映射函数，并证明了其单射性的重要性。</p><p>（2）局部建模对于注意力机制的有效性仍然至关重要，尽管它以其大感受野和出色<br>的长程建模能力而闻名。</p><h2 id="线性注意力"><a href="#线性注意力" class="headerlink" title="线性注意力"></a>线性注意力</h2><p>博客之前介绍过线性注意力从$sim(Q,K)&#x3D;softmax(QK^T&#x2F;\sqrt{d})$变为$sim(Q,K)&#x3D;\phi(Q)\phi(K)^T$，从而能先计算KV而不是QK来减少计算复杂度。</p><h2 id="注意力函数的单射性"><a href="#注意力函数的单射性" class="headerlink" title="注意力函数的单射性"></a>注意力函数的单射性</h2><p>作者证明了，Softmax 注意力是单射的，但线性注意力不是单射的。</p><p>如下图所示，线性注意力无法区分具有不同强度的相同语义。</p><p><img src="/2025/20250812/1.jpg"></p><p>作者通过往softmax的输入前加入非单射函数，结果发现准确率下降了，认定注意力函数单射属性的关键作用。</p><h3 id="赋予线性注意力单射性"><a href="#赋予线性注意力单射性" class="headerlink" title="赋予线性注意力单射性"></a>赋予线性注意力单射性</h3><p>最简单的解决方法就是使线性注意力拥有单射性，作者简单地将线性注意力的规范化从除法变换为减法，提出了单射线性注意力（InLine）：<br>$$<br>InL_K(Q_i)&#x3D;[\phi(Q_i)^T\phi(K_1),…\phi(Q_i)^T\phi(K_N)]^T-\frac{1}{N}\sum_{s&#x3D;1}^N \phi(Q_i)^T\phi(K_s)+\frac{1}{N}<br>$$</p><h2 id="局部建模能力"><a href="#局部建模能力" class="headerlink" title="局部建模能力"></a>局部建模能力</h2><p>Softmax 注意力为局部窗口分配了<br>大量的注意力，表明其相较于其他两种注意力范<br>式具有更强的局部建模能力。</p><p><img src="/2025/20250812/2.jpg"></p><p>两个关键观察结果显现：</p><ol><li>屏蔽局部 token 会显著降低模型性能，而随机屏蔽相同数量的 token 对结果影响较小。</li><li>当局部 token 被屏蔽时，Softmax 注意力机制的性能比 InLine 注意力机制受到更严重的影响。这些发现证明了局部建模对于两种注意力机制的重要性，并证明了 Softmax 注意力机制相对于 InLine 注意力机制的优势主要归因于其更强的局部建模能力。</li></ol><h3 id="赋予线性注意力单射性-1"><a href="#赋予线性注意力单射性-1" class="headerlink" title="赋予线性注意力单射性"></a>赋予线性注意力单射性</h3><p>作者采用MLP 来预测 InLine 注意力的额外局部注意力残差。</p><p>具体而言，<br>$$<br>O_i&#x3D;InL_K(Q_i)^TV+\sum_{j&#x3D;1}^9r_jV_j^{N(i)},r&#x3D;MLP(\bar x)<br>$$<br>V代表3*3领域内的值$Q_i$。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250812/3.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SINDER：Repairing the Singular Defects of DINOv2</title>
    <link href="/2025/20250810/"/>
    <url>/2025/20250810/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p> (ECCV 2024 Oral)</p><span id="more"></span><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>不同图像中的缺陷 patch token 几乎相同。</p><p>为了验证这一点，作者计算了每张图像的平均缺陷 token，然后计算它们之间的平均成对角度，结果为 5.5 度。这证实了最后一层中的缺陷方向本质上与输入图像无关。这一观察结果与前人中的观点不同，前人声称高范数token 包含图像级别的全局信息。（有点类似attention sinks）</p><p>奇异缺陷方向的定义<strong>完全源自预训练网络的权重</strong>；在推理过程中，它并不依赖于输入图像。</p><p>根据作者的统计量，这些高范数 token 主要不包含任何输入信息，无论是局部的还是全局的。</p><p>作者从<a href="https://en.wikipedia.org/w/index.php?title=Power%20iteration&oldid=1188380344"><strong>power iteration</strong></a>中受到启发，这是一种求主特征向量的方法，迭代过程为$b_{k+1}&#x3D;\frac{Ab_k}{||Ab_k||}$。</p><p>促使作者从奇异值分解的角度来探讨缺陷问题。</p><p>注意作者不区分奇异的缺陷方向与其相反方向。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>Singular Defect Repairing（SIN-DER）</p><p>SINDER 旨在通过修改少量参数，使用平滑的正则化来修复前向传播中遇到的第一个缺陷层。</p><p>首先定义logit$l_t$为规范化patch token与$v_i$之间内积的绝对值。<br>$$<br>l_t&#x3D;\left| \frac{x_t}{||x_t||}\cdot v_i\right| \tag{5}<br>$$<br>然后将缺陷 Token 集合 D 定义为那些偏离平均 Logit 超过 掩码阈值μ &#x3D; 4 倍标准差的 Token。</p><p>对于一个有缺陷的token，使用其3x3空间邻近tokens的加权平均来定义其学习目标，另$x_{t1}$为$x_t$的邻近token。</p><p>计算系数：<br>$$<br>c_{tt_1}&#x3D;\frac{exp(-l_{t1}&#x2F;\tau)}{\sum_{s\in N_t}exp(-l_s&#x2F;\tau)}<br>$$<br>然后将该系数与3x3的高斯核相乘并归一化。</p><p>并线性组合为token$x_t$的学习目标：<br>$$<br>\tilde x_t&#x3D;\sum_{t1\in N_t} \tilde c_{tt1}x_{t1}<br>$$<br>损失函数：<br>$$<br>L&#x3D;\frac{1}{|D|}\sum_{t\in D}||x-\tilde x_t|| \tag{8}<br>$$<br><strong>进一步限制学习参数数量</strong>:</p><p>考虑到使用比原始训练集少得多的图像对模型进行微调，控制可训练参数的数量以避免损害模型在下游任务中的泛化能力变得至关重要。</p><p>具体而言，作者在微调过程中使用SVD，并冻结参数U和V。</p><p><img src="/2025/20250810/1.jpg"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>如果我们只是简单的对奇异值clip，则结果如下。随着 γ 的减小，缺陷补丁的范数也随之减小，缺陷token 的数量也减少。然而，当 gamma 过小时，特征图的语义似乎被破坏。</p><p>所以使用学成的最优奇异值更好，而不是根据某些手动设计的阈值进行裁剪。</p><p><img src="/2025/20250810/3.jpg"></p><p><strong>学成奇异值与原始奇异值之间的差异</strong>：</p><p>线基本都在下面。</p><p><img src="/2025/20250810/2.jpg"></p><h2 id="后续"><a href="#后续" class="headerlink" title="后续"></a>后续</h2><p>作者后续又用<strong>左奇异向量</strong>预测“爆炸”后的方向；用<strong>右奇异向量</strong>解释“触发”条件（爆炸子空间）；用<strong>特征值分解</strong>解释“衰减”层的行为。2502.07004</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Ecotransformer-无需乘法的注意力机制</title>
    <link href="/2025/20250807/"/>
    <url>/2025/20250807/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（arxiv 2025）2507.20096</p><p>众所周知，注意力为$softmax(\frac{QK^T}{\sqrt{D_k}})V$。</p><span id="more"></span><p>而QK内积可以写为$&lt;Q,K&gt;&#x3D;\frac{1}{2}(||Q||^2_2+||K||^2_2-||Q-K||^2_2)$。</p><p>若Q、K被L2规范化，则括号内前两项为常数，化简后最终为$softmax(\frac{-1}{2&#x2F;\sqrt{D}}||Q-K||^2_2)$。</p><p>于是作者把其扩充为softmax(QK距离)，作者使用了L1距离乘上$\lambda$系数。</p><p><img src="/2025/20250807/1.jpg"></p><h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><p>思路其实比较简单。</p><p>在此之前也有<a href="https://arxiv.org/pdf/2105.02723v1">《Do You Even Need Attention? A Stack of Feed-Forward Layers Does Surprisingly Well on ImageNet》</a>干脆全用FFN。</p><p>和用逐元素乘法代替点积的AFT，不过被ICLR 2021 拒了。</p><p><img src="/2025/20250807/2.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>可训练动态掩码稀疏注意力</title>
    <link href="/2025/20250805/"/>
    <url>/2025/20250805/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（arxiv 2025）</p><p>《<a href="http://arxiv.org/pdf/2508.02124">Trainable Dynamic Mask Sparse Attention</a>》</p><p>Smalldoge出品，该组织专注于小型语言模型，专注于<strong>效率</strong>和<strong>易用性</strong>。</p><span id="more"></span><h2 id="现有方法"><a href="#现有方法" class="headerlink" title="现有方法"></a>现有方法</h2><p>现有方法要么是利用softmax注意力的稀疏性。</p><p>如《From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification》提出了Sparsemax，即$Sparsemax(x)&#x3D;relu(x-\lambda(x))$。</p><p>其中$\lambda(x)$是使得p的各分量之和为1的常数。</p><p>其实softmax能写为$softmax(x)&#x3D;exp(x-\lambda(x))$，而sparsemax正是利用了$exp(x)\approx relu(1+x)$。</p><p>或者要么是利用长内容的稀疏性来选择性计算，比如滑动注意力和之前介绍过的NSA。</p><p>另一方面，也有从KV入手，比如之前attention sinks介绍过多那一些。</p><p>为了解决之前方法的限制，必须利用自注意力稀疏性进行必要计算和利用长上下文稀疏性进行选择性计算。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><strong>内容感知动态稀疏掩码</strong>：<br>$$<br>\delta&#x3D;exp(\tau(v(\Delta)*A))<br>$$<br>其中$\Delta$是可学习的采样权重矩阵，A是门控参数，$\tau$是非负函数，这里是有点是softplus，类似VIB中的处理。</p><p>再应用 Top-k 选择和标准的因果掩码$m_t^c$：<br>$$<br>m_t&#x3D;f(top_w(\delta+m_t^c))<br>$$</p><p><strong>位置感知的稀疏注意力权重</strong>：</p><p>然后就是类似标准的因果掩码：<br>$$<br>o_t&#x3D;softmax(\frac{q_tk^T}{\sqrt{d_h}}+m_t)v<br>$$<br>由于被掩码位置的注意力权重注定为 0，因此在计算 QKT 点积时，可以完全<strong>跳过</strong>这些位置的计算 。这使得计算复杂度从标准注意力的$O(n^2d_h)$ 降低到 $O(nwd_h)$，其中 n 是序列长度，w 是保留的窗口大小，$d_h$是头维度 。</p><p>这部分是通过硬件和类似flash attention的分块操作处理的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">dynamic_mask_attention</span>(<span class="hljs-params">h_t, position_embeddings, causal_m, past_key_value,</span><br><span class="hljs-params"> W_Q, W_K, W_V, W_dt, A, W_O,</span><br><span class="hljs-params"> num_heads, scaling, keep_window_size</span>):<br> input_shape = h_t.shape[:-<span class="hljs-number">1</span>]<br> <span class="hljs-comment"># [b, q_len]</span><br> hidden_shape = (*input_shape,-<span class="hljs-number">1</span>, h_t.shape[-<span class="hljs-number">1</span>] // num_heads)<br> <span class="hljs-comment"># linear projections</span><br> q_t = W_Q(h_t).view(hidden_shape).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br> k_t = W_K(h_t).view(hidden_shape).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br> v_t = W_V(h_t).view(hidden_shape).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br> o_t = torch.zeros_like(q_t)<br> <span class="hljs-comment"># [b, n_h, q_len, d_h]</span><br> <span class="hljs-comment"># [b, n_h, q_len, d_h]</span><br> <span class="hljs-comment"># [b, n_h, q_len, d_h]</span><br> <span class="hljs-comment"># [b, n_h, q_len, d_h]</span><br> <span class="hljs-comment"># apply rotary position embeddings</span><br> q_t, k_t = apply_rotary_pos_emb(q_t, k_t, *position_embeddings)<br> <span class="hljs-comment"># concatenate past key and value states</span><br> k, v = past_key_value.update(k_t, v_t)<br> <span class="hljs-comment"># [b, n_h, k_len, d_h]</span><br> <span class="hljs-comment"># calculate dynamic mask</span><br> dt = W_dt(v.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(v.shape[<span class="hljs-number">0</span>], v.shape[-<span class="hljs-number">2</span>],-<span class="hljs-number">1</span>)) <span class="hljs-comment"># [b, k_len, n_h]</span><br> dt = torch.exp(A * F.softplus(dt)).transpose(-<span class="hljs-number">1</span>,-<span class="hljs-number">2</span>)<br> <span class="hljs-comment"># [b, n_h, k_len]</span><br> m_t = dt[:, :, <span class="hljs-literal">None</span>, :].expand(-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>, h_t.shape[<span class="hljs-number">1</span>],-<span class="hljs-number">1</span>)<br> <span class="hljs-comment"># [b, n_h, q_len, k_len]</span><br> active_m = torch.zeros_like(m_t)<br> m_t = m_t.masked_fill(causal_m != <span class="hljs-number">0</span>,-<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>))<br> topk_indices = torch.topk(m_t, keep_window_size, dim=-<span class="hljs-number">1</span>, <span class="hljs-built_in">sorted</span>=<span class="hljs-literal">False</span>).indices<br> active_m = active_m.scatter(-<span class="hljs-number">1</span>, topk_indices, <span class="hljs-number">1.0</span>)<br> m_t = m_t.masked_fill(active_m == <span class="hljs-number">0.0</span>,-<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>))<br> <span class="hljs-comment"># calculate sparse attention weight</span><br> <span class="hljs-keyword">for</span> b_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(hidden_shape[<span class="hljs-number">0</span>]):<br> <span class="hljs-keyword">for</span> h_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_heads):<br> <span class="hljs-keyword">for</span> q_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(hidden_shape[<span class="hljs-number">1</span>]):<br> q_elem = q_t[b_idx, h_idx, q_idx, :]<br> indices = topk_indices[b_idx, h_idx, q_idx]<br> k_vecs = k[b_idx, h_idx, indices, :]<br> v_vecs = v[b_idx, h_idx, indices, :]<br> a_elem = torch.<span class="hljs-built_in">sum</span>(q_elem.unsqueeze(<span class="hljs-number">0</span>) * k_vecs, dim=-<span class="hljs-number">1</span>)<br> a_elem = a_elem * scaling + m_t[b_idx, h_idx, q_idx, indices]<br> a_elem = F.softmax(a_elem, dim=-<span class="hljs-number">1</span>)<br> o_elem = torch.<span class="hljs-built_in">sum</span>(a_elem.unsqueeze(<span class="hljs-number">1</span>) * v_vecs, dim=<span class="hljs-number">0</span>)<br> o_t[b_idx, h_idx, q_idx, :] = o_elem<br> o_t = o_t.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()<br> h_t = W_O(o_t.view(*input_shape,-<span class="hljs-number">1</span>))<br> <span class="hljs-keyword">return</span> h_t<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态攻击与防御速览</title>
    <link href="/2025/20250727/"/>
    <url>/2025/20250727/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>视觉-语言预训练模型(VLP安全)</p><p>共41篇论文。</p><span id="more"></span><p><strong>目录</strong></p><p>[TOC]</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="多模态一系列模型"><a href="#多模态一系列模型" class="headerlink" title="多模态一系列模型"></a>多模态一系列模型</h3><p>李沐老师的《多模态论文串讲》或博客上的《多模态速览》</p><h3 id="BERT-attack"><a href="#BERT-attack" class="headerlink" title="BERT-attack"></a>BERT-attack</h3><p>(EMNLP 2020)</p><p>利用以 BERT为代表的预训练掩码语言模型生成对抗样本。</p><p><strong>1.计算重要性分数</strong></p><p>把这个词从句子中去掉（用一个无意义的词如 <code>[MASK]</code> 替换），然后看模型的预测结果与正确标签 <code>Y</code> 的置信度下降了多少。下降得越多，说明这个词越重要。</p><p><strong>2.筛选关键词</strong></p><p>在计算完所有词的重要性分数后，算法会根据分数从高到低进行排序。</p><p><strong>3.为重要词寻找最佳替换</strong></p><p>如果 <code>w_j</code> 是一个完整的词（没有被切分成子词），就直接筛选候选词。</p><p>如果 <code>w_j</code> 被切分成了多个子词，算法会使用 <strong>PPL (Perplexity，困惑度)</strong> 来进行排序和筛选。</p><blockquote><p>分词后的序列$X&#x3D;(x_0,x_1,…,x_t)$的ppl计算公式为：<br>$$<br>PPL(x)&#x3D;exp(-\frac{1}{t}\sum _ {i}^tlogp_\theta(x_i\mid x _ {&lt;i}))<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">c_loss = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>ppl = c_loss(word_predictions.view(N*L, -<span class="hljs-number">1</span>), all_substitutes.view(-<span class="hljs-number">1</span>)) <span class="hljs-comment"># [ N*L ] </span><br>ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-<span class="hljs-number">1</span>)) <span class="hljs-comment"># N </span><br></code></pre></td></tr></table></figure></blockquote><p><strong>4.攻击</strong></p><p>遍历候选替换词集合 <code>C</code> 中的每一个词 <code>c_k</code>。</p><p>用候选词 <code>c_k</code> 替换掉原句中的重要词 <code>w_j</code>，生成一个新句子 <code>S&#39;</code>。</p><p>将新句子 <code>S&#39;</code> 输入模型进行预测。如果模型的预测结果 <strong>不再是</strong> 正确标签 <code>Y</code>，则攻击成功。</p><p><img src="/2025/20250727/bertattack.jpg"></p><h3 id="Projected-Gradient-Descent-PGD"><a href="#Projected-Gradient-Descent-PGD" class="headerlink" title="Projected Gradient Descent(PGD)"></a>Projected Gradient Descent(PGD)</h3><p>(ICLR 2018)</p><p>梯度攻击常用方法。</p><p>在此之前有快速梯度符号方法（FGSM）等方法，具体而言，取梯度的符号方向，并乘以一个小的超参数。<br>$$<br>x+\epsilon sgn(\nabla_xL(\theta,x,y))<br>$$<br>PGD可以被看作是FGSM的迭代版本。</p><p><strong>初始化:</strong> 在原始样本x的$\epsilon$-邻域内随机选择一个初始点$x _ {adv(0)}$。这个随机初始化有助于避免陷入局部最优。<br>$$<br>x _ {adv(0)}&#x3D;x+\text{random_perturbation}<br>$$<br><strong>迭代更新:</strong> 在进行T次迭代的每一步t中： </p><ol><li><p>计算损失函数关于当前对抗样本$x _ {adv}^{(t)}$的梯度： $$ g^{(t)} &#x3D; \nabla _ {x _ {adv}^{(t)}} J(\theta, x _ {adv}^{(t)}, y _ {true}) $$ </p></li><li><p>沿着梯度符号方向更新对抗样本，步长为$\alpha$： $$ x _ {adv}^{(t+1)} &#x3D; x _ {adv}^{(t)} + \alpha \cdot \text{sign}(g^{(t)}) $$ </p></li><li><p>将更新后的样本投影回原始样本x的$\epsilon$-邻域内。</p></li></ol><p>L∞范数的话，投影操作为： $$ x _ {adv}^{(t+1)} &#x3D; x+\text{clip}(x _ {adv}^{(t+1)}-x,  - \epsilon,  + \epsilon) $$ </p><p>L2范数的话，投影操作为： $$ x _ {adv}^{(t+1)} &#x3D; x+(x _ {adv}^{(t+1)}-x)\cdot\text{min}(1,\frac{\epsilon}{||x _ {adv}^{(t+1)}-x||_2})$$ </p><p>其实就是把它clip到扰动范围内。</p><p>直接来看<a href="https://github.com/Jeffkang-94/pytorch-adversarial-attack/blob/master/attack/pgd.py">代码</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PGD</span>(<span class="hljs-title class_ inherited__">Attacker</span>):<br>...<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        :param x: Inputs to perturb</span><br><span class="hljs-string">        :param y: Ground-truth label</span><br><span class="hljs-string">        :param target : Target label </span><br><span class="hljs-string">        :return adversarial image</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        x_adv = x.detach().clone()<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.config[<span class="hljs-string">&#x27;random_init&#x27;</span>] :<br>            x_adv = <span class="hljs-variable language_">self</span>._random_init(x_adv)<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.config[<span class="hljs-string">&#x27;attack_steps&#x27;</span>]):<br>            x_adv.requires_grad = <span class="hljs-literal">True</span><br>            <span class="hljs-variable language_">self</span>.model.zero_grad()<br>            logits = <span class="hljs-variable language_">self</span>.model(x_adv) <span class="hljs-comment">#f(T((x))</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.target <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<span class="hljs-comment">#是否需要错误分类到 self.target</span><br>                <span class="hljs-comment"># Untargeted attacks - gradient ascent</span><br>                loss = F.cross_entropy(logits, y,  reduction=<span class="hljs-string">&quot;sum&quot;</span>)<br>                loss.backward()                      <br>                grad = x_adv.grad.detach()<br>                grad = grad.sign()<br>                x_adv = x_adv + <span class="hljs-variable language_">self</span>.config[<span class="hljs-string">&#x27;attack_lr&#x27;</span>] * grad<br>            <span class="hljs-keyword">else</span>:<br>                ...<br>                x_adv = x_adv - <span class="hljs-variable language_">self</span>.config[<span class="hljs-string">&#x27;attack_lr&#x27;</span>] * grad<br>            <span class="hljs-comment"># Projection</span><br>            x_adv = x + torch.clamp(x_adv - x, <span class="hljs-built_in">min</span>=-<span class="hljs-variable language_">self</span>.config[<span class="hljs-string">&#x27;eps&#x27;</span>], <span class="hljs-built_in">max</span>=<span class="hljs-variable language_">self</span>.config[<span class="hljs-string">&#x27;eps&#x27;</span>])<br>            x_adv = x_adv.detach()<br>            x_adv = torch.clamp(x_adv, *<span class="hljs-variable language_">self</span>.clamp)<br><br>        <span class="hljs-keyword">return</span> x_adv<br></code></pre></td></tr></table></figure><h2 id="对抗攻击"><a href="#对抗攻击" class="headerlink" title="对抗攻击"></a>对抗攻击</h2><h3 id="白盒攻击"><a href="#白盒攻击" class="headerlink" title="白盒攻击"></a>白盒攻击</h3><h4 id="不可见"><a href="#不可见" class="headerlink" title="不可见"></a>不可见</h4><h5 id="ACMMM-2022-Towards-adversarial-attack-on-vision-language-pre-training-models"><a href="#ACMMM-2022-Towards-adversarial-attack-on-vision-language-pre-training-models" class="headerlink" title="(ACMMM 2022)Towards adversarial attack on vision language pre-training models"></a>(ACMMM 2022)Towards adversarial attack on vision language pre-training models</h5><p>Co-attack</p><p><img src="/2025/20250727/tvlp.jpg"></p><p><strong>攻击多模态嵌入（Attacking Multimodal Embedding）</strong><br>Co-Attack 的目标是促使被扰动的多模态嵌入偏离原始多模态嵌入。其损失函数定义为:</p><p>$$ \text{max } \mathcal{L}(E_m(E_i(x_i’), E_t(x_t’)), E_m(E_i(x_i), E_t(x_t))) + \alpha_1 \mathcal{L}(E_m(E_i(x_i’), E_t(x_t’)), E_m(E_i(x_i), E_t(x_t))) $$ </p><p>这里：</p><ul><li>$E_m(\cdot, \cdot)$ 代表多模态编码器。</li><li>$E_i(\cdot)$ 代表图像编码器。</li><li>$E_t(\cdot)$ 代表文本编码器。</li><li>$x_i$ 是原始输入图像。</li><li>$x_t$ 是原始输入文本。</li><li>$x_i’$ 是扰动后的图像。</li><li>$x_t’$ 是扰动后的文本。</li><li>$\mathcal{L}$ 是一个损失函数，通常用于衡量嵌入之间的差异（例如，KL散度损失）。</li><li>$\alpha_1$ 是一个超参数，控制第二项的贡献，第二项对应 $\delta _ {i\&amp;t}$，表示图像和文本扰动产生的合扰动。</li></ul><p>该优化问题通常通过类似 PGD 的程序解决。</p><p><strong>攻击单模态嵌入（Attacking Unimodal Embedding）</strong><br>Co-Attack 旨在促使被扰动的图像模态嵌入偏离被扰动的文本模态嵌入。其损失函数定义为:</p><p>$$ \text{max } \mathcal{L}(E_i(x_i’), E_i(x_i)) + \alpha_2 \cdot \mathcal{L}(E_i(x_i’), E_t(x_t’)) $$ </p><p>在这两种情况下，攻击流程都是先扰动离散的文本输入，然后根据文本扰动的结果扰动连续的图像输入 。对于图像扰动，通常使用基于梯度的 PGD 攻击。对于文本扰动，则使用 BERT-Attack 方法。</p><p>CoAttack能拉大距离和夹角。</p><p><img src="/2025/20250727/tvlp1.jpg"></p><p>论文中其实没有从理论上解释为什么能做到。只通过做实验表明，可以发现平均角度都提高了。</p><p><img src="/2025/20250727/tvlp2.jpg"></p><h5 id="ACM-MM-2023-Advclip-Downstream-agnostic-adversarial-examples-in-multimodal-contrastive-learning"><a href="#ACM-MM-2023-Advclip-Downstream-agnostic-adversarial-examples-in-multimodal-contrastive-learning" class="headerlink" title="(ACM MM 2023)Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning"></a>(ACM MM 2023)Advclip: Downstream-agnostic adversarial examples in multimodal contrastive learning</h5><p><img src="/2025/20250727/advclip1.jpg"></p><p>AdvCLIP使用了GAN来生成一个通用对抗补丁将其加到数据集的图像上，从而得到一个对抗样本。<br>$$<br>\tilde{x_i^v}&#x3D;x\odot(1-m)+G(z)\odot m<br>$$<br>我们直接从损失函数来看是如何构建模型和训练模型的。</p><p>除了GAN的G损失和D损失，还有：</p><p><strong>重建损失</strong>：$||\tilde{x_i^v}-x||_2$</p><p><strong>对比损失</strong>：即，使用InfoNCE来衡量编码器输出的向量之间的相似度。</p><p>具体而言，拉大将良性图像和对抗图像的特征距离。由于InfoNCE不是对称的，所以要正反共计算两次损失函数。</p><p><strong>拓扑偏差损失</strong>：</p><p>也就是对应图中的Topology-deviation。旨在破坏对抗样本与其对应正常样本之间的拓扑相似性，即在表示空间中基于样本间相似度构建的邻域关系图。</p><p>论文里是这么说的。构件图后，再通过交叉熵计算正常图和对抗图的损失函数。</p><p><img src="/2025/20250727/advclip0.jpg"></p><p>似乎和代码有些不同，我们直接来看代码。</p><p>首先要进行进行归一化。</p><p>接着计算点积，$S&#x3D;OO^T$。再将其转为距离，$D&#x3D;1-S$。并给每个点到自身的距离（即对角线元素）临时设置为一个很大的数（这里是3）。</p><p>定义$\rho_i$为最近的正距离$\rho&#x3D;\text{min}D$。再把它转为新的相似度矩阵，$S&#x3D;1-(D-\rho)$。</p><p>S的范围是-1到1，$\frac{S+1}{2}$把S转为[0,1]上。最好再把它进行行归一化，这就是定义的概率（因为概率要满足sum为1。）</p><p>然后就是计算交叉熵了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">umap</span>(<span class="hljs-params">output_net, target_net, eps=<span class="hljs-number">0.0000001</span></span>):<br>    <span class="hljs-comment"># Normalize each vector by its norm</span><br>    (n, d) = output_net.shape<br>    output_net_norm = torch.sqrt(torch.<span class="hljs-built_in">sum</span>(output_net ** <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>))<br>    output_net = output_net / (output_net_norm + eps)<br>    output_net[output_net != output_net] = <span class="hljs-number">0</span><br>    target_net_norm = torch.sqrt(torch.<span class="hljs-built_in">sum</span>(target_net ** <span class="hljs-number">2</span>, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>))<br>    target_net = target_net / (target_net_norm + eps)<br>    target_net[target_net != target_net] = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># Calculate the cosine similarity</span><br>    model_similarity = torch.mm(output_net, output_net.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>    model_distance = <span class="hljs-number">1</span>-model_similarity <span class="hljs-comment">#[0,2]</span><br>    model_distance[<span class="hljs-built_in">range</span>(n), <span class="hljs-built_in">range</span>(n)] = <span class="hljs-number">3</span><br>    model_distance = model_distance - torch.<span class="hljs-built_in">min</span>(model_distance, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>    model_distance[<span class="hljs-built_in">range</span>(n), <span class="hljs-built_in">range</span>(n)] = <span class="hljs-number">0</span><br>    model_similarity = <span class="hljs-number">1</span>-model_distance<br>    target_similarity = torch.mm(target_net, target_net.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>))<br>    target_distance = <span class="hljs-number">1</span>-target_similarity<br>    target_distance[<span class="hljs-built_in">range</span>(n), <span class="hljs-built_in">range</span>(n)] = <span class="hljs-number">3</span><br>    target_distance = target_distance - torch.<span class="hljs-built_in">min</span>(target_distance,dim=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>].view(-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)<br>    target_distance[<span class="hljs-built_in">range</span>(n), <span class="hljs-built_in">range</span>(n)] = <span class="hljs-number">0</span><br>    target_similarity = <span class="hljs-number">1</span> - target_distance<br>    <span class="hljs-comment"># Scale cosine similarity to 0..1</span><br>    model_similarity = (model_similarity + <span class="hljs-number">1.0</span>) / <span class="hljs-number">2.0</span><br>    target_similarity = (target_similarity + <span class="hljs-number">1.0</span>) / <span class="hljs-number">2.0</span><br>    <span class="hljs-comment"># Transform them into probabilities</span><br>    model_similarity = model_similarity / torch.<span class="hljs-built_in">sum</span>(model_similarity, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>    target_similarity = target_similarity / torch.<span class="hljs-built_in">sum</span>(target_similarity, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># Calculate the KL-divergence</span><br>    loss = CE(target_similarity,model_similarity)<br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure><h4 id="可见"><a href="#可见" class="headerlink" title="可见"></a>可见</h4><h5 id="arxiv-2021-Reading-isn’t-believing-Adversarial-attacks-on-multi-modal-neurons"><a href="#arxiv-2021-Reading-isn’t-believing-Adversarial-attacks-on-multi-modal-neurons" class="headerlink" title="(arxiv 2021)Reading isn’t believing: Adversarial attacks on multi-modal neurons"></a>(arxiv 2021)Reading isn’t believing: Adversarial attacks on multi-modal neurons</h5><p>作者发现在CLIP模型中，文本标签可以覆盖纹理和图像形状。当CLIP模型读取标签、看到纹理 并分类形状时，这种对抗性攻击风格会获得额外的选项。</p><p>比如我们明目张胆给图像加入一个文本标签，结果CLIP直接把标签当做正确的。</p><p><img src="/2025/20250727/rib.jpg"></p><p>哪怕打错字，也有效。</p><p><img src="/2025/20250727/rib1.jpg"></p><p>字体越大越有效。</p><p><img src="/2025/20250727/rib2.jpg"></p><h3 id="黑盒攻击"><a href="#黑盒攻击" class="headerlink" title="黑盒攻击"></a>黑盒攻击</h3><h4 id="样本级扰动"><a href="#样本级扰动" class="headerlink" title="样本级扰动"></a>样本级扰动</h4><h5 id="ICCV-2023-Set-level-guidance-attack-Boosting-adversarial-transferability-of-vision-language-pre-training-models"><a href="#ICCV-2023-Set-level-guidance-attack-Boosting-adversarial-transferability-of-vision-language-pre-training-models" class="headerlink" title="(ICCV 2023)Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models"></a>(ICCV 2023)Set-level guidance attack: Boosting adversarial transferability of vision-language pre-training models</h5><p>首次对视觉语言预训练（VLP）模型中对抗样本的可迁移性进行了研究。</p><h6 id="可迁移性"><a href="#可迁移性" class="headerlink" title="可迁移性"></a>可迁移性</h6><p><img src="/2025/20250727/setlevel.jpg"></p><p>作者认为，对抗样本的迁移性退化主要是由于现有攻击方法的局限性：</p><ul><li>Sep-Attack 的一个主要局限性是它没有考虑不同模态之间的交互作用。作为一种针对每个模态的独立攻击方法，它无法建模在多模态学习中成功攻击至关重要的模态间对应关系。这在图像-文本检索等多模态任务中尤为明显，其中真实值不是离散标签（例如，图像分类），而是与输入模态相对应的另一种模态数据。Sep-Attack 中完全缺乏跨模态交互，严重限制了对抗样本的泛化能力，并降低了它们在不同 VLP 模型之间的迁移性。</li><li>虽然 Co-Attack 旨在利用模态之间的协作生成对抗样本，但它仍存在一个关键缺点，阻碍了其在其他 VLP 模型中的迁移性。与单模态学习不同，多模态学习涉及多个互补模态，并具有许多对许多的跨模态对齐，这给实现足够的对抗迁移性带来了独特的挑战。然而，Co-Attack 仅使用单个图像-文本对来生成对抗数据，限制了其他模态中多个标签提供的指导多样性。跨模态指导的这种缺乏多样性，使得对抗样本与白盒模型的对齐模式高度相关。因此，对抗样本的通用性受到限制，它们在迁移到其他模型时的效果也会下降。</li></ul><h6 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h6><p>一些符号计法：</p><p>$B[,\epsilon]$表示合法搜索空间，$\epsilon$分别表示图像最大扰动范围或者文本中科改变单词的最大数量。</p><p>正如名字所说，<strong>“集合”</strong>，算法不是简单地针对单个图像和单个标题进行优化，而是构建了一个<strong>对抗性标题集合</strong>和一个<strong>图像集合</strong>，并利用这些集合来“引导”对抗性样本的生成，从而使攻击效果更稳定和强大。</p><p><strong>第一步</strong>，使用余弦相似度来构建对抗性标题集合 <code>t&#39;</code>。</p><p><strong>第二步</strong>，通过加入<strong>高斯噪声</strong>来构建图像集合 <code>v</code>。</p><p><strong>第三步</strong>，<strong>集合</strong>地生成对抗性图像和对抗性文本。</p><p><img src="/2025/20250727/setlevel1.jpg"></p><h5 id="arxiv-2023-Sa-attack-Improving-adversarial-transferability-of-vision-language-pre-training-models-via-self-augmentation"><a href="#arxiv-2023-Sa-attack-Improving-adversarial-transferability-of-vision-language-pre-training-models-via-self-augmentation" class="headerlink" title="(arxiv 2023)Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation"></a>(arxiv 2023)Sa-attack: Improving adversarial transferability of vision-language pre-training models via self-augmentation</h5><p>中山大学网安院长为通讯。未录用未公开代码。</p><p><img src="/2025/20250727/saatt.jpg"></p><p>方法包含三个步骤：</p><ul><li>从良性图像和良性文本中生成对抗性中间文本。</li><li>使用增强的良性文本和对抗性中间文本，结合良性图像，生成对抗性图像。¸ </li><li>使用增强的良性图像和对抗性图像，结合对抗性中间文本，生成对抗性文本。不同颜色代表不同模块。图中各变量的描述见表</li></ul><p>其他部分和上一篇类似。</p><p>主要区别有几个，噪声改为均值为0，方差为0.05的<strong>均匀分布</strong>，并会对图像的clip到[0,1]。</p><h5 id="TMM-2023-Exploring-transferability-of-multimodal-adversarial-samples-for-vision-language-pre-training-models-with-contrastive-learning"><a href="#TMM-2023-Exploring-transferability-of-multimodal-adversarial-samples-for-vision-language-pre-training-models-with-contrastive-learning" class="headerlink" title="(TMM 2023)Exploring transferability of multimodal adversarial samples for vision language pre-training models with contrastive learning"></a>(TMM 2023)Exploring transferability of multimodal adversarial samples for vision language pre-training models with contrastive learning</h5><h6 id="两大贡献"><a href="#两大贡献" class="headerlink" title="两大贡献"></a>两大贡献</h6><p>(1) 通过将扰动优化与多模态对抗样本生成统一于一个基于梯度的框架中，能够更有效地攻击那些语义相似的多模态信息中的脆弱点。</p><p>(2) 运用对比学习，从多角度扰动良性样本的内在结构及图文对的上下文一致性，从而提升了生成的多模态对抗样本的可迁移性。</p><h6 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h6><p><img src="/2025/20250727/etm.jpg"></p><p>使用$L_\infty$来约束对抗扰动。通过最大化给定前序 token 的似然来约束<code>t′</code>的流畅性。使用BERT 得分来约束对抗文本<code>t&#39;</code>。</p><p><strong>图像-文本语义相似度损失</strong>$L _ {adv}$:<br>$$<br>L _ {adv} &#x3D; \begin{cases}<br>\min J(F_s(i’), F_s(t’)) \\<br>\text{s.t. } |i’ - i| _ {\infty} \leq \epsilon \\<br>\text{s.t. } \text{similarity}(t’, t) \leq \beta<br>\end{cases}<br>$$</p><p><strong>软约束损失</strong>$L _ {prep}$：</p><p>由于文本的离散性，所以需要将文本转为连续分布。</p><p>采用Gumbel-Softmax来建模文本数据。</p><p>具体而言，对于一个词序列$t&#x3D;[\omega_1,\omega_2,…,\omega_n]$，其中每个$\omega_j$属于固定词表V，使用由矩阵$\Theta\in R ^ {n\times V}$参数化的Gumbel-Softmax分布$P _ {\theta}$。用于采样$\pi$。<br>$$<br>\begin{align}<br>(\pi_k)_j &amp;&#x3D; \frac{\exp((\Theta _ {k,j} + g _ {k,j})&#x2F;\tau)}{\sum _ {\nu&#x3D;1} ^ {V} \exp((\Theta _ {k,\nu} + g _ {k,\nu})&#x2F;\tau)}<br>\\e(\pi) &amp;&#x3D; e(\pi_1) \cdots e(\pi_n)<br>\end{align}<br>$$<br>其中$g _ {k,j}\sim Gumbel(0,1)$。</p><p>上面第一条式子其实就是Gumbel Softmax，具体而言，$softmax((logp_i-log(-log\epsilon_i))&#x2F;\tau),\epsilon\sim U[0,1]$。</p><p>Gumbel 分布的分布函数为$F(x;\mu,\beta)&#x3D;e ^ {-e ^ {-(x-\mu)&#x2F;\beta}}$。若想从中采样，则$x&#x3D;F ^ {-1}(u)&#x3D;\mu-\beta ln(-ln(u)),u\sim U(0,1)$。</p><blockquote><p>证明：<br> $$<br>P(F ^ {-1}(u)\leq x)&#x3D;P(u\leq F(x))&#x3D;F(x)<br>$$</p></blockquote><p>为了生成对抗文本的流畅性，在给定前序 token 时最大化下一个 token 预测的似然。<br>$$<br>L _ {prep}(\pi)&#x3D;-\sum _ {k&#x3D;1} ^ {n} log p _ {dis}(\pi_k\mid \pi_1 …\pi _ {k-1})<br>$$<br>其中log是下一个词分布与先前预测词分布之间的交叉熵。</p><p>采用下式来计算 BERT 得分，用来保持语义一致性并约束语义鸿沟：<br>$$<br>L _ {sim}&#x3D;\sum _ {k&#x3D;1} ^ {n} w_k\cdot\text{max} _ {j&#x3D;1,…,m}\phi(t)_k^T\phi(t’)_j<br>$$</p><p>$\phi$是生成embedding的语言模型。</p><p><strong>跨模态对比损失</strong>$L _ {itm}$：<br>$$<br>L _ {nce}(i, t^+, t^-) &#x3D; E \left[ \log \frac{e ^ {(sim(i, t^+)&#x2F;\tau)}}{\sum _ {k&#x3D;1} ^ {K} e ^ {(sim(i, \hat{t}_k)&#x2F;\tau)}} \right]<br>$$<br>同样使用InfoNCE，并类似之前要计算两次。<br>$$<br>L _ {itm}&#x3D;\frac{1}{2}[L _ {nce}(i’,i+,i-)+L _ {nce}(i’,i+,i-)]<br>$$<br><strong>模态内对比损失</strong>$L _ {i2i}$：</p><p>同样使用InfoNCE，将同一模态中与良性样本语义不同的对抗样本推开。具体来说，将良性图像$i$在随机数据增强下的一些随机视图视为负例$i^-$ ，并从测试集中随机抽取图像作为正例。</p><h5 id="IEEE-Computer-Society-2024-Transferable-multimodal-attack-on-vision-language-pre-training-models"><a href="#IEEE-Computer-Society-2024-Transferable-multimodal-attack-on-vision-language-pre-training-models" class="headerlink" title="(IEEE Computer Society 2024)Transferable multimodal attack on vision-language pre-training models"></a>(IEEE Computer Society 2024)Transferable multimodal attack on vision-language pre-training models</h5><p>pass</p><h5 id="NeurIPS-2023-Vlattack-Multimodal-adversarial-attacks-on-vision-language-tasks-via-pre-trained-models"><a href="#NeurIPS-2023-Vlattack-Multimodal-adversarial-attacks-on-vision-language-tasks-via-pre-trained-models" class="headerlink" title="(NeurIPS 2023)Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models"></a>(NeurIPS 2023)Vlattack: Multimodal adversarial attacks on vision-language tasks via pre-trained models</h5><h6 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h6><p>(1) 第一个探索预训练和微调的 VL 模型之间的对抗脆弱性的。</p><p>(2) 提出 VLAttack 从不同层次搜索对抗样本。对于单模态层次，提出 BSA 策略以在各种下游任务上统一扰动最优化目标。对于多模态层次，设计 ICSAC通过在不同模态上交叉搜索扰动来生成对抗图像-文本对。</p><h6 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h6><p><img src="/2025/20250727/vlatt.jpg"></p><p><strong>单模态：</strong></p><p><strong>图像攻击</strong>。作者提出了块级相似度攻击 (BSA) 来破坏通用的基于上下文的表示。<br>$$<br>\mathcal{L} &#x3D; \underbrace{\sum _ {i&#x3D;1}^{M _ {\alpha}} \sum _ {j&#x3D;1}^{M_j^i} \text{Cos}(F _ {\alpha}^{i,j}(I), F _ {\alpha}^{i,j}(I’))} _ {\text{Image Encoder}} + \underbrace{\sum _ {k&#x3D;1}^{M _ {\beta}} \sum _ {t&#x3D;1}^{M_t^k} \text{Cos}(F _ {\beta}^{k,t}(I, T), F _ {\beta}^{k,t}(I’, T))} _ {\text{Transformer Encoder}}<br>$$</p><p>其中 $M _ {\alpha}$ 是图像编码器中的块数，$M_j^i$ 是在 $i\text{-th}$ 块中生成的展平图像特征嵌入的数量。类似地，$M _ {\beta}$ 是 Transformer 编码器中的块数，$M_t^k$ 是在 $k\text{-th}$ 块中生成的图像词元特征数。$F _ {\alpha}^{i,j}$ 是在图像编码器的 $i\text{-th}$ 层中获得的 $j\text{-th}$ 特征向量，$F _ {\beta}^{k,t}$ 是在 Transformer 编码器的 $k\text{-th}$ 层中获得的 $t\text{-th}$ 特征向量。图像编码器仅以单个图像 $I$ 或 $I’$ 作为输入，但 Transformer 编码器将同时使用图像和文本作为输入。采用余弦相似度来计算扰动特征与良性特征之间的距离。</p><p>使用PGD来进行迭代。</p><blockquote><p>其实也还是前面的那一套。</p></blockquote><p><strong>文本攻击</strong>。直接应用 BERT-Attack 来生成文本扰动。</p><p><strong>多模态：</strong></p><p>从第一阶段生成的有效候选文本集 T 中，根据语义相似度得分 γi进行排序，选出最相似的 K个候选文本 {T’1, …, T’K}。</p><p>对于每一个顶级的候选文本 T’k： 协同优化。算法在<strong>已有的对抗性图像 I</strong> (来自第一阶段图像攻击的结果) 和<strong>当前的候选文本 T’k</strong> 的基础上，再次调用 <code>BSA</code> 函数。</p><p><img src="/2025/20250727/vlatt1.jpg"></p><h5 id="arxiv-2024-As-firm-as-their-foundations-Can-open-sourced-foundation-models-be-used-to-create-adversarial-examples-for-downstream-tasks"><a href="#arxiv-2024-As-firm-as-their-foundations-Can-open-sourced-foundation-models-be-used-to-create-adversarial-examples-for-downstream-tasks" class="headerlink" title="(arxiv 2024)As firm as their foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?"></a>(arxiv 2024)As firm as their foundations: Can open-sourced foundation models be used to create adversarial examples for downstream tasks?</h5><p>哈佛大学出品。提出了“补丁表示错位”（Patch Representation Misalignment，PRM）的跨任务攻击策略。</p><p><img src="/2025/20250727/PRM.jpg"></p><p>用 $F_l(x), F_l(x′) $来表示从第 l 层获得的中间编码器特征。</p><p><img src="/2025/20250727/PRM1.jpg"></p><p>和前面的主要区别是对每层操作，而非对最后进行操作。</p><h4 id="通用扰动"><a href="#通用扰动" class="headerlink" title="通用扰动"></a>通用扰动</h4><h5 id="拒ICLR-2025-One-perturbation-is-enough-On-generating-universal-adversarial-pertur-bations-against-vision-language-pre-training-models"><a href="#拒ICLR-2025-One-perturbation-is-enough-On-generating-universal-adversarial-pertur-bations-against-vision-language-pre-training-models" class="headerlink" title="(拒ICLR 2025)One perturbation is enough: On generating universal adversarial pertur bations against vision-language pre-training models"></a>(拒ICLR 2025)One perturbation is enough: On generating universal adversarial pertur bations against vision-language pre-training models</h5><h6 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h6><p><img src="/2025/20250727/opie.jpg"></p><p>loss基本就是InfoNCE，范数损失那一套。还有用到了《Set-level guidance attack》中的集合思想。</p><p>创新点没看出来。除了使用Deepfool替换前人常用的PGD，然后还被审稿者怼了。</p><h6 id="拒稿理由"><a href="#拒稿理由" class="headerlink" title="拒稿理由"></a>拒稿理由</h6><p><a href="https://openreview.net/forum?id=PdA9HAxO4w">https://openreview.net/forum?id=PdA9HAxO4w</a></p><p>批评地挺狠的。</p><p>AC：这项工作的主要弱点在于所提出的通用对抗性文本生成方法<strong>既低效又无效</strong>。生成的扰动质量存疑，因为仅替换词语并不能确保难以察觉，导致修改后的文本容易被识别。此外，原始文本与对抗性文本之间的高语义相似性表明攻击效果<strong>微乎其微</strong>，削弱了其有效性。使用大型语言模型来验证难以察觉性也缺乏说服力。</p><h6 id="Deepfool"><a href="#Deepfool" class="headerlink" title="Deepfool"></a>Deepfool</h6><p>既然论文没啥价值，那就介绍一下Deepfool。</p><p>出自CVPR 2016，《DeepFool: a simple and accurate method to fool deep neural networks》</p><p>对抗的目标是：$\Delta(x;\hat{k}):&#x3D; \min _ {r} |r|_2  : \mathrm{subject} : \mathrm{to} : \hat{k}(x+r) \not &#x3D; \hat{k}(x)$</p><p>其中k是估计。</p><p><strong>对于二分类</strong></p><p>k可以写作$sign(f(x))&#x3D;sign(w^Tx+b)$。</p><p>那么点到决策边界的距离为$r_\ast(x_0)&#x3D;-\frac{f(x_0)}{||w||_2}w$。</p><p>而f的一阶近似是$f(x_0+r)\approx f(x_0)+\nabla^Tf(x_0)r$，而$w&#x3D;\nabla f(x_0),b&#x3D;f(x_0)$。</p><p>故<br>$$<br>\begin{align}<br>r_i&amp;&#x3D;-\frac{f(x_i)}{||\nabla f(x_0)||^2_2}\nabla f(x_i)<br>\\<br>x _ {i+1}&amp;&#x3D;x_i+r_i<br>\end{align}<br>$$<br><img src="/2025/20250727/deepfool.jpg"></p><p><strong>多分类也是相似的。</strong></p><p>决策边界也还是直线，类似OVR。<br>$$<br>\hat{k}(x)&#x3D;\text{argmax}_k f_k(x)<br>$$<br><img src="/2025/20250727/deepfool1.jpg"></p><p>但值得注意的是，<a href="https://github.com/LTS4/DeepFool/blob/master/Python/deepfool.py">代码</a>中所用的不是$x _ {i+1}&#x3D;x_i+r_i$，而是$x _ {i+1}&#x3D;x_i+(1+\eta)r_i$，其中$\eta$默认为0.02。</p><p>论文中并无提到这样做的理由。但我觉得只加一倍且过程中间使用了约等于，可能会导致并不越过决策边界，最好是再越高边界一点。</p><h5 id="SIGIR-2024-Universal-adversarial-perturbations-for-vision-language-pre-trained-models"><a href="#SIGIR-2024-Universal-adversarial-perturbations-for-vision-language-pre-trained-models" class="headerlink" title="(SIGIR 2024)Universal adversarial perturbations for vision-language pre-trained models"></a>(SIGIR 2024)Universal adversarial perturbations for vision-language pre-trained models</h5><p>Effective and Transferable Universal Adversarial Attack（ETU）</p><p>主要贡献：</p><ul><li>这是首次在“黑箱”情景下学习通用对抗样本（UAPs），以检验视觉-语言预训练模型的鲁棒性。同时，该研究揭示了在多模态场景中发起<br>有效通用攻击所面临的关键挑战，为未来该领域的研究奠定了基石。</li><li>设计了一种新颖高效且可迁移的通用对抗扰动（UAP）生成方法，该方法通过综合考虑多模态交互，提升了 UAP 的实用性和迁移能力。提出了一种新颖的局部 UAP 强化技术和 ScMix数据增强方法，以增强对抗攻击的有效性和可迁移性。</li></ul><p><img src="/2025/20250727/etu.jpg"></p><p><strong>目标：</strong><br>$$<br>\arg\max _ {\delta} \mathcal{L}_1 &#x3D; \sum _ {i&#x3D;1}^{n} \left( \ell(f_x(x_i + \delta), f_x(x_i)) + \ell(f_x(x_i + \delta), f_t(t_i)) \right)<br>$$</p><p><strong>进行裁剪数据增强：</strong></p><p>记$\mathcal{A}$为随机裁剪子区域并将其调整至与原始图像相同的尺寸。<br>$$<br>\arg\max _ {\delta} \mathcal{L}_2 &#x3D; \sum _ {i&#x3D;1}^{n} \left( \ell(f_x(x_i + \mathcal{A}_s(\delta)), f_x(x_i)) + \ell(f_x(x_i + \mathcal{A}_s(\delta)), f_t(t_i)) \right)<br>$$</p><p><strong>使用ScMix进行数据增强：</strong></p><p><img src="/2025/20250727/etu1.jpg"><br>$$<br>\begin{align}<br>p_i &amp;&#x3D; \eta \cdot f_x(x_i^1) + (1 - \eta) \cdot f_x(x_i^2),<br>\\<br>\tilde{x}_i &amp;&#x3D; \underbrace{\beta_1 \cdot \hat{x}_i + \beta_2 \cdot x_j,} _ {\text{Cross-mix}}<br>\\<br>\hat{x}_i &amp;&#x3D; \underbrace{\eta \cdot x_i^1 + (1 - \eta) \cdot x_i^2,} _ {\text{Self-mix}}<br>\\<br>\text{s.t. } \eta &amp;&#x3D; \max(\eta’, 1 - \eta’), \quad \eta’ \sim \text{Beta}(\alpha, \alpha)<br>\end{align}<br>$$<br>其中$\beta_1&gt;\beta_2\in[0,1)$。</p><p>记$\mathcal{A}$为增强操作。<br>$$<br>\begin{align}<br>\arg\max _ {\delta} \mathcal{L}_3 &amp;&#x3D; \sum _ {i&#x3D;1}^{n} (\ell(f_x(x_i + \mathcal{A}_s(\delta)), p_i)  \\<br>&amp;+<br>\ell(f_x(x_i + \mathcal{A}_s(\delta)), f_x(x_i)) \\<br>&amp;+ \ell(f_x(x_i + \mathcal{A}_s(\delta)), f_t(t_i)) )<br>\end{align}<br>$$</p><h2 id="对抗防御"><a href="#对抗防御" class="headerlink" title="对抗防御"></a>对抗防御</h2><h3 id="PromptTuning"><a href="#PromptTuning" class="headerlink" title="PromptTuning"></a>PromptTuning</h3><h4 id="ICCV-2023-Workshop-Defense-prefix-for-preventing-typographic-attacks-on-clip"><a href="#ICCV-2023-Workshop-Defense-prefix-for-preventing-typographic-attacks-on-clip" class="headerlink" title="(ICCV 2023 Workshop)Defense-prefix for preventing typographic attacks on clip"></a>(ICCV 2023 Workshop)Defense-prefix for preventing typographic attacks on clip</h4><p>主要应对可见的攻击。</p><p><img src="/2025/20250727/dpfp0.jpg"></p><h5 id="算法-4"><a href="#算法-4" class="headerlink" title="算法"></a>算法</h5><p>主动学习一个前缀。</p><p>即一张老鼠的照片→一张<code>[DP]</code>老鼠的照片、</p><p>作者认为上图会类似于“一张老鼠的照片”的文本特征，但不会与”一张<code>[DP]</code>老鼠的照片“的特征相似。（？）</p><blockquote><p> 有点不理解。难道比如说，有可能学到”一张老虎老鼠的照片”，这样就能破坏掉embedding的相似度？</p></blockquote><p><img src="/2025/20250727/dpfp.jpg"></p><h4 id="ECCV-2024-Adversarial-prompt-tuning-for-vision-language-models"><a href="#ECCV-2024-Adversarial-prompt-tuning-for-vision-language-models" class="headerlink" title="(ECCV 2024)Adversarial prompt tuning for vision-language models"></a>(ECCV 2024)Adversarial prompt tuning for vision-language models</h4><p>作者于复旦大学实习期间完成的。</p><p><img src="/2025/20250727/advp.jpg"></p><p>默认情况下，CLIP的默认prompt模板为 “a photo of a [CLASS]”。</p><p>在该论文中，让模型去学习prompt $t_j&#x3D;[\text{context} _ \text{front}][\text{CLASS} _  j]$</p><p>攻击样本使用PGD生成。</p><h4 id="CVPR-2024-One-prompt-word-is-enough-to-boost-adversarial-robustness-for-pre-trained-vision-language-models"><a href="#CVPR-2024-One-prompt-word-is-enough-to-boost-adversarial-robustness-for-pre-trained-vision-language-models" class="headerlink" title="(CVPR 2024)One prompt word is enough to boost adversarial robustness for pre-trained vision-language models"></a>(CVPR 2024)One prompt word is enough to boost adversarial robustness for pre-trained vision-language models</h4><p>默认情况下，CLIP的默认prompt模板为 “a photo of a [CLASS]”。</p><p>在该论文中，让模型去学习prompt$t_j&#x3D;[\text{context}_\text{front}][\text{CLASS} _ j][\text{context} _ \text{end}]$</p><p><img src="/2025/20250727/opwie.jpg"></p><h4 id="ICIC-2024-Mixprompt-Enhancing-generalizability-and-adversarial-robustness-for-vision-language-models-via-prompt-fusion"><a href="#ICIC-2024-Mixprompt-Enhancing-generalizability-and-adversarial-robustness-for-vision-language-models-via-prompt-fusion" class="headerlink" title="(ICIC 2024)Mixprompt: Enhancing generalizability and adversarial robustness for vision-language models via prompt fusion"></a>(ICIC 2024)Mixprompt: Enhancing generalizability and adversarial robustness for vision-language models via prompt fusion</h4><p>pass</p><h4 id="MICCAI-2024-Promptsmooth-Certifying-robustness-of-medical-vision-language-mod-els-via-prompt-learning"><a href="#MICCAI-2024-Promptsmooth-Certifying-robustness-of-medical-vision-language-mod-els-via-prompt-learning" class="headerlink" title="(MICCAI 2024)Promptsmooth: Certifying robustness of medical vision-language mod els via prompt learning"></a>(MICCAI 2024)Promptsmooth: Certifying robustness of medical vision-language mod els via prompt learning</h4><p>医学方向。</p><p>面对问题：Gaussian 噪声。</p><p><img src="/2025/20250727/prosmooth.jpg"></p><p><strong>Few-Shot PromptSmooth：</strong></p><p>损失函数定义为攻击样本的预测与真实标签之间的交叉熵损失。</p><p><a href="https://github.com/nhussein/promptsmooth/blob/e8265ed075725d38fec9225062466c9fcf6c32da/certify_promptsmooth_plip.py#L358">代码</a>并没有训练而是直接load已经训练好的。</p><p>说明Few Shot和Zero Shot是分开训练的，且无Few Shot的训练代码。关于此，已提Issue。</p><p><strong>Zero-Shot PromptSmooth：</strong></p><p>使用熵来定义损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">output = model(inputs) <br>loss = avg_entropy(output)<br></code></pre></td></tr></table></figure><p>而</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">avg_entropy</span>(<span class="hljs-params">outputs</span>):<br>    logits = outputs - outputs.logsumexp(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) <span class="hljs-comment"># logits = outputs.log_softmax(dim=1) [N, 1000]</span><br>    avg_logits = logits.logsumexp(dim=<span class="hljs-number">0</span>) - np.log(logits.shape[<span class="hljs-number">0</span>]) <span class="hljs-comment"># avg_logits = logits.mean(0) [1, 1000]</span><br>    min_real = torch.finfo(avg_logits.dtype).<span class="hljs-built_in">min</span><br>    avg_logits = torch.clamp(avg_logits, <span class="hljs-built_in">min</span>=min_real)<br>    <span class="hljs-keyword">return</span> -(avg_logits * torch.exp(avg_logits)).<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><p>注意，代码中有两种写法。但都是一样的。<br>$$<br>logsoftmax&#x3D;log(\frac{e^{z_i}}{\sum e^{z_i}})&#x3D;z_i-log(\sum e^{z_i})<br>$$</p><p>$$<br>log(mean(p))&#x3D;logsumexp(log(p))-log(N)<br>$$</p><h4 id="NeurIPS-2024-Few-shot-adversarial-prompt-learning-on-vision-language-models"><a href="#NeurIPS-2024-Few-shot-adversarial-prompt-learning-on-vision-language-models" class="headerlink" title="(NeurIPS 2024)Few-shot adversarial prompt learning on vision-language models"></a>(NeurIPS 2024)Few-shot adversarial prompt learning on vision-language models</h4><p>Few-shot Adversarial Prompt learning (FAP)</p><p><img src="/2025/20250727/fap.jpg"></p><p><img src="/2025/20250727/fap1.jpg"></p><p>损失函数上进行改进。</p><h5 id="对抗文本-图像对比损失"><a href="#对抗文本-图像对比损失" class="headerlink" title="对抗文本-图像对比损失"></a>对抗文本-图像对比损失</h5><p>$$<br>\mathcal{L} _ {\text{final}} &#x3D; \mathcal{L} _ {\text{CE}}(\cos(\mathbf{z}^{(I,P_v)}, \mathbf{z}^{(t,P_t)}), y) + \lambda   \mathcal{L} _ {\text{KL}}(\cos(\mathbf{z}^{(I,P_v)}, \mathbf{z}^{(t,P_t)}), \cos(\tilde{\mathbf{z}}^{(I,P_v)}, \mathbf{z}^{(t,P_t)}))<br>$$</p><h5 id="单模态对抗感知损失"><a href="#单模态对抗感知损失" class="headerlink" title="单模态对抗感知损失"></a>单模态对抗感知损失</h5><p>$$<br>\mathcal{L} _ {\cos}&#x3D;\cos(\mathbf{z}^{(I,P_v)},\tilde{\mathbf{z}}^{(I,P_v)})+1<br>$$</p><p><strong>总损失：</strong><br>$$<br>\mathcal{L} _ {\text{final}} &#x3D; \mathcal{L} _ {\text{CE}}(\cos(\mathbf{z}^{(I,P_v)}, \mathbf{z}^{(t,P_t)}), y) + \lambda \mathcal{L} _ {\cos} \cdot \mathcal{L} _ {\text{KL}}(\cos(\mathbf{z}^{(I,P_v)}, \mathbf{z}^{(t,P_t)}), \cos(\tilde{\mathbf{z}}^{(I,P_v)}, \mathbf{z}^{(t,P_t)}))<br>$$</p><h4 id="ECCV-2024-Adversarial-prompt-distillation-for-vision-language-models"><a href="#ECCV-2024-Adversarial-prompt-distillation-for-vision-language-models" class="headerlink" title="(ECCV 2024)Adversarial prompt distillation for vision-language models"></a>(ECCV 2024)Adversarial prompt distillation for vision-language models</h4><p><img src="/2025/20250727/APD.jpg"></p><p><img src="/2025/20250727/APD1.jpg"></p><p>使用了教师模型和学生模型。算法所有内容都在这两幅图中。</p><p>值得注意的是，作者还探究了教师模型不更新的情况（称为离线APD），结果发现离线APD也能超过基准。</p><p><img src="/2025/20250727/apd2.jpg"></p><h4 id="CVPR-2025-Tapt-Test-time-adversarial-prompt-tuning-for-robust-inference-in-vision-language-models"><a href="#CVPR-2025-Tapt-Test-time-adversarial-prompt-tuning-for-robust-inference-in-vision-language-models" class="headerlink" title="(CVPR 2025)Tapt: Test time adversarial prompt tuning for robust inference in vision-language models"></a>(CVPR 2025)Tapt: Test time adversarial prompt tuning for robust inference in vision-language models</h4><p>目前github仓库为空，无代码公开。</p><p><img src="/2025/20250727/tapt.jpg"></p><p><strong>1.数据增强</strong></p><p>给定一个测试图像，TAPT 首先通过随机增强 A 生成 M 个随机增强视图。</p><p><strong>2.基于多视图熵的样本选择</strong></p><p>通过熵$-plogp$来选择。</p><p><strong>3.对抗-干净嵌入对齐</strong></p><p>对抗测试图像 x 可能会使图像编码器生成的图像嵌入相对于干净图像的嵌入发生偏移，从而可能误导模型。</p><p>通过约束方差和均值来做到。</p><p><img src="/2025/20250727/tapt1.jpg"></p><h3 id="ContrastiveTuning"><a href="#ContrastiveTuning" class="headerlink" title="ContrastiveTuning"></a>ContrastiveTuning</h3><h4 id="ICLR-2023-Understanding-zero-shot-adversarial-robustness-for-large-scale-models"><a href="#ICLR-2023-Understanding-zero-shot-adversarial-robustness-for-large-scale-models" class="headerlink" title="(ICLR 2023)Understanding zero-shot adversarial robustness for large-scale models"></a>(ICLR 2023)Understanding zero-shot adversarial robustness for large-scale models</h4><p>Zero-Shot 对抗鲁棒性</p><p><strong>大规模预训练 CLIP 模型针对 zero-shot 对抗鲁棒性的适应方法：</strong></p><p><img src="/2025/20250727/uzsar.jpg"></p><h5 id="算法-5"><a href="#算法-5" class="headerlink" title="算法"></a>算法</h5><p>大规模视觉-语言模型的 zero-shot 泛化能力可能源自其语言监督。</p><p>如果仅用独热标签微调视觉编码器，可能会破坏这一联合特征空间，损害这种 zero-shot 泛化能力。这些观察促使作者在生成对抗样本时以及在模型适应期间的训练目标中考虑使用文本信息。</p><p>作者提出了文本引导对比对抗（TeCoA）训练损失。</p><p><img src="/2025/20250727/uzsar1.jpg"></p><p>具体而言，</p><p><img src="/2025/20250727/uzsar2.jpg"></p><p>其实就是交叉熵。</p><h4 id="CVPR-2024-Pre-trained-model-guided-f-ine-tuning-for-zero-shot-adversarial-robustness"><a href="#CVPR-2024-Pre-trained-model-guided-f-ine-tuning-for-zero-shot-adversarial-robustness" class="headerlink" title="(CVPR 2024)Pre-trained model guided f ine-tuning for zero-shot adversarial robustness"></a>(CVPR 2024)Pre-trained model guided f ine-tuning for zero-shot adversarial robustness</h4><p>PMG‑AFT</p><p><img src="/2025/20250727/PMGF.jpg"></p><p>通过TeCoA来生成对抗样本。</p><p><strong>损失函数：</strong></p><p><strong>鲁棒性信息分支：</strong></p><p>对应上图的黑线，使用的是交叉熵。</p><p><strong>泛化信息分支：</strong></p><p>对应的是上图的橙线。</p><p>将对抗样本输入到目标模型和 原始预训练模型中，得到adv和ori-adv。<br>$$<br>\begin{align}<br>P _ {adv}&amp;&#x3D;softmax(I _ {adv}\cdot T^T)<br>\\<br>P _ {ori-adv}&amp;&#x3D;softmax(I _ {ori-adv}\cdot T^T)<br>\\<br>L _ {general}&amp;&#x3D;\frac{1}{N}\sum D _ {KL}(P _ {adv_j}||P _ {ori-adv_j})<br>\end{align}<br>$$<br><strong>正则化损失：</strong><br>$$<br>\begin{align}<br>P _ {clean}&amp;&#x3D;softmax(I\cdot T^T)<br>\\<br>L _ {clean}&amp;&#x3D;\frac{1}{N}\sum D _ {KL}(P _ {adv_j}||P _ {clean_j})<br>\end{align}<br>$$</p><h4 id="CoRR-2024-Revisiting-the-adversarial-robustness-of-vision-language-models-a-multimodal-perspective"><a href="#CoRR-2024-Revisiting-the-adversarial-robustness-of-vision-language-models-a-multimodal-perspective" class="headerlink" title="(CoRR 2024)Revisiting the adversarial robustness of vision language models: a multimodal perspective"></a>(CoRR 2024)Revisiting the adversarial robustness of vision language models: a multimodal perspective</h4><p><img src="/2025/20250727/MMCOA.jpg"></p><p>多模态攻击依旧采用PGD和BERT-attack相结合的方式，并为了更针对CLIP，将PGD改为对比损失。</p><p>损失函数和TeCoA基本一致，不过是改为了两套结构，交叉计算（见上图的圆圈箭头）。</p><h4 id="ICML-2024-Oral-PMLR-2024-Robust-CLIP-Unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-language-models"><a href="#ICML-2024-Oral-PMLR-2024-Robust-CLIP-Unsupervised-adversarial-fine-tuning-of-vision-embeddings-for-robust-large-vision-language-models" class="headerlink" title="(ICML 2024 Oral&#x2F; PMLR 2024)Robust CLIP: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models"></a>(ICML 2024 Oral&#x2F; PMLR 2024)Robust CLIP: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models</h4><p><img src="/2025/20250727/teaser0.png"></p><p>把TeCoA的交叉熵改为L2。</p><p><img src="/2025/20250727/fare.jpg"></p><p>正如代码所示，</p><p><strong>TeCoA:</strong></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">python -m train<span class="hljs-selector-class">.adversarial_training_clip</span>  <span class="hljs-attr">--loss</span> ce <span class="hljs-attr">--inner_loss</span> ce <br></code></pre></td></tr></table></figure><p><strong>FARE：</strong></p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs stylus">python -m train<span class="hljs-selector-class">.adversarial_training_clip</span>  <span class="hljs-attr">--loss</span> l2 <span class="hljs-attr">--inner_loss</span> l2 <br></code></pre></td></tr></table></figure><p>值得注意的是，FARE使用了干净的模型，而TeCoA不能使用干净的模型。我觉得这样比较是不公平的。而且你都有干净的模型了，为什么不直接用？</p><p>论文另外顺便还证明了L2损失和优化余弦是等价的。</p><p><img src="/2025/20250727/FARE1.jpg"></p><h3 id="AdversarialTraining-Two-stageTraining"><a href="#AdversarialTraining-Two-stageTraining" class="headerlink" title="AdversarialTraining-Two-stageTraining"></a>AdversarialTraining-Two-stageTraining</h3><h4 id="CVPR-2024-Revisiting-adversarial-training-at-scale"><a href="#CVPR-2024-Revisiting-adversarial-training-at-scale" class="headerlink" title="(CVPR 2024)Revisiting adversarial training at scale"></a>(CVPR 2024)Revisiting adversarial training at scale</h4><p><strong>训练代码未公开。</strong></p><p>在可承受的计算成本下，利用巨型模型和网络规模数据进行对抗训练成为可能。将这一新引入的框架命名为 AdvXL。</p><p>在模型缩放方面，将模型参数从之前最大的 200M 大小增加到 1B；在数据缩放方面，在从包含约 1M 图像的中等规模ImageNet-1K 到包含超过 1B 图像的网络规模数据集上对模型进行对抗训练。为了使对抗训练的缩放计算上可行，引入了一种高效的方法，采用简单的两阶段训练计划，即首先进行轻量级预训练，然后进行密集微调。</p><h5 id="预训练阶段"><a href="#预训练阶段" class="headerlink" title="预训练阶段"></a>预训练阶段</h5><p>模型以较短的 token 长度和较弱的攻击进行训练，持续时间相对较长。</p><p>使用了三种图像token缩减策略。随机掩码、块掩码、图像缩放。</p><p><img src="/2025/20250727/advxl.jpg"></p><p>应用少量 PGD 步骤，PGD-1。</p><h5 id="密集微调"><a href="#密集微调" class="headerlink" title="密集微调"></a>密集微调</h5><p>以全分辨率和更强的攻击进行训练，时间安排相对较短。</p><p><img src="/2025/20250727/advxl1.jpg"></p><p>相比预训练阶段，使用更多的PGD步骤，如PGD-3。</p><h5 id="新的对比损失"><a href="#新的对比损失" class="headerlink" title="新的对比损失"></a>新的对比损失</h5><p>作者使用了前人的对比损失。</p><p>$$\mathcal{L}(f^I, f^T, I, T) &#x3D; -\frac{1}{2n} \sum_i \left( \log \frac{\exp(h_i^{I^\top} h_i^T &#x2F; \tau)}{\sum_j \exp(h_i^{I^\top} h_j^T &#x2F; \tau)} + \log \frac{\exp(h_i^{T^\top} h_i^I &#x2F; \tau)}{\sum_j \exp(h_i^{T^\top} h_j^I &#x2F; \tau)} \right)$$</p><p>其中 $n$ 表示批量大小； $\tau$ 是一个可学习的温度参数；$h_i^I &#x3D; f^I(I_i) &#x2F; |f^I(I_i)|$ 和 $h_i^T &#x3D; f^T(T_i) &#x2F; |f^T(T_i)|$ 表示图像-文本对 $(I_i, T_i)$ 的规范化投影特征。需要注意的是，作者选择 CLIPA 训练的文本编码器 作为初始的 $f^T$权重，并在训练过程中保持其冻结。</p><blockquote><p>不知道在哪里用这个损失。</p><p>这篇论文写得不好，不清晰。也没给伪代码或者训练过程。</p></blockquote><h5 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h5><p>仅训练为L∞鲁棒，在L2、L1也能表现良好。</p><p><img src="/2025/20250727/advxl2.jpg"></p><h4 id="NeurIPS-2020-Spotlight-Large-scale-adversarial-training-for-vision-and-language-representation-learning"><a href="#NeurIPS-2020-Spotlight-Large-scale-adversarial-training-for-vision-and-language-representation-learning" class="headerlink" title="(NeurIPS 2020 Spotlight)Large-scale adversarial training for vision-and-language representation learning"></a>(NeurIPS 2020 Spotlight)Large-scale adversarial training for vision-and-language representation learning</h4><p>Villa 包含两个训练阶段：</p><h5 id="（i）任务无关的对抗预训练；"><a href="#（i）任务无关的对抗预训练；" class="headerlink" title="（i）任务无关的对抗预训练；"></a>（i）任务无关的对抗预训练；</h5><p>由于图像和文本模态的独特特性，作者建议一次仅对一个模态添加扰动。</p><h5 id="（ii）任务特定的对抗微调。"><a href="#（ii）任务特定的对抗微调。" class="headerlink" title="（ii）任务特定的对抗微调。"></a>（ii）任务特定的对抗微调。</h5><p><strong>损失函数：</strong></p><p>$L _ {std}$为干净数据上的交叉熵损失。</p><p>$R _ {at}$为保留标签的对抗训练损失。<br>$$<br>R _ {at}(\theta)&#x3D;\max _ {||\delta _ {img}||\le \epsilon}L(f_\theta(x _ {img}+\delta _ {img},x _ {txt}),y)+\max _ {||\delta _ {text}||\le \epsilon}L(f_\theta(x _ {img},x _ {txt}+\delta _ {txt}),y)<br>$$<br>其中L是交叉熵。范数是Frobenius 范数。</p><p>所以PGD的投影不同于常用的$L^{\infty}$范数，而是球面投影。</p><p>$R _ {kl}$是一个更细粒度的对抗正则化项。</p><p>其实就是把前面的交叉熵换成KL散度：$KL(p||q)+KL(q||p)$。</p><p>另外，K 步 PGD 需要 K 次前向-后向传播，这在计算上是繁重的。</p><p>在 K 步之后，只有最后一步的扰动被用于模型训练。为了实现大规模训练的对抗训练并促进多样化的对抗样本，作者在每个小批次中只loss.backward()，不更新模型，而是等处理了 <code>gradient_accumulation_steps</code> （默认值为16）个小批次之后，再用它们累积起来的总梯度来更新一次模型。</p><p>作者称之为，累积“免费”的参数梯度。</p><p><img src="/2025/20250727/villa.jpg"></p><h3 id="AdversarialDetection-One-shotDetection"><a href="#AdversarialDetection-One-shotDetection" class="headerlink" title="AdversarialDetection One-shotDetection"></a>AdversarialDetection One-shotDetection</h3><h4 id="撤ICLR-2025-Mirrorcheck-Efficient-adversarial-defense-for-vision-language-models"><a href="#撤ICLR-2025-Mirrorcheck-Efficient-adversarial-defense-for-vision-language-models" class="headerlink" title="(撤ICLR 2025)Mirrorcheck: Efficient adversarial defense for vision-language models"></a>(撤ICLR 2025)Mirrorcheck: Efficient adversarial defense for vision-language models</h4><p><img src="/2025/20250727/teaser_mrr.png"></p><p>使用一个对抗检测器，来将图像分类为对抗或者干净类。</p><h5 id="拒稿理由-1"><a href="#拒稿理由-1" class="headerlink" title="拒稿理由"></a>拒稿理由</h5><p><a href="https://openreview.net/forum?id=p4jCBTDvdu">https://openreview.net/forum?id=p4jCBTDvdu</a></p><p>评分1366</p><p>作者不rebutal。</p><h3 id="AdversarialDetection-StatefulDetection"><a href="#AdversarialDetection-StatefulDetection" class="headerlink" title="AdversarialDetection StatefulDetection"></a>AdversarialDetection StatefulDetection</h3><h4 id="ACM-MM-2024-AdvQDet-Detecting-query-based-adversarial-attacks-with-adversarial-contrastive-prompt-tuning"><a href="#ACM-MM-2024-AdvQDet-Detecting-query-based-adversarial-attacks-with-adversarial-contrastive-prompt-tuning" class="headerlink" title="(ACM MM 2024)AdvQDet: Detecting query-based adversarial attacks with adversarial contrastive prompt tuning"></a>(ACM MM 2024)AdvQDet: Detecting query-based adversarial attacks with adversarial contrastive prompt tuning</h4><p>与Tapt同作者。</p><p><strong>疑似没给训练代码</strong>。</p><p>对抗对比提示调优（ACPT）</p><p>通过 ACPT，引入了一个检测框架 AdvQDet，能够在 5 次查询内以 &gt; 99% 的检测率检测到 7 种最先进的基于查询的攻击。</p><h5 id="应对攻击"><a href="#应对攻击" class="headerlink" title="应对攻击"></a>应对攻击</h5><p>【待补充】</p><p>查询攻击。</p><h5 id="算法-6"><a href="#算法-6" class="headerlink" title="算法"></a>算法</h5><p><img src="/2025/20250727/AdvQDet.jpg"></p><h6 id="经过-ACPT-微调的图像编码器"><a href="#经过-ACPT-微调的图像编码器" class="headerlink" title="经过 ACPT 微调的图像编码器"></a>经过 ACPT 微调的图像编码器</h6><p><img src="/2025/20250727/AdvQDet1.jpg"></p><p>使用InfoNCE。</p><h6 id="相似度计算模块"><a href="#相似度计算模块" class="headerlink" title="相似度计算模块"></a>相似度计算模块</h6><p>提取并保存每个查询图像的嵌入到嵌入库 Q中。</p><p>嵌入库带来了两个问题：1) 存储成本和 2) 计算成本。</p><p>可以使用自动混合精度（AMP）技术。可以使用一些成熟的技术来加速高维相似度搜索。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AdvQDet</span>(<span class="hljs-title class_ inherited__">StateModule</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, arguments</span>):<br>        add_prompt_len=<span class="hljs-number">20</span><br><br>        checkpoint = torch.load(<span class="hljs-string">&#x27;/path/to/checkpoint.pth.tar&#x27;</span>)<br>        <br>        <span class="hljs-variable language_">self</span>.device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>        model, <span class="hljs-variable language_">self</span>.preprocess = clip.load(<span class="hljs-string">&quot;ViT-B/32&quot;</span>, device=<span class="hljs-variable language_">self</span>.device, prompt_len=add_prompt_len)<br> <br>        convert_models_to_fp32(model)<br>        model = torch.nn.DataParallel(model)<br><br>        model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-variable language_">self</span>.encoder = model.module.encode_image<br>    <br>        add_prompter = TokenPrompter(prompt_len=add_prompt_len)<br>        add_prompter = torch.nn.DataParallel(add_prompter).to(<span class="hljs-variable language_">self</span>.device)<br><br>        add_prompter.load_state_dict(checkpoint[<span class="hljs-string">&#x27;add_prompter&#x27;</span>])<br>        add_prompter.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-variable language_">self</span>.ind_prompt = add_prompter()<br><br>        model_dtype = <span class="hljs-built_in">next</span>(model.parameters()).dtype<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Model dtype: <span class="hljs-subst">&#123;model_dtype&#125;</span>&quot;</span>)<br>        add_prompter_dtype = <span class="hljs-built_in">next</span>(add_prompter.parameters()).dtype<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Add_prompter dtype: <span class="hljs-subst">&#123;add_prompter_dtype&#125;</span>&quot;</span>)<br>        <br>        <span class="hljs-variable language_">self</span>.input_shape = arguments[<span class="hljs-string">&quot;input_shape&quot;</span>]<br><br>        <span class="hljs-variable language_">self</span>.cache = &#123;&#125;<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">getDigest</span>(<span class="hljs-params">self, img</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(img.shape) != <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;expected 3d image&quot;</span>)<br><br>        pil_img = transforms.ToPILImage()(img)<br>        image = <span class="hljs-variable language_">self</span>.preprocess(pil_img).unsqueeze(<span class="hljs-number">0</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            embed = multiGPU_CLIP(<span class="hljs-variable language_">self</span>.encoder, image, <span class="hljs-variable language_">self</span>.ind_prompt).squeeze(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">return</span> embed<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">resetCache</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.cache = &#123;&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">add</span>(<span class="hljs-params">self, img, prediction</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(img.shape) != <span class="hljs-number">3</span>:<br>            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;expected 3d image&quot;</span>)<br>        encoding = <span class="hljs-variable language_">self</span>.getDigest(img)<br>        <span class="hljs-variable language_">self</span>.cache[encoding] = prediction<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">resultsTopk</span>(<span class="hljs-params">self, img, k</span>):<br>        img = torch.clamp(img, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<br>        embed = <span class="hljs-variable language_">self</span>.getDigest(img)<br>        dists = []<br>        preds = []<br>        <span class="hljs-keyword">for</span> query_embed, pred <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.cache.items():<br>            dist = torch.cosine_similarity(embed, query_embed, dim=<span class="hljs-number">0</span>).item()<br>            dists.append(dist)<br>            preds.append(pred)<br>        top_dists = np.argsort(dists)<br>        result = [(dists[i], preds[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> top_dists][::-<span class="hljs-number">1</span>]<br><br>        <span class="hljs-keyword">return</span> result<br></code></pre></td></tr></table></figure><h6 id="防御动作"><a href="#防御动作" class="headerlink" title="防御动作"></a>防御动作</h6><p>一旦检测到查询为攻击，防御者可采取以下几种应对措施：</p><p>1）拒绝该查询，适用于假正例率较低的情况，否则可能损害用户体验；</p><p>2）限制用户的查询次数和频率，此举会吸引攻击者的注意力；</p><p>3）向用户返回有意扰动的输出，但仍存在泄露梯度（或其他）信息的风险；</p><p>4）封禁账户或封锁 IP 地址，这是一种激进措施，仅在极端情况下采用；</p><p>5）直接返回之前相似查询的缓存结果，这是一种稳妥的做法，既不会向用户暴露新信息，也不会影响用户体验。</p><h2 id="后门-投毒攻击"><a href="#后门-投毒攻击" class="headerlink" title="后门&amp;投毒攻击"></a>后门&amp;投毒攻击</h2><h3 id="Visual-Trigger"><a href="#Visual-Trigger" class="headerlink" title="Visual Trigger"></a>Visual Trigger</h3><h4 id="ICLR-2022-Oral-Poisoning-and-backdooring-contrastive-learning"><a href="#ICLR-2022-Oral-Poisoning-and-backdooring-contrastive-learning" class="headerlink" title="(ICLR 2022 Oral)Poisoning and backdooring contrastive learning"></a>(ICLR 2022 Oral)Poisoning and backdooring contrastive learning</h4><p><img src="/2025/20250727/pabcl.jpg"></p><p>探讨了两种在图像上放置后门的方法。在一致情景下，将补丁置于图像的左上角；而在随机情景下，将补丁随机放置在图像中的某个位置。</p><p>给定目标图像 x′ 和期望的目标标签 y′，首先构建一个与标签 y′ 相关的描述集 Y ′。</p><p><strong>没怎么介绍算法。</strong></p><h4 id="IEEE-Symposium-on-Security-and-Privacy-2022-Badencoder-Backdoor-attacks-to-pre-trained-encoders-in-self-supervised-learning"><a href="#IEEE-Symposium-on-Security-and-Privacy-2022-Badencoder-Backdoor-attacks-to-pre-trained-encoders-in-self-supervised-learning" class="headerlink" title="(IEEE Symposium on Security and Privacy 2022)Badencoder: Backdoor attacks to pre trained encoders in self-supervised learning"></a>(IEEE Symposium on Security and Privacy 2022)Badencoder: Backdoor attacks to pre trained encoders in self-supervised learning</h4><p><img src="/2025/20250727/badencoder.jpg"></p><p>干净的预训练图像编码器和后门编码器分别表示为 f 和 f ′。</p><p><strong>有效性损失：</strong></p><p>$$L_0 &#x3D; - \frac{\sum _ {i&#x3D;1}^t \sum _ {j&#x3D;1}^{r_i} \sum _ {x \in D_s} s(f’(x \oplus e_i), f’(x _ {ij}))}{|D_s| \cdot \sum _ {i&#x3D;1}^t r_i}$$</p><p>$$L_1 &#x3D; - \frac{\sum _ {i&#x3D;1}^t \sum _ {j&#x3D;1}^{r_i} s(f’(x _ {ij}), f(x _ {ij}))}{\sum _ {i&#x3D;1}^t r_i}$$</p><p><strong>效用损失:</strong><br>$$<br>L_2&#x3D;-\frac{1}{\mathcal{D} _ s}\sum _ {x\in \mathcal{D} _ s}s(f’(x),f(x))<br>$$<br>作者发现，使用简单且物理上可实现的触发器（例如，位于图像右下角的白色方阵）的BadEncoder 已经能够实现这两个目标。因此，为了简化，本文中不对触发器进行优化，并将此类联合优化留作未来工作。</p><h4 id="CVPR-2024-Data-poisoning-based-backdoor-attacks-to-contrastive-learning"><a href="#CVPR-2024-Data-poisoning-based-backdoor-attacks-to-contrastive-learning" class="headerlink" title="(CVPR 2024)Data poisoning based backdoor attacks to contrastive learning"></a>(CVPR 2024)Data poisoning based backdoor attacks to contrastive learning</h4><p>CorruptEncoder</p><p>通过利用随机裁剪机制制作中毒图像，因为这是 CL 成功的关键。若缺少随机裁剪，编码器的性能将大幅下降。</p><h5 id="制作有毒数据集"><a href="#制作有毒数据集" class="headerlink" title="制作有毒数据集"></a>制作有毒数据集</h5><p>理论分析表明，为了最大化这一概率从而提升攻击效果，1) 背景图像的大小应约为参考对象的两倍，2) 参考对象应位于背景图像的角落，3) 触发器应位于背景图像中除去参考对象后剩余部分的中心。</p><p><img src="/2025/20250727/dpba.jpg"></p><p><img src="/2025/20250727/dpba1.jpg"></p><p>【待补充】这部分有数学证明。</p><p><strong>流程：</strong></p><p><img src="/2025/20250727/algo1.jpg"></p><p><img src="/2025/20250727/algo2.jpg"></p><p><strong>触发器设置：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建一个纯白色的方块作为触发器</span><br>trigger = Image.new(<span class="hljs-string">&quot;RGB&quot;</span>, (trigger_size, trigger_size), ImageColor.getrgb(<span class="hljs-string">&quot;white&quot;</span>))<br></code></pre></td></tr></table></figure><h5 id="CorruptEncoder"><a href="#CorruptEncoder" class="headerlink" title="CorruptEncoder+"></a>CorruptEncoder+</h5><p>联合优化以下两个项：</p><p>$$<br>\max _ {D_p} [S_C(f_o, f_e; \theta _ {DUD_p}) + \lambda \cdot S_C(f_o, f _ {cls}; \theta _ {DUD_p})]<br>$$</p><p>其中 $S_C(\cdot, \cdot)$ 表示两个特征向量之间的余弦相似度，$\theta _ {DUD_p}$ 是 (被后门攻击的) 编码器在污染训练数据集上预训练的权重。$f_o$、$f_e$ 和 $f _ {cls}$ 分别表示参考对象 $o$、触发器 $e$ 和目标类的簇中心的特征向量。</p><h4 id="CVPR-2024-Badclip-Dual-embedding-guided-backdoor-attack-on-multimodal-contrastive-learning"><a href="#CVPR-2024-Badclip-Dual-embedding-guided-backdoor-attack-on-multimodal-contrastive-learning" class="headerlink" title="(CVPR 2024)Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning"></a>(CVPR 2024)Badclip: Dual-embedding guided backdoor attack on multimodal contrastive learning</h4><p>【待补充】</p><p>从贝叶斯规则的角度出发，提出了一个双嵌入引导的后门攻击框架。具体而言，确保视觉触发模式在嵌入空间中逼近文本目标语义，使得由后门学习引起的细微参数变化难以被检测。</p><p>此外，优化视觉触发模式，使中毒样本与目标视觉特征对齐，从而阻碍通过干净微调进行的后门遗忘。</p><p><img src="/2025/20250727/bad1.jpg"></p><h3 id="Multi-modal-Trigger"><a href="#Multi-modal-Trigger" class="headerlink" title="Multi-modal Trigger"></a>Multi-modal Trigger</h3><h4 id="CVPR-2024spotlight-Badclip-Trigger-aware-prompt-learning-for-backdoor-attacks-on-clip"><a href="#CVPR-2024spotlight-Badclip-Trigger-aware-prompt-learning-for-backdoor-attacks-on-clip" class="headerlink" title="(CVPR 2024spotlight)Badclip: Trigger aware prompt learning for backdoor attacks on clip"></a>(CVPR 2024spotlight)Badclip: Trigger aware prompt learning for backdoor attacks on clip</h4><p>首个通过提示学习研究CLIP 后门攻击的团队</p><p><img src="/2025/20250727/badclip.jpg"></p><p>【待补充】</p><p><a href="https://github.com/jiawangbai/BadCLIP">https://github.com/jiawangbai/BadCLIP</a></p><h3 id="Multi-modal-Poisoning"><a href="#Multi-modal-Poisoning" class="headerlink" title="Multi-modal Poisoning"></a>Multi-modal Poisoning</h3><h4 id="ICML-2023-拒ICLR-2023-Data-poisoning-attacks-against-multimodal-encoders"><a href="#ICML-2023-拒ICLR-2023-Data-poisoning-attacks-against-multimodal-encoders" class="headerlink" title="(ICML 2023&#x2F;拒ICLR 2023)Data poisoning attacks against multimodal encoders"></a>(ICML 2023&#x2F;拒ICLR 2023)Data poisoning attacks against multimodal encoders</h4><h5 id="拒稿理由-2"><a href="#拒稿理由-2" class="headerlink" title="拒稿理由"></a>拒稿理由</h5><p><a href="https://openreview.net/forum?id=7qSpaOSbRVO">https://openreview.net/forum?id=7qSpaOSbRVO</a></p><h2 id="后门-投毒防御"><a href="#后门-投毒防御" class="headerlink" title="后门&amp;投毒防御"></a>后门&amp;投毒防御</h2><h3 id="Backdoor-Removal-Fine-tuning"><a href="#Backdoor-Removal-Fine-tuning" class="headerlink" title="Backdoor Removal Fine-tuning"></a>Backdoor Removal Fine-tuning</h3><h4 id="ICCV-2023-Cleanclip-Mitigating-data-poisoning-attacks-in-multimodal-contrastive-learning"><a href="#ICCV-2023-Cleanclip-Mitigating-data-poisoning-attacks-in-multimodal-contrastive-learning" class="headerlink" title="(ICCV 2023)Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning"></a>(ICCV 2023)Cleanclip: Mitigating data poisoning attacks in multimodal contrastive learning</h4><p><img src="/2025/20250727/cleanclip.jpg"><br>$$<br>\mathcal{L} _ {\text{CLIP}} &#x3D; -\frac{1}{2N} \left\{ \sum _ {j&#x3D;1}^{N} \log \frac{\exp(\langle I_j^e, T_j^e \rangle &#x2F; \tau)}{\underbrace{\sum _ {k&#x3D;1}^{N} \exp(\langle I_j^e, T_k^e \rangle &#x2F; \tau)} _ {\text{Contrasting images with texts}}} + \sum _ {k&#x3D;1}^{N} \log \frac{\exp(\langle I_k^e, T_k^e \rangle &#x2F; \tau)}{\underbrace{\sum _ {j&#x3D;1}^{N} \exp(\langle I_j^e, T_k^e \rangle &#x2F; \tau)} _ {\text{Contrasting texts with images}}} \right\}<br>$$</p><p>$$<br>\mathcal{L} _ {SS} &#x3D; -\frac{1}{2N} \left( \sum _ {j&#x3D;1}^{N} \log \underbrace{\left[ \frac{\exp(\langle I_j^e, \tilde{I}_j^e \rangle &#x2F; \tau)}{\sum _ {k&#x3D;1}^{N} \exp(\langle I_j^e, \tilde{I}_k^e \rangle &#x2F; \tau)} \right]} _ {\text{Contrasting images with the augmented images}} + \sum _ {j&#x3D;1}^{N} \log \underbrace{\left[ \frac{\exp(\langle T_j^e, \tilde{T}_j^e \rangle &#x2F; \tau)}{\sum _ {k&#x3D;1}^{N} \exp(\langle T_j^e, \tilde{T}_k^e \rangle &#x2F; \tau)} \right]} _ {\text{Contrasting texts with the augmented texts}} \right)<br>$$</p><p>$$<br>\mathcal{L} _ {\text{CleanCLIP}} &#x3D; \lambda_1 \mathcal{L} _ {\text{CLIP}} + \lambda_2 \mathcal{L} _ {SS}<br>$$</p><h4 id="拒ICLR-2024-Better-safe-than-sorry-Pre-training-CLIP-against-targeted-data-poisoning-and-backdoor-attacks"><a href="#拒ICLR-2024-Better-safe-than-sorry-Pre-training-CLIP-against-targeted-data-poisoning-and-backdoor-attacks" class="headerlink" title="(拒ICLR 2024)Better safe than sorry: Pre training CLIP against targeted data poisoning and backdoor attacks"></a>(拒ICLR 2024)Better safe than sorry: Pre training CLIP against targeted data poisoning and backdoor attacks</h4><h5 id="拒稿理由-3"><a href="#拒稿理由-3" class="headerlink" title="拒稿理由"></a>拒稿理由</h5><p><a href="https://openreview.net/forum?id=Ge0GEOvifh">https://openreview.net/forum?id=Ge0GEOvifh</a></p><p>主要弱点包括：实验不足、需要在实践中大量调整以获得正确的超参数，以及以下假设：1) 单一模态（图像&#x2F;文本）上的对比学习对中毒&#x2F;后门攻击免疫。2) 使用降低的学习率在可能被中毒&#x2F;后门攻击的数据上进行一个周期的 CLIP 训练是安全的。</p><h3 id="Robust-Training-Pre-training"><a href="#Robust-Training-Pre-training" class="headerlink" title="Robust Training Pre-training"></a>Robust Training Pre-training</h3><h4 id="NeurIPS-2023-Robust-contrastive-language-image-pretraining-against-data-poisoning-and-backdoor-attacks"><a href="#NeurIPS-2023-Robust-contrastive-language-image-pretraining-against-data-poisoning-and-backdoor-attacks" class="headerlink" title="(NeurIPS 2023)Robust contrastive language-image pretraining against data poisoning and backdoor attacks"></a>(NeurIPS 2023)Robust contrastive language-image pretraining against data poisoning and backdoor attacks</h4><p><img src="/2025/20250727/roclip.jpg"></p><h5 id="标准CLIP："><a href="#标准CLIP：" class="headerlink" title="标准CLIP："></a><strong>标准CLIP：</strong></h5><p>$$<br>\mathcal{L} _ {\text{CLIP}} &#x3D; -\frac{1}{2N} \sum _ {j&#x3D;1}^{N} \log \left[ \frac{\exp(\langle \mathbf{z}_j^I, \mathbf{z}_j^T \rangle &#x2F; \tau)}{\sum _ {k&#x3D;1}^{N} \exp(\langle \mathbf{z}_j^I, \mathbf{z}_k^T \rangle &#x2F; \tau)} \right] - \frac{1}{2N} \sum _ {k&#x3D;1}^{N} \log \left[ \frac{\exp(\langle \mathbf{z}_k^I, \mathbf{z}_k^T \rangle &#x2F; \tau)}{\sum _ {j&#x3D;1}^{N} \exp(\langle \mathbf{z}_j^I, \mathbf{z}_k^T \rangle &#x2F; \tau)} \right]<br>$$</p><h5 id="RoCLIP："><a href="#RoCLIP：" class="headerlink" title="RoCLIP："></a><strong>RoCLIP</strong>：</h5><h6 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h6><p>被投毒样本的图像-描述对与数据中的其他干净样本并不相似。因此，它们的梯度与干净样本的梯度无法良好对齐。</p><p><img src="/2025/20250727/roclip2.jpg"></p><h6 id="算法-7"><a href="#算法-7" class="headerlink" title="算法"></a>算法</h6><p>用两种技术来打破被投毒图像-描述对之间的关联：（1）一个庞大且不断变化的随机选择描述池；（2）对图像和描述同时进行增强。</p><p><strong>描述池</strong>：</p><p><img src="/2025/20250727/roclip3.jpg"></p><p>选择相对较大的池大小，以便每个干净的图像都能找到与其原始标题相似的标题；(2) 每隔 K轮次使用该方法进行训练，在其他轮次中使用标准的 CLIP 损失进行训练。</p><p>选择了 2% 的总数据集大小作为池大小。</p><p><strong>数据增强：</strong></p><p>图像增强策略中，采用了随机裁剪、水平翻转、颜色抖动、灰度转换 和模糊处理。</p><p>文本增强采用了 同义词替换、随机交换和随机删除等。</p><p>RoCLIP 首先从 P 个描述符 $P &#x3D; {z_i^T} _ {i&#x3D;1}^P$ 中均匀随机采样一个池。</p><p>在训练过程中, 对于每个示例 $(x_j^I, x_j^T)$，在小批量中, 首先用增强策略对它的图像和文本进行增强, 然后将其增强后的图像表示 $\tilde{z}_j^I$ 与池中最相似的增强描述符 $\tilde{z}_j^T$ 进行匹配, 即<br>$$z _ {nn(j)}^T &#x3D; \operatorname{argmin} _ {z_p^T \in P} |\tilde{z}_j^I - z_p^T|_2$$有效地, 形成了正的图像-描述符表示对 $(\tilde{z}_j^I, z _ {nn(j)}^T)$ 并用它代替 $(z_j^I, z_j^T)$。</p><p>类似于 CLIP 损失, 从小批量中获取负对。也就是说, 对于一个小批量的 N 个图像-描述符对 ${(x_j^I, x_j^T)} _ {j&#x3D;1}^N$, 以及它们的嵌入 ${(\tilde{z}_j^I, \tilde{z}_j^T)} _ {j&#x3D;1}^N$, 损失定义为:<br>$$<br>\mathcal{L} _ {\text{RoCLIP}} &#x3D; -\frac{1}{2N}\sum _ {j&#x3D;1}^{N}\log\left[\frac{\exp(\langle \tilde{z}_j^I, z _ {nn(j)}^T \rangle&#x2F;\tau)}{\sum _ {k&#x3D;1}^N \exp(\langle \tilde{z}_j^I, z _ {nn(k)}^T \rangle&#x2F;\tau)}\right] - \frac{1}{2N}\sum _ {k&#x3D;1}^{N}\log\left[\frac{\exp(\langle \tilde{z}_k^I, z _ {nn(k)}^T \rangle&#x2F;\tau)}{\sum _ {j&#x3D;1}^N \exp(\langle \tilde{z}_j^I, z _ {nn(k)}^T \rangle&#x2F;\tau)}\right]<br>$$</p><p>对于池 $P$, 考虑一个先进先出的队列, 它初始化为随机的 caption 表示。在每个小批量训练后, 通过获取小批量中 $N$ 个样本的 caption 表示并将它们连接到队列的末尾来更新 $P$。从队列中丢弃最旧的 $N$ 个元素, 这个数量等于训练批量大小。</p><h5 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h5><p><img src="/2025/20250727/roclip1.jpg"></p><h3 id="Backdoor-Detection-Backdoor-Model-Detection"><a href="#Backdoor-Detection-Backdoor-Model-Detection" class="headerlink" title="Backdoor Detection Backdoor Model Detection"></a>Backdoor Detection Backdoor Model Detection</h3><h4 id="CVPR-2023-Detecting-backdoors-in-pre-trained-encoders"><a href="#CVPR-2023-Detecting-backdoors-in-pre-trained-encoders" class="headerlink" title="(CVPR 2023)Detecting backdoors in pre-trained encoders"></a>(CVPR 2023)Detecting backdoors in pre-trained encoders</h4><p>DECREE </p><h5 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h5><p>这是首个针对 SSL 中预训练编码器的后门检测方法。</p><p><img src="/2025/20250727/ssl.jpg"></p><p>为了解决现有检测方法的不足，DECREE 直接扫描编码器。具体而言，对于目标编码器，DECREE 首先搜索一个最小触发模式，使得任何带有该触发的输入共享相似的嵌入。然后，利用识别出的触发模式来判断给定编码器是良性的还是被植入后门的。</p><p>仅考虑针对视觉编码器的后门攻击。</p><h5 id="现有局限性"><a href="#现有局限性" class="headerlink" title="现有局限性"></a>现有局限性</h5><p>为了识别编码器是否被植入后门，防御者可以利用现有的后门扫描工具（如 Neural Cleanse (NC) 和 ABS）来检查使用该编码器的下游分类器，而无需直接扫描编码器本身。然而，这一策略存在局限性，如后文所示。</p><p>另一类后门扫描器，如 MNTD，采用元分类器来区分良性模型和后门模型。它们首先训练数千个良性和后门模型，然后在这些模型提取的特征上训练一个元分类器。在自监督学习（SSL）情景下这种设计可能因成本过高而不太实际。</p><p>例如，通过对比学习创建一个后门编码器需要 48 小时。而 MNTD则需构建 2048 个良性编码器和 2048 个被植入后门的编码器。</p><p>下表为各检测器表现。</p><p><img src="/2025/20250727/ssl1.jpg"></p><p>然而，当下游分类器的训练数据集（STL-10 和 GTSRB）不包含攻击目标时，如最后两行所示，NC 和 ABS 均未能检测到编码器中的后门。这对现有后门扫描器提出了两点启示：</p><p>(1) 它们必须掌握攻击目标及相应下游任务的知识，这对于一个编码器而言，鉴于存在大量不同的下游任务，获取这些信息并非易事。</p><p>(2) 它们需要获取下游任务的原始训练数据集以构建用于检测的分类器，而这些数据可能是私有的。</p><p>另外，对于<strong>零样本预测场景</strong>，零样本分类器直接计算图像嵌入与每个候选标题文本嵌入之间的相似度，并选择与输入图像嵌入最相似的标题。在此场景中，显然现有的后门扫描器不适用。故需要一种能够在嵌入空间中处理攻击的后门检测方法</p><h5 id="motivation-1"><a href="#motivation-1" class="headerlink" title="motivation"></a>motivation</h5><p><img src="/2025/20250727/ssl2.jpg"></p><p><strong>观察一</strong>：尽管 SSL 在预训练过程中不需要标签，但通过训练后的编码器，相同标签的样本的嵌入倾向于聚集在一起，而不同标签的样本则倾向于分散。</p><p><strong>观察二</strong>：被植入后门的编码器会为带有触发器的样本生成高度相似的嵌入，而干净的编码器则不会。</p><p><strong>观察三</strong>：与干净的编码器相比，被植入后门的编码器需要更小的扰动就能使样本落入稠密区域。</p><p><strong>直觉</strong>：稠密区域即为攻击目标所在之处。故模型设计旨在判断编码器的嵌入空间中是否存在一个中心稠密区域（被干净样本的嵌入所包围）。直观上，带有中心稠密区域的后门编码器只需施加微小扰动即可将干净样本推向该稠密区域。而干净的编码器则不具备这样的稠密区域，这意味着通过在样本上添加小触发器无法轻易实现嵌入间的高相似度。因此，该技术在编码器层面检测后门，无需依赖目标标签。</p><h5 id="算法-8"><a href="#算法-8" class="headerlink" title="算法"></a>算法</h5><p><img src="/2025/20250727/ssl4.jpg"></p><h3 id="Backdoor-Detection-Trigger-Inversion"><a href="#Backdoor-Detection-Trigger-Inversion" class="headerlink" title="Backdoor Detection Trigger Inversion"></a>Backdoor Detection Trigger Inversion</h3><h4 id="ICCV-2023-Tijo-Trigger-inversion-with-joint-optimization-for-defending-multimodal-backdoored-models"><a href="#ICCV-2023-Tijo-Trigger-inversion-with-joint-optimization-for-defending-multimodal-backdoored-models" class="headerlink" title="(ICCV 2023)Tijo: Trigger inversion with joint optimization for defending multimodal backdoored models"></a>(ICCV 2023)Tijo: Trigger inversion with joint optimization for defending multimodal backdoored models</h4><p><img src="/2025/20250727/tijo.jpg"></p><h4 id="USENIX-Security-2024-Mudjacking-Patching-backdoor-vulnerabilities-in-foundation-models"><a href="#USENIX-Security-2024-Mudjacking-Patching-backdoor-vulnerabilities-in-foundation-models" class="headerlink" title="(USENIX Security 2024)Mudjacking: Patching backdoor vulnerabilities in foundation models"></a>(USENIX Security 2024)Mudjacking: Patching backdoor vulnerabilities in foundation models</h4><p>误分类输入$x_b$和参考输入$x_r$。<br>$$<br>\begin{align}<br>\mathcal{L}_e&amp;&#x3D;-sim(h’(x_b),h’(x_r))<br>\\<br>\mathcal{L}_l&amp;&#x3D;-\frac{1}{|D _ {val}|+1}\sum _ {x\in D _ {val}\cup x_r }sim(h(x),h’(x))<br>\\<br>\mathcal{L}_g&amp;&#x3D;-\frac{1}{|D _ {val}|+1}\sum _ {x\in D _ {val}\cup x_r }sim(h’(x\oplus t_b),h’(x))<br>\end{align}<br>$$<br>另，给定这样的目标函数，利用一种解释方法来计算 xb 中每个像素&#x2F;词的属性得分。较高的属性得分可能表明相应的像素&#x2F;词对目标函数有更大的影响。<br>$$<br>l(h,x_b,x_r)&#x3D;1-sin(h(x_b),h(x_r)) \tag{5}<br>$$<br><img src="/2025/20250727/mud.jpg"></p><h3 id="Backdoor-Detection-Backdoor-Sample-Detection"><a href="#Backdoor-Detection-Backdoor-Sample-Detection" class="headerlink" title="Backdoor Detection Backdoor Sample Detection"></a>Backdoor Detection Backdoor Sample Detection</h3><h4 id="AAAI-2024-Seer-Backdoor-detection-for-vision-language-models-through-searching-target-text-and-image-trigger-jointly"><a href="#AAAI-2024-Seer-Backdoor-detection-for-vision-language-models-through-searching-target-text-and-image-trigger-jointly" class="headerlink" title="(AAAI 2024)Seer: Backdoor detection for vision-language models through searching target text and image trigger jointly"></a>(AAAI 2024)Seer: Backdoor detection for vision-language models through searching target text and image trigger jointly</h4><p>pass</p><h4 id="ICLR-2025-Detecting-backdoor-samples-in-contrastive-language-image-pretraining"><a href="#ICLR-2025-Detecting-backdoor-samples-in-contrastive-language-image-pretraining" class="headerlink" title="(ICLR 2025)Detecting backdoor samples in contrastive language image pretraining"></a>(ICLR 2025)Detecting backdoor samples in contrastive language image pretraining</h4><p><strong>简化局部异常值因子</strong>（SLOF）:<br>$$<br>SLOF_k(q)&#x3D;\frac{1}{k}\sum _ {o\in NN_k(q)}\frac{k-dist(q)}{k-dist(o)}<br>$$<br>其中$k-dist(x)$是样本到其第k个最近邻的距离。</p><p><strong>局部内在维度</strong> (LID)：</p><p><img src="/2025/20250727/LID.jpg"></p><p><strong>维度感知异常值检测 (DAO)</strong>：<br>$$<br>DAO_k(q)&#x3D;\frac{1}{k}\sum _ {o\in NN_k(q)}\left(\frac{k-dist(q)}{k-dist(o)}\right)^{\hat {LID^\ast _ {F_o}}}<br>$$</p><p><img src="/2025/20250727/dbs.jpg"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>Safety at Scale: A Comprehensive Survey of Large Model Safety</li><li><a href="https://kexue.fm/">https://kexue.fm</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Attention Sink</title>
    <link href="/2025/20250726/"/>
    <url>/2025/20250726/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Attention Sink是指某些（初始）token具有较大的注意力得分。最早明确提出于StreamingLLM (2309.17453)。</p><span id="more"></span><h1 id="Attention-Sink有用武之地"><a href="#Attention-Sink有用武之地" class="headerlink" title="Attention Sink有用武之地"></a>Attention Sink有用武之地</h1><h2 id="StreamingLLM：Attention-Sink用来实现长度外推"><a href="#StreamingLLM：Attention-Sink用来实现长度外推" class="headerlink" title="StreamingLLM：Attention Sink用来实现长度外推"></a>StreamingLLM：Attention Sink用来实现长度外推</h2><p>（ICLR 2024）</p><p>长度外推性是指我们在短序列上训练的模型，能否不用微调地用到长序列上并依然保持不错的效果。</p><p>像ROPE这种带有旋转周期特性的，自带外推属性，但一旦超过一定角度，外推就会失效。（<a href="https://kexue.fm/archives/9431/comment-page-1#%E6%80%9D%E7%BB%B4%E8%AF%AF%E5%8C%BA">苏神：像RoPE算是外推能力较好的位置编码，也只能外推10%到20%左右的长度而保持效果不变差。</a>）</p><p>所以就需要对此进行特殊设计。</p><p>StreamingLLM认为，主要挑战有外推性和解码阶段KV缓存过大。</p><p>一个直观的方法就是窗口注意力，既能减少KV缓存，又能适当地解决了外推问题，避免了推理阶段出现了没见过的相对距离。</p><p><img src="/2025/20250726/sllm1.jpg"></p><p>但这些方法要么时间复杂度较高，要么表现不好。</p><p>为了理解为什么窗口注意力失败，StreamingLLM作者发现自回归大语言模型中存在一个有趣的现象：无论这些初始词元与语言模型任务的相关性如何，都会分配出大量注意力得分。</p><p><img src="/2025/20250726/sllm2.jpg"></p><p>作者认为认为这是因为 Softmax 操作要求所有上下文词元的注意力得分之和为一。因此，即使当前查询在许多先前词元中没有强烈的匹配，模型仍然需要将这些不需要的注意力值分配到某处，以使总和为一。</p><p>也有从softmax入手解决以上问题的，比如<a href="https://www.evanmiller.org/attention-is-off-by-one.html">SoftMax-Off-by-One</a>和<a href="https://arxiv.org/abs/2504.20966">Softpick</a>。</p><p>初始词元作为黑洞词元的原因是直观的：由于自回归语言模型的特性，初始词元对几乎所有后续词元都是可见的，这使得它们更容易被训练成注意力黑洞。</p><p>故作者在窗口注意力的基础上，保留保留注意力汇聚词元的 KV——4个初始词元。</p><p><img src="/2025/20250726/sllm3.jpg"></p><p>总之，StreamingLLM 能够在<strong>不扩展 LLMs 上下文长度</strong>的情况下，从 KV 缓存中的词元高效生成连贯文本。</p><p>此外，类似的论文还有LM-Infinite（2308.16137）和<a href="https://zhuanlan.zhihu.com/p/619703849">Perpetual Sampling </a>，它们都指出初始tokens的重要性，但没有更深入地指出attention sink这一现象。</p><p><img src="/2025/20250726/sllm4.jpg"></p><h2 id="FastGen-四种特殊tokens策略"><a href="#FastGen-四种特殊tokens策略" class="headerlink" title="FastGen:四种特殊tokens策略"></a>FastGen:四种特殊tokens策略</h2><p>同年ICLR2024的FastGen（2310.01801）进一步考虑了更多的KV缓存种类。</p><p><strong>特殊词元</strong>。例如句子开始词元 &lt; s&gt; ，指令词元 [INST] 等。此策略称为 $C_{special}$。</p><p><strong>标点符号</strong>。仅在 KV 缓存中保留标点符号，称之为$C_{punct}$。</p><p><strong>局部性</strong>。类似窗口注意力。称之为$C_{local}$</p><p><strong>频繁项</strong>。监控每个词元的注意力得分的累积和，然后将这些得分视为词元 频率，并在 KV 缓存中仅保留最频繁的词元。称之为$C_{frequent}$。</p><p><img src="/2025/20250726/fastgen.jpg"></p><h2 id="H2O：更灵活的位置"><a href="#H2O：更灵活的位置" class="headerlink" title="H2O：更灵活的位置"></a>H2O：更灵活的位置</h2><p>2306.14048</p><p>（NeurIPS 2023）</p><p>H2O（Heavy Hitter Oracle）可以说是StreamingLLM的前身，StreamingLLM作者也从中得到启发。它没从长度外推入手，而是只从KV cache入手。</p><p><img src="/2025/20250726/H2.jpg"></p><p>相当于仍保持StreamingLLM的特殊Tokens数不变，但不限制其位置为初始，而使用贪心算法来实现。</p><p><img src="/2025/20250726/H3.jpg"></p><p><strong>伪代码：</strong></p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">def generation_loop (...):<br><span class="hljs-comment"># Prologue</span><br>...<br><span class="hljs-comment"># Generate</span><br>for i in range( gen_len ):<br>for <span class="hljs-keyword">j </span>in range( num_layers ):<br>for k in range( num_gpu_batches ):<br>load_weight (i, <span class="hljs-keyword">j+1, </span>k)<br>load_cache (i, <span class="hljs-keyword">j, </span>k+<span class="hljs-number">1</span>)<br>store_hidden (i, <span class="hljs-keyword">j, </span>k -<span class="hljs-number">1</span>)<br>load_hidden (i, <span class="hljs-keyword">j, </span>k+<span class="hljs-number">1</span>)<br>compute_layer (i, <span class="hljs-keyword">j, </span>k)<br>store_cache (i, <span class="hljs-keyword">j, </span>k -<span class="hljs-number">1</span>)<br><span class="hljs-keyword">sync </span>()<br><span class="hljs-comment"># Epilogue</span><br>...<br><span class="hljs-comment"># h is the hidden states ( activations )</span><br>def attention_forward (h, ...):<br><span class="hljs-comment"># the read/write buffer are intermediate stops for prefetching</span><br>if <span class="hljs-keyword">prefill </span>:<br>h, new_k_cache , new_v_cache = compute_attention (h, ...)<br><span class="hljs-comment"># select K heavy hitters and K recent tokens</span><br>new_k_cache , new_v_cache = select(new_k_cache , new_v_cache , K)<br><span class="hljs-keyword">cache_write_buffer </span>.store(new_k_cache , new_v_cache )<br><span class="hljs-symbol">else:</span><br>k_cache , v_cache = <span class="hljs-keyword">cache_read_buf </span>.pop ()<br><span class="hljs-comment"># evict_ids track the entries that will be evicted</span><br>h, new_k_cache , new_v_cache , evict_ids =<br>compute_attention (h, k_cache , v_cache , ...)<br><span class="hljs-keyword">cache_write_buffer </span>.store(new_k_cache , new_v_cache , evict_ids )<br>return h<br>def store_cache (...):<br>if <span class="hljs-keyword">prefill </span>:<br><span class="hljs-comment"># store cache directly</span><br>...<br><span class="hljs-symbol">else:</span><br>k_new , v_new , evict_ids = <span class="hljs-keyword">cache_write_buffer </span>.pop ()<br><span class="hljs-comment"># circular queue for the last K entries</span><br><span class="hljs-comment"># extract the index for the oldest token at i-th iteration</span><br>oldest = ((i - <span class="hljs-number">1</span>)<br><span class="hljs-comment"># update the KV cache (k_home and v_home )</span><br><span class="hljs-keyword">cache_replace </span>(k_home , evict_ids , k_new , K, oldest)<br><span class="hljs-keyword">cache_replace </span>(v_home , evict_ids , v_new , K, oldest)<br></code></pre></td></tr></table></figure><h2 id="PyramidKV：初始token以外的attention-sink"><a href="#PyramidKV：初始token以外的attention-sink" class="headerlink" title="PyramidKV：初始token以外的attention sink"></a>PyramidKV：初始token以外的attention sink</h2><p>（拒 ICLR 2025）</p><p><img src="/2025/20250726/pkv0.jpg"></p><p>作者发现注意力不仅在除了初始tokens以外的位置也会呈现出attention sink，而且还会呈现出阶梯状。</p><p>故可以在不同层动态调整键值缓存大小，在较低层分配更多缓存，在较高层分配较少缓存。</p><p><img src="/2025/20250726/pkv.jpg"></p><p>该论文的发现和《<a href="https://arxiv.org/abs/2502.00919">Attention Sinks and Outlier Features: A ‘Catch, Tag, and Release’ Mechanism for Embeddings</a>》的观点类似。</p><p>验证了attention sink是由低秩结构产生的。</p><p><img src="/2025/20250726/catch.jpg"></p><p>并从数学的角度考虑了一个双层attention、能求平均值的模型的attention map，得出其形状为：</p><p><img src="/2025/20250726/catch1.jpg"></p><h2 id="SepLLM-针对标点符号的attention-sink"><a href="#SepLLM-针对标点符号的attention-sink" class="headerlink" title="SepLLM:针对标点符号的attention sink"></a>SepLLM:针对标点符号的attention sink</h2><p><img src="/2025/20250726/sep.jpg"></p><p>作者发现，LLMs 并未将注意力集中在具有语义意义的 token（如名词和动词）上，而是倾向于优先关注看似“无意义”的分隔符 token（如“.”或“\n”）以进行信息检索。这一观察表明，段落信息被压缩并嵌入到这些分隔符 token 中，从而无需直接从内容 token中提取即可实现高效的信息检索。</p><p><img src="/2025/20250726/sep1.jpg"></p><p>但作者其实并不只使用标点符号，也加入了StreamingLLM所用的初始tokens。具体如下图所示：</p><p><img src="/2025/20250726/sep2.jpg"></p><p>其和FastGen有些类似，FastGen相当于Past Window Cache为无限大，而不是局部的。</p><h1 id="Attention-Sink是必要的吗？"><a href="#Attention-Sink是必要的吗？" class="headerlink" title="Attention Sink是必要的吗？"></a>Attention Sink是必要的吗？</h1><p>Attention Sink已广泛应用于流式&#x2F;长上下文生成、KV 缓存优化、推理加速、模型量化等场景。</p><p>ICLR 2025的《When Attention Sink Emerges in Language Models: An Empirical View》详细研究了attention sink，但仍表示“是否有利于语言模型的下游性能仍不明确”。</p><p>此外，也涌现了Softpick、<a href="https://arxiv.org/abs/2505.06708">gated attention</a>等无attention sink的模型，足以说明并不一定是必需的。</p><h2 id="Attention-Sink-的产生"><a href="#Attention-Sink-的产生" class="headerlink" title="Attention Sink 的产生"></a>Attention Sink 的产生</h2><p>（ICLR 2025）的《When Attention Sink Emerges in Language Models: An Empirical View》指出：</p><p><strong>1.</strong> 在语言模型通过充足训练数据进行有效训练后，注意力汇聚现象显现。采用较小学习率训练的语言模型中，此现象显得不那么明显。而权重衰减则促进了注意力汇聚的形成。</p><p><strong>2.</strong> 汇点位置与损失函数及数据分布高度相关，并可移动至除首个 token 之外的其他位置。</p><p><strong>3.</strong> 位置嵌入、FFN 设计、LN 位置以及多头设计不会影响注意力汇的出现。注意力汇聚点更像是一种键偏置，它存储了额外的注意力，同时不参与价值计算。这一现象（至少部分）源于 token 对注意力得分的内部依赖，这种依赖是由Softmax 规范化引起的。通过用其他注意力操作（如无规范化的 Sigmoid 注意力）替代 Softmax 注意力来缓解这种依赖后，在参数规模高达 1B 的语言模型中，注意力汇聚点不再出现。</p><p>《Why do LLMs attend to the first token?》指出：</p><p><strong>1</strong>.attention sinks对于控制秩崩溃、表示崩溃和过度挤压是有用的。</p><p><strong>2</strong>.更大的模型和经过更长上下文训练的模型应该具有更强的汇点。</p><p><strong>3</strong>.无论在预训练期间是否包含 〈bos〉 ，attention sinks都会形成。</p><p>（COLM 2024）《Massive Activations in Large Language Models》提出了Massive Activations的特征，并指出与attention sink。</p><p><img src="/2025/20250726/massive.jpg"></p><p>layernorm可以消除影响。</p><p><img src="/2025/20250726/massive5.jpg"></p><p>Massive Activations本质上是充当bias的作用。</p><p><img src="/2025/20250726/massive1.jpg"></p><p>《Unveiling Super Experts in Mixture-of-Experts Large Language Models》进一步研究了MOE中的Massive Activations，并发现了超级专家。</p><p>ICLR 2025 的《Systematic Outliers in Large Language Models》研究了更多的情况。</p><p><img src="/2025/20250726/sys.jpg"></p><p>出现异常的位置：</p><p><img src="/2025/20250726/sys2.jpg"></p><p>另外，该文还发现尽管这些token受到了显著的注意力，其对应的值向量（V）却显示出相对较小的幅度。这表明，虽然异常值token吸引了注意力，但它们可能对最终输出的直接贡献较少。</p><p><img src="/2025/20250726/sys3.jpg"></p><p>论文中没有提到，但是可以从论文中看到的一点是，rmsnorm和layernorm类似，也可以消除一定影响。</p><p><img src="/2025/20250726/sys5.jpg"></p><h1 id="Attention-Sink是引起幻觉的元凶"><a href="#Attention-Sink是引起幻觉的元凶" class="headerlink" title="Attention Sink是引起幻觉的元凶"></a>Attention Sink是引起幻觉的元凶</h1><p>像博客之前介绍过的OPERA则指出了这一点。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OATS-通过稀疏与低秩分解实现异常值感知的剪枝</title>
    <link href="/2025/20250724/"/>
    <url>/2025/20250724/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（ICLR 2025)</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>OATS假定模型权重$W\approx S+L$，其中S为稀疏，$||S_0||\le k$；L为低秩，$Rank(L)\le r$。</p><span id="more"></span><p>对于此，可以转化为$\min ||W-S-L||_F^2$的求解问题。</p><p>OATS利用前人提出的交替阈值算法，该算法通过奇异值阈值处理迭代地交替求解低秩项 L，并通过硬阈值处理求解稀疏项 S。<br>$$<br>HardThreshold(A,k)&#x3D;M\odot A<br>$$<br>在执行HardThreshold时，也可以进行 HardThreshold和N：M稀疏性。</p><p><img src="/2025/20250724/1.png"></p><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>单独使用交替阈值化方法会产生次优结果，因为大规模 Transformer 的活性值呈现出少量大振幅特征（<a href="https://arxiv.org/abs/2402.17762">Massive Activations</a>），改变这些特征（例如通过稀疏和低秩近似）会对模型性能产生负面影响。</p><p>所以作者还另定义了一个矩阵D，用了捕捉活性值的二阶矩。<br>$$<br>D&#x3D;\sqrt{diag(X^TX)}<br>$$<br>然后变为$WD\approx S+L$。</p><h2 id="参数指定"><a href="#参数指定" class="headerlink" title="参数指定"></a>参数指定</h2><p><img src="/2025/20250724/3.png"></p><p>最终流程为：</p><p><img src="/2025/20250724/4.png"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>和其他量化相比：</strong></p><p><img src="/2025/20250724/2.png"></p><p><strong>秩比和迭代次数对 OATS 性能的影响：</strong></p><p><img src="/2025/20250724/5.png"></p><p><strong>稀疏项和低秩项捕捉了图像的不同区域:</strong></p><p><img src="/2025/20250724/6.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OPERA：通过过度信任惩罚和回顾分配减轻多模态大语言模型中的幻觉</title>
    <link href="/2025/20250722/"/>
    <url>/2025/20250722/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（CVPR 2024)</p><span id="more"></span><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>MLLMs 也面临着一个称为“幻觉”问题的重大挑战。具体而言，MLLMs 经常对用户提供的图像和提示产生不正确的声明，例如生成无关或无意义的响应，在颜色、数量和位置方面识别图像中不存在的错误物体。</p><p>各种方法 [23, 33, 34, 39]被提出以减少 MLLM 中的幻觉。</p><p>作者发现，幻觉许多幻觉内容的出现与列状注意力模式生成的后续词元相吻合。</p><p><img src="/2025/20250722/1.jpg"></p><p>值得注意的是，这些列状注意力模式通常出现在缺乏实质性信息的词元上，例如句号或引号。</p><p>一个表现出列状注意力模式的词元通常包含有限的信息，却对所有后续词元的预测产生显著影响，大多数后续内容包含推理或幻觉。基于上述观察，作者假设此类词元作为摘要词元，即从序列中的先前词元中聚合关键知识并指导后续词元生成。</p><p><img src="/2025/20250722/2.jpg"></p><p>在解码词元$x_t$时，每个候选假设将根据 Logit 中的Top-$N _ {beam}$ 概率选择 $N _ {beam}$ 个候选词元。最后，解码过程将输出获得最佳束得分的假设。</p><p>另外，基于$Logit p(x_t|x _ {&lt;t}) $，发展了几种解码策略。OPERA 基于束搜索 （bean search），这是一种基于累积得分的解码策略。简而言之，给定一个束大小 $N _ {beam}$ ，束搜索会保留 Nbeam 个候选序列，其中每个候选序列是一个解码序列 $x^{N _ {beam}}$，带有束得分。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="过度信任-Logit-惩罚"><a href="#过度信任-Logit-惩罚" class="headerlink" title="过度信任 Logit 惩罚"></a>过度信任 Logit 惩罚</h3><p><img src="/2025/20250722/3.jpg"></p><p><strong>裁剪局部窗口：</strong><br>$$<br>W _ {t-1}^k &#x3D; \{ w^i \} _ {i &#x3D; t-k}^{t-1}, \quad \text{s.t. } w^i &#x3D; \{ \omega _ {i,j} \} _ {j &#x3D; t-k}^{i}\tag{3}<br>$$</p><p><strong>注意力缩放：</strong></p><p>$$<br>W _ {t-1}^k &#x3D; \{ w^i \} _ {i &#x3D; t-k}^{t-1}, \quad \text{s.t. } w^i &#x3D; \{\sigma \omega _ {i,j} \} _ {j &#x3D; t-k}^{i}\tag{4}<br>$$</p><p><strong>对注意力矩阵的下三角部分进行列-wise 乘法运算：</strong><br>$$<br>\phi(\omega _ {&lt;t}) &#x3D; \prod _ {i &#x3D; c}^{t-1} \sigma \omega _ {i,c}, \quad \text{s.t. } c &#x3D; \arg\max _ {t-k \leq j \leq t-1} \prod _ {i &#x3D; j}^{t-1} \sigma \omega _ {i,j}<br>\tag{5}<br>$$<br>最终使用beam search限制候选集$\mathcal{Y}$：<br>$$<br>p(x_t\mid x _ {&lt;t})&#x3D;Softmax[\mathcal{H}(h_t)-\alpha\phi(\omega _ {\le t})] _ {x_t},\text{ s.t. }x_t\in \mathcal{Y}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">attn_pos = key_position<br>attn_local = attn_last[..., attn_pos[<span class="hljs-string">&quot;response_start&quot;</span>]:, attn_pos[<span class="hljs-string">&quot;response_start&quot;</span>]:]<br>attn_local = scale_factor * attn_local<span class="hljs-comment"># scale_factor默认为50</span><br><br><span class="hljs-comment"># 计算 rollback_scores</span><br>attn_local_scores = torch.zeros((...), dtype=torch.float16).to(candidate_token_scores.device)<br><span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(attn_local.shape[-<span class="hljs-number">1</span>]):<br>    local_score = <span class="hljs-number">1e-7</span> * attn_local[..., j:, j].prod(-<span class="hljs-number">1</span>).data<br>    attn_local_scores[..., j] = local_score.to(torch.float32)<br><br><span class="hljs-comment"># 计算 image attention 惩罚</span><br>cur_response_lens = attn_local.shape[-<span class="hljs-number">1</span>]<br>attn_i = attn_last[..., -<span class="hljs-number">1</span>, attn_pos[<span class="hljs-string">&quot;image_start&quot;</span>]:attn_pos[<span class="hljs-string">&quot;image_end&quot;</span>]+<span class="hljs-number">1</span>].<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>)<br>attn_scores = attn_i<br><br>rollback_scores, rollback_locs = attn_local_scores.<span class="hljs-built_in">max</span>(-<span class="hljs-number">1</span>)<br>rollback_loc = rollback_locs.mode().values.data<br><br>penalty_scores = -attn_scores <span class="hljs-keyword">if</span> cur_response_lens &lt;= <span class="hljs-number">10</span> <span class="hljs-keyword">else</span> rollback_scores<br>candidate_token_scores -= penalty_weights * penalty_scores<br>current_state[<span class="hljs-string">&quot;candidate_token_scores&quot;</span>] = candidate_token_scores.clone()<br><br></code></pre></td></tr></table></figure><h3 id="回顾-分配策略"><a href="#回顾-分配策略" class="headerlink" title="回顾-分配策略"></a>回顾-分配策略</h3><p>然而，仍然存在一些情况，其中所有候选词都受到惩罚且幻觉已经发生。这是由于前几个后续词元过度信任了摘要词元，而惩罚机制未能纠正它们。因此，一个直观但激进的想法是，如果我们能排除导致幻觉的词元，并在摘要词元之后重新选择合适的前几个词元，这种模式将大大减弱。</p><p><img src="/2025/20250722/4.jpg"></p><p>$$<br>N _ {overlap}&#x3D;\sum _ {c\in\mathcal{C}}1 _ {c&#x3D;s},\text{ s.t. }s&#x3D;Mode(\mathcal{C})<br>$$<br>若$N _ {overlap}\ge r$，考虑回溯，并将$s&#x3D;Mode(\mathcal{C})$视为摘要词元的位置。</p><p>假设序列$\{x_0, x_1, . . . , x_s, . . . , x _ {t−1}\}$在 摘 要 词 元 xs 处 展 示 了<br>知 识 聚 合 模 式，将 解 码 过 程 回 滚 到 序 列$\{x_0, x_1, . . . , xs\}$并在补集$\mathcal{Y}&#x2F;{x _ {s+1}}$中选择新的下一个词元。由于后续的回滚将比之前的更向前，手动指定回滚位置 s 必须是单调不递减的。此外，为回滚配置了一个最大时间 β ，如果$x_s$ 已经达到了最大回滚次数，我们考虑回滚到 $\{x_0, x_1, . . . , x _ {s−1}\}$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">try</span>:<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">all</span>((rollback_loc_gather == rollback_loc).long().<span class="hljs-built_in">sum</span>() &gt; <span class="hljs-built_in">int</span>(threshold) <span class="hljs-keyword">for</span> _, rollback_loc_gather <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(rollback_loc_gathers)):<br>                    <span class="hljs-keyword">if</span> rollback_loc &lt; <span class="hljs-number">10</span>: <span class="hljs-comment"># or rollback_loc + 1 &lt; rollback_pos:</span><br>                        <span class="hljs-keyword">assert</span> <span class="hljs-literal">False</span><br>                    <span class="hljs-comment"># locate the rollback position</span><br>                    <span class="hljs-comment"># —— 1) 确定 rollback_pos，更新 max_rollback_time —— </span><br>                    rollback_pos = rollback_loc + <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">if</span> max_rollback_time[rollback_pos] &gt;= num_attn_candidates:<br>                        <span class="hljs-comment"># print(f&quot;Already reach the maximum rollback times at position &#123;rollback_pos&#125;, so shift the rollback position to &#123;rollback_pos-1&#125;&quot;)</span><br>                        <span class="hljs-comment"># 已在该位置回滚过足够多次，先尝试往前退一格</span><br>                        rollback_pos = rollback_pos - <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">if</span> max_rollback_time[rollback_pos] &gt;= num_attn_candidates:<span class="hljs-comment"># 如果前一位置也达到极限，则抛错进入 except 分支</span><br>                            <span class="hljs-keyword">assert</span> <span class="hljs-literal">False</span><br>                        <span class="hljs-keyword">else</span>:<br>                            max_rollback_time[rollback_pos] += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">else</span>:<br>                        max_rollback_time[rollback_pos] += <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">if</span> cur_response_lens - rollback_pos &gt; history_length + <span class="hljs-number">1</span>:<span class="hljs-comment">## 如果当前位置与历史窗口长度差太大，也往前移动</span><br>                        rollback_pos = <span class="hljs-built_in">max</span>(<span class="hljs-number">1</span>, cur_response_lens - history_length - <span class="hljs-number">1</span>)<br>                    <span class="hljs-comment"># print(f&quot;rollback from pos &#123;cur_response_lens-1&#125; to pos &#123;rollback_pos&#125; for the time &#123;int(max_rollback_time[rollback_pos])&#125;&quot;)</span><br><br>                    <span class="hljs-comment"># discard the rollbacked states in history</span><br>                    <span class="hljs-comment"># —— 2) 丢弃过多历史状态 —— </span><br>    <span class="hljs-comment"># history_states 按时间顺序保存着每一步的快照，最后一条是刚加入的 current_state</span><br>    <span class="hljs-comment"># 要回到 rollback_pos，就要把最新的 (cur_len - rollback_pos - 1) 条状态弹出</span><br>                    <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cur_response_lens-rollback_pos-<span class="hljs-number">2</span>):<br>                        history_states.pop(-<span class="hljs-number">1</span>)<br>                        history_rollback_locs.pop(-<span class="hljs-number">1</span>)<br>                        reject_token_pos_gather[-(j+<span class="hljs-number">1</span>)] = []<br><br>                    <span class="hljs-comment"># Revive all of variables in the state of the rollback position</span><br>                    input_ids = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;input_ids&quot;</span>]<br>                    beam_scorer = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;beam_scorer&quot;</span>]<br>                    beam_indices = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;beam_indices&quot;</span>]<br>                    cur_len = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;cur_len&quot;</span>]<br><br>                    attn_previous = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;attn_previous&quot;</span>].to(input_ids.device)<br>                    candidate_token_scores = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;candidate_token_scores&quot;</span>]<br>                    candidate_tokens = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;candidate_tokens&quot;</span>]<br><br>                    beam_scores = history_states[-<span class="hljs-number">2</span>][<span class="hljs-string">&quot;beam_scores&quot;</span>]<br>                    beam_next_tokens = history_states[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;beam_next_tokens&quot;</span>]<br>                    beam_idx = history_states[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;beam_idx&quot;</span>]<br><br>                    <span class="hljs-comment"># first inference to get model kwargs</span><br>                    <span class="hljs-comment"># —— 4) 重新前向两次 —— </span><br>    <span class="hljs-comment"># 第一次：先构造 model_kwargs（恢复上下文）</span><br>                    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;images&quot;</span> <span class="hljs-keyword">in</span> model_kwargs_ori.keys():<br>                        model_kwargs = model_kwargs_ori.copy()<br>                        model_kwargs[<span class="hljs-string">&quot;attention_mask&quot;</span>] = torch.cat([<br>                            model_kwargs[<span class="hljs-string">&quot;attention_mask&quot;</span>], torch.ones((<br>                                input_ids.shape[<span class="hljs-number">0</span>], input_ids[:,:-<span class="hljs-number">1</span>].shape[<span class="hljs-number">1</span>] - model_kwargs[<span class="hljs-string">&quot;attention_mask&quot;</span>].shape[<span class="hljs-number">1</span>]<br>                            )).to(input_ids.device)], <span class="hljs-number">1</span>)<br><br>                        model_inputs_tmp = <span class="hljs-variable language_">self</span>.prepare_inputs_for_generation(input_ids[:,:-<span class="hljs-number">1</span>], **model_kwargs)<br>                    <span class="hljs-keyword">else</span>:<br>                        answer_embeds = <span class="hljs-variable language_">self</span>.model.embed_tokens(input_ids[:,<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>])<br>                        model_kwargs = model_kwargs_ori.copy()<br>                        model_kwargs[<span class="hljs-string">&quot;inputs_embeds&quot;</span>] = torch.cat([model_kwargs[<span class="hljs-string">&quot;inputs_embeds&quot;</span>], answer_embeds], <span class="hljs-number">1</span>)<br>                        model_kwargs[<span class="hljs-string">&quot;attention_mask&quot;</span>] = torch.cat(<br>                            [model_kwargs[<span class="hljs-string">&quot;attention_mask&quot;</span>], torch.ones_like(input_ids[:,<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>]).to(input_ids.device)], <span class="hljs-number">1</span>)<br><br>                        model_inputs_tmp = <span class="hljs-variable language_">self</span>.prepare_inputs_for_generation(input_ids[:,<span class="hljs-number">1</span>:-<span class="hljs-number">1</span>], **model_kwargs)<br><br>                    outputs_tmp = <span class="hljs-variable language_">self</span>(<br>                        **model_inputs_tmp,<br>                        return_dict=<span class="hljs-literal">True</span>,<br>                        output_attentions=output_attentions,<br>                        output_hidden_states=output_hidden_states,<br>                    )<br>                    model_kwargs = <span class="hljs-variable language_">self</span>._update_model_kwargs_for_generation(<br>                        outputs_tmp, model_kwargs, is_encoder_decoder=<span class="hljs-variable language_">self</span>.config.is_encoder_decoder<br>                    )<br><br>                    <span class="hljs-comment"># another inference to get outputs and logits</span><br>                    model_inputs_tmp = <span class="hljs-variable language_">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)<br><br>                    outputs = <span class="hljs-variable language_">self</span>(<br>                        **model_inputs_tmp,<br>                        return_dict=<span class="hljs-literal">True</span>,<br>                        output_attentions=output_attentions,<br>                        output_hidden_states=output_hidden_states,<br>                    )<br>                    next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>, :]<br>                    <span class="hljs-keyword">del</span> outputs_tmp, model_inputs_tmp<br><br>                    <span class="hljs-comment"># discard the last rollbacked state in history</span><br>                    history_states.pop(-<span class="hljs-number">1</span>)<br>                    history_rollback_locs.pop(-<span class="hljs-number">1</span>)<br>                    reject_token_pos_gather[rollback_pos+<span class="hljs-number">1</span>] = []<br><br>                    <span class="hljs-comment"># set penalty on the corresponding candidates</span><br>                    <span class="hljs-comment"># —— 5) 在 logits 上对已拒绝 token 施加极大惩罚 —— </span><br>    <span class="hljs-comment"># 将所有 logits 减去一个大数，使这些 token 在后续 topk 中永远得分最小</span><br>                    next_token_logits -= <span class="hljs-number">999.</span> + next_token_logits.<span class="hljs-built_in">min</span>(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>).values.data<br>                    next_token_logits = next_token_logits.view(batch_size, num_beams * vocab_size)<br>                    beam_idx = beam_idx.view(batch_size, num_beams)<br>                    beam_next_tokens = beam_next_tokens.view(batch_size, num_beams)<br>                    reject_token_pos = beam_idx * vocab_size + beam_next_tokens<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(reject_token_pos_gather[rollback_pos]) &gt; <span class="hljs-number">0</span>:<br>                        reject_token_pos = torch.cat([reject_token_pos_gather[rollback_pos], reject_token_pos], -<span class="hljs-number">1</span>)<br>                    reject_token_pos_gather[rollback_pos] = reject_token_pos<br>                    next_token_logits = next_token_logits.scatter_(-<span class="hljs-number">1</span>, reject_token_pos, -<span class="hljs-number">999.</span>)<br>                    next_token_logits = next_token_logits.view(batch_size * num_beams, vocab_size)<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">assert</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">except</span>:<br>                next_token_logits.fill_(-<span class="hljs-number">999.</span>)<br>                next_token_logits = next_token_logits.scatter_(-<span class="hljs-number">1</span>, candidate_tokens, candidate_token_scores)<br></code></pre></td></tr></table></figure><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250722/1.png"></p><p><img src="/2025/20250722/2.png"></p><p>令人奇异的是，beam search的幻觉率居然比top-k、top-p相对更低。</p><p>但想想似乎也存在一定的合理性。提出top-p的《The Curious Case of Neural Text Degeneration》也说明了beam search更容易重复，而缺乏新意。</p><h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>后续的还有2407.15130、2409.20429、2501.15269、2503.08342。</p><p>视觉方面也有相关研究：2503.07772</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>通过VIB减少大型视觉-语言模型中的幻觉现象</title>
    <link href="/2025/20250720/"/>
    <url>/2025/20250720/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（Corr 2025&#x2F; AAAI 2025）</p><p>《Mitigating Hallucinations in Large Vision-Language Models by Adaptively Constraining Information Flow》</p><span id="more"></span><p>广州大学白佳奇（一作）、李默涵、田志宏（通讯）老师团队</p><h2 id="motivation"><a href="#motivation" class="headerlink" title="motivation"></a>motivation</h2><p><img src="/2025/20250720/0.jpg"></p><p>相似度分布的平滑性影响与物体幻觉的出现相关联。</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><p>i): 首次利用 VIB 来缓解目标幻觉问题，降低了当软视觉 token 映射到 LLM的词嵌入时对无关视觉特征的过度自信。</p><p>ii): 提出了 AdaVIB，一种基于熵的噪声控制策略，能够自适应地约束视觉 token 所传达的信息，考虑到与 LLM词嵌入的相似度分布的平滑性。</p><p>iii): 综合实验证明了AdaVIB 在缓解目标幻觉方面的有效性。所提出的方法在不同模型架构的两个目标幻觉基准上均取得了持续的改进。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2025/20250720/1.jpg"></p><p><a href="https://github.com/jiaqi5598/AdaVIB/blob/875a41adf37e8fb879a3abdfcc1ab703819c5c90/minigpt4/models/mini_gpt4.py">代码</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MiniGPT4</span>(<span class="hljs-title class_ inherited__">Blip2Base</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<br>        ):<br>        <span class="hljs-keyword">if</span> mm_projector_type == <span class="hljs-string">&quot;vib&quot;</span>:<br><br>            <span class="hljs-variable language_">self</span>.llama_proj = nn.Linear(<br>                <span class="hljs-variable language_">self</span>.Qformer.config.hidden_size, <span class="hljs-variable language_">self</span>.llama_model.config.hidden_size<br>            )  <span class="hljs-comment"># don&#x27;t modify this, because it should be loaded from pre-trained checkpoints.</span><br><br>            <span class="hljs-variable language_">self</span>.llama_proj_std = nn.Linear(<br>                <span class="hljs-variable language_">self</span>.Qformer.config.hidden_size, <span class="hljs-variable language_">self</span>.llama_model.config.hidden_size<br>            )<br><br>            <span class="hljs-variable language_">self</span>.mu_p = nn.Parameter(torch.randn(<span class="hljs-variable language_">self</span>.llama_model.config.hidden_size))<br>            <span class="hljs-variable language_">self</span>.std_p = nn.Parameter(torch.randn(<span class="hljs-variable language_">self</span>.llama_model.config.hidden_size))<br><br>        <span class="hljs-keyword">elif</span> mm_projector_type == <span class="hljs-string">&quot;linear&quot;</span>:<br>            <span class="hljs-variable language_">self</span>.llama_proj = nn.Linear(<br>                <span class="hljs-variable language_">self</span>.Qformer.config.hidden_size, <span class="hljs-variable language_">self</span>.llama_model.config.hidden_size<br>            )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">universal_sentence_embedding</span>(<span class="hljs-params">self, sentences, mask, sqrt=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">        :param sentences: [batch_size, seq_len, hidden_size]</span><br><span class="hljs-string">        :param mask: [batch_size, seq_len]</span><br><span class="hljs-string">        :param sqrt:</span><br><span class="hljs-string">        :return: [batch_size, hidden_size]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        <span class="hljs-comment"># need to mask out the padded chars</span><br>        sentence_sums = torch.bmm(<br>            sentences.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>), mask.<span class="hljs-built_in">float</span>().unsqueeze(-<span class="hljs-number">1</span>)<br>        ).squeeze(-<span class="hljs-number">1</span>)<br>        divisor = (mask.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>).view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).<span class="hljs-built_in">float</span>())<br>        <span class="hljs-keyword">if</span> sqrt:<br>            divisor = divisor.sqrt()<br>        sentence_sums /= divisor<br>        <span class="hljs-keyword">return</span> sentence_sums<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_emb_entropy</span>(<span class="hljs-params">self, inputs_llama, llama_emb_weights</span>):<span class="hljs-comment">#对应式10、11</span><br>        batch_size = inputs_llama.size(<span class="hljs-number">0</span>)<br>        prompt_len = inputs_llama.size(<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            _mask = torch.ones([batch_size, prompt_len]).to(inputs_llama.device)<br>            _pooling_states = <span class="hljs-variable language_">self</span>.universal_sentence_embedding(inputs_llama, mask=_mask)<br>            _prob = torch.matmul(_pooling_states, llama_emb_weights.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)).softmax(-<span class="hljs-number">1</span>)<br>            _entropy = - (_prob * (_prob + <span class="hljs-number">1e-8</span>).log()).<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> _entropy, _prob<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">estimate</span>(<span class="hljs-params">self, emb, emb2mu, emb2std</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Estimates mu and std from the given input embeddings.&quot;&quot;&quot;</span><br>        mean = emb2mu(emb)<br>        std = torch.nn.functional.softplus(emb2std(emb))<span class="hljs-comment">#VIB原论文有的，避免方差为负。</span><br>        <span class="hljs-keyword">return</span> mean, std<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">kl_div</span>(<span class="hljs-params">self, mu_q, std_q, mu_p, std_p</span>):<br>        k = mu_q.size(-<span class="hljs-number">1</span>)<br>        mu_diff = mu_p - mu_q<br>        mu_diff_sq = torch.mul(mu_diff, mu_diff)<br>        logdet_std_q = torch.<span class="hljs-built_in">sum</span>(<span class="hljs-number">2</span> * torch.log(torch.clamp(std_q, <span class="hljs-built_in">min</span>=<span class="hljs-number">1e-8</span>)), dim=-<span class="hljs-number">1</span>)<br>        logdet_std_p = torch.<span class="hljs-built_in">sum</span>(<span class="hljs-number">2</span> * torch.log(torch.clamp(std_p, <span class="hljs-built_in">min</span>=<span class="hljs-number">1e-8</span>)), dim=-<span class="hljs-number">1</span>)<br>        fs = torch.<span class="hljs-built_in">sum</span>(torch.div(std_q ** <span class="hljs-number">2</span>, std_p ** <span class="hljs-number">2</span>), dim=-<span class="hljs-number">1</span>) + torch.<span class="hljs-built_in">sum</span>(torch.div(mu_diff_sq, std_p ** <span class="hljs-number">2</span>), dim=-<span class="hljs-number">1</span>)<br>        kl_divergence = (fs - k + logdet_std_p - logdet_std_q) * <span class="hljs-number">0.5</span><br>        <span class="hljs-keyword">return</span> kl_divergence<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reparameterize</span>(<span class="hljs-params">self, mu, std, sample_size</span>):<br>        batch_size = mu.size(<span class="hljs-number">0</span>)<br>        z = torch.randn(sample_size, batch_size, mu.size(<span class="hljs-number">1</span>), mu.size(<span class="hljs-number">2</span>)).to(mu.device)<br>        <span class="hljs-keyword">return</span> mu + std * z<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">vib_layer</span>(<span class="hljs-params">self, query_output_state, is_training, ib_sample_size</span>):<br>        batch_size = query_output_state.size(<span class="hljs-number">0</span>)<br>        prompt_len = query_output_state.size(<span class="hljs-number">1</span>)<br>        mu, std = <span class="hljs-variable language_">self</span>.estimate(query_output_state, <span class="hljs-variable language_">self</span>.llama_proj, <span class="hljs-variable language_">self</span>.llama_proj_std)<br><br>        _mask = torch.ones([batch_size, prompt_len], requires_grad=<span class="hljs-literal">False</span>).to(mu.device)<br>        mu_pooling = <span class="hljs-variable language_">self</span>.universal_sentence_embedding(mu, _mask)<br>        std_pooling = <span class="hljs-variable language_">self</span>.universal_sentence_embedding(std, _mask)<br><br>        mu_p = <span class="hljs-variable language_">self</span>.mu_p.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).expand(batch_size, -<span class="hljs-number">1</span>)<br>        std_p = torch.nn.functional.softplus(<span class="hljs-variable language_">self</span>.std_p.view(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>).expand(batch_size, -<span class="hljs-number">1</span>))<br>        kl_loss = <span class="hljs-variable language_">self</span>.kl_div(mu_pooling, std_pooling, mu_p, std_p)<br><br>        <span class="hljs-keyword">if</span> is_training:<br>            z = <span class="hljs-variable language_">self</span>.reparameterize(mu, std, sample_size=ib_sample_size)<br>            sampled_logits = <span class="hljs-variable language_">self</span>.get_logits(z)<br>            logits = sampled_logits<br>        <span class="hljs-keyword">else</span>:<br>            logits = mu<br><br>        <span class="hljs-keyword">return</span> logits, kl_loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_alpha_fn</span>(<span class="hljs-params">self, _entropy, v_size</span>):<span class="hljs-comment">#对应论文式12</span><br>        <span class="hljs-keyword">return</span> - (_entropy / math.log(v_size)).log()<br>    <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode_img</span>(<span class="hljs-params">self, image, is_training, beta=<span class="hljs-number">1.</span>, self_adaptive=<span class="hljs-literal">False</span>, ib_sample_size=<span class="hljs-number">3</span></span>):<br>        <br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.mm_projector_type == <span class="hljs-string">&quot;vib&quot;</span>:<br><br>            inputs_llama, kl_loss = <span class="hljs-variable language_">self</span>.vib_layer(query_output.last_hidden_state, is_training, ib_sample_size)<br>            llama_emb_weight = <span class="hljs-variable language_">self</span>.llama_model.model.embed_tokens.weight<br>            _entropy, _prob = <span class="hljs-variable language_">self</span>._emb_entropy(inputs_llama, llama_emb_weight)<br><br>            <span class="hljs-keyword">if</span> self_adaptive <span class="hljs-keyword">and</span> is_training:<br>                _alpha = <span class="hljs-variable language_">self</span>._alpha_fn(_entropy, v_size=llama_emb_weight.size(<span class="hljs-number">0</span>))<br>                sample_size = _alpha.size(<span class="hljs-number">0</span>) // kl_loss.size(<span class="hljs-number">0</span>)<br>                _alpha = _alpha.reshape(sample_size, -<span class="hljs-number">1</span>).mean(<span class="hljs-number">0</span>)<br>                kl_loss = beta * (kl_loss * _alpha).mean()<br>            <span class="hljs-keyword">else</span>:<br>                kl_loss = beta * kl_loss.mean()<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>大模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SparseVIT:无手工先验的图像操作定位网络</title>
    <link href="/2025/20250719/"/>
    <url>/2025/20250719/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（AAAI 2025）</p><h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>图像篡改定位（IML）用以识别图像中特定的篡改区域。</p><p>由于操作后图像上不可避免地会留下操作痕迹，这些痕迹可以分为语义和非语义（语义无关）特征。语义无关特征指的是突出低级痕迹信息的特征，这些特征独立于图像的语义内容。</p><p>几乎所有现有的IML模型都遵循了“语义分割骨干网络”和“手工制作的非语义特征提取”的设计。</p><span id="more"></span><p><img src="/2025/20250719/1.png"></p><p>大多数基本用到比如DCT、Sobel这些手工制作的，或者自带先验的特征。</p><p>而SparseVIT则抛弃了这种先验。显得更end to end。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2025/20250719/2.png"></p><h3 id="稀疏自注意力"><a href="#稀疏自注意力" class="headerlink" title="稀疏自注意力"></a>稀疏自注意力</h3><p><img src="/2025/20250719/3.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">alter_sparse</span>(<span class="hljs-params">x, sparse_size=<span class="hljs-number">8</span></span>):<br>    x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">assert</span> x.shape[<span class="hljs-number">1</span>]%sparse_size == <span class="hljs-number">0</span> &amp; x.shape[<span class="hljs-number">2</span>]%sparse_size == <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;image size should be divisible by block_size&#x27;</span><br>    grid_size = x.shape[<span class="hljs-number">1</span>]//sparse_size<br>    out, H, Hp, C = block(x, grid_size)<br>    out = out.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>).contiguous()<br>    out = out.reshape(-<span class="hljs-number">1</span>, sparse_size, sparse_size, C)<br>    out = out.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> out, H, Hp, C   <br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">alter_unsparse</span>(<span class="hljs-params">x, H, Hp, C, sparse_size=<span class="hljs-number">8</span></span>):<br>    x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>)<br>    x = x.reshape(-<span class="hljs-number">1</span>, Hp//sparse_size, Hp//sparse_size, sparse_size, sparse_size, C)<br>    x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>).contiguous()<br>    out = unblock(x, H)<br>    out = out.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><p>有点像空洞卷积。</p><p><img src="/2025/20250719/4.png"></p><h3 id="轻量级有效预测头Learnable-Feature-Fusion-LFF"><a href="#轻量级有效预测头Learnable-Feature-Fusion-LFF" class="headerlink" title="轻量级有效预测头Learnable Feature Fusion (LFF)"></a>轻量级有效预测头Learnable Feature Fusion (LFF)</h3><p><img src="/2025/20250719/5.png"></p><p>其中参考了<a href="https://arxiv.org/abs/2103.17239">《Going deeper with Image Transformers》</a>中的LayerScale。</p><p><img src="/2025/20250719/6.png"></p><p>最终LFF可以表示为<br>$$<br>\begin{align}<br>F_i&#x3D;Linear(C_i,C)(F_i),i&amp;&#x3D;1..4\quad;F_i&#x3D;Upsample()(F_i),i&#x3D;5,6<br>\\<br>M_p&amp;&#x3D;ADD(F_i\times\gamma)<br>\\<br>M_p&amp;&#x3D;Linear(C,1)(M_p)<br>\\<br>M_p&amp;&#x3D;Upsample()(M_p)<br>\end{align}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Multiple</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, </span><br><span class="hljs-params">                 init_value = <span class="hljs-number">1e-6</span>,</span><br><span class="hljs-params">                 embed_dim = <span class="hljs-number">256</span>,</span><br><span class="hljs-params">                 predict_channels = <span class="hljs-number">1</span>,</span><br><span class="hljs-params">                 norm_layer = partial(<span class="hljs-params">nn.LayerNorm, eps=<span class="hljs-number">1e-6</span></span>) </span>):<br>        <span class="hljs-built_in">super</span>(Multiple, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.gamma1 = nn.Parameter(init_value * torch.ones((embed_dim)),requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.gamma2 = nn.Parameter(init_value * torch.ones((embed_dim)),requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.gamma3 = nn.Parameter(init_value * torch.ones((embed_dim)),requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.gamma4 = nn.Parameter(init_value * torch.ones((embed_dim)),requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.gamma5 = nn.Parameter(init_value * torch.ones((embed_dim)),requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-variable language_">self</span>.gamma6 = nn.Parameter(init_value * torch.ones((embed_dim)),requires_grad=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># self.drop_path = nn.Identity()</span><br>        <span class="hljs-variable language_">self</span>.norm = norm_layer(embed_dim)<br>        <br>        <span class="hljs-variable language_">self</span>.conv_layer1 = nn.Conv2d(in_channels=<span class="hljs-number">320</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.conv_layer2 = nn.Conv2d(in_channels=<span class="hljs-number">320</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.conv_layer3 = nn.Conv2d(in_channels=<span class="hljs-number">320</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.conv_layer4 = nn.Conv2d(in_channels=<span class="hljs-number">320</span>, out_channels=<span class="hljs-number">512</span>, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br>        <span class="hljs-variable language_">self</span>.conv_last = nn.Conv2d(embed_dim, predict_channels, kernel_size= <span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        c1, c2, c3, c4, c5, c6 = x<br>        <br>        c1 = <span class="hljs-variable language_">self</span>.conv_layer1(c1)<br>        c2 = <span class="hljs-variable language_">self</span>.conv_layer2(c2)<br>        c3 = <span class="hljs-variable language_">self</span>.conv_layer3(c3)<br>        c4 = <span class="hljs-variable language_">self</span>.conv_layer4(c4)<br>        b, c , h, w = c1.shape<br>        c5 = F.interpolate(c5, size=(h, w), mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>, align_corners=<span class="hljs-literal">False</span>)<br>        c6 = F.interpolate(c6, size=(h, w), mode=<span class="hljs-string">&#x27;bilinear&#x27;</span>, align_corners=<span class="hljs-literal">False</span>)<br>        c1 = c1.flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        c2 = c2.flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        c3 = c3.flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        c4 = c4.flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <br>        c5 = c5.flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        c6 = c6.flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        x = <span class="hljs-variable language_">self</span>.gamma1*c1 + <span class="hljs-variable language_">self</span>.gamma2*c2 + <span class="hljs-variable language_">self</span>.gamma3*c3 + <span class="hljs-variable language_">self</span>.gamma4*c4 + <span class="hljs-variable language_">self</span>.gamma5*c5 + <span class="hljs-variable language_">self</span>.gamma6*c6<br>        x= x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(b, c, h, w)<br>        x = (<span class="hljs-variable language_">self</span>.norm(x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>))).permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>).contiguous()<br>        x = <span class="hljs-variable language_">self</span>.conv_last(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://github.com/scu-zjz/SparseViT">https://github.com/scu-zjz/SparseViT</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态速览</title>
    <link href="/2025/20250716/"/>
    <url>/2025/20250716/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于李沐老师的<a href="https://www.bilibili.com/video/BV1Vd4y1v77v/">《多模态论文串讲》</a>，发布于2023年初左右。</p><p>发布之始已经看过。由于近日学习的需要，重拾并整理。</p><span id="more"></span><hr><h2 id="多模态基本任务"><a href="#多模态基本任务" class="headerlink" title="多模态基本任务"></a>多模态基本任务</h2><p>虽然今天只会涉及一部分，但我们也有必要大概了解。</p><p><strong>多模态理解 (Multimodal Understanding)：</strong></p><p>这类任务侧重于让AI理解不同模态之间的关联和意义。</p><ul><li><strong>图像描述生成 (Image Captioning)：</strong> 给定一张图像，AI生成一段描述图像内容的文本。例如，看到一张“一只猫坐在沙发上”的图片，AI能生成文字描述“一只橘猫正舒适地躺在棕色沙发上”。</li><li><strong>视觉问答 (Visual Question Answering, VQA)：</strong> 结合图像和自然语言问题，AI回答问题。例如，给出一张图片并问“图中有几只狗？”，AI需要理解图像内容并提取相关信息进行回答。</li><li><strong>视频理解 (Video Understanding)：</strong> 分析视频内容，包括识别视频中的活动、事件、人物等。这可以包括视频摘要、行为识别、以及对视频内容的问答。</li><li><strong>跨模态检索 (Cross-modal Retrieval)：</strong> 使用一种模态的数据（如文本查询）来检索另一种模态的数据（如图像或视频），反之亦然。例如，输入一段文字描述“日落时分的沙滩”，系统能检索出相关的日落海滩图片。</li><li><strong>情感识别 (Sentiment Analysis)：</strong> 结合文本、语音语调和面部表情等多种模态信息来更准确地判断用户的情绪。</li><li><strong>多模态机器翻译 (Multimodal Machine Translation)：</strong> 在翻译文本的同时，考虑视觉或听觉信息，以提高翻译的准确性和语境理解。</li></ul><p><strong>多模态生成 (Multimodal Generation)：</strong></p><p>这类任务要求AI根据一种或多种模态的输入，生成另一种或多种模态的输出。</p><ul><li><strong>文本到图像生成 (Text-to-Image Generation)：</strong> 根据文本描述生成对应的图像。例如，输入“一只穿着宇航服的猫”，AI能够生成符合描述的图像。Stable Diffusion 和 DALL-E 等模型就是这方面的典型代表。</li><li><strong>图像到文本生成 (Image-to-Text Generation)：</strong> （这与图像描述生成类似，但在某些语境下，生成更具创造性或特定风格的文本也属于此范畴）。</li><li><strong>文本到语音生成 (Text-to-Speech, TTS)：</strong> 将文本转换为自然流畅的语音。</li><li><strong>语音到文本生成 (Speech-to-Text, STT)：</strong> 将语音转换为文字，通常称为语音识别。</li><li><strong>图像&#x2F;视频到语音&#x2F;音乐生成：</strong> 根据图像或视频内容生成相关的声音或音乐。</li><li><strong>3D 内容生成：</strong> 结合文本或其他模态信息，生成三维模型或场景。</li></ul><p><strong>多模态交互 (Multimodal Interaction)：</strong></p><p>这类任务侧重于AI与用户之间，或AI系统内部不同模态之间的动态交互。</p><ul><li><strong>对话系统 (Conversational AI)：</strong> 结合语音、文本、甚至视觉信息，实现更自然、更富有语境的对话。例如，一个智能助手不仅能理解用户的语音指令，还能根据屏幕上的视觉信息给出更精准的回答。</li><li><strong>机器人学与具身智能 (Robotics and Embodied AI)：</strong> 让机器人能够通过视觉、听觉、触觉等多种感官感知环境，并做出相应的动作。</li><li><strong>人机交互 (Human-Computer Interaction, HCI)：</strong> 探索更自然、直观的多模态交互方式，如手势控制、眼动追踪等。</li></ul><p><strong>多模态表示学习 (Multimodal Representation Learning)：</strong></p><p>这类任务的目标是学习如何将不同模态的数据映射到一个共同的、有意义的表示空间中，以便于后续的任务处理。</p><ul><li><strong>联合表示 (Joint Representation)：</strong> 将来自不同模态的数据映射到同一个向量空间中，使得不同模态之间的语义相似性可以在这个空间中衡量。</li><li><strong>协同表示 (Coordinated Representation)：</strong> 学习不同模态的单独表示，但这些表示可以通过一些机制（如共享的语义概念）相互关联。</li></ul><h2 id="从ViLT引入"><a href="#从ViLT引入" class="headerlink" title="从ViLT引入"></a>从ViLT引入</h2><p>当前的视觉与语言预训练（VLP）方法高度依赖于图像特征抽取过程。则效率&#x2F;速度方面存在问题，即仅仅提取输入特征所需的计算量远大于多模态交互步骤；且在表达能力方面，因为它的表达能力受限于视觉嵌入器及其预定义的视觉词表。</p><p><img src="/2025/20250716/1.jpg"></p><p>早期的都依赖预训练的目标检测器。</p><p><strong>第一种就是早期的工作。</strong> 比如VSE（visual-semantic embeddings）或者VSE++。</p><p>VSE++的主要贡献是将难例负样本纳入损失函数中，主要是在损失函数的创新。</p><p><strong>第二种是CLip。</strong></p><p>前两种模态之间只进行简单的交互。</p><p>比如Clip只做简单的点积。</p><p><img src="/2025/20250716/2.jpg"></p><p><strong>第三种就是OSCAR 、VILBERT、UNITER等模型的思路。</strong></p><p>这一代开始认为，模态之间更深层的交互更重要。</p><p><strong>OSCAR:</strong></p><p><img src="/2025/20250716/OSCAR.jpg"></p><p><strong>VILBERT:</strong></p><p><img src="/2025/20250716/VILBERT.jpg"></p><p><strong>UNITER:</strong></p><p><img src="/2025/20250716/UNITER.jpg"></p><p>UNITER和OSCAR的处理方式让我想到了博客之前介绍的Fuyu。</p><p>前三种的缺点则是模型参数过大，效率&#x2F;速度较慢。</p><p><strong>第四种则是VilT。</strong></p><p>自VIT出来后，由于patch都能蕴含信息，它实际上和目标检测的锚框是相似的，故ViLT 把预训练的目标检测器换成了一层可学习的 Patch Embedding。</p><p>当然伴随着参数的减小，模型的性能也变差了，不如第三类。（想到了No free lunch theorem)</p><p>而我们的数据集是更偏向图像而非文本。所以理论上，需要更强的视觉模型，而ViLT的文本部分强于视觉部分。</p><p>另一个缺点是训练时间很慢。</p><p>而ViLT发现Word Patch Alignment（WPA） loss非常慢。故可以考虑把它删去。</p><h2 id="ALBEF"><a href="#ALBEF" class="headerlink" title="ALBEF"></a>ALBEF</h2><p>我们把以上的得到的内容综合，这正是ALBEF（Align before Fuse, 出自Salesforce Research，提出了Blip之类的高质量多模态模型）。</p><p><img src="/2025/20250716/3.jpg"></p><p> 其中结构如图，并只用到了ITC、ITM、MLM loss。</p><p>让我们具体详解。</p><p>ViLT弃用预训练的目标检测器是因为觉得参数过大会导致推理速度太慢了，而ALBEF则认为<strong>预训练</strong>的目标检测器没有端到端训练，故视觉和文本特征并不是对齐的。这是第一个贡献。</p><p> 另外，作者认为网络上的数据太嘈杂了，为了从噪声网络数据中更好地学习，作者还提出了动量蒸馏，这是一种自训练方法，它从动量模型（出自MoCo，与主模型是同一个模型，参数由指数滑动平均得出）生成的伪目标中学习。这是第二个贡献。</p><p><img src="/2025/20250716/3.jpg"></p><p>Image Embedding 部分使用了标准的 ViT 模型，而文本部分相当于BERT，但是只用前 6 层做文本encoder，剩余部分当做多模态交互的部分。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>对于前6层使用的ITC loss，是完全按照MoCo的方法来的。先使用softmax对相似度进行归一化（惯用操作了），并将对比损失定义为交叉熵损失。</p><p>对于后面使用的ITM loss：</p><ul><li>给定图片和文本，然后经过 ALBEF 的模型后，得到特征，再过一个FC层，以此做二分类，判断是否为一对。</li><li>但是判断正样本有点难，但是判断负样本很容易，因此准确度会上升得很快。</li><li>为解决上面的问题，这里通过某种方式选择最接近正样本的负样本。<ul><li>hard negatives ：ITM 利用 ITC 把同一 batch 中图片和所有文本都算一遍余弦相似度。 利用最相似的做负样本。</li></ul></li></ul><p>对于后面使用的MLM loss，mask 掉一些文本，然后将 mask 过后的句子和图片一起通过 ALBEF 模型，最后把之前完整的句子预测出来。</p><p>要注意的一点是，MLM loss的输入和前两个 Loss 是不同的，分别是($I$,$T$)、($I$,$T_{mask}$)，说明模型用了两次<code>forward()</code> 函数。</p><h3 id="Momentum-Distillation（动量蒸馏）"><a href="#Momentum-Distillation（动量蒸馏）" class="headerlink" title="Momentum Distillation（动量蒸馏）"></a>Momentum Distillation（动量蒸馏）</h3><p>这部分是用来从噪声网络数据中更好地学习。使用的是自训练方法。</p><p>关于噪声和自训练，有谷歌提出的Noisy Student。</p><p>具体而言，使用标准交叉熵损失训练一个教师模型。然后，使用教师模型在无标签的图像上生成伪标签，伪标签可以是软的(连续分布)，也可以是硬的(独热分布)。然后基于上述有标签和伪标签图像，使用标准交叉熵损失训练一个学生模型。最后，不断迭代这个过程，将学生模型作为新的教师模型，以生成新的伪标签并训练新的学生模型。</p><p><img src="/2025/20250716/4.jpg"></p><p>博客之前介绍过的《Multi-teacher Self-training for Semi-supervised Node Classification with Noisy Labels》也是类似的方法。</p><p><img src="/2025/20250716/multiteacher.png"></p><p>ALBEF的动量模型是个持续演进的教师模型。在训练过程中，训练基础模型使其预测与动量模型的预测相匹配。</p><p>这部分只对ITC、MLM做，而不对ITM做。</p><p>由于ITM已经有hard negative，所以就没必要再做了。</p><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p><img src="/2025/20250716/5.jpg"></p><h2 id="ViLT"><a href="#ViLT" class="headerlink" title="ViLT"></a>ViLT</h2><p><img src="/2025/20250716/VILT.jpg"></p><p>结构方面类似于UNITER和OSCAR，但变得更简单了。</p><p>后续的Fuyu则进一步更简化。</p><p><img src="/2025/20250716/fuyu.jpg"></p><p>感觉模型发展就是不断往更简化的模式去。就像对比学习中的SimCLR、BYOL再到何恺明更简单的SimSiam。</p><p>真可谓大道至简。</p><h2 id="VLMo"><a href="#VLMo" class="headerlink" title="VLMo"></a>VLMo</h2><p>微软出品。</p><ul><li>模型结构上的改进 Mixture-of-Modality-Experts</li><li>训练方式改进：分阶段模型预训练</li></ul><p>模型架构：</p><p><img src="/2025/20250716/6.jpg"></p><p>分阶段训练：</p><p><img src="/2025/20250716/7.jpg"></p><h3 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h3><p>1.更大模型 —— BeiTv3<br>2.文本图像都可以mask ——VL-BeiT</p><p><img src="/2025/20250716/8.jpg"></p><p>3.单模态可以帮助多模态，多模态也可以帮助单模态——BeiTv3<br>4.更多模态，如视频等——MetaLM</p><h2 id="BLip"><a href="#BLip" class="headerlink" title="BLip"></a>BLip</h2><p>ALBEF原班人马所做，因此结合了 ALBEF 和 VLMo。</p><p>《BLIP: <strong>Bootstrapping</strong> Language-Image Pre-training for <strong>Unified</strong> Vision-Language Understanding and Generation》</p><p><strong>Bootstrap：</strong> 先用嘈杂数据训练模型，再用比较干净的数据训练模型。</p><p><img src="/2025/20250716/9.jpg"></p><p><strong>Unified：</strong> 统一了图像-语言的理解与生成任务。</p><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p><img src="/2025/20250716/10.jpg"></p><h2 id="CoCa"><a href="#CoCa" class="headerlink" title="CoCa"></a>CoCa</h2><p>谷歌出品，《CoCa: Contrastive Captioners are Image-Text Foundation Models》</p><p>也是ALBEF的一个后续工作。</p><p><img src="/2025/20250716/11.jpg"></p><p>图像部分使用Encoder，文本部分全用Decoder。</p><p>因为效率问题所以如此设计。解耦的自回归解码器设计的一个主要优势是它可以高效地计算两种训练损失。由于单向语言模型是在完整句子上使用因果掩码进行训练的，因此解码器可以通过一次前向传播高效生成对比损失和生成损失的输出。而前面提到的方法需要两次。</p><h2 id="BEITV3"><a href="#BEITV3" class="headerlink" title="BEITV3"></a>BEITV3</h2><p>《Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks》</p><h3 id="引言：大一统"><a href="#引言：大一统" class="headerlink" title="引言：大一统"></a>引言：大一统</h3><p><strong>第一</strong>，Transformer的成功从语言问题和多模态问题中得到了体现。网络结构的合一使我们能够无缝处理多种模态。对于视觉-语言建模，由于下游任务的不同性质，有多种应用 Transformer 的方式。</p><p>例如，双塔结构用于高效检索，编码器-解码器网络用于生成任务，融合编码器结构用于图像-文本编码。</p><p>然而，大多数基础模型必须根据特定架构手动转换终端任务格式。此外，参数通常在不同模态之间无法有效共享。在本工作中，采用 Multiway Transformers 进行通用建模。其实是VLMO的Mixture of Modality Experts。</p><p><strong>第二</strong>，基于掩码数据建模的预训练任务已被成功应用于各种模态。当前的视觉-语言基础模型通常多任务其他预训练目标（如图像-文本匹配），导致缩放不友好且效率低下。</p><p>相反，该作仅使用一个预训练任务，即掩码-预测，来训练一个通用的多模态基础模型。通过将图像视为一种外语，我们以相同的方式处理文本和图像，而没有根本性的建模差异。因此，图像-文本对被用作“平行句子”，以学习不同模态之间的对齐.</p><p><strong>第三</strong>，普遍地扩大模型规模和数据规模可以提高基础模型的泛化质量，从而使它们能够转移到各种下游任务中。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>架构：</p><p><img src="/2025/20250716/12.jpg"></p><p>EBiT-3 可以转移到各种视觉和视觉-语言下游任务。通过一个共享的 Multiway Transformer。</p><p><img src="/2025/20250716/13.jpg"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.李沐老师的<a href="https://www.bilibili.com/video/BV1Vd4y1v77v/">《多模态论文串讲》</a></p><p>2.<a href="https://zhuanlan.zhihu.com/p/663234588">腾讯技术工程-浅析多模态大模型的前世今生</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>快手推荐系统模型速览</title>
    <link href="/2025/20250714/"/>
    <url>/2025/20250714/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>推荐模型的一个大概结构如图所示。</p><p><img src="/2025/20250714/recsys_overview.png"></p><p>我们将要介绍快手的几个推荐系统模型。QARM和OneRec。</p><p>我们将按时间顺序介绍。</p><span id="more"></span><h2 id="QARM"><a href="#QARM" class="headerlink" title="QARM"></a>QARM</h2><p>QARM&#x3D; Quantitative Alignment Multi-Modal Recommendation</p><p>作者认为由于计算代价过大，导致业内通常使用非端到端的模型——二步部署方案，先使用MLLM提取embedding，再进一步训练。</p><p>Feature-&gt;Encoder-&gt;Embedding-&gt;Model</p><p>作者认为这种非端到端的模型有以下问题：</p><p><strong>（1）embedding不匹配。</strong></p><p>MLLM通常是由图文匹配训练的，而基于ID的特征并不是。</p><p><strong>（2）embedding遗忘。</strong></p><p>新添加的多模态特征不会随着推荐系统的训练而更新，而其他基于离散ID的特征（用户ID、物品ID等）可以实时端到端优化。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>检索模型遵循的经典设计：</p><p><img src="/2025/20250714/1.jpg"></p><p>这也就是推荐系统常用的“双塔模型”。</p><p>QARM整体架构：</p><p><img src="/2025/20250714/2.jpg"></p><h4 id="对齐"><a href="#对齐" class="headerlink" title="对齐"></a>对齐</h4><p>首先通过以下方式生成高质量的项目对：</p><ul><li><p>基于User2Item检索模型，针对每位用户正向点击的目标项目，从其最近50次正向点击的项目集合中，选取在ID表示空间内相似度最高的项目作为触发项。</p></li><li><p>基于Item2Item检索模型，利用已学成稳定且高相似度的物品对作为数据源，例如从我们的Swing检索模型中导出的数据。</p></li></ul><p>再通过对比学习进行批量（为$\beta$）训练：<br>$$<br>\begin{align}<br>M _ {trigger}&amp;&#x3D;MLLM(T _ {trigger}^{text},T _ {trigger}^{audio},T _ {trigger}^{image})\\<br>M _ {target}&amp;&#x3D;MLLM(T _ {target}^{text},T _ {target}^{audio},T _ {target}^{image})\\<br>\mathcal{L} _ {align}&amp;&#x3D;Batch\_Contrastive(M _ {trigger},M _ {target},\beta)<br>\end{align}<br>$$</p><h4 id="Quantitative-Code"><a href="#Quantitative-Code" class="headerlink" title="Quantitative Code"></a>Quantitative Code</h4><p>作者首先是将多模态表征量化压缩成语义ID, 再将语义ID输入到推荐模型中。</p><p>作者采用了两种量化机制。</p><p>一种是最常用的VQ，首先训练一个codebook，然后利用top-k最近邻搜索来哈希表示。</p><p>由于预训练的MLLM已经能表示复杂项目的相关系数，故不再训练codebook，而直接采用所有项目的对齐作为codebook。</p><p>另一种是RQ，即Residual-Quantized。出自RQ-VAE。</p><p><img src="/2025/20250714/1.png"></p><p>具体而言，</p><p><img src="/2025/20250714/VQ.jpg"></p><p><img src="/2025/20250714/2.png"></p><h4 id="端到端训练"><a href="#端到端训练" class="headerlink" title="端到端训练"></a>端到端训练</h4><p>最后将这几个编码作为新的ID特征送入模型中，进行端到端训练：</p><p><img src="/2025/20250714/3.png"></p><h2 id="OneRec"><a href="#OneRec" class="headerlink" title="OneRec"></a>OneRec</h2><p>虽然也是快手出品，但研究团队完全换了一批人。</p><p>该模型用统一的生成模型替换了级联学习框架。是第一个端到端的生成模型。</p><p><img src="/2025/20250714/3.jpg"></p><p>该模型基于tranformer。首先使用QARM的对齐和Quantitative Code的方法来作为Tokenizer，转化为模型可以接受的输入。</p><p>完成模型的预训练之后, 由于模型是只预测next-token。所以还会再利用 DPO 等强化学习进行对齐训练。最后，在预测的时候, 通过 beam search 筛选出视频流。</p><h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><p><img src="/2025/20250714/4.jpg"></p><h3 id="编码器部分"><a href="#编码器部分" class="headerlink" title="编码器部分"></a>编码器部分</h3><h4 id="多尺度特征工程"><a href="#多尺度特征工程" class="headerlink" title="多尺度特征工程"></a>多尺度特征工程</h4><h5 id="用户静态路径"><a href="#用户静态路径" class="headerlink" title="用户静态路径"></a>用户静态路径</h5><p>用户静态路径生成核心用户特征的紧凑表示，包含用户标识符（uid）、年龄（age）、性别（gender）等，随后将其变换为模型的隐藏维度。</p><h5 id="短期路径"><a href="#短期路径" class="headerlink" title="短期路径"></a>短期路径</h5><p>挑选用户最近交互的 ($L_s&#x3D;20$) 个视频的 &lt;video identifier, author identifier, tag, timestamp, playtime, duration, labels&gt; 等信息进行融合:</p><h5 id="正反馈路径"><a href="#正反馈路径" class="headerlink" title="正反馈路径"></a>正反馈路径</h5><p>将用户给与最多正反馈的 ($L_p&#x3D;200$） 个视频的 &lt;video identifier, author identifier, tag, timestamp, playtime, duration, labels&gt; 等信息进行融合:</p><p>对于以上三个部分，变换方式都是：$f_u$为各特征拼接，$h_u&#x3D;Dense(LeakyReLU(Dense(f_u)))$。</p><h5 id="终身路径"><a href="#终身路径" class="headerlink" title="终身路径"></a>终身路径</h5><p>终身行为路径旨在处理包含多达 100,000 个视频序列的超长用户交互历史。直接对此类序列应用注意力机制在计算上是不可行的。</p><p><strong>行为压缩</strong></p><p>使用类似QARM的分层 K-均值，使用最接近簇中心的项目作为该簇的代表。</p><p><strong>特征聚合</strong></p><p>依旧使用上面的：$f_u$为各特征拼接，$h_u&#x3D;Dense(LeakyReLU(Dense(f_u)))$。</p><p>最后再通过 QFormer 压缩。</p><h3 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h3><p>对于每个目标视频 $m$，解码器输入序列通过将可学习的序列起始 token 与视频的语义标识符连接而构造：</p><p>$$<br>S_m &#x3D; \{s_{\text{EOS}}^1, s_m^1, s_m^2, \dots, s_m^{L_s}\}<br>$$</p><p>$$<br>d_m^{(0)} &#x3D; \text{Emb_lookup}(S_m)<br>$$</p><p>解码器通过 $L_{\text{dec}}$ Transformer 层处理此序列。每层执行顺序操作：</p><p>$$<br>d_m^{(l+1)} &#x3D; d_m^{(l)} + \text{CausalSelfAttn}(d_m^{(l)})<br>$$</p><p>$$<br>d_m^{(l+1)} &#x3D; d_m^{(l+1)} + \text{CrossAttn}(d_m^{(l+1)}, Z_{\text{enc}}, Z_{\text{enc}})<br>$$</p><p>$$<br>d_m^{(l+1)} &#x3D; d_m^{(l+1)} + \text{MoE}(\text{RMSNorm}(d_m^{(l+1)}))<br>$$</p><p>每个解码器层都包含一个混合专家模型 (MoE) 前馈网络，以增强模型容量，同时保持计算效率。MoE 层采用 $N_{\text{experts}}$ 个专家网络，并采用 top-k 路由策略：</p><p>$$<br>\text{MoE}(x) &#x3D; \sum_{j&#x3D;1}^k \text{Gate}_j(x) \cdot \text{Expert}_j(x)<br>$$</p><p>其中 $\text{Gate}_j(x)$ 表示由路由机制确定的权重，$\text{Expert}_j(x)$ 表示第 $j$ 个选定的专家网络的输出。并使用了DeepseekV3的无损失的负载均衡策略。</p><h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p><img src="/2025/20250714/5.jpg"></p><p>在训练过程中，这些塔使用相应的目标标签作为辅助任务计算二元交叉熵（BCE）损失。每个塔的隐状态，连同用户和物品的表示，被输入到最终层的多层感知机（MLP）中。该 MLP 之后是一个单塔输出 P-Score，它使用所有目标的标签计算二元交叉熵损失。</p><p>使用的是GRPO的修改版（被称为早起阶段GRPO，ECPO），具体而言，多了31式：</p><p><img src="/GRPO.jpg"></p><h4 id="生成格式正则化"><a href="#生成格式正则化" class="headerlink" title="生成格式正则化"></a>生成格式正则化</h4><p>引入强化学习与 ECPO会显著增加了非法输出的生成，即在推<br>理过程中生成没有对应物品 ID 的语义 ID 序列。</p><p><img src="/2025/20250714/7.jpg"></p><p>这是由于挤压效应引起的，即当A&lt;0时，一部分合法token被挤压到非法token，使得模型难以区分合法token。</p><p>作者直接引入格式奖励以鼓励模型正确生成。具体而言，对合法样本设置为1，对非法样本设置为0，即丢弃。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>OneRec Technical Report.<a href="http://arxiv.org/abs/2506.13695">http://arxiv.org/abs/2506.13695</a></p><p>OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment.<a href="https://arxiv.org/abs/2502.18965">https://arxiv.org/abs/2502.18965</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
      <tag>推荐系统</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>线性注意力速览</title>
    <link href="/2025/20250711/"/>
    <url>/2025/20250711/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自注意力机制是Transformer模型的重要部分，但是自注意力的计算和内存的复杂度都与序列长度的二次方成正比，这带来了巨大的计算和内存瓶颈。于是乎，就有了线性注意力的提出。</p><span id="more"></span><h2 id="最简单的形式"><a href="#最简单的形式" class="headerlink" title="最简单的形式"></a>最简单的形式</h2><p>我们知道标准的attention可以写成：（忽略缩放因子）<br>$$<br>O&#x3D;softmax(QK^T)V<br>$$<br>现在的计算顺序是$(QK^T)V$，如果我们能把计算顺序改为$Q(K^TV)$，那么计算复杂度将从$O(N^2d)$到$O(Nd^2)$。</p><p>那我们该如何把softmax化掉呢？</p><p>我们可以联想到SVM等中所用到的核函数：K( x, x′) &#x3D;φ( x) ⋅φ( x′)。</p><p>于是我们也可以寻求$exp(q,k)\approx\phi(q)^T\psi(k)$。</p><p>而《<a href="https://arxiv.org/abs/1812.01243">Efficient Attention: Attention with Linear Complexities</a>》给出的选择是，对Q和K的不同维度进行归一化：$P&#x3D;softmax_d(Q)softmax_N(K)^TV$。</p><p>更多的其他方法还有<a href="https://arxiv.org/abs/2103.02143">Random Feature Attention</a>、<a href="https://arxiv.org/abs/2009.14794">Performer</a>等。</p><p>进一步的我们可以省略为$O&#x3D;Q(K^TV)$。</p><p>我们可以写成$o_t&#x3D;(\sum_{j&#x3D;1}^t v_jk_j^T)q_t$。</p><p>我们把$q_t$分开，也就是$o_t&#x3D;S_tq_t$，$S_t&#x3D;S_{t-1}+v_tk_t^T$</p><h2 id="统一形式"><a href="#统一形式" class="headerlink" title="统一形式"></a>统一形式</h2><p>我们得到了上面的式子，可以看成attention可以写成以$S_t$为state的线性RNN的形式。</p><p>进一步的我们可以假定统一形式为:<br>$$<br>S_t&#x3D;A_tS_{t-1}+B_t<br>$$<br>当然我们也不是胡乱设置A和B的，这条式子类似梯度下降的迭代，故我们也可以参考梯度下降。这正是《<a href="https://arxiv.org/abs/2407.04620">Learning to (Learn at Test Time): RNNs with Expressive Hidden States</a>》的做法：</p><p>我们希望学习一个函数$M:key\to value$，并定义一个距离度量$loss_M(key,value)&#x3D;dis(M(key),value)$，梯度下降为$M_{next}&#x3D;M-\beta\nabla loss_M(k,v)$。</p><p>对于距离我们可以采取负点积或（平方）欧几里得距离，也就引出了以下的线性注意力：</p><p><img src="/2025/20250711/linear-attention-derivations.png"></p><p>而（平方）欧几里得距离的梯度下降实际上可以与机器学习中的<a href="https://en.wikipedia.org/wiki/Delta_rule">Delta rule</a>联系起来，这也就是DeltaNet名字的由来。</p><p>虽然$B_t$大多为$v_tk_t^T$的倍数，但也有不符合这一形式的。这也是为什么我们这里采用$S_t&#x3D;A_tS_{t-1}+B_t$而不是$S_t&#x3D;A_tS_{t-1}+B_tv_tk_t^T$。</p><table><thead><tr><th>线性注意力</th><th>$A_t$</th><th>$B_t$</th></tr></thead><tbody><tr><td>Vanilla Linear Attention</td><td>I</td><td>$v_tk_t^T$</td></tr><tr><td>RetNet（2023.07）</td><td>$\gamma$</td><td>$v_tk_t^T$</td></tr><tr><td>Mamba 2（2024.05）</td><td>$diag(\alpha_tI)$</td><td>$v_tk_t^T$</td></tr><tr><td>DeltaNet（2024.06）</td><td>$\alpha_i(I-\beta_tk_tk_t^T)$</td><td>$\beta_tv_tk_t^T$</td></tr><tr><td>Gated DeltaNet（2024.12）</td><td>$I-\beta_tk_tk_t^T$</td><td>$\beta_tv_tk_t^T$</td></tr><tr><td>RWKV-5（2023.11）&#x2F;RWKV-6（2024.05）</td><td>$diag(w_t)$</td><td>$v_tk_t^T$</td></tr><tr><td>HGRN-2（2024.07）</td><td>$diag(w_t)$</td><td>$v_t(1-w_t)^T$</td></tr><tr><td>RWKV-7（2025.03）</td><td>$diag(w_t)-\kappa_i(a_i\odot \kappa_i^T)$</td><td>$v_tk_t^T$</td></tr></tbody></table><p>RKKV-7较复杂，$\kappa$代表对键进行部分移除，还会进行归一化，w代表衰减率。（且RWKV并不是$o_t&#x3D;S_qo_t$的形式，像RWKV6则是$o_t&#x3D;(S_{t-1}+(d\odot v_t)k_t^T)q_t$，似乎在论文中并未找到如何设计的缘由，可能更多是Bo Peng的灵感一现）</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://leloykun.github.io/ponder/blockmat-linear-attn/">https://leloykun.github.io/ponder/blockmat-linear-attn/</a></p><p><a href="https://kexue.fm/archives/11033">https://kexue.fm/archives/11033</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>方班128期研讨班涉及的一些论文阅读</title>
    <link href="/2025/20250706/"/>
    <url>/2025/20250706/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《方班研讨班课需要把握的要点》——方班示范班第128期研讨厅（复盘课）暨电子科技大学方班实验班成立仪式</p><p>虽然这次讲座的核心并不是论文，而是学习方法，但其中提到的不少论文也值得阅读。</p><p>全讲座大概3小时，不得不感慨知识密度之密集和院士精力之充沛。</p><hr><span id="more"></span><h2 id="LLMembed"><a href="#LLMembed" class="headerlink" title="LLMembed"></a>LLMembed</h2><p>acl 2024<br>模型结构如下，即综合多个模型，在训练过程中仅训练分类器头的参数。对于 llama2，使用多个网络深度提取嵌入，并通过池化操作进行融合，以提高嵌入的泛化能力。</p><p><img src="/2025/20250706/LLMebed.jpg"></p><h2 id="Toward-Efficient-Inference-for-Mixture-of-Experts"><a href="#Toward-Efficient-Inference-for-Mixture-of-Experts" class="headerlink" title="Toward Efficient Inference for Mixture of Experts"></a>Toward Efficient Inference for Mixture of Experts</h2><p>（NIPS 2024） FAIR</p><p><strong>动态门控和专家缓冲优化</strong></p><p><img src="/2025/20250706/moe.jpg"></p><p><strong>负载均衡优化</strong></p><p>首先进行的是贪心算法，按专家的平均历史负载对专家进行排序，并依次分配专家。</p><p><strong>反相关平衡。</strong> 当专家激活独立时（LM、MT‑Encoder），贪婪算法是有效的，但当 激活相关时（MT‑Decoder），它的效果较差。改进是计算负载加上了0.5*皮尔逊相关系数。</p><h2 id="Parameter-Disparities-Dissection-for-Backdoor-Defense-in-Heterogeneous-Federated-Learning"><a href="#Parameter-Disparities-Dissection-for-Backdoor-Defense-in-Heterogeneous-Federated-Learning" class="headerlink" title="Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning"></a>Parameter Disparities Dissection for Backdoor Defense in Heterogeneous Federated Learning</h2><p>（NIPS2024）</p><p><img src="/2025/20250706/FDCR.jpg"></p><h3 id="客户端差异聚类-Fisher-Client-Discrepancy-Cluster-FCDC"><a href="#客户端差异聚类-Fisher-Client-Discrepancy-Cluster-FCDC" class="headerlink" title="客户端差异聚类 (Fisher Client Discrepancy Cluster, FCDC)"></a>客户端差异聚类 (Fisher Client Discrepancy Cluster, FCDC)</h3><p>该组件旨在检测和隔离恶意攻击者 。</p><p>1.<strong>计算参数重要性</strong>: 每个客户端在本地训练后，在其本地数据上计算 FIM (Fisher信息矩阵，Fisher Information Matrix, FIM） 的近似值，以获得一个代表每个参数重要性的向量 。（对应伪代码的5）</p><p>在神经网络理论区域，Fisher矩阵可以用hessian近似，而hessian是一个计算量比较大的数值（所以使用hessian的牛顿法比梯度下降法更不流行）。足以见FIM计算量并不小，所以需要近似以加速。</p><p>论文近似的方法为直接使用对角线元素。FIM 的对角线元素衡量的是<strong>单个参数</strong>的重要性，而非对角线元素则描述了<strong>不同参数之间的相关性或交互影响</strong>。取对角线相当于忽略了不同参数之间的相关性。</p><p>然后对该重要性得分进行归一化 （对应伪代码的6）。</p><p>2.<strong>重加权梯度</strong>: 服务器根据每个客户端计算出的参数重要性得分，对其上传的梯度进行重加权 。这一步强调了客户端认为对其本地任务重要的参数更新。</p><p><img src="/2025/20250706/update.jpg"></p><p>3.<strong>衡量差异</strong>: 服务器从这些重加权的梯度中计算出一个聚合的全局梯度 。然后，它衡量每个客户端的重加权梯度与这个全局梯度之间的差异（平方差）。其直觉是，专注于不同分布的恶意客户端将显示出较大的差异 。</p><p>4.<strong>聚类与排除</strong>: 服务器使用一种无参数的聚类算法 (FINCH) 对这些差异值进行聚类，将客户端分组 。平均差异较大的聚类被标记为恶意，并通过将其聚合权重设置为零来从最终聚合中排除 。</p><h3 id="参数重缩放聚合-Fisher-Parameter-Rescale-Aggregation-FPRA"><a href="#参数重缩放聚合-Fisher-Parameter-Rescale-Aggregation-FPRA" class="headerlink" title="参数重缩放聚合 (Fisher Parameter Rescale Aggregation, FPRA)"></a>参数重缩放聚合 (Fisher Parameter Rescale Aggregation, FPRA)</h3><p>在过滤掉恶意客户端后，FPRA 旨在通过智能地聚合良性客户端的更新来改进学习过程 。</p><ul><li><p>FPRA 不在聚合过程中平等对待所有参数，而是根据每个参数元素的重要性来重新缩放其更新 。</p></li><li><p>其目标是为被认为重要的参数提供更显著的更新（更高的“可变范围”），从而加速它们对目标分布的适应，同时减弱次要参数的影响 。</p></li></ul><p><img src="/2025/20250706/sig.jpg"></p><h2 id="RAPIER"><a href="#RAPIER" class="headerlink" title="RAPIER"></a>RAPIER</h2><p>（NDSS2024）</p><p>动机：良性与恶意流量数据之间具有分布差异</p><p>观察到正常数据的分布倾向于相似且稠密，而恶意数据（可能由大量恶意软件生成）的分布则趋于稀疏。</p><p><img src="/2025/20250706/rapier.jpg"></p><p><strong>标签矫正。</strong></p><p>使用MADE来进行分布估计。</p><p><img src="/2025/20250706/MADE.jpg"></p><p>对标签修正后的数据集通过集成学习推断 真实标签。即构建了七个经典机器学习分类器的集成，包括线性判别分析、AdaBoost、随机森林、逻辑回归、高斯朴素贝叶斯、SVC 和 XGBoost。</p><p><strong>数据增强。</strong></p><p>对标 签校正的原始训练集进行数据增强。使用GAN进行三种数据增强。</p><p><img src="/2025/20250706/gan.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>VisualMixer-通过像素重排来保护视觉DNN任务的训练数据</title>
    <link href="/2025/20250705/"/>
    <url>/2025/20250705/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（NDSS 2024）该论文提出了一种通过打乱像素来保护DNN图像数据视觉隐私的方法。（很奇妙的方法）</p><span id="more"></span><h2 id="现有技术的局限性："><a href="#现有技术的局限性：" class="headerlink" title="现有技术的局限性："></a><strong>现有技术的局限性</strong>：</h2><ul><li><p><strong>差分隐私 (Differential Privacy, DP)</strong>：虽然DP能提供强大的隐私保证，但它通过添加噪声来实现，这些噪声对于人眼来说很容易被过滤掉，因此无法有效保护图像的“视觉特征” 。同时，要达到足够的视觉混淆程度，DP引入的噪声会严重损害模型的准确性 。</p></li><li><p><strong>同态加密 (Homomorphic Encryption, HE)</strong>：HE允许在加密数据上进行计算，但其巨大的计算开销使得它在处理高维图像数据时不太实用 。</p></li><li><p><strong>可信执行环境 (TEE)</strong>：TEE依赖特定的硬件，并且其有限的资源和兼容性问题限制了在DNN计算中的应用 。</p></li></ul><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>主要关注以下三种攻击。<br><strong>访问客户端上传的数据。</strong> 服务器上的攻击者可以直接访问客户端上传的数据。即使客户端对其图像进行了混淆，攻击者仍会尝试通过暴力破解或启发式攻击来恢复原始视觉特征。 </p><p><strong>重构 DNN 训练数据。</strong> 攻击者可以对训练好的 DNN 模型进行成员推断攻击，以识别训练过程中所<br>用数据的所有权 。此外，他们可以利用数据增强方法，如基于 GAN 的数据重构，生成视觉特征，并根据客户端上传数据训练的模型权重恢复部分私有训练数据的信息。 </p><p><strong>恢复中间梯度和特征。</strong> 攻击者基于训练和推理过程中的中间梯度和特征图重构视觉上可区分的图像</p><h3 id="视觉特征熵-Visual-Feature-Entropy-VFE"><a href="#视觉特征熵-Visual-Feature-Entropy-VFE" class="headerlink" title="视觉特征熵 (Visual Feature Entropy, VFE)"></a>视觉特征熵 (Visual Feature Entropy, VFE)</h3><p>论文首先定义了一个视觉特征熵的概念。</p><p><img src="/2025/20250705/1.jpg"></p><p>论文中说：VFE值越高，代表图像的梯度变化越密集，视觉上越难识别，隐私性越强 。这个指标是与具体任务无关的（task-agnostic），因此具有普适性 。</p><p>其实这和用于边缘检测的sobel算子很相似。</p><p><img src="/2025/20250705/1.png"></p><p>另一角度来说，包含边缘信息可能包含更多的纹理和细节，可能包含更多有用的特征。</p><h3 id="VisualMixer-VIM"><a href="#VisualMixer-VIM" class="headerlink" title="VisualMixer (VIM)"></a>VisualMixer (VIM)</h3><p>这一部分用来打乱图像。</p><p><strong>非均匀混淆 (Non-uniform Shuffling)</strong>：与在整张图上采用统一策略不同，VisualMixer认为图像的不同区域包含不同的信息量 。它根据VFE值对图像进行非均匀处理：</p><ul><li><p>对于VFE值较低的区域（通常是背景等平滑区域），使用较大的混洗窗口（Window Size, WS），混淆强度更高 。</p></li><li><p>对于VFE值较高的区域（通常是包含重要特征的区域），使用较小的混洗窗口，以保留更多结构信息，保证数据可用性 。</p></li></ul><p><strong>空间与通道混洗 (Spatial and Per-channel Shuffling)</strong>：在每个<strong>选定的窗口</strong> 内，VisualMixer执行两种混洗操作：</p><ul><li><p><strong>空间混洗</strong>：在二维空间上随机打乱像素的位置 。</p></li><li><p><strong>通道混洗</strong>：在每个颜色通道（R, G, B）内部独立地混洗像素值 。这能有效破坏颜色特征，同时对纹理和结构特征影响较小，这些特征对视觉任务更为重要 。</p></li></ul><p><img src="/2025/20250705/model.jpg"></p><p>per-channel-shuffling让我想到了<a href="https://zhuanlan.zhihu.com/p/636551276">RWKV中的time mixing</a>，虽然它们不是一个东西。</p><p><img src="/2025/20250705/2.jpg"></p><h3 id="ST-Adam-优化器"><a href="#ST-Adam-优化器" class="headerlink" title="ST-Adam 优化器"></a>ST-Adam 优化器</h3><p>经过VisualMixer处理的图像，其梯度在训练过程中会产生剧烈波动，即<strong>梯度震荡 (gradient oscillation)</strong> 。这使得像Adam这样的标准优化器难以收敛，或者容易陷入局部最优解 。</p><table><thead><tr><th align="left">步骤</th><th align="left">ST-Adam</th><th align="left">Adam</th></tr></thead><tbody><tr><td align="left"><strong>1. 梯度计算</strong></td><td align="left">$$g _ {t}&#x3D;\nabla f(w _ {t})$$</td><td align="left">$$g _ {t}&#x3D;\nabla f(w _ {t})$$</td></tr><tr><td align="left"><strong>2. 动量更新</strong></td><td align="left">$$m _ {t}&#x3D;\beta m _ {t-1}+(1-\beta)g _ {t}$$</td><td align="left">$$m _ {t}&#x3D;\beta m _ {t-1}+(1-\beta)g _ {t}$$</td></tr><tr><td align="left"><strong>3. 二阶矩更新</strong></td><td align="left">$$v _ {t}&#x3D; \gamma v _ {t-1}+(1-\gamma)g _ {t}^{2}$$</td><td align="left">$$v _ {t}&#x3D; \gamma v _ {t-1}+(1-\gamma)g _ {t}^{2}$$</td></tr><tr><td align="left"><strong>4. 偏差校正</strong></td><td align="left"><strong>无此步骤</strong></td><td align="left">$$\hat{m} _ {t}&#x3D;\frac{m _ {t}}{1-\beta^{t}}$$ $$\hat{v} _ {t}&#x3D;\frac{v _ {t}}{1-\gamma^{t}}$$</td></tr><tr><td align="left"><strong>5. 参数更新</strong></td><td align="left">$$w _ {t+1}&#x3D;w _ {t}-\eta \frac{m _ {t}}{\sqrt{v _ {t}}+\epsilon}$$</td><td align="left">$$w _ {t+1}&#x3D;w _ {t}-\eta \frac{\hat{m} _ {t}}{\sqrt{\hat{v} _ {t}}+\epsilon}$$</td></tr></tbody></table><p>作者认为：在 VisualMixer的场景中，由于混合操作，梯度更有可能在有限的范围内剧烈变化。也就是说，灵活的更新步长对我们的模型训练过程来说更像是一种毒药而非益处。尽管自适应更新步长可以让模型更快地收敛，但它也使得模型更有可能陷入局部极小。因此，保持更新步长稳定是更好的选择，能避免模型陷入局部极小。</p>]]></content>
    
    
    
    <tags>
      
      <tag>计算机视觉</tag>
      
      <tag>隐私保护</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于核复杂度的无需训练的防御方法</title>
    <link href="/2025/20250617/"/>
    <url>/2025/20250617/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>arxiv，《<a href="https://arxiv.org/abs/2506.11611">KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity</a>》</p><span id="more"></span><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2025/20250617/1.jpg"></p><p><img src="/2025/20250617/2.jpg"></p><p><img src="/2025/20250617/3.jpg"></p><p>（没有找到附录。）</p><p>然后有：</p><p><img src="/2025/20250617/4.jpg"></p><p>我们可以可以移除具有较高 KC 得分的边，以确保图神经网络（GNN）的性能。</p><p>最终伪代码为：</p><p><img src="/2025/20250617/5.jpg"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2025/20250617/6.jpg"></p><p>并和RGCN、ProGNN、GNN-SVD 、GNN-Jaccard、GNNGuard 相比都是SOTA。</p><blockquote><p>模型有点太老了。</p></blockquote><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><p>首先，它主要关注结构扰动，意味着它本身并不针对结点特征的对抗攻击。</p><p>其次，作为一种边净化技术，KCES 不直接适用于本质上以边为中心的基于图形的任务。</p><p>第三，尽管其实现与模型无关，但KCES 的理论基础依赖于 GNN 特有的假设。故只能用于GNN。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>传统kmeans并不是（局部）最优</title>
    <link href="/2025/20250615/"/>
    <url>/2025/20250615/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>(ICML 2025) 《Modified K-means Algorithm with Local Optimality Guarantees》</p><p>K-means是广泛使用的聚类算法，尽管它不一定能保证全局最优，但大家都约定速成地认为K-means会收敛到局部最优解。</p><p><strong>但其实不然。</strong></p><span id="more"></span><h2 id="定义问题"><a href="#定义问题" class="headerlink" title="定义问题"></a>定义问题</h2><p>在提出反例之前我们先提出一些定义。</p><p>我们将广泛使用的欧式距离拓展到Bregman 散度。<br>$$<br>D_\phi(x,y)&#x3D;\phi(x)-\phi(y)-&lt;x-y,\nabla\phi(y)&gt;<br>$$<br><strong>Bregman散度</strong>是借用一个严格凸函数φ（·），该散度即为φ（x）和h（x）的差，其中h（x）为φ（y）在x处的线性近似。</p><p><img src="/2025/20250615/2.png"></p><p>更多的：</p><p><img src="/2025/20250615/3.jpg"></p><p>两种局部最优的定义：</p><p><strong>D-local: Discrete locally optimal solution</strong></p><p>即若把某个数据点移动到任意簇后，损失都会升高。</p><p><strong>C-local: Continuous locally optimal solution</strong></p><p>即在某个数据点的附近一个领域内，无论怎么移动，损失都会升高。</p><p>可以看出D-local更强。</p><h2 id="反例"><a href="#反例" class="headerlink" title="反例"></a>反例</h2><p>点的数量为 N &#x3D; 5，簇的数量为K &#x3D; 2，相似度度量为 $D(x, y) &#x3D;||x-y||^2_2$，给定的数据集和初始中心如下：</p><p>x1 &#x3D; −4, x2 &#x3D; −2, x3 &#x3D; 0, x4 &#x3D; 1.5, x5 &#x3D; 2.5,<br>c1 &#x3D; x3 &#x3D; 0, c2 &#x3D; x5 &#x3D; 2.5<br>$$<br>p^\ast&#x3D;\begin{bmatrix}<br>1,1,1,0,0\\0,0,0,1,1<br>\end{bmatrix},c_1^\ast&#x3D;-2,c_2^\ast&#x3D;2<br>$$<br>但是他并不满足C-local或D-local。</p><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><p>为了满足C-local，我们找数据点 <code>n</code> 满足至少两个或以上的簇中心在可允许误差范围内同样近。修改它的簇为另外的簇。</p><p>为了满足D-local，我们检查所有的数据点，遍历移动到其他簇是否损失会降低。</p><p><img src="/2025/20250615/1.jpg"></p><p><img src="/2025/20250615/2.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>V-JEPA 2</title>
    <link href="/2025/20250613/"/>
    <url>/2025/20250613/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在23年，本Blog已经介绍过JEPA，也就是lecun推出的世界模型结构。</p><p>Lecun一直不相信当前的LLM，而布局于“世界模型”。</p><p>当时主要是对图片处理，也就是Image-JEPA。而V-JEPA代表是对视频（Video）处理。</p><span id="more"></span><p>不妨让我们回顾一下。</p><h2 id="JEPA和I-JEPA"><a href="#JEPA和I-JEPA" class="headerlink" title="JEPA和I-JEPA"></a>JEPA和I-JEPA</h2><p>Joint-Embedding Predictive Architecture（JEPA）是一种架构，它从单个上下文块中学习预测图像中不同目标块的表示，并使用掩码策略来引导模型生成语义表示。</p><p><img src="/2025/20250613/3.jpg"></p><p>而I-JEPA的框架为：</p><p><img src="/2025/20250613/2.jpg"></p><p>其中这一结构也有点类似对比学习。</p><p><img src="/2025/20250613/1_0brkjcnDkgEcWpFpe0bxeg.webp"></p><p>I-JEPA有点像Simsiam，而后续介绍的V-JEPA有点像BYOL。</p><blockquote><p> 题外话：</p><p>有人指出BYOL起作用是因为其中的batch norm，BYOL作者写了一篇《BYOL works even without batch statistics》做了一些实验证明了BN并不是必要的。BN只是保证模型稳定的技巧，一个合适的初始化才是更重要的。</p></blockquote><h2 id="V-JEPA-2"><a href="#V-JEPA-2" class="headerlink" title="V-JEPA 2"></a>V-JEPA 2</h2><p>基本概述如下图所示。V-JEPA 2 采用分阶段训练流程，首先在互联网规模的视频上进行无动作的预训练，随后利用少量交互数据进行后训练。在第一阶段，采用掩码降噪特征预测目标 ，模型在学成的表示空间中预测视频的掩码片段。使用多达 10 亿参数和超过 100 万小时的视频来训练 V-JEPA 2 编码器。</p><p>在互联网规模的视频上进行预训练后，利用第一阶段学成的表示，在少量交互数据上训练了一个动作条件的世界模型 V-JEPA 2-AC。动作条件世界模型是一个拥有 3 亿参数的 Transformer 网络，采用了块因果注意力机制，能够自回归地预测在给定动作和先前状态下的下一帧视频的表示。该模型在给定子目标的情况下，仅使用来自 Droid 数据集 的 62 小时未标记交互数据，可用于规划 Franka 机械臂的动作，并在新环境中通过单目 RGB 摄像头 zero-shot 执行抓取操作任务。</p><p><img src="/2025/20250613/1.jpg"></p><p>算法架构如下图所示：</p><p><img src="/2025/20250613/4.jpg"></p><p>左图有点像BYOL，会有EMA和stop grad。</p><p>V-JEPA 2-AC会涉及以下损失：</p><p><img src="/2025/20250613/5.jpg"></p><p>对于接下来的机械臂动作规划，算法会最小化目标条件的能量函数来规划固定时间范围内的动作序列。然后执行第一个动作，观察新状态，并重复该过程。</p><p><img src="/2025/20250613/6.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>学会排序算法</title>
    <link href="/2025/20250610/"/>
    <url>/2025/20250610/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>Learning-to-Rank (LTR)</strong> 是一种运用机器学习技术来解决排序问题的领域。它的核心目标是，训练一个模型，让这个模型能自动地对一个项目列表进行优化排序，使得排序结果尽可能地好。</p><p>常用的算法有RankNet和LambdaRank等。</p><span id="more"></span><h2 id="RankNet"><a href="#RankNet" class="headerlink" title="RankNet"></a>RankNet</h2><p>我们定义一个概率$P _ {ij}$代表i排在j前面的真实概率，我们定义1代表更重要&#x2F;相关，0.5代表相同重要&#x2F;相关，0代表更不重要&#x2F;相关。</p><p>同时，我们会对输出的分数$s_i$和$s_j$进行转换，RankNet使用的是<br>$$<br>\hat{P} _ {ij} &#x3D; \frac{1}{1 + e^{-\alpha(s_i - s_j)}}<br>$$<br>其中$\alpha$用了控制sigmoid（$\sigma$）的形状，注意这里是$\sigma(s_i-s_j)$，而不同于AUC中的$\sigma(s_i)-\sigma(s_j)$。</p><p>定义交叉熵损失函数为：</p><p>$$ C _ {ij} &#x3D; -P _ {ij} \log(\hat{P} _ {ij}) - (1 - P _ {ij}) \log(1 - \hat{P} _ {ij}) $$</p><p>求偏导有：<br>$$<br>\begin{align}<br>\frac{\partial{C}}{\partial{w_k}}&amp;&#x3D;\frac{\partial{C}}{\partial{s_i}}\frac{\partial{s_i}}{\partial{w_k}}+\frac{\partial{C}}{\partial{s_j}}\frac{\partial{s_j}}{\partial{w_k}}\\&amp;&#x3D;\alpha\left(P _ {ij}-\hat P _ {ij}\right)\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)<br>\\&amp;&#x3D;\lambda _ {ij}\left(\frac{\partial{s_i}}{\partial{w_k}}-\frac{\partial{s_j}}{\partial{w_k}}\right)<br>\end{align}<br>$$<br>$\lambda _ {ij}$也被成为lambda梯度。</p><h2 id="LambdaRank"><a href="#LambdaRank" class="headerlink" title="LambdaRank"></a>LambdaRank</h2><p>RankNet一个显明的缺点是，不直接与各种排序指标相关。故LambdaRank将其引入到其算法中，修改为<br>$$<br>\lambda _ {LambdaRank}&#x3D;\lambda _ {ij}\cdot|\Delta metric|<br>$$<br>Xgboost中有<code>rank:ndcg</code>、<code>rank:map</code>、<code>rank:pairwise</code>，前两种分别对应NDCG和map指标，后一种类似RankNet，即$|\Delta|$&#x3D;1。</p><h2 id="与AUC自定义损失的联系"><a href="#与AUC自定义损失的联系" class="headerlink" title="与AUC自定义损失的联系"></a>与AUC自定义损失的联系</h2><p>AUC其实也是一种排序问题，但我们通常使用交叉熵隐含地去优化分类任务以达到更好的排序。</p><p>另外，在这里的交叉熵它可以写成softplus的形式，这也是0-1损失的替代损失函数之一，另外还有hinge-loss等等。所以，排序问题和直接优化AUC，采取的思想是类似的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>流模型与流匹配</title>
    <link href="/2025/20250609/"/>
    <url>/2025/20250609/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>流模型与流匹配介绍。</p><span id="more"></span><h1 id="归一化流（Normalizing-Flow）"><a href="#归一化流（Normalizing-Flow）" class="headerlink" title="归一化流（Normalizing Flow）"></a>归一化流（Normalizing Flow）</h1><h2 id="先导"><a href="#先导" class="headerlink" title="先导"></a>先导</h2><p>我们首先会定义了一个分布 $p _ {z}(z)$,有时也称为基础分布，以及一个由深度神经网络给出的非线性函数 $x&#x3D;f(z,w)$，该函数将潜在空间（latent space）转换为数据空间。</p><p> $p _ {z}(z)$ 会尽可能简单,例如高斯分布，以便从这种模型中进行采样。</p><p>为了衡量分布的相似度，$g^\ast &#x3D;argmax\sum logP(x)\approx argmin (KL(P _ {data}||P_G))$，我们需要计算此模型的似然函数,我们需要数据空间分布，这取决于神经网络的逆函数。</p><p>我们将其写为 $z&#x3D;g(x,w)$，它满足 $z&#x3D;g(f(z,w),w)$。即对于w的每个值,函数 $f(z,w)$ 和 $g(x,w)$ 是可逆的。</p><p>然后我们可以使用变量变换公式（change of variable theorem）来计算数据密度:</p><p>$$<br>p _ {x}(x|w)&#x3D;p _ {z}(g(x,w))|det J(x)|<br>$$</p><p>其中 $J(x)$ 是雅可比矩阵，$J _ {ij}(x)&#x3D;\frac{\partial g _ {i}(x,w)}{\partial x _ {j}}$ 。</p><p><img src="/2025/20250609/change.jpg"></p><p>然而，要求可逆映射的一个后果是，隐式空间的维度必须与数据空间的维度相同，这可能导致高维数据(如图像，动辄就是几百乘几百的维度)的大型模型的复杂性增加。比如计算 $D\times D$ 矩阵的行列式的成本是 $\mathcal{O}(D^{3})$。</p><p>所以，我们希望对模型施加一些进一步的限制,以便更有效地计算雅可比矩阵行列式。</p><p>如果我们考虑一个独立数据点训练集 $\mathcal{D}&#x3D;{x _ {1},…,x _ {N}}$，则：</p><p>$$<br>\begin{align}<br>lnp(\mathcal{D}|w)&amp;&#x3D;\sum _ {n&#x3D;1}^{N}lnp _ {x}(x _ {n}|w)<br>\\<br>&amp;&#x3D;\sum _ {n&#x3D;1}^{N}{ln<del>p _ {z}(g(x _ {n},w))+ln|det</del>J(x _ {n})|}<br>\end{align}<br>$$</p><p>并且我们的目标是通过似然函数来训练神经网络。为了能够模拟各种分布,我们希望转换函数 $x&#x3D;f(z,w)$ 具有高度的灵活性,因此我们使用<strong>深度</strong> 神经网络架构。如果我们使网络中的每一层都是可逆的,那么我们可以确保整个函数是可逆的。为了看到这一点,考虑三个连续的变换,每个变换对应网络的一层,形式如下: </p><p>$$<br>x&#x3D;f^{A}(f^{B}(f^{C}(z)))<br>$$</p><p>那么逆函数由下式给出：</p><p>$$<br>z&#x3D;g^{C}(g^{B}(g^{A}(x)))<br>$$</p><p>对于雅克比矩阵也可以使用链式法则化为各个层的乘积。</p><p>这种建模分布的方法称为归一化流（Normalizing Flow）。流，是因为通过一系列映射转换概率分布的过程与流体的流动有些类似。归一化，是因为逆映射的效果是将复杂的数据分布转换为归一化形式,通常是高斯或正态分布。</p><h2 id="耦合流（Coupling-Flows）"><a href="#耦合流（Coupling-Flows）" class="headerlink" title="耦合流（Coupling Flows）"></a>耦合流（Coupling Flows）</h2><p>我们的目标是为单个可逆函数层,以便我们可以将许多层组合在一起来定义一个高度灵活的可逆函数类。</p><p>首先考虑一个形式为$x&#x3D;az+b$，它很容易求逆,得到 $z&#x3D;\frac{1}{a}(x-b)$ 。</p><p>但是，线性变换在复合下是封闭的——一系列线性变换等效于一个整体线性变换。此外,高斯分布的线性变换仍然是高斯分布。因此,即使我们有许多这样的线性变换,我们也只会得到一个高斯分布。</p><p>问题是，我们能否在保持线性变换可逆性的同时,允许额外的灵活性,以便结果分布是非高斯的呢？</p><p>这个问题的一个解决方案是由一种称为RealNVP<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><span class="hint--top hint--rounded" aria-label="Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio. Density estimation using Real NVP">[1]</span></a></sup> 的归一化流模型给出,它是“real-valued non-volume preserving”的缩写。</p><p>想法是将潜在变量向量z分成两部分 $z&#x3D;(z _ {A},z _ {B})$,使得如果z的维度是 $D _ {i}$ 而 $Z _ {A}$ 的维度是d,那么 $z _ {B}$ 的维度是 $D-d _ {n}$ 。</p><p>我们同样将输出向量 $x&#x3D;(x _ {A},x _ {B})$ 划分,其中 $x _ {A}$ 的维度为d。</p><p>而对于输出向量的第一部分,我们简单地复制输入: $x _ {A}&#x3D;z _ {A}$ ，$x _ {B}$ 的维度为 $D-d _ {\circ}$</p><p>向量的第二部分经过线性变换,但现在线性变换中的系数由 $Z _ {A}$ 的非线性函数给出：</p><p>$$x _ {B}&#x3D;exp(s(z _ {A},w))\odot z _ {B}+b(z _ {A},w)$$</p><p>其中 $s(z _ {A},w)$ 和 $b(z _ {A},w)$ 是神经网络的实值输出,指数函数确保乘法项为非负。这里表示涉及两个向量逐元素乘法的Hadamard积。</p><p>由于使用了神经网络函数, $x$ 的值可以是非常灵活的函数。</p><p>整体变换也很容易可逆：给定一个 $x&#x3D;(x _ {A},x _ {B})$的值，我们首先计算$z _ {A}&#x3D;x _ {A}$ ，然后有$z _ {B}&#x3D;exp(-s(z _ {A},w))\odot(x _ {B}-b(z _ {A},w))$。</p><p><img src="/2025/20250609/realnvp.jpg"></p><p>值得注意的是， $s(z;w)$ 和 $b(z;w)$ 无需可逆。</p><p>现在考虑雅可比矩阵及其行列式。我们可以将雅可比矩阵分成块,对应于z和x的划分,得到:</p><p>$$<br>J&#x3D;\begin{bmatrix}\frac{\partial z_A}{\partial x_A}&amp;\frac{\partial z_A}{\partial x_B}\\ \frac{\partial z_B}{\partial x_A}&amp;\frac{\partial z_B}{\partial x_B}\end{bmatrix}&#x3D;\begin{bmatrix}I _ {d}&amp;0\\ \frac{\partial z _ {B}}{\partial x _ {A}}&amp;diag(exp(-s))\end{bmatrix}<br>$$</p><p>我们不在乎左下角的值，因为它是一个下三角矩阵，行列式直接是主对角线元素的乘积。因此,雅可比行列式简单地由 $exp(-s(z _ {A},w))$ 的元素乘积给出。</p><p>这种方法的一个明显限制是 $z _ {A}$ 的值不受变换的影响。这很容易通过添加另一层来解决,在该层中 $z _ {A}$ 和 $z _ {B}$ 的角色被反转。然后可以重复这种双层结构多次。</p><p><img src="/2025/20250609/morenvp.jpg"></p><p>整体训练过程涉及创建数据点的mini-batches。对于形式为 $\mathcal{N}(z|0,I)$ 的潜在分布,对数密度简单地是 $-||z||^{2}&#x2F;2$,仅差一个加性常数。<br>$$<br>\ln(\frac{1}{\sqrt{2\pi}}\exp(-\frac{z^2}{2}))&#x3D;\ln(\frac{1}{\sqrt{2\pi}})+\frac{-||z||^2}{2}<br>$$<br>另外，Real NVP模型中会使用BatchNorm，这也会使得多一些变换行列式。</p><p>RealNVP 模型属于一类称为耦合流（Coupling Flows）的正则化流,其中线性变换$x _ {B}&#x3D;exp(s(z _ {A},w))\odot z _ {B}+b(z _ {A},w)$被更一般的形式$x _ {B}&#x3D;h(z _ {B},g(z _ {A},w))$ 所替代。</p><p>其中 $h(z _ {B}g)$ 是一个关于 $z _ {B}$ 的函数，对于任何给定的g值都是可逆的,称为耦合函数。函数 $g(z _ {A},w)$ 称为条件器（conditioner）,通常由神经网络表示。</p><p>笔者也动手实现了realnvp的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CouplingLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim, hidden_dim, mask_type=<span class="hljs-string">&#x27;even&#x27;</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.input_dim = input_dim<br>        <span class="hljs-variable language_">self</span>.mask = torch.zeros(input_dim, device=device)<br>        <span class="hljs-keyword">if</span> mask_type == <span class="hljs-string">&#x27;even&#x27;</span>:<br>            <span class="hljs-variable language_">self</span>.mask[::<span class="hljs-number">2</span>] = <span class="hljs-number">1</span>  <span class="hljs-comment"># Condition on even, transform odd</span><br>        <span class="hljs-keyword">else</span>: <span class="hljs-comment"># &#x27;odd&#x27;</span><br>            <span class="hljs-variable language_">self</span>.mask[<span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = <span class="hljs-number">1</span> <span class="hljs-comment"># Condition on odd, transform even</span><br>        <br>        <span class="hljs-variable language_">self</span>.s_net = nn.Sequential(<br>            nn.Linear(input_dim, hidden_dim),<br>            nn.ReLU(),<br>            nn.Linear(hidden_dim, hidden_dim),<br>            nn.ReLU(),<br>            nn.Linear(hidden_dim, input_dim),<br>            nn.Tanh() <span class="hljs-comment"># To stabilize: limits scale factor somewhat</span><br>        )<br>        <span class="hljs-variable language_">self</span>.t_net = nn.Sequential(<br>            nn.Linear(input_dim, hidden_dim),<br>            nn.ReLU(),<br>            nn.Linear(hidden_dim, hidden_dim),<br>            nn.ReLU(),<br>            nn.Linear(hidden_dim, input_dim)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot; x -&gt; z (inverse transformation for density estimation) &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># x_masked conditions the transformation</span><br>        x_masked = x * <span class="hljs-variable language_">self</span>.mask <br>        s = <span class="hljs-variable language_">self</span>.s_net(x_masked) * (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.mask) <span class="hljs-comment"># s applies only to non-masked part</span><br>        t = <span class="hljs-variable language_">self</span>.t_net(x_masked) * (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.mask) <span class="hljs-comment"># t applies only to non-masked part</span><br>        <br>        <span class="hljs-comment"># Inverse affine transformation: z_B = (x_B - t) * exp(-s)</span><br>        z = (x * (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.mask) - t) * torch.exp(-s) + x_masked<br>        <br>        <span class="hljs-comment"># Log determinant of Jacobian: sum of -s for the transformed part</span><br>        log_det_jacobian = (-s).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> z, log_det_jacobian<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inverse</span>(<span class="hljs-params">self, z</span>):<br>        <span class="hljs-string">&quot;&quot;&quot; z -&gt; x (forward transformation for sampling) &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># z_masked conditions the transformation</span><br>        z_masked = z * <span class="hljs-variable language_">self</span>.mask<br>        s = <span class="hljs-variable language_">self</span>.s_net(z_masked) * (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.mask)<br>        t = <span class="hljs-variable language_">self</span>.t_net(z_masked) * (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.mask)<br>        <br>        <span class="hljs-comment"># Affine transformation: x_B = z_B * exp(s) + t</span><br>        x = (z * (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.mask)) * torch.exp(s) + t + z_masked<br>        <br>        <span class="hljs-comment"># For generation, we also need log_det_jacobian (or its inverse) if we were to calc prob of x</span><br>        <span class="hljs-comment"># but here we just need x. The log_det_jacobian for x = f(z) would be sum(s).</span><br>        log_det_jacobian_inv = s.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># log |det J_f| = -log|det J_g|</span><br>        <span class="hljs-keyword">return</span> x, log_det_jacobian_inv<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RealNVP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_layers, input_dim, hidden_dim</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.layers = nn.ModuleList()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_layers):<br>            mask_type = <span class="hljs-string">&#x27;even&#x27;</span> <span class="hljs-keyword">if</span> i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;odd&#x27;</span><br>            <span class="hljs-variable language_">self</span>.layers.append(CouplingLayer(input_dim, hidden_dim, mask_type))<br>        <br>        <span class="hljs-comment"># Base distribution (e.g., standard normal)</span><br>        <span class="hljs-variable language_">self</span>.base_distribution = torch.distributions.Normal(<br>            torch.zeros(input_dim, device=device), <br>            torch.ones(input_dim, device=device)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>): <span class="hljs-comment"># For density estimation x -&gt; z</span><br>        log_det_jacobian_sum = <span class="hljs-number">0</span><br>        z = x<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">reversed</span>(<span class="hljs-variable language_">self</span>.layers): <span class="hljs-comment"># Inverse direction for density</span><br>            z, log_det_jacobian = layer.forward(z) <span class="hljs-comment"># This is g(x)</span><br>            log_det_jacobian_sum += log_det_jacobian<br>        <span class="hljs-keyword">return</span> z, log_det_jacobian_sum<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inverse</span>(<span class="hljs-params">self, z</span>): <span class="hljs-comment"># For sampling z -&gt; x</span><br>        <span class="hljs-comment"># log_det_jacobian_inv_sum = 0 # Not typically needed for just sampling</span><br>        x = z<br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers: <span class="hljs-comment"># Forward direction for sampling</span><br>            x, _ = layer.inverse(x) <span class="hljs-comment"># This is f(z)</span><br>            <span class="hljs-comment"># log_det_jacobian_inv_sum += log_det_jacobian_inv # If we needed prob(x) from f(z)</span><br>        <span class="hljs-keyword">return</span> x<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">log_prob</span>(<span class="hljs-params">self, x</span>):<br>        z, log_det_jacobian_sum = <span class="hljs-variable language_">self</span>.forward(x)<br>        <span class="hljs-comment"># log p(x) = log p(z) + log |det J_g|</span><br>        log_likelihood_z = <span class="hljs-variable language_">self</span>.base_distribution.log_prob(z).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> log_likelihood_z + log_det_jacobian_sum<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">sample</span>(<span class="hljs-params">self, num_samples</span>):<br>        z = <span class="hljs-variable language_">self</span>.base_distribution.sample((num_samples,))<br>        x_generated = <span class="hljs-variable language_">self</span>.inverse(z)<br>        <span class="hljs-keyword">return</span> x_generated<br></code></pre></td></tr></table></figure><p>值得注意的是，图像是离散数据，而Realnvp是针对连续数据，我们需要做一些处理才可能成功运行。</p><p>一个最简单的方法是使用autoencoder，并用潜在向量来训练realnvp。神经网络是连续的，它可以把离散的图像转为连续的潜在向量，且能通过decoder很好地重建回离散图像。</p><p>另一个方法是反量化（Dequantization）。一个最简单的反量化是对每个离散值添加一个范围在$ [0, 1)$之间的随机噪声，并将其映射为0~1之间，比如使用sigmoid等。</p><p>另一种做法是变分反量化，在flow++<sup id="fnref:2" class="footnote-ref"><a href="#fn:2" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel. Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design">[2]</span></a></sup>中提出（flow++还引入了自注意力），即将反量化过程本身也变成了一个需要学习和优化的模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">VariationalDequantization</span>(<span class="hljs-title class_ inherited__">Dequantization</span>):<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, var_flows, alpha=<span class="hljs-number">1e-5</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Inputs: </span><br><span class="hljs-string">            var_flows - A list of flow transformations to use for modeling q(u|x)</span><br><span class="hljs-string">            alpha - Small constant, see Dequantization for details</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>().__init__(alpha=alpha)<br>        <span class="hljs-variable language_">self</span>.flows = nn.ModuleList(var_flows)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">dequant</span>(<span class="hljs-params">self, z, ldj</span>):<br>        z = z.to(torch.float32)<br>        img = (z / <span class="hljs-number">255.0</span>) * <span class="hljs-number">2</span> - <span class="hljs-number">1</span> <span class="hljs-comment"># We condition the flows on x, i.e. the original image</span><br>        <br>        <span class="hljs-comment"># Prior of u is a uniform distribution as before</span><br>        <span class="hljs-comment"># As most flow transformations are defined on [-infinity,+infinity], we apply an inverse sigmoid first.</span><br>        deq_noise = torch.rand_like(z).detach()<br>        deq_noise, ldj = <span class="hljs-variable language_">self</span>.sigmoid(deq_noise, ldj, reverse=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">for</span> flow <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.flows:<br>            deq_noise, ldj = flow(deq_noise, ldj, reverse=<span class="hljs-literal">False</span>, orig_img=img)<br>        deq_noise, ldj = <span class="hljs-variable language_">self</span>.sigmoid(deq_noise, ldj, reverse=<span class="hljs-literal">False</span>)<br>        <br>        <span class="hljs-comment"># After the flows, apply u as in standard dequantization</span><br>        z = (z + deq_noise) / <span class="hljs-number">256.0</span><br>        ldj -= np.log(<span class="hljs-number">256.0</span>) * np.prod(z.shape[<span class="hljs-number">1</span>:])<br>        <span class="hljs-keyword">return</span> z, ldj<br></code></pre></td></tr></table></figure><p><strong>处理前：</strong> 可以发现训练不一定能成功，有时会坍塌。</p><p><img src="/2025/20250609/fault6.png"></p><p><img src="/2025/20250609/fault16.png"></p><p><strong>处理后：</strong></p><p><img src="/2025/20250609/success.png"></p><p> OpenAI在RealNVP改进得到了Glow模型<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Diederik P. Kingma, Prafulla Dhariwal.Glow: Generative Flow with Invertible 1x1 Convolutions">[3]</span></a></sup>。</p><p>其核心是可逆1x1卷积层，其中巧妙地利用了LU分解。</p><p>即任意矩阵都可以分解为$W&#x3D;PLU$，则$log|det(W)|&#x3D;\sum log|diag(U)|$</p><p>故Glow中采用的是，先随机生成一个正交矩阵，然后做LU分解，得到P,L,U。</p><p>固定P，固定U的对角线的正负号，约束L为对角线全1的下三角阵，U为上三角阵，优化训练L,U的其余参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> scipy <span class="hljs-keyword">import</span> linalg <span class="hljs-keyword">as</span> la<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InvConv2dLU</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channel</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        weight = np.random.randn(in_channel, in_channel)<br>        q, _ = la.qr(weight)<br>        w_p, w_l, w_u = la.lu(q.astype(np.float32)) <span class="hljs-comment"># PLU分解</span><br>        w_s = np.diag(w_u) <span class="hljs-comment"># 计算U对角线元素</span><br>        w_u = np.triu(w_u, <span class="hljs-number">1</span>) <span class="hljs-comment"># 约束U为上三角阵（不包括对角线）</span><br>        u_mask = np.triu(np.ones_like(w_u), <span class="hljs-number">1</span>) <span class="hljs-comment"># 全1上三角阵</span><br>        l_mask = u_mask.T <span class="hljs-comment"># 全1下三角阵</span><br><br>        w_p = torch.from_numpy(w_p)<br>        w_l = torch.from_numpy(w_l)<br>        w_s = torch.from_numpy(w_s)<br>        w_u = torch.from_numpy(w_u)<br><br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;w_p&quot;</span>, w_p)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;u_mask&quot;</span>, torch.from_numpy(u_mask))<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;l_mask&quot;</span>, torch.from_numpy(l_mask))<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;s_sign&quot;</span>, torch.sign(w_s)) <span class="hljs-comment"># U对角线的正负号</span><br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;l_eye&quot;</span>, torch.eye(l_mask.shape[<span class="hljs-number">0</span>])) <span class="hljs-comment"># 单位阵</span><br>        <span class="hljs-variable language_">self</span>.w_l = nn.Parameter(w_l) <span class="hljs-comment"># 可学习参数：L的下三角元素</span><br>        <span class="hljs-variable language_">self</span>.w_s = nn.Parameter(logabs(w_s)) <span class="hljs-comment"># 可学习参数：U的对角元素的对数值</span><br>        <span class="hljs-variable language_">self</span>.w_u = nn.Parameter(w_u) <span class="hljs-comment"># 可学习参数：U的上三角元素</span><br><br>        <span class="hljs-variable language_">self</span>.weight = (<br>            <span class="hljs-variable language_">self</span>.w_p <span class="hljs-comment"># P为固定值</span><br><span class="hljs-meta">            @ (<span class="hljs-params">self.w_l * self.l_mask + self.l_eye</span>) </span><span class="hljs-comment"># 下三角阵L对角线元素全为1，其余参数可学习</span><br><span class="hljs-meta">            @ (<span class="hljs-params">(<span class="hljs-params">self.w_u * self.u_mask</span>) + torch.diag(<span class="hljs-params">self.s_sign * torch.exp(<span class="hljs-params">self.w_s</span>)</span>)</span>) </span><span class="hljs-comment"># 上三角阵U对角线的正负号固定，其余参数可学习</span><br>        )<br>        <span class="hljs-variable language_">self</span>.weight = <span class="hljs-variable language_">self</span>.weight.unsqueeze(<span class="hljs-number">2</span>).unsqueeze(<span class="hljs-number">3</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        _, _, height, width = <span class="hljs-built_in">input</span>.shape<br>        out = F.conv2d(<span class="hljs-built_in">input</span>, <span class="hljs-variable language_">self</span>.weight)<br>        logdet = height * width * torch.<span class="hljs-built_in">sum</span>(<span class="hljs-variable language_">self</span>.w_s)<br>        <span class="hljs-keyword">return</span> out, logdet<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverse</span>(<span class="hljs-params">self, output</span>):<br>        <span class="hljs-keyword">return</span> F.conv2d(output, <span class="hljs-variable language_">self</span>.weight.squeeze().inverse().unsqueeze(<span class="hljs-number">2</span>).unsqueeze(<span class="hljs-number">3</span>))<br></code></pre></td></tr></table></figure><h2 id="自回归流（Autoregressive-Flows）"><a href="#自回归流（Autoregressive-Flows）" class="headerlink" title="自回归流（Autoregressive Flows）"></a>自回归流（Autoregressive Flows）</h2><p>一个与正则化流相关的公式可以从注意到一组变量的联合分布总是可以表示为每个变量的条件分布的乘积来推导。我们可以不失一般性地写出：</p><p>$$p(x _ {1},…,x _ {D})&#x3D;\prod _ {i&#x3D;1}^{D}p(x _ {i}|x _ {1:i-1})$$ </p><p>其中 $x _ {1:i-1}$ 表示 $x _ {1},…,x _ {i-10}$ 这种分解可用于构建一类称为掩码式自回归流(masked autoregressive flow，MAF)的归一化流,其定义为：</p><p>$$x _ {i}&#x3D;h(z _ {i},g _ {i}(x _ {1:i-1},w _ {i}))$$</p><p>这里 $h(z _ {i},\cdot)$ 是耦合函数,其被选择为相对于易于 $z _ {i}$ 求逆的,而g是条件器,通常由深度神经网络表示。掩码式指的是使用单个神经网络来实现上式以及一个二元掩码，该掩码强制网络权重的子集为零以实现自回归约束。</p><p><img src="/2025/20250609/ag.jpg"></p><p>在这种情况下,用于评估似然函数的逆向计算需要给出：</p><p>$z _ {i}&#x3D;h^{-1}(x _ {i},g _ {i}(x _ {1:i-1},w _ {i}))$</p><p>因为上式中用于评估的各个函数 $z _ {1},…,z _ {D}$ 可以并行评估，因此可以在现代硬件上高效执行。</p><p>与上式对应的雅可比矩阵的元素 $\partial z _ {i}&#x2F;\partial x _ {j}$ 它们形成一个上三角矩阵,其行列式由对角元素的乘积给出,因此也可以高效评估。</p><p>然而,从这个模型中进行采样必须通过评估$x _ {i}&#x3D;h(z _ {i},g _ {i}(x _ {1:i-1},w _ {i}))$来完成,这本质上是有序的，因此很慢。</p><p>为了避免这种低效的采样，可以定义一个逆自回归流（inverse autoregressive flows，IAF）给出 ：</p><p>$$x _ {i}&#x3D;h(z _ {i},\tilde{g} _ {i}(z _ {1:i-1},w _ {i}))$$</p><p>现在采样就更快了。</p><p>然而，其逆函数也还是具有序列性。</p><p>$$z _ {i}&#x3D;h^{-1}(x _ {i},\tilde{g} _ {i}(z _ {1:i-1},w _ {i}))$$</p><h2 id="连续流（Continuous-Flow）"><a href="#连续流（Continuous-Flow）" class="headerlink" title="连续流（Continuous Flow）"></a>连续流（Continuous Flow）</h2><h3 id="神经微分方程（Neural-differential-equations）"><a href="#神经微分方程（Neural-differential-equations）" class="headerlink" title="神经微分方程（Neural differential equations）"></a>神经微分方程（<strong>Neural differential equations</strong>）</h3><p>深度网络具有很深的层数是很有用。</p><p>所以，我们可以探索神经网络达到无限层数的极限会发生什么。</p><p>考虑一个残差网络,其中每一层的处理生成的输出由输入向量与输入向量的某些参数化非线性函数相加给出: </p><p>$$z^{(t+1)}&#x3D;z^{(t)}+f(z^{(t)},w)$$ </p><p>其中 $t&#x3D;1,…,T$ 标记网络中的层。</p><p>为了简便，我们在每一层都使用了相同的函数,并共享参数向量w。</p><p>在极限情况下,隐藏单元激活向量成为连续变量的函数 $z(t)$,我们可以将此向量在网络中的演变表示为一个微分方程:</p><p>$$\frac{dz(t)}{dt}&#x3D;f(z(t),w)$$</p><p>中的公式被称为一个神经常微分方程或神经 ODE。</p><p>如果我们用向量 $z(0)$ 表示网络的输入,那么输出 $z(T)$ 是通过积分微分方程获得的：</p><p>$$z(T)&#x3D;\int _ {0}^{T}f(z(t),w)dt$$</p><p>这个积分可以使用标准的数值积分包来评估。求解微分方程的最简单方法是欧拉前向积分方法,它对应于表达式$z^{(t+1)}&#x3D;z^{(t)}+f(z^{(t)},w)$。</p><p>在实践中,更强大的数值积分算法可以自适应地调整它们的函数评估以实现。特别是,它们可以自适应地选择的值,这些值通常不是均匀分布的。</p><h3 id="神经ODE反向传播"><a href="#神经ODE反向传播" class="headerlink" title="神经ODE反向传播"></a>神经ODE反向传播</h3><p>我们得到了神经ODE的形式，那么如何通过优化损失函数来确定w的值呢？假设我们有一个数据集,其中包含输入向量 $z(0)$ 的值,以及相关的输出目标向量和损失函数 $L(\cdot)$,该函数取决于输出向量 $z(T)$。</p><p>一种方法可能是使用自动微分来对ODE求解器在正向传递期间执行的所有操作进行微分。尽管这很直接，但从内存角度来看代价很高,并且在控制数值误差方面不是最优的。</p><p>相反,Chen等人<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ricky T. Q. Chen. Neural Ordinary Differential Equations">[4]</span></a></sup>将ODE求解器视为一个黑盒,并使用一种称为伴随敏感性方法的技术,这可以看作是显式反向传播的连续类比。</p><p>为了将反向传播应用于神经ODE,我们定义一个称为伴随的量,其定义为：</p><p>$$a(t)&#x3D;\frac{dL}{dz(t)}$$ </p><p>我们看到 $a(T)$ 对应于损失关于输出向量的常规导数。伴随子满足其自身的微分方程,该方程为：</p><p>$$\frac{da(t)}{dt}&#x3D;-a(t)^{T}\nabla _ {z}f(z(t),w)$$ </p><blockquote><p>证明：<br>$$<br>\begin{align}<br>a(t)&amp;&#x3D;\frac{dL}{dz(t)}\\<br>a(t+\delta t)&amp;\approx\frac{dL}{dz(t+\delta t)}\\<br>z(t+\delta t)&amp;\approx z(t)+f(z(t),w)\delta t\\<br>\therefore \frac{\partial L}{\partial z(t)}&amp;&#x3D;\frac{\partial L}{\partial z(t+\delta t)}\frac{\partial z(t+\delta t)}{\partial z(t)}\\<br>a(t)&amp;&#x3D;a(t+\delta t)(I+\nabla_zf()\delta t)<br>\end{align}<br>$$</p></blockquote><p>这是一个微积分链式法则的连续版本。这可以通过从 $a(T)$ 开始反向积分来求解,这同样可以使用黑盒ODE求解器来完成。</p><p>关于网络参数的导数。当一个参数值在网络的多个连接中共享时,总导数由每个连接的导数之和形成。</p><p>对于我们的神经ODE,其中相同的参数向量 w在整个网络中共享,这个求和变成了对的积分,其形式为：</p><p>$$\nabla _ {w}L&#x3D;-\int _ {0}^{T}a(t)^{T}\nabla _ {w}f(z(t),w)dt$$ </p><p> $\nabla _ {z}f$ 和 $\nabla _ {w}f$ 可以使用自动微分高效地计算。注意,上述结果同样适用于一个更通用的神经网络函数 $f(z(t),t,w)$ 它在除了通过 $z(t)$ 隐式依赖之外,还对有显式依赖。</p><p>使用伴随方法训练的神经ODE相比于传统层状网络的一个优点是,不需要存储前向传播的中间结果,因此内存成本是恒定的。</p><p>此外，神经 ODE可以自然地处理连续时间数据,其中观测值在任意时间发生。如果误差函数 L 取决于除输出值之外的 $z(t)$ 的值,那么需要多次运行反向模型求解器,每次运行对应一对连续的输出,以便将单个解分解为多个连续解,从而访问中间状态。</p><h3 id="神经ODE-flows"><a href="#神经ODE-flows" class="headerlink" title="神经ODE flows"></a>神经ODE flows</h3><p>神经ODE定义了一种高度灵活的从输入向量 $z(0)$ 到输出向量 $z(T)$ 的变换,该变换以微分方程的形式给出：</p><p>$$\frac{dz(t)}{dt}&#x3D;f(z(t),w)$$</p><p>如果我们定义一个关于输入向量 $p(z(0))$ 的基础分布,那么神经ODE 会将其随时间向前传播,为每个的值给出一个分布 $p(z(t))$,从而得到一个关于输出向量 $p(z(T))$ 的分布。</p><p>Chen<sup id="fnref:4" class="footnote-ref"><a href="#fn:4" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ricky T. Q. Chen. Neural Ordinary Differential Equations">[4]</span></a></sup>表明,对于神经ODE,密度的变换可以通过积分一个由下式给出的微分方程来评估：</p><p>$$\frac{\partial~lnp(z(t))}{\partial t}&#x3D;-Tr(\frac{\partial f}{\partial z(t)})$$</p><p>其中 $\partial f&#x2F;\partial z$ 表示雅可比矩阵,其元素为 $\partial f _ {i}&#x2F;\partial z _ {jc}$。</p><p>作者使用的是泰勒级数展开式等来证明。</p><p>我们也可以进一步展开为：<br>$$<br>\frac{\partial~lnp(z(t))}{\partial t}&#x3D;-\nabla\cdot f&#x3D;-Tr(\frac{\partial f}{\partial z(t)})<br>$$</p><blockquote><p>证明：</p><p>根据连续性方程（其密度的局部变化率等于流入该点的通量（flux）的负散度，也叫传输方程）：<br>$$<br>\frac{\partial p(z)}{\partial t}+\nabla\cdot (p(z)f)&#x3D;0<br>$$<br>根据向量微积分的乘法法则：<br>$$<br>\nabla\cdot (pf)&#x3D;\nabla p (f)+p(\nabla \cdot f)<br>$$<br>根据多元链式法则，<br>$$<br>\frac{\partial<del>lnp(z(t))}{\partial t}&#x3D;\frac{1}{p}(\frac{\partial p}{\partial t}+(\nabla p)f)<br>$$<br>把二者结合起来有：<br>$$<br>\frac{\partial</del>lnp(z(t))}{\partial t}&#x3D;-\nabla\cdot f<br>$$<br>而一个向量场的散度在数学上等于其雅可比矩阵的迹。</p></blockquote><p> 这个积分可以使用标准的ODE 求解器来执行。同样,可以从基础密度 $p(z(0))$ 中采样,该分布被选择为一个简单的分布,例如高斯分布,并通过再次使用ODE 求解器积分将值传播到输出。</p><p>由此产生的框架被称为一个连续归一化流。连续归一化流可以使用用于神经ODE的伴随敏感性方法进行训练,这可以看作是反向传播的连续时间等价物。</p><p>由于涉及雅可比矩阵的迹而不是使用行列式，因此它可能看起来在计算上更高效。<br>通常,计算一个 $D\times D$ 矩阵的行列式需要 $\mathcal{O}(D^{3})$ 次操作,而计算迹只需要 $\mathcal{O}(D)$ 次操作。</p><p>然而,如果行列式是下三角形的,如在许多归一化流的形式中,那么行列式是主对角线项的乘积,因此也涉及 $\mathcal{O}(D)$ 次操作。由于计算雅可比矩阵的各个元素需要单独的前向传播,而前向传播本身需要 $\mathcal{O}(D)$ 次操作,因此计算迹或行列式(对于下三角矩阵)总共需要 $\mathcal{O}(D^{2})$ 次操作。</p><p>然而,通过使用 Hutchinson 迹估计器<sup id="fnref:5" class="footnote-ref"><a href="#fn:5" rel="footnote"><span class="hint--top hint--rounded" aria-label="Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud. FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models">[5]</span></a></sup>,可以将计算迹的成本降低到 $\mathcal{O}(D)$。</p><p>该估计器对于一个矩阵A采用以下形式 ：</p><p>$$Tr(A)&#x3D;\mathbb{E} _ {\epsilon}[\epsilon^{T}A\epsilon]$$</p><p>其中是$\epsilon$一个均值为零、协方差为一的随机向量,例如,高斯 $\mathcal{N}(0,I)$。</p><p>我们不需要计算整个矩阵A，就能得到$\epsilon^{T}A\epsilon$的值。</p><p>具体而言，</p><p>（1）计算神经网络的前向传播f(z)</p><p>（2）计算f(z)和$\epsilon$的点积，得到$L&#x3D;\epsilon\cdot f(z)$</p><p>（3）执行反向传播，计算L关于z的梯度，则梯度为$\epsilon^T\frac{\partial f}{\partial z}$</p><p>（4）最后计算梯度和$\epsilon$的点积，$(\epsilon^T\frac{\partial f}{\partial z})\cdot \epsilon$</p><p>然后我们可以使用有限数量的样本来近似迹,形式为：</p><p>$$Tr(A)\approx\frac{1}{M}\sum _ {m&#x3D;1}^{M}\epsilon _ {m}^{T}A\epsilon _ {m}$$</p><p>这里其实牵扯到一个叫<strong>雅可比向量积 (Jacobian-Vector Product，JVP)<strong>的概念，即一个</strong>行向量 <code>v^T</code></strong> 与一个<strong>雅可比矩阵 <code>J</code></strong> 的乘积</p><h1 id="流匹配（Flow-Matching）"><a href="#流匹配（Flow-Matching）" class="headerlink" title="流匹配（Flow Matching）"></a>流匹配（Flow Matching）</h1><p>该方法<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yaron Lipman. FLOW MATCHING FOR GENERATIVE MODELING。">[6]</span></a></sup>于2022年提出。</p><p>Flow代表继续延续连续归一化流（CNF）的核心。即一个类似流动的过程，或者说基于连续性方程&#x2F;传输方程的思想。</p><p>Matching类似之前博客曾提过的分数匹配。</p><p>而与前面的流模型不同，流匹配是一种在训练过程中不需要求解ODE来训练的方法。</p><p>类似分数匹配，我们之间对向量场进行构建回归目标，（我们继续延续流模型中的符号，部分符号会和流匹配论文中常用的符号略有不同）：<br>$$<br>\mathcal{L} _ {FM}&#x3D;E _ {t\sim U(0,1),X_t\sim p_t}||f_t(X_t,t)-f^\theta_t(X_t,t)||^2<br>$$<br>然而我们并不知道$p_t$和真实向量场$f$，所以就有了条件流匹配的出现。</p><h2 id="条件流匹配（Conditional-Flow-Matching）"><a href="#条件流匹配（Conditional-Flow-Matching）" class="headerlink" title="条件流匹配（Conditional Flow Matching）"></a>条件流匹配（Conditional Flow Matching）</h2><p>CFM 的核心思想是选择一个额外变量 z 和一个条件概率路径$p(x\mid t,z)$。<br>$$<br>\mathcal{L} _ {CFM}&#x3D;E _ {t\sim U(0,1),X_t\sim p_t,z\sim q(z)}||f_t(X_t,t;z)-f_t^\theta(X_t,t)||^2<br>$$<br>Lipman 等人<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yaron Lipman. FLOW MATCHING FOR GENERATIVE MODELING。">[6]</span></a></sup>证明了$\mathcal{L} _ {FM}$和$\mathcal{L} _ {CFM}$只相差一个与$\theta$无关的常数，二者梯度相等。证明其实很简单，把期望内的括号展开分离即可。</p><p>对于z，我们可以只考虑$x_1$，定义$\mu(z,t)&#x3D;tx_1$，$\sigma(z,t)&#x3D;t\sigma _ {const}-t+1$，则<br>$$<br>\begin{align}<br>p _ {t}(\mathbf{x} | \mathbf{z}) &amp;&#x3D; \mathcal{N}\left( \mathbf{x} | t \mathbf{x}_1, (t \sigma _ {const} - t + 1)^2 \mathbf{I} \right) \\<br>f(\mathbf{x}, t ; \mathbf{z}) &amp;&#x3D; \frac{ \mathbf{x}_1 - (1 - \sigma _ {const}) \mathbf{x} }{ 1 - (1 - \sigma _ {const}) t }<br>\end{align}<br>$$<br>这也就是原始的CFM<sup id="fnref:6" class="footnote-ref"><a href="#fn:6" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yaron Lipman. FLOW MATCHING FOR GENERATIVE MODELING。">[6]</span></a></sup>，也叫<strong>锥形高斯路径</strong>（<strong>Conical Gaussian paths</strong>）。</p><p>对于z，我们也可以将$x_0$和$x_1$联合起来考虑，定义$\mu(z,t)&#x3D;(1-t)x_0+tx_1$，$\sigma(z,t)&#x3D;\sigma _ {const}$，则<br>$$<br>\begin{align}<br>p _ {t}(\mathbf{x} | \mathbf{z}) &amp;&#x3D; \mathcal{N}\left( \mathbf{x} | t \mathbf{x}_1+(1-t)x_0, \sigma _ {const}^2 \mathbf{I} \right) \\<br>f(\mathbf{x}, t ; \mathbf{z}) &amp;&#x3D; x_1-x_0<br>\end{align}<br>$$<br>这也就是independent CFM（iCFM)<sup id="fnref:7" class="footnote-ref"><a href="#fn:7" rel="footnote"><span class="hint--top hint--rounded" aria-label="Tong, A., Malkin, N. Improving and generalizing flow-based generative models with minibatch optimal transport.">[7]</span></a></sup>，也叫<strong>线性插值。</strong> 它也与最优传输（Optimal Transport）有关，正如其论文标题所述。</p><p><strong>完整流程：</strong></p><p>1.采样  t∼Uniform(0,1)</p><p>2.采样  z∼q(z)</p><p>3.计算$\mu_z$和$\sigma_t(z)$</p><p>4.计算$x_t\sim\mathcal{N}(x\mid\mu(z,t),\sigma(z,t))$</p><p>5.计算向量场$f$</p><p>6.计算损失</p><p>7.更新参数</p><h2 id="MeanFlow"><a href="#MeanFlow" class="headerlink" title="MeanFlow"></a>MeanFlow</h2><p>我们直接跳到最新的研究。由何恺明组提出的《Mean Flows for One-step Generative Modeling》<sup id="fnref:8" class="footnote-ref"><a href="#fn:8" rel="footnote"><span class="hint--top hint--rounded" aria-label="Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter,  Kaiming He. Mean Flows for One-step Generative Modeling">[8]</span></a></sup>。</p><p>回顾一下前文：</p><p>$$\frac{dz(t)}{dt}&#x3D;f(z(t),w)$$</p><p>这实际上代表着瞬时速度。</p><p>然而瞬时速度并不是直接指向数据点，所以无法一步生成，而要一步生成需要平均速度才能实现。</p><p><img src="/2025/20250609/averspeed.jpg"></p><p>展开来说，前文的flow matching尽管使用的是直线插值，但是实际上是线性加噪而非线性去噪。导致我们无法一步生成。</p><p>我们也可以使用一些形象的方法来解释。</p><p>当只有一个力源的情况下，它的场线是直线（左图和中图），这与我们的加噪相似，我们也可以类似PFGM，将这一过程理解为数据点被力场推着走向目标位置。</p><p>然而，我们不止一个力源，所以最终场（右图）并不是一条直线。</p><p><img src="/2025/20250609/vis.jpg"></p><p>我们也很难去设计一种力，使得合并后的最终场是直线形式。</p><p>面对天体运动，与其我们引入复杂的本轮均轮来完善地心说，更好的方法是直接引入日心说。</p><p>故MeanFlow另起炉灶，虽然继续使用同样的力源，但不继续做文章，而是使用了平均速度。</p><p>我们延用论文中的符号。</p><p>瞬时速度：</p><p>$$<br>\frac{dz(t)}{dt}&#x3D;v(z(t),w)<br>$$</p><p>而平均速度场 u 和瞬时速度场 v 有以下关系：</p><p>$$(t - r)u(z_t, r, t) &#x3D; \int_r^t v(z _ {\tau}, \tau) d\tau$$</p><p>$$\frac{d}{dt} (t - r)u(z_t, r, t) &#x3D; \frac{d}{dt} \int_r^t v(z _ {\tau}, \tau) d\tau \implies u(z_t, r, t) + (t - r)\frac{d}{dt}u(z_t, r, t) &#x3D; v(z_t, t)$$</p><p>若我们想利用平均速度一步就能得到结果, 则就需要平均速度场满足</p><p>$$<br>\underbrace{u(z_t, r, t)} _ {\text{average vel.}} &#x3D; \underbrace{v(z_t, t)} _ {\text{instant. vel.}} - \underbrace{(t - r)\frac{d}{dt}u(z_t, r, t)} _ {\text{time derivative}}<br>$$</p><p>对于最后一项有：</p><p>$$<br>\frac{d}{dt}u(z_t, r, t) &#x3D; \frac{dz_t}{dt}\partial_z u + \frac{dr}{dt}\partial_r u + \frac{dt}{dt}\partial_t u<br>$$</p><p>其中：</p><p>$$<br>\frac{dz_t}{dt} &#x3D; v(z_t, t), \quad \frac{dr}{dt} &#x3D; 0, \quad \frac{dt}{dt} &#x3D; 1<br>$$</p><p>在上文的 Hutchinson 迹估计器中，JVP已见其威力，我们不需要计算整个雅克比矩阵A，就能得到矩阵的迹。同样地我们在这里也可以对上式使用。</p><p>最终设定损失函数：<br>$$<br>\mathcal{L}(\theta) &#x3D; \mathbb{E}\left[ || u_\theta(z_t, r, t) - \text{sg}(u _ {\text{tgt}}) ||^2_2 \right]<br>$$</p><p>其中</p><p>$$<br>u _ {\text{tgt}} &#x3D; v(z_t, t) - (t-r)(v(z_t, t)\partial_z u_\theta + \partial_t u_\theta)<br>$$<br>在这里我们会使用sg(stop gradient)，因为我们并不需要它的导数，也避免了再去求高阶导数。</p><p><img src="/2025/20250609/meanflow.jpg"></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><p>本文参考了多篇公开资料（博客、代码等等）<sup id="fnref:9" class="footnote-ref"><a href="#fn:9" rel="footnote"><span class="hint--top hint--rounded" aria-label="生成扩散模型漫谈（三十）：从瞬时速度到平均速度. https://kexue.fm/archives/10958">[9]</span></a></sup><sup id="fnref:10" class="footnote-ref"><a href="#fn:10" rel="footnote"><span class="hint--top hint--rounded" aria-label="Flow Matching: Matching flows instead of scores. https://jmtomczak.github.io/blog/18/18_fm.html">[10]</span></a></sup><sup id="fnref:11" class="footnote-ref"><a href="#fn:11" rel="footnote"><span class="hint--top hint--rounded" aria-label="Yaron Lipman. Flow Matching Guide and Code">[11]</span></a></sup><sup id="fnref:12" class="footnote-ref"><a href="#fn:12" rel="footnote"><span class="hint--top hint--rounded" aria-label="A Visual Dive into Conditional Flow Matching. https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/#d-footnote-9">[12]</span></a></sup><sup id="fnref:13" class="footnote-ref"><a href="#fn:13" rel="footnote"><span class="hint--top hint--rounded" aria-label="Deep Learning Course at the University of Amsterdam (MSc AI), Fall 2023. https://github.com/phlippe/uvadlc_notebooks">[13]</span></a></sup><sup id="fnref:14" class="footnote-ref"><a href="#fn:14" rel="footnote"><span class="hint--top hint--rounded" aria-label="流模型(Flow-based Model). https://0809zheng.github.io/2022/05/01/flow.html">[14]</span></a></sup><sup id="fnref:15" class="footnote-ref"><a href="#fn:15" rel="footnote"><span class="hint--top hint--rounded" aria-label="Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling. Improving Variational Inference with Inverse Autoregressive Flow">[15]</span></a></sup><sup id="fnref:16" class="footnote-ref"><a href="#fn:16" rel="footnote"><span class="hint--top hint--rounded" aria-label="George Papamakarios, Theo Pavlakou, Iain Murray. Masked Autoregressive Flow for Density Estimation">[16]</span></a></sup>和一些教科书<sup id="fnref:17" class="footnote-ref"><a href="#fn:17" rel="footnote"><span class="hint--top hint--rounded" aria-label="PRML">[17]</span></a></sup>和李宏毅的课程<sup id="fnref:18" class="footnote-ref"><a href="#fn:18" rel="footnote"><span class="hint--top hint--rounded" aria-label="李宏毅的Flow-based Generative Model课程">[18]</span></a></sup>。</p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>Laurent Dinh, Jascha Sohl-Dickstein, Samy Bengio. Density estimation using Real NVP<a href="#fnref:1" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:2" class="footnote-text"><span>Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel. Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design<a href="#fnref:2" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:3" class="footnote-text"><span>Diederik P. Kingma, Prafulla Dhariwal.Glow: Generative Flow with Invertible 1x1 Convolutions<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:4" class="footnote-text"><span>Ricky T. Q. Chen. Neural Ordinary Differential Equations<a href="#fnref:4" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:5" class="footnote-text"><span>Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud. FFJORD: Free-form Continuous Dynamics for Scalable Reversible Generative Models<a href="#fnref:5" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:6" class="footnote-text"><span>Yaron Lipman. FLOW MATCHING FOR GENERATIVE MODELING。<a href="#fnref:6" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:7" class="footnote-text"><span>Tong, A., Malkin, N. Improving and generalizing flow-based generative models with minibatch optimal transport.<a href="#fnref:7" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:8" class="footnote-text"><span>Zhengyang Geng, Mingyang Deng, Xingjian Bai, J. Zico Kolter,  Kaiming He. Mean Flows for One-step Generative Modeling<a href="#fnref:8" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:9" class="footnote-text"><span>生成扩散模型漫谈（三十）：从瞬时速度到平均速度. <a href="https://kexue.fm/archives/10958">https://kexue.fm/archives/10958</a><a href="#fnref:9" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:10" class="footnote-text"><span>Flow Matching: Matching flows instead of scores. <a href="https://jmtomczak.github.io/blog/18/18_fm.html">https://jmtomczak.github.io/blog/18/18_fm.html</a><a href="#fnref:10" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:11" class="footnote-text"><span>Yaron Lipman. Flow Matching Guide and Code<a href="#fnref:11" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:12" class="footnote-text"><span>A Visual Dive into Conditional Flow Matching. <a href="https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/#d-footnote-9">https://dl.heeere.com/conditional-flow-matching/blog/conditional-flow-matching/#d-footnote-9</a><a href="#fnref:12" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:13" class="footnote-text"><span>Deep Learning Course at the University of Amsterdam (MSc AI), Fall 2023. <a href="https://github.com/phlippe/uvadlc_notebooks">https://github.com/phlippe/uvadlc_notebooks</a><a href="#fnref:13" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:14" class="footnote-text"><span>流模型(Flow-based Model). <a href="https://0809zheng.github.io/2022/05/01/flow.html">https://0809zheng.github.io/2022/05/01/flow.html</a><a href="#fnref:14" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:15" class="footnote-text"><span>Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, Max Welling. Improving Variational Inference with Inverse Autoregressive Flow<a href="#fnref:15" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:16" class="footnote-text"><span>George Papamakarios, Theo Pavlakou, Iain Murray. Masked Autoregressive Flow for Density Estimation<a href="#fnref:16" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:17" class="footnote-text"><span>PRML<a href="#fnref:17" rev="footnote" class="footnote-backref"> ↩</a></span></span></li><li><span id="fn:18" class="footnote-text"><span>李宏毅的Flow-based Generative Model课程<a href="#fnref:18" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>物理</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经典的GNN被淘汰了吗？</title>
    <link href="/2025/20250606/"/>
    <url>/2025/20250606/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>各种GNN结构层出不穷，比如Graphformer等transformer模型。不禁产生一个疑问：经典的GNN被淘汰了吗？</p><p>以下这几篇正是对这一疑问的驳斥。</p><p>正好对应图领域中的链接预测、节点分类、图层次三大任务。其中后两篇均为Yuankai Luo的作品。<span id="more"></span></p><p>（NIPS 2023） <a href="https://arxiv.org/abs/2306.10453">Evaluating Graph Neural Networks for Link Prediction</a></p><p>（NIP2 2024 ）<a href="https://arxiv.org/abs/2406.08993">Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification</a></p><p>（ICML 2025 ）<a href="https://arxiv.org/abs/2502.09263">Unlocking the Potential of Classic GNNs for Graph-level Tasks: Simple Architectures Meet Excellence</a></p><h2 id="节点预测"><a href="#节点预测" class="headerlink" title="节点预测"></a>节点预测</h2><p>（1）作者注意到，多个模型的性能被低估。对于某些方法，如标准 GNNs，<br>这是由于调参不当所致。一旦进行适当的调参，它们甚至可以在某些指标上达到最佳整体性能。</p><p><img src="/2025/20250606/links.png"></p><p>（2）大多数模型的排序指标标准差往往较高。例如，MRR的标准差在 Cora、Citeseer 和 Pubmed 上分别高达 8.82 、8.96 和 7.75 。此外，在 ogbl-ddi数据集上，Hits@20 的标准差甚至达到了 10.47 和 15.56。</p><p>高方差意味着模型性能不稳定。这使得比较不同方法的结果变得困难，因为真实性能存在于更大的值域内。这进一步增加了复制模型性能的复杂性，因为即使与报告结果存在较大差异，也可能仍处于方差范围内。</p><p>也导致作者可能会取偏高的作为结果，而难以体现模型的平均性能。</p><p>（3）惯用的AUC 并不是链接预测的合适指标。</p><h3 id="改进评估方案"><a href="#改进评估方案" class="headerlink" title="改进评估方案"></a>改进评估方案</h3><p>现有的链接预测评估过程是将一个正例与一组 K 随机选择的负例进行排名。所有正例都使用相同的 K 个负例（ogbl-citation2 除外，它每个正例使用 1000 个）。</p><p>然而这种评估方法会有以下问题：</p><p><strong>问题1：非个性化的负例。</strong></p><p>现有的评估情景对所有正例使用相同的负例集合（ogbl-citation2除外）。这种策略，称为全局负采样，不是通常追求的目标。相反，我们通常更感兴趣的是预测特定结点将发生的链接。例如，一个连接朋友用户的社交网络。</p><p>在这种情景下，我们可能有兴趣为用户推荐新朋友 u 。这需要学习一个分类器 f ，它为链接存在的概率进行赋值。在评估此任务时，我们希望将 u 连接到现有朋友的链接排在那些不连接的链接之上。</p><p>例如，如果 u 与 a 是朋友但不是 b 的朋友，我们希望 f (u, a) &gt; f (u, b) 。然而，现有的评估情景并未明确测试这一点。而是将一个真实样本 (u, a) 与一个可能无关的负例进行比较，例如 (c, d) 。这与此类图上链接预测的实际应用不符。</p><p><strong>问题 2: 易判负例（Easy Negative Samples）。</strong></p><p>现有评估情景中随机选取负例进行使用。然而，考虑到大多数图的规模<br>较大，随机采样的负例很可能选择两个彼此无关的结点。这类结点对<br>分类起来轻而易举。</p><p>作者进一步表明，即使在较小的数据集中，这一问题依然存在。</p><p><strong>新方法：</strong></p><p>作者的策略 HeaRT 通过以下方式解决这些挑战：</p><p>(a) 为每个样本个性化负例</p><p> (b) 使用启发式方法选择难负例。这使得负例能够与每个正例直接相关，同时又不失挑战性。</p><p><img src="/2025/20250606/heart.png"></p><h2 id="节点分类"><a href="#节点分类" class="headerlink" title="节点分类"></a>节点分类</h2><p>具体采用的数据集有：</p><p><img src="/2025/20250606/nodedata.png"></p><p><strong>在同质图上：</strong></p><p><img src="/2025/20250606/nodedata1.png"></p><p>经典 GNN 仅需对超参数进行微调，便能在同质图上的节点分类任务中表现出极强的竞争力，在许多情况下甚至超越了最先进的图Transformer！</p><p><strong>在异质图上：</strong></p><p><img src="/2025/20250606/nodedata2.png"></p><p>传统 GNN依旧在异质图中是强有力的竞争者。</p><p><strong>在大规模图上：</strong></p><p><img src="/2025/20250606/nodedata3.png"></p><p>传统 GNN在这些大规模图数据集上取得了最佳结果，无论是同质性还是异质性图，均超越了最先进的图 Transformer。这表明消息传递在大<br>规模图上学习结点表示仍然非常有效。</p><h2 id="图层次"><a href="#图层次" class="headerlink" title="图层次"></a>图层次</h2><p>作者并不是使用传统的GNN，而是提出了GNN+，即GNN整合了六种流行技术边特征集成、Normalization、Dropout、残差连接、FFN以及位置编码。</p><h3 id="边特征集成"><a href="#边特征集成" class="headerlink" title="边特征集成"></a>边特征集成</h3><p>$$<br>h_v^l&#x3D;\sigma(\sum_{u\in N(v)\cup{v}}\frac{1}{\sqrt{\hat d_u\hat d_v}}h_u^{l-1}W^l)<br>$$</p><p>边特征集成即：<br>$$<br>h_v^l&#x3D;\sigma(\sum_{u\in N(v)\cup{v}}\frac{1}{\sqrt{\hat d_u\hat d_v}}h_u^{l-1}W^l+e_{uv}W^l_e)<br>$$<br>其中$e_{uv}、W^l_e$分别为u和v之间的特征向量和第$l$层的可训练权重矩阵。</p><h3 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h3><p>作者采用的是随机游走结构编码（RWSE）。</p><p>之前的博客曾介绍过相关的Graph Neural Networks with Learnable Structural and Positional Representations，在这里不再赘述。<br>$$<br>x_v&#x3D;[x_v||x_v^{RWSE}]W_{PE}<br>$$</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2025/20250606/graph.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>时间序列预测与后门攻击</title>
    <link href="/2025/20250603/"/>
    <url>/2025/20250603/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>后门攻击简单来说，是攻击者通过在训练过程中嵌入触发器（trigger）来操纵测试时的预测。</p><p>但后面攻击普遍用于CV或分类中，时间序列（预测）中比较少。</p><span id="more"></span><h2 id="通用方法"><a href="#通用方法" class="headerlink" title="通用方法"></a>通用方法</h2><p>发表于SaTML’23的《<a href="https://arxiv.org/pdf/2211.07915">Backdoor Attacks on Time Series: A Generative Approach</a>》，SaTML是比较新的IEEE会议。</p><p>既然是通用，其暗示即简单。事实也如此。</p><p><img src="/2025/20250603/method.png"></p><p>为了解决生成器训练的冷启动问题，首先在来自 D 的所有干净样本上对时间序列分类器 f 进行预训练，直到其交叉熵损失 LCE 稳定下<br>降。</p><p>预训练完成后，我们同时训练触发器生成器 g 和部分训练的分类器 f 。</p><p>在每次迭代中，两个网络都按照类似的过程逐步更新：</p><p>1）在污染样本上训练 g ，以最小化针对后门类别 $y_t$ 的分类损失（第 15-17 行）；</p><p>2）使用 g 生成污染数据集 D′ ；</p><p>3）在污染数据集 D′ 上训练 f ，其中污染样本被重新标记为 $y_t$。</p><p>整个过程中，后门触发模式被限制在信号幅度的 10% 以内，即 0.1 ∗ (xmax − xmin) ，以增强隐蔽性。</p><h2 id="针对多元时间序列预测"><a href="#针对多元时间序列预测" class="headerlink" title="针对多元时间序列预测"></a>针对多元时间序列预测</h2><p>NIP2 24 《<a href="https://arxiv.org/abs/2410.02195">BACKTIME: Backdoor Attacks on Multivariate Time Series Forecasting</a>》</p><p>多元时间序列（MTS）数据与单变量时间序列相比，会有变量间相关系数的存在，这会使对MTS 数据的攻击更为复杂。</p><p>在多变量时间序列预测中，常见做法是将数据集切分为时间窗口，作为预测模型的输入。</p><p>然而，在中毒数据集中，识别这些切分的时间窗口是否被中毒面临两大挑战。</p><p>（1）这些时间窗口的长度可能与触发器或目标模式的长度不匹配。</p><p>（2）当将数据集切分为时间窗口时，这些窗口可能仅包含触发器或目<br>标模式的一部分。</p><p>为解决这些问题，作者假设只有当输入包含触发器的所有组成部分时，注入的后门才会被激活。</p><p>对于后门攻击，我们需要确定三个关键要素：</p><p>（1）攻击何处。这个由攻击者指定</p><p>（2）何时攻击，即选择攻击的时间点</p><p>（3）如何攻击，即指定注入的触发器。</p><h3 id="攻击时间选择"><a href="#攻击时间选择" class="headerlink" title="攻击时间选择"></a>攻击时间选择</h3><p>预测误差较高的时间戳更容易遭受攻击。</p><p>利用预训练的干净模型计算预测与真实值之间的 MAE，并进一步选择MAE 最高的前 α个时间戳。</p><h3 id="攻击方法"><a href="#攻击方法" class="headerlink" title="攻击方法"></a>攻击方法</h3><p>首先，通过利用MLP 来捕捉目标变量 S 内部的变量间相关性，生成一个加权图。然后，进一步基于学成的加权图利用图卷积网络（GCN）进行触发器生成。</p><p>由于时间序列跨度比较大，故作者采取的是使用DFT并取低维特征来降低维度和提取信息。即$z_i&#x3D;Filter(DFT(x_i),k)$。</p><p>并使用MLP来构建相关性的图，即$A _ {i,j}&#x3D;cos(MLP(z_i)，MLP(z_j))$。</p><p>使用长度为$t^{BEF}$的时间窗口对触发器之前的历史数据进行切片。随后，我们基于切片后的历史<br>数据，利用 GCN 进行触发器生成：<br>$$<br>\hat g_{t_i}&#x3D;GCN(X^{ATK}[t_i-t^{BEF}-t^{TGR}:t_i-t^{TGR},S],A),\forall t_i\in T^{ATK}<br>$$<br>然而，GCN 倾向于激进地增加输出 $\hat g _ {t_i}$的幅度。作者对这种行为的一个潜在解释是，较大的触发幅度会导致显著的偏差，而具有此类偏差特征的数据点更容易被预测模型学成，尽管它们违背了隐蔽性的要求。</p><p>为了解决这一问题，作者提出引入一个非线性缩放函数tanh(·)，对输出幅度施加强制性限制来生成隐蔽的触发器。<br>$$<br>g_{t_i}&#x3D;\Delta^{TGR}\cdot tanh(\hat g_{t_i}) \tag{6}<br>$$</p><p>由于原始问题是个双层优化问题，故作者引入了一个替代预测模型$f_s$，以提供精确解的实用近似</p><p>同样的，也会使用预热来避免难以训练或难以收敛。在预热阶段，我们仅训练替代模型，使其具备合理的预测能力。一旦预热阶段结束，我们将同时更新替代模型和触发器生成器。</p><p>第一阶段，替代模型更新：<br>$$<br>l _ {cln}&#x3D;L _ {CLN}(f_s(X^{ATK} _ {t_i,h}),X^{ATK} _ {t _ i,f})\tag{7}<br>$$<br>其中使用平滑L1loss。</p><p>第二阶段，触发器生成器更新：</p><p>固定代理模型的参数后，攻击损失可以形式化为：<br>$$<br>l_{atk}&#x3D;\sum _ {t_i&#x3D;t}^{t+t^{PTN}} L _ {ATK}(f_s(X^{ATK}_{t_i,h}),X^{ATK} _ {t_i,f})\eta(t_i)<br>$$<br>PTN表示目标。</p><p>现实世界数据集中的多变量时间序列（MTS）数据普遍存在高频波动或噪声。然而该方法并不能天然保证触发器具有高频信号。因此，为了弥补这一差距，引入了以下规范化损失：<br>$$<br>l_{norm}&#x3D;AVG\left(\left|\sum_{i&#x3D;0}^{t^{TGR}} g_{t_i}[i,:]\right|\right)\tag{9}<br>$$<br>可能有点云里雾里，不妨我们来看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config, atk_vars, target_pattern, train_mean, train_std,</span><br><span class="hljs-params">                 train_data, test_data, train_data_stamps, test_data_stamps, device</span>):<br>        <span class="hljs-variable language_">self</span>.net = MODEL_MAP[<span class="hljs-variable language_">self</span>.config.surrogate_name](<span class="hljs-variable language_">self</span>.config.Surrogate).to(device)<br>        <span class="hljs-variable language_">self</span>.optimizer = optim.Adam(<span class="hljs-variable language_">self</span>.net.parameters(), lr=config.learning_rate)<br><br><br>        train_set = TimeDataset(train_data, train_mean, train_std, device, num_for_hist=<span class="hljs-number">12</span>, num_for_futr=<span class="hljs-number">12</span>, timestamps=train_data_stamps)<br>        channel_features = fft_compress(train_data, <span class="hljs-number">200</span>)<br>        <span class="hljs-variable language_">self</span>.attacker = Attacker(train_set, channel_features, atk_vars, config, target_pattern, device)<br>        <span class="hljs-variable language_">self</span>.use_timestamps = config.Dataset.use_timestamps<br><br>        <span class="hljs-variable language_">self</span>.prepare_data()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.attacker.train()<br>        poison_metrics = []<br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.num_epochs):<br>            <span class="hljs-variable language_">self</span>.net.train()  <span class="hljs-comment"># ensure dropout layers are in train mode</span><br><br>            <span class="hljs-keyword">if</span> epoch &gt; <span class="hljs-variable language_">self</span>.warmup:<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(<span class="hljs-variable language_">self</span>.attacker, <span class="hljs-string">&#x27;atk_ts&#x27;</span>):<br>                    <span class="hljs-comment"># select the attacked timestamps</span><br>                    <span class="hljs-variable language_">self</span>.attacker.select_atk_timestamp(poison_metrics)<br>                <span class="hljs-comment"># attacker poison the training data</span><br>                <span class="hljs-variable language_">self</span>.attacker.sparse_inject()<br><br>            poison_metrics = []<br><br>            <span class="hljs-variable language_">self</span>.train_loader = DataLoader(<span class="hljs-variable language_">self</span>.train_set, batch_size=<span class="hljs-variable language_">self</span>.batch_size, shuffle=<span class="hljs-literal">True</span>)<br>            pbar = tqdm.tqdm(<span class="hljs-variable language_">self</span>.train_loader, desc=<span class="hljs-string">f&#x27;Training data <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;self.num_epochs&#125;</span>&#x27;</span>)<br><br>            <span class="hljs-keyword">for</span> batch_index, batch_data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(pbar):<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-variable language_">self</span>.use_timestamps:<br>                    encoder_inputs, labels, clean_labels, idx = batch_data<br>                    x_mark = torch.zeros(encoder_inputs.shape[<span class="hljs-number">0</span>], encoder_inputs.shape[-<span class="hljs-number">1</span>], <span class="hljs-number">4</span>).to(<span class="hljs-variable language_">self</span>.device)<br>                <span class="hljs-keyword">else</span>:<br>                    encoder_inputs, labels, clean_labels, x_mark, y_mark, idx = batch_data<br>                encoder_inputs = torch.squeeze(encoder_inputs).to(<span class="hljs-variable language_">self</span>.device).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>                labels = torch.squeeze(labels).to(<span class="hljs-variable language_">self</span>.device).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>                <span class="hljs-variable language_">self</span>.optimizer.zero_grad()<br><br>                x_des = torch.zeros_like(labels)<br>                outputs = <span class="hljs-variable language_">self</span>.net(encoder_inputs, x_mark, x_des, <span class="hljs-literal">None</span>)<br>                outputs = <span class="hljs-variable language_">self</span>.train_set.denormalize(outputs)<br>                loss_per_sample = F.smooth_l1_loss(outputs, labels, reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>                loss_per_sample = loss_per_sample.mean(dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br><br>                poison_metrics.append(torch.stack([loss_per_sample.cpu().detach(), idx.cpu().detach()], dim=<span class="hljs-number">1</span>))<br>                loss = loss_per_sample.mean()<br>                loss.backward()<br>                <span class="hljs-variable language_">self</span>.optimizer.step()<br><br>            <span class="hljs-keyword">if</span> epoch &gt; <span class="hljs-variable language_">self</span>.warmup:<br>                <span class="hljs-variable language_">self</span>.attacker.update_trigger_generator(<span class="hljs-variable language_">self</span>.net, epoch, <span class="hljs-variable language_">self</span>.num_epochs, use_timestamps=<span class="hljs-variable language_">self</span>.use_timestamps)<br><br><br></code></pre></td></tr></table></figure><p>其中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sparse_inject</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.dataset.init_poison_data()<br><br>        n, c, T = <span class="hljs-variable language_">self</span>.dataset.data.shape<br>        n = <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.atk_vars)<br>        trigger_len = <span class="hljs-variable language_">self</span>.trigger_generator.output_dim<br>        pattern_len = <span class="hljs-variable language_">self</span>.target_pattern.shape[-<span class="hljs-number">1</span>]<br><br>        <span class="hljs-keyword">for</span> beg_idx <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.atk_ts.tolist():<br>            data_bef_tgr = <span class="hljs-variable language_">self</span>.dataset.data[<span class="hljs-variable language_">self</span>.atk_vars, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, beg_idx - <span class="hljs-variable language_">self</span>.trigger_generator.input_dim:beg_idx]<br>            data_bef_tgr = <span class="hljs-variable language_">self</span>.dataset.normalize(data_bef_tgr)<br>            data_bef_tgr = data_bef_tgr.reshape(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.trigger_generator.input_dim)<br><br>            triggers = <span class="hljs-variable language_">self</span>.trigger_generator(data_bef_tgr)[<span class="hljs-number">0</span>]<br>            triggers = <span class="hljs-variable language_">self</span>.dataset.denormalize(triggers).reshape(n, <span class="hljs-number">1</span>, -<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># inject the trigger and target pattern</span><br>            <span class="hljs-variable language_">self</span>.dataset.poisoned_data[<span class="hljs-variable language_">self</span>.atk_vars, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, beg_idx:beg_idx + trigger_len] = triggers.detach()<br>            <span class="hljs-variable language_">self</span>.dataset.poisoned_data[<span class="hljs-variable language_">self</span>.atk_vars, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, beg_idx + trigger_len:beg_idx + trigger_len + pattern_len] = \<br>                <span class="hljs-variable language_">self</span>.target_pattern + <span class="hljs-variable language_">self</span>.dataset.poisoned_data[<span class="hljs-variable language_">self</span>.atk_vars, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, beg_idx - <span class="hljs-number">1</span>:beg_idx]<br><br></code></pre></td></tr></table></figure><p><strong>注入触发器</strong>: <code>self.dataset.poisoned_data[...] = triggers.detach()</code> 这一行将刚刚生成的 <code>triggers</code> 注入到 <code>poisoned_data</code>（中毒数据副本）中，位置是从 <code>beg_idx</code> 开始，持续 <code>trigger_len</code> 的长度。<code>.detach()</code> 用于切断梯度，因为注入过程本身不需要反向传播。</p><p><strong>注入目标模式</strong>: 紧接着触发器之后，代码注入了<strong>目标模式</strong>。</p><ul><li><code>self.target_pattern</code>: 这是预先定义好的、攻击者希望模型在看到触发器后输出的模式。</li><li><code>+ self.dataset.poisoned_data[...]</code>: 这里有一个值得注意的细节，目标模式并不是直接覆盖，而是<strong>加上了</strong>紧邻攻击点前一个时间步的数据**。** 这样做可能是为了让注入的模式在数值上与周围的数据更“平滑”地衔接，减少异常感，使攻击更隐蔽。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">update_trigger_generator</span>(<span class="hljs-params">self, net, epoch, epochs, use_timestamps=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        update the trigger generator using the soft identification.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> use_timestamps:<br>            tgr_slices = <span class="hljs-variable language_">self</span>.get_trigger_slices(<span class="hljs-variable language_">self</span>.fct_input_len - <span class="hljs-variable language_">self</span>.trigger_len,<br>                                                 <span class="hljs-variable language_">self</span>.trigger_len + <span class="hljs-variable language_">self</span>.pattern_len + <span class="hljs-variable language_">self</span>.fct_output_len)<br>        <span class="hljs-keyword">else</span>:<br>            tgr_slices, tgr_timestamps = <span class="hljs-variable language_">self</span>.get_trigger_slices(<span class="hljs-variable language_">self</span>.fct_input_len - <span class="hljs-variable language_">self</span>.trigger_len,<br>                                                             <span class="hljs-variable language_">self</span>.trigger_len + <span class="hljs-variable language_">self</span>.pattern_len + <span class="hljs-variable language_">self</span>.fct_output_len)<br>        pbar = tqdm.tqdm(tgr_slices, desc=<span class="hljs-string">f&#x27;Attacking data <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;epochs&#125;</span>&#x27;</span>)<br>        <span class="hljs-keyword">for</span> slice_id, <span class="hljs-built_in">slice</span> <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(pbar):<br>            <span class="hljs-built_in">slice</span> = <span class="hljs-built_in">slice</span>.to(<span class="hljs-variable language_">self</span>.device)<br>            <span class="hljs-built_in">slice</span> = <span class="hljs-built_in">slice</span>[:, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, :]<br>            n, c, l = <span class="hljs-built_in">slice</span>.shape<br>            data_bef = <span class="hljs-built_in">slice</span>[<span class="hljs-variable language_">self</span>.atk_vars, :,<br>                       <span class="hljs-variable language_">self</span>.fct_input_len - <span class="hljs-variable language_">self</span>.trigger_len - <span class="hljs-variable language_">self</span>.bef_tgr_len:<span class="hljs-variable language_">self</span>.fct_input_len - <span class="hljs-variable language_">self</span>.trigger_len]<br>            data_bef = data_bef.reshape(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.bef_tgr_len)<br><br>            triggers, perturbations = <span class="hljs-variable language_">self</span>.predict_trigger(data_bef)<br><br>            <span class="hljs-comment"># add the trigger to the slice. x[t-trigger_len:x] = trigger</span><br>            triggers = triggers.reshape(<span class="hljs-variable language_">self</span>.atk_vars.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.trigger_len)<br>            <span class="hljs-built_in">slice</span>[<span class="hljs-variable language_">self</span>.atk_vars, :, <span class="hljs-variable language_">self</span>.fct_input_len - <span class="hljs-variable language_">self</span>.trigger_len:<span class="hljs-variable language_">self</span>.fct_input_len] = triggers<br><br>            <span class="hljs-comment"># add the pattern to the slice. x[t:t+ptn_len] = x[t-1-trigger_len] + target_pattern</span><br>            <span class="hljs-built_in">slice</span>[<span class="hljs-variable language_">self</span>.atk_vars, :, <span class="hljs-variable language_">self</span>.fct_input_len:<span class="hljs-variable language_">self</span>.fct_input_len + <span class="hljs-variable language_">self</span>.pattern_len] = \<br>                <span class="hljs-variable language_">self</span>.target_pattern + <span class="hljs-built_in">slice</span>[<span class="hljs-variable language_">self</span>.atk_vars, :, <span class="hljs-variable language_">self</span>.fct_input_len - <span class="hljs-variable language_">self</span>.trigger_len - <span class="hljs-number">1</span>].unsqueeze(-<span class="hljs-number">1</span>)<br><br>            <span class="hljs-comment"># mimic the soft identification, i.e., the input and output only contain a part of the trigger and pattern</span><br>            batch_inputs_bkd = [<span class="hljs-built_in">slice</span>[..., i:i + <span class="hljs-variable language_">self</span>.fct_input_len] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.pattern_len)]<br>            batch_labels_bkd = [<span class="hljs-built_in">slice</span>[..., i + <span class="hljs-variable language_">self</span>.fct_input_len:i + <span class="hljs-variable language_">self</span>.fct_input_len + <span class="hljs-variable language_">self</span>.fct_output_len].detach()<br>                                <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.pattern_len)]<br>            batch_inputs_bkd = torch.stack(batch_inputs_bkd, dim=<span class="hljs-number">0</span>)<br>            batch_labels_bkd = torch.stack(batch_labels_bkd, dim=<span class="hljs-number">0</span>)<br><br>            batch_inputs_bkd = batch_inputs_bkd[:, :, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, :]<br>            batch_labels_bkd = batch_labels_bkd[:, :, <span class="hljs-number">0</span>, :]<br>            batch_inputs_bkd = <span class="hljs-variable language_">self</span>.dataset.normalize(batch_inputs_bkd)<br><br>            <span class="hljs-comment"># calculate eta in the soft identification to reweight the loss</span><br>            loss_decay = (<span class="hljs-variable language_">self</span>.pattern_len - torch.arange(<span class="hljs-number">0</span>, <span class="hljs-variable language_">self</span>.pattern_len, dtype=torch.float32).to(<br>                <span class="hljs-variable language_">self</span>.device)) / <span class="hljs-variable language_">self</span>.pattern_len<br><br>            <span class="hljs-variable language_">self</span>.attack_optim.zero_grad()<br>            batch_inputs_bkd = batch_inputs_bkd.squeeze(<span class="hljs-number">2</span>).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>            batch_labels_bkd = batch_labels_bkd.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>            <span class="hljs-keyword">if</span> use_timestamps:<br>                batch_x_mark = [tgr_timestamps[slice_id][i:i + <span class="hljs-variable language_">self</span>.fct_input_len] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.pattern_len)]<br>                batch_y_mark = [<br>                    tgr_timestamps[slice_id][i + <span class="hljs-variable language_">self</span>.fct_input_len:i + <span class="hljs-variable language_">self</span>.fct_input_len + <span class="hljs-variable language_">self</span>.fct_output_len] <span class="hljs-keyword">for</span> i<br>                    <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.pattern_len)]<br>                batch_x_mark = torch.stack(batch_x_mark, dim=<span class="hljs-number">0</span>)<br>                batch_y_mark = torch.stack(batch_y_mark, dim=<span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">else</span>:<br>                batch_x_mark = torch.zeros(batch_inputs_bkd.shape[<span class="hljs-number">0</span>], batch_inputs_bkd.shape[<span class="hljs-number">1</span>], <span class="hljs-number">4</span>).to(<span class="hljs-variable language_">self</span>.device)<br><br>            x_des = torch.zeros_like(batch_labels_bkd)<br>            outputs_bkd = net(batch_inputs_bkd, batch_x_mark, x_des, <span class="hljs-literal">None</span>)<br>            outputs_bkd = <span class="hljs-variable language_">self</span>.dataset.denormalize(outputs_bkd)<br><br>            loss_bkd = F.mse_loss(outputs_bkd[:, :, <span class="hljs-variable language_">self</span>.atk_vars], batch_labels_bkd[:, :, <span class="hljs-variable language_">self</span>.atk_vars],<br>                                  reduction=<span class="hljs-string">&#x27;none&#x27;</span>)<br>            loss_bkd = torch.mean(loss_bkd, dim=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>            loss_bkd = torch.<span class="hljs-built_in">sum</span>(loss_bkd * loss_decay)  <span class="hljs-comment"># reweight the loss</span><br>            loss_norm = torch.<span class="hljs-built_in">abs</span>(torch.<span class="hljs-built_in">sum</span>(perturbations, dim=<span class="hljs-number">1</span>)).mean()<br>            loss = loss_bkd + <span class="hljs-variable language_">self</span>.lam_norm * loss_norm<br><br>            loss.backward()<br>            <span class="hljs-variable language_">self</span>.attack_optim.step()<br>        <span class="hljs-variable language_">self</span>.atk_scheduler.step()<br></code></pre></td></tr></table></figure><h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>欧洲航天局举办了一场<a href="https://www.kaggle.com/competitions/trojan-horse-hunt-in-space">关于时间序列的后门提取比赛</a>，形式新颖，但<strong>不允许中国人获得奖金。</strong></p><p>但也期待获胜方案是如何解决的。AmbrosM目前位列第一，他一向具有很高深的方法。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
      <tag>时间序列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>卷积层与位置编码</title>
    <link href="/2025/20250601/"/>
    <url>/2025/20250601/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在NLP领域中，比如经典的textcnn是没有位置编码，那么CNN中的位置信息是从何而来的呢？</p><span id="more"></span><h2 id="CNN中蕴含了多少信息？"><a href="#CNN中蕴含了多少信息？" class="headerlink" title="CNN中蕴含了多少信息？"></a>CNN中蕴含了多少信息？</h2><p> ICLR 2020的《How much position information do convolutional neural network encode?》给了我们答案。</p><p>作者设计了一套位置编码网络 (PosENet）来探索位置信息。</p><p><img src="/2025/20250601/posnet.jpg"></p><p>即提取VGG的五层特征，插值到同一size后concat起来，再接几层卷积层来提取特征，以映射到指定的gt。</p><p>gt如右图所示，它是自己指定的，共有五种。它的设计基于这样的假设——若CNN并没有编码位置信息，只凭图片信息是无法学习到这样的形状的。</p><p>作者在不同模式下做了训练，可以发现设计的网络自身很难学习到位置信息，这佐证了CNN是可以学到位置信息的。</p><p><img src="/2025/20250601/posnet1.jpg"></p><h3 id="和位置信息有关的因素"><a href="#和位置信息有关的因素" class="headerlink" title="和位置信息有关的因素"></a>和位置信息有关的因素</h3><p>作者进一步分析，得到了以下结论。</p><p><strong>层数越多，位置信息能学习得更好。</strong> 一方面，卷积层在空间上等同于一个 5 × 5 卷积层 (VGG论文)。另一种可能性是位置信息可能以需要超过一阶推理的方式进行表示。</p><p><strong>卷积核越大，位置信息能学习得更好。</strong> 位置信息可能在层内和特征空间中以更大的感受野分布，从而更好地解析位置信息。</p><p><img src="/2025/20250601/posnet2.jpg"></p><h3 id="位置信息的来源"><a href="#位置信息的来源" class="headerlink" title="位置信息的来源"></a>位置信息的来源</h3><p>作者进一步做了实验，并给出了一个令人想不到的结论——是<strong>zero padding</strong> 驱动位置信息的学习！</p><p><img src="/2025/20250601/posnet3.jpg"></p><h2 id="CNN可以使用位置编码吗"><a href="#CNN可以使用位置编码吗" class="headerlink" title="CNN可以使用位置编码吗"></a>CNN可以使用位置编码吗</h2><p>既然CNN已经能学习到位置编码，那么我们继续使用会有帮助吗？</p><p>Facebook出品的《Convolutional Sequence to Sequence Learning》正是使用了位置编码的卷积神经网络之作。</p><p>架构如下：</p><p><img src="/2025/20250601/stru.webp"></p><p>输入的embedding使用绝对位置编码。即输入为$x&#x3D;(x_1,x_2,…)$，$p&#x3D;(p_1,p_2,…)$，则$e&#x3D;x&#x3D;(x_1+p_1,x_2+p_2,…)$。</p><p>另一个关键的地方是使用了attention。</p><p>其中$d_i&#x3D;W_dh_i+b_d+g_i$。</p><p>实际上也是一种QKV注意力，QKV如图所示。即e也会参与到attention中的参与中。</p><p><img src="/2025/20250601/qkv.png"></p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AttentionLayer</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, conv_channels, embed_dim, bmm=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-comment"># projects from output of convolution to embedding dimension</span><br>        <span class="hljs-variable language_">self</span>.in_projection = Linear(conv_channels, embed_dim)<br>        <span class="hljs-comment"># projects from embedding dimension to convolution size</span><br>        <span class="hljs-variable language_">self</span>.out_projection = Linear(embed_dim, conv_channels)<br><br>        <span class="hljs-variable language_">self</span>.bmm = bmm <span class="hljs-keyword">if</span> bmm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> torch.bmm<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, target_embedding, encoder_out, encoder_padding_mask</span>):<br>        residual = x<br><br>        <span class="hljs-comment"># attention</span><br>        x = (<span class="hljs-variable language_">self</span>.in_projection(x) + target_embedding) * math.sqrt(<span class="hljs-number">0.5</span>)<br>        x = <span class="hljs-variable language_">self</span>.bmm(x, encoder_out[<span class="hljs-number">0</span>])<br><br>        <span class="hljs-comment"># don&#x27;t attend over padding</span><br>        <span class="hljs-keyword">if</span> encoder_padding_mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            x = (<br>                x.<span class="hljs-built_in">float</span>()<br>                .masked_fill(encoder_padding_mask.unsqueeze(<span class="hljs-number">1</span>), <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>))<br>                .type_as(x)<br>            )  <span class="hljs-comment"># FP16 support: cast to float and back</span><br><br>        <span class="hljs-comment"># softmax over last dim</span><br>        sz = x.size()<br>        x = F.softmax(x.view(sz[<span class="hljs-number">0</span>] * sz[<span class="hljs-number">1</span>], sz[<span class="hljs-number">2</span>]), dim=<span class="hljs-number">1</span>)<br>        x = x.view(sz)<br>        attn_scores = x<br><br>        x = <span class="hljs-variable language_">self</span>.bmm(x, encoder_out[<span class="hljs-number">1</span>])<br><br>        <span class="hljs-comment"># scale attention output (respecting potentially different lengths)</span><br>        s = encoder_out[<span class="hljs-number">1</span>].size(<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> encoder_padding_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            x = x * (s * math.sqrt(<span class="hljs-number">1.0</span> / s))<br>        <span class="hljs-keyword">else</span>:<br>            s = s - encoder_padding_mask.type_as(x).<span class="hljs-built_in">sum</span>(<br>                dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span><br>            )  <span class="hljs-comment"># exclude padding</span><br>            s = s.unsqueeze(-<span class="hljs-number">1</span>)<br>            x = x * (s * s.rsqrt())<br><br>        <span class="hljs-comment"># project back</span><br>        x = (<span class="hljs-variable language_">self</span>.out_projection(x) + residual) * math.sqrt(<span class="hljs-number">0.5</span>)<br>        <span class="hljs-keyword">return</span> x, attn_scores<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">make_generation_fast_</span>(<span class="hljs-params">self, beamable_mm_beam_size=<span class="hljs-literal">None</span>, **kwargs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Replace torch.bmm with BeamableMM.&quot;&quot;&quot;</span><br>        <span class="hljs-keyword">if</span> beamable_mm_beam_size <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>.bmm<br>            <span class="hljs-variable language_">self</span>.add_module(<span class="hljs-string">&quot;bmm&quot;</span>, BeamableMM(beamable_mm_beam_size))<br></code></pre></td></tr></table></figure><h2 id="题外话"><a href="#题外话" class="headerlink" title="题外话"></a>题外话</h2><p>以上说明了我们不能禁锢于成见。</p><p>该博客启发于NORCE举办的地质预报挑战赛，笔者幸运地获得了第一名。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://sh-tsang.medium.com/review-convolutional-sequence-to-sequence-learning-convs2s-510a9eddce05">https://sh-tsang.medium.com/review-convolutional-sequence-to-sequence-learning-convs2s-510a9eddce05</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>计算机视觉</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Child Mind Institute历届比赛获胜方案</title>
    <link href="/2025/20250530/"/>
    <url>/2025/20250530/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Child Mind Institute（下称CMI）之前在 Kaggle 上举办了两场竞赛，一场与检测睡眠状态有关，另一场与检测有问题的互联网使用有关。加上这个月开始举办的关于检测强迫性重复行为的比赛总计3场。</p><span id="more"></span><h2 id="识别睡眠状态"><a href="#识别睡眠状态" class="headerlink" title="识别睡眠状态"></a><a href="https://www.kaggle.com/competitions/child-mind-institute-detect-sleep-states/overview">识别睡眠状态</a></h2><h3 id="比赛数据介绍"><a href="#比赛数据介绍" class="headerlink" title="比赛数据介绍"></a>比赛数据介绍</h3><p>数据量986.46MB</p><p>该数据集包含大约500个手腕佩戴的加速度计数据的多日记录，这些记录被注释为两种事件类型：睡眠开始，和睡眠结束。任务是检测加速度计序列中这两种事件的发生。</p><p>对于这些数据，有以下几条具体的指导方针：</p><ul><li>一个睡眠周期必须至少持续30分钟。</li><li>一个睡眠期可能会被不超过30分钟的活动中断。</li><li>除非手表被认为在整段时间内佩戴着，否则无法检测到不睡觉的时间窗口。</li><li>夜晚中最长的睡眠窗口是唯一被记录的。</li><li>如果无法识别有效的睡眠窗口，则不会记录该夜的入睡或唤醒事件。</li><li>睡眠事件不需要跨越日界线，因此没有硬性规定定义在给定时间内可能发生的次数。然而，每晚不应分配超过一个窗口。例如，同一天内，一个人有01:00-06:00和19:00-23:30的睡眠窗口是有效的，尽管分配在连续的夜晚。</li><li>记录的夜晚数大致等于该系列中的24小时周期数。</li></ul><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><p>评估指标是mean average precision（mAP）</p><h3 id="第一名-sakami"><a href="#第一名-sakami" class="headerlink" title="第一名@sakami"></a>第一名@sakami</h3><p><img src="/2025/20250530/model_structure.jpeg"></p><ul><li>SEScale</li></ul><p>在输入缩放方面，使用了 SEModule。（<a href="https://arxiv.org/abs/1709.01507%EF%BC%89">https://arxiv.org/abs/1709.01507）</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SEScale</span>(nn.Module):<br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, ch: <span class="hljs-built_in">int</span>, r: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>       <span class="hljs-built_in">super</span>().__init__()<br>       <span class="hljs-variable language_">self</span>.fc1 = nn.Linear(ch, r)<br>       <span class="hljs-variable language_">self</span>.fc2 = nn.Linear(r, ch)<br><br>   <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: torch.FloatTensor</span>) -&gt; torch.FloatTensor:<br>       h = <span class="hljs-variable language_">self</span>.fc1(x)<br>       h = F.relu(h)<br>       h = <span class="hljs-variable language_">self</span>.fc2(h).sigmoid()<br>       <span class="hljs-keyword">return</span> h * x<br></code></pre></td></tr></table></figure><ul><li>Minute connection</li></ul><p>当真实事件发生时，分钟存在偏差。为解决这个问题，在最终层中，与分钟相关的特征被分别连接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, num_x: torch.FloatTensor, cat_x: torch.LongTensor</span>) -&gt; torch.FloatTensor:<br>    cat_embeddings = [embedding(cat_x[:, :, i]) <span class="hljs-keyword">for</span> i, embedding <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.category_embeddings)]<br>    num_x = <span class="hljs-variable language_">self</span>.numerical_linear(num_x)<br><br>x = torch.cat([num_x] + cat_embeddings, dim=<span class="hljs-number">2</span>)<br>x = <span class="hljs-variable language_">self</span>.input_linear(x)<br>x = <span class="hljs-variable language_">self</span>.conv(x.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)).transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)<br><br><span class="hljs-keyword">for</span> gru <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.gru_layers:<br>    x, _ = gru(x)<br><br>x = <span class="hljs-variable language_">self</span>.dconv(x.transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)).transpose(-<span class="hljs-number">1</span>, -<span class="hljs-number">2</span>)<br>minute_embedding = <span class="hljs-variable language_">self</span>.minute_embedding(cat_x[:, :, <span class="hljs-number">0</span>])<br>x = <span class="hljs-variable language_">self</span>.output_linear(torch.cat([x, minute_embedding], dim=<span class="hljs-number">2</span>))<br><span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>每系列数据被分为每日的数据块，间隔为 0.35 天。在训练过程中，每个数据块的一半在每个 epoch 中使用。</p><h4 id="目标值处理"><a href="#目标值处理" class="headerlink" title="目标值处理"></a>目标值处理</h4><p>基于与真实事件距离创建衰减目标，距离越大，值越小。</p><p><img src="/2025/20250530/target.png"></p><p>目标在每个 epoch 中都会更新以进一步衰减。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># update target</span><br>targets = np.where(targets == <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>, (targets - (<span class="hljs-number">1.0</span> / config.n_epochs)).clip(<span class="hljs-built_in">min</span>=<span class="hljs-number">0.0</span>))<br></code></pre></td></tr></table></figure><h4 id="周期性滤波器"><a href="#周期性滤波器" class="headerlink" title="周期性滤波器"></a>周期性滤波器</h4><p>当测量设备被移除时，数据中存在日周期性。这被用来基于规则预测这些周期，并用作输入和预测的过滤器。</p><p><img src="/2025/20250530/inbox.png"></p><h4 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h4><p>1.数据特点</p><p>目标事件的第二个值总是设置为 0。</p><p>2.创建第二级模型</p><p>第一级模型的预测被训练为识别与真实值在特定范围内的事件为正。</p><p>第二级模型将这些转化为每分钟真实值事件存在的概率。</p><p>第二级模型首先以每分钟为单位对第一级模型的预测进行平均，然后使用高度为 0.001、距离为 8 的 <code>find_peaks</code> 检测这些平均值中的峰值。</p><ul><li>根据检测到的峰值，从原始时间序列中创建数据块，每个峰值前后各捕获 8 分钟。<ul><li>其中的<code>step_size</code>至关重要，因为正负样本的比例取决于包含的步数，这会影响后续阶段的准确性。因此调整了步数以获得最佳性能。</li><li>如果数据块是连接的，它们被视为一个数据块。</li></ul></li></ul><p>对于每个数据块，汇总了第 1 个模型的预测结果以及其他特征，如 anglez 和 enmo。这些汇总特征随后用于训练 LightGBM 和 CatBoost 等模型。</p><p>将每个数据块视为一个序列来训练 CNN-RNN、CNN 和 Transformer 模型。</p><h3 id="第二名-K-MAT"><a href="#第二名-K-MAT" class="headerlink" title="第二名@K_MAT"></a>第二名@K_MAT</h3><h4 id="第一阶段：事件检测与睡眠-清醒分类"><a href="#第一阶段：事件检测与睡眠-清醒分类" class="headerlink" title="第一阶段：事件检测与睡眠&#x2F;清醒分类"></a>第一阶段：事件检测与睡眠&#x2F;清醒分类</h4><p><img src="/2025/20250530/1ststage.png"></p><p><img src="/2025/20250530/1ststagepp.png"></p><h4 id="第二阶段：考虑每天最多-2-次事件的限制，重新评估置信度"><a href="#第二阶段：考虑每天最多-2-次事件的限制，重新评估置信度" class="headerlink" title="第二阶段：考虑每天最多 2 次事件的限制，重新评估置信度"></a>第二阶段：考虑每天最多 2 次事件的限制，重新评估置信度</h4><p><img src="/2025/20250530/2ndstage_r.png"></p><h4 id="第三阶段：尽可能添加更多事件"><a href="#第三阶段：尽可能添加更多事件" class="headerlink" title="第三阶段：尽可能添加更多事件"></a>第三阶段：尽可能添加更多事件</h4><ul><li>准备 2 个 CNN 模型</li><li>每个模型在第一阶段对 10(5 折 x2 种子)的预测进行平均</li><li>对每个模型运行第二阶段</li><li>2 个模型的 Weighted Boxes Fusion（WBF）式集成</li></ul><p>WBF出自<a href="https://arxiv.org/pdf/1910.13302">《Weighted boxes fusion: Ensembling boxes from different object detection models》</a>。</p><p>在这里，具体而言：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">weighted_fusion_ensemble</span>(<span class="hljs-params">df_0: pd.DataFrame,</span><br><span class="hljs-params">                             df_1: pd.DataFrame,</span><br><span class="hljs-params">                             distance_threshold: <span class="hljs-built_in">int</span> = <span class="hljs-number">12</span>,</span><br><span class="hljs-params">                             model_weights: <span class="hljs-built_in">list</span> = [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>]</span>) -&gt; pd.DataFrame:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    对来自两个模型 (df_0, df_1) 的预测结果（包含 &#x27;step&#x27; 和 &#x27;score&#x27;）进行加权融合。</span><br><span class="hljs-string">    融合是按 &#x27;series_id&#x27; 分组进行的。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    参数:</span><br><span class="hljs-string">    df_0 (pd.DataFrame): 第一个模型的预测结果 DataFrame，</span><br><span class="hljs-string">                         必须包含 &#x27;series_id&#x27;, &#x27;step&#x27;, &#x27;score&#x27; 列。</span><br><span class="hljs-string">    df_1 (pd.DataFrame): 第二个模型的预测结果 DataFrame，</span><br><span class="hljs-string">                         必须包含 &#x27;series_id&#x27;, &#x27;step&#x27;, &#x27;score&#x27; 列。</span><br><span class="hljs-string">    distance_threshold (int): &#x27;step&#x27; 值的距离阈值。如果来自 df_0 和 df_1 的两个 step</span><br><span class="hljs-string">                              的绝对差小于此阈值，则认为它们匹配。</span><br><span class="hljs-string">    model_weights (list): 包含两个浮点数的列表，分别代表 df_0 和 df_1 的基础权重。</span><br><span class="hljs-string">                          这些权重会被归一化。</span><br><span class="hljs-string"></span><br><span class="hljs-string">    返回:</span><br><span class="hljs-string">    pd.DataFrame: 包含融合后预测结果的 DataFrame。</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># 1. 归一化模型权重，使其总和为1</span><br>    normalized_model_weights = [model_weights[<span class="hljs-number">0</span>] / <span class="hljs-built_in">sum</span>(model_weights),<br>                                model_weights[<span class="hljs-number">1</span>] / <span class="hljs-built_in">sum</span>(model_weights)]<br><br>    large_val = <span class="hljs-number">1e8</span>  <span class="hljs-comment"># 用于标记已处理的 step 的一个大数值</span><br>    series_ids = df_0[<span class="hljs-string">&#x27;series_id&#x27;</span>].unique()  <span class="hljs-comment"># 获取所有唯一的 series_id</span><br>    out_df_list = []  <span class="hljs-comment"># 用于存储每个 series_id 处理后的结果</span><br><br>    <span class="hljs-comment"># 2. 按 series_id 遍历处理</span><br>    <span class="hljs-keyword">for</span> series_id <span class="hljs-keyword">in</span> series_ids:<br>        <span class="hljs-comment"># 复制当前 series_id 的数据，避免修改原始 DataFrame</span><br>        df_0_id = df_0[df_0[<span class="hljs-string">&#x27;series_id&#x27;</span>] == series_id].copy()<br>        df_1_id = df_1[df_1[<span class="hljs-string">&#x27;series_id&#x27;</span>] == series_id].copy()<br><br>        <span class="hljs-comment"># 按 &#x27;score&#x27; 降序排序，确保高分预测优先处理/匹配</span><br>        df_0_id = df_0_id.sort_values(<span class="hljs-string">&quot;score&quot;</span>, ascending=<span class="hljs-literal">False</span>).reset_index(drop=<span class="hljs-literal">True</span>)<br>        df_1_id = df_1_id.sort_values(<span class="hljs-string">&quot;score&quot;</span>, ascending=<span class="hljs-literal">False</span>).reset_index(drop=<span class="hljs-literal">True</span>)<br><br>        <span class="hljs-comment"># 提取 &#x27;step&#x27; 和 &#x27;score&#x27; 为 numpy 数组以提高效率</span><br>        <span class="hljs-comment"># df_0_id 的 steps_0 和 scores_0 将作为融合的基础</span><br>        steps_0 = df_0_id[<span class="hljs-string">&#x27;step&#x27;</span>].values.copy()<br>        scores_0 = df_0_id[<span class="hljs-string">&#x27;score&#x27;</span>].values.copy()<br>        steps_1 = df_1_id[<span class="hljs-string">&#x27;step&#x27;</span>].values.copy()<br>        scores_1 = df_1_id[<span class="hljs-string">&#x27;score&#x27;</span>].values.copy()<br><br>        not_assigned_predictions_from_df1 = [] <span class="hljs-comment"># 存储 df_1 中未匹配的预测</span><br><br>        <span class="hljs-comment"># 3. 遍历 df_1 中的每一个预测，尝试与 df_0 中的预测进行匹配和融合</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(steps_1)):<br>            step_1_current = steps_1[i]<br>            score_1_current = scores_1[i]<br><br>            <span class="hljs-comment"># 计算当前 df_1 预测与 df_0 所有（未被标记为 large_val 的）预测之间的距离</span><br>            dists = np.<span class="hljs-built_in">abs</span>(steps_0 - step_1_current)<br>            argmin_dist = np.argmin(dists) <span class="hljs-comment"># 找到距离最小的 df_0 预测的索引</span><br>            min_dist_val = dists[argmin_dist]<br><br>            <span class="hljs-comment"># 如果最小距离小于阈值，则认为匹配成功</span><br>            <span class="hljs-keyword">if</span> min_dist_val &lt; distance_threshold:<br>                <span class="hljs-comment"># 获取匹配到的 df_0 预测的 step 和 score</span><br>                matched_step_0 = steps_0[argmin_dist]<br>                matched_score_0 = scores_0[argmin_dist]<br><br>                <span class="hljs-comment"># 计算用于加权平均的权重，这些权重基于模型的基础权重和各自预测的原始分数</span><br>                <span class="hljs-comment"># 这种加权方式使得原始分数越高的预测在融合中占主导地位</span><br>                weight_for_avg_0 = normalized_model_weights[<span class="hljs-number">0</span>] * matched_score_0<br>                weight_for_avg_1 = normalized_model_weights[<span class="hljs-number">1</span>] * score_1_current<br><br>                <span class="hljs-comment"># 计算融合后的新 step 和 new score</span><br>                <span class="hljs-comment"># 分母为0的情况（理论上score&gt;0时不会发生）需要注意，但这里未显式处理</span><br>                denominator = weight_for_avg_0 + weight_for_avg_1<br>                <span class="hljs-keyword">if</span> denominator == <span class="hljs-number">0</span>: <span class="hljs-comment"># 避免除以零，尽管在score&gt;0时不太可能</span><br>                    new_score = (matched_score_0 + score_1_current) / <span class="hljs-number">2</span><br>                    new_step = (matched_step_0 + step_1_current) / <span class="hljs-number">2</span><br>                <span class="hljs-keyword">else</span>:<br>                    new_score = (matched_score_0 * weight_for_avg_0 + score_1_current * weight_for_avg_1) / denominator<br>                    new_step = (matched_step_0 * weight_for_avg_0 + step_1_current * weight_for_avg_1) / denominator<br><br>                <span class="hljs-comment"># 更新 df_0_id 中匹配到的预测的 score 和 step</span><br>                df_0_id.loc[argmin_dist, <span class="hljs-string">&quot;score&quot;</span>] = new_score<br>                df_0_id.loc[argmin_dist, <span class="hljs-string">&quot;step&quot;</span>] = new_step<br><br>                <span class="hljs-comment"># 将 df_0 中已匹配的 step 标记为一个大值，防止它被再次匹配</span><br>                steps_0[argmin_dist] = large_val<br>                <span class="hljs-comment"># 同时更新 scores_0 数组，尽管它在后续迭代中不直接用于距离计算，但保持一致性</span><br>                scores_0[argmin_dist] = new_score<br><br><br>            <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 如果没有在 df_0 中找到足够近的匹配项</span><br>                <span class="hljs-comment"># 将这个来自 df_1 的未匹配预测暂存起来</span><br>                <span class="hljs-comment"># 其分数按其模型的权重进行缩放</span><br>                unmatched_pred_df1 = df_1_id.iloc[[i]].copy() <span class="hljs-comment"># 获取原始行</span><br>                unmatched_pred_df1[<span class="hljs-string">&#x27;score&#x27;</span>] = score_1_current * normalized_model_weights[<span class="hljs-number">1</span>]<br>                not_assigned_predictions_from_df1.append(unmatched_pred_df1)<br><br>        <span class="hljs-comment"># 4. 处理 df_0 中未被任何 df_1 预测匹配到的预测</span><br>        <span class="hljs-comment"># 这些预测的分数也需要按其模型的权重进行缩放</span><br>        <span class="hljs-comment"># `steps_0 != large_val` 条件用于选取那些未被标记（即未被匹配和融合）的原始 df_0 预测</span><br>        df_0_id.loc[steps_0 != large_val, <span class="hljs-string">&quot;score&quot;</span>] *= normalized_model_weights[<span class="hljs-number">0</span>]<br><br>        <span class="hljs-comment"># 5. 收集当前 series_id 的结果</span><br>        out_df_list.append(df_0_id)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(not_assigned_predictions_from_df1) &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 合并所有来自 df_1 的未匹配预测</span><br>            concatenated_not_assigned = pd.concat(not_assigned_predictions_from_df1)<br>            out_df_list.append(concatenated_not_assigned)<br><br>    <span class="hljs-comment"># 6. 将所有 series_id 的处理结果合并成一个 DataFrame</span><br>    final_out_df = pd.concat(out_df_list).reset_index(drop=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> final_out_df<br></code></pre></td></tr></table></figure><p>优先将两个模型中“相似”（<code>step</code>距离近）的预测进行智能融合，融合后的结果会赋予更高的权重（通过原始分数的参与）。</p><p>对于未能找到匹配项的预测，则保留它们，但其分数会根据其来源模型的权重进行调整。</p><ul><li>运行第三阶段</li></ul><p><img src="/2025/20250530/Ensemble.png"></p><h3 id="第三名-FNOA"><a href="#第三名-FNOA" class="headerlink" title="第三名@FNOA"></a>第三名@FNOA</h3><p><img src="/2025/20250530/sleep3.png"></p><h4 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h4><p>对于最终提交，GRU + UNET 模型仅使用 7 个特征。</p><h5 id="数据集构建"><a href="#数据集构建" class="headerlink" title="数据集构建"></a>数据集构建</h5><p>将序列分为一天一段的序列，并将粒度从 5 秒降低到 30 秒。</p><p>因此得到了长度为 2880 的序列，其中通常包含一个睡眠开始和一个醒来。</p><h5 id="噪声处理也重要"><a href="#噪声处理也重要" class="headerlink" title="噪声处理也重要"></a>噪声处理也重要</h5><p>当在相同的小时、分钟和秒内，同一系列中重复出现完全相同的值时，这基本上就是噪声。红线是检测到的噪声。</p><p><img src="/2025/20250530/sleep31.png"></p><h2 id="问题性互联网使用"><a href="#问题性互联网使用" class="headerlink" title="问题性互联网使用"></a>问题性互联网使用</h2><p><strong>shakeup的比赛</strong></p><h3 id="比赛数据介绍-1"><a href="#比赛数据介绍-1" class="headerlink" title="比赛数据介绍"></a>比赛数据介绍</h3><p>6.73GB</p><ul><li><code>Demographics</code> - 参与者的年龄和性别信息。</li><li><code>Internet Use</code> - 每天使用电脑&#x2F;互联网的小时数。</li><li><code>Children&#39;s Global Assessment Scale</code> - 心理健康临床医生用于评估 18 岁以下青少年一般功能的数值量表。</li><li><code>Physical Measures</code> - 血压、心率、身高、体重和腰围、臀围的测量数据集合。</li><li><code>FitnessGram Vitals and Treadmill</code> - 使用 NHANES 跑步机协议评估的心血管健康测量。</li><li><code>FitnessGram Child</code> - 健康相关的体能评估，测量五个不同参数，包括有氧能力、肌肉力量、肌肉耐力、柔韧性和身体成分。</li><li><code>Bio-electric Impedance Analysis</code> - 关键身体成分指标的测量，包括 BMI、脂肪、肌肉和水分含量。</li><li><code>Physical Activity Questionnaire</code> - 关于儿童在过去 7 天内参与剧烈活动的情况。</li><li><code>Sleep Disturbance Scale</code> - 用于对儿童睡眠障碍进行分类的量表。</li><li><code>Actigraphy</code> - 通过研究级生物追踪器对生态物理活动进行客观测量。</li><li><code>Parent-Child Internet Addiction Test</code> - 20 项量表，用于测量与强迫性互联网使用相关的特征和行为，包括强迫性、逃避性和依赖性。</li></ul><p>目标 <code>sii</code> ： <code>0</code> 对应 <code>None</code> ， <code>1</code> 对应 <code>Mild</code> ， <code>2</code> 对应 <code>Moderate</code> ， <code>3</code> 对应 <code>Severe</code> 。</p><h3 id="评估指标-1"><a href="#评估指标-1" class="headerlink" title="评估指标"></a>评估指标</h3><p>基于二次加权 kappa 系数</p><h3 id="第一名-LENNART-HAUPTS"><a href="#第一名-LENNART-HAUPTS" class="headerlink" title="第一名@LENNART HAUPTS"></a>第一名@LENNART HAUPTS</h3><p>模型为投票集成，包括：</p><ul><li>LGBMRegressor</li><li>两个 XGBoost 回归器</li><li>CatBoostRegressor</li><li>ExtraTreesRegressor</li></ul><h4 id="数据清洗、特征工程和插补"><a href="#数据清洗、特征工程和插补" class="headerlink" title="数据清洗、特征工程和插补"></a>数据清洗、特征工程和插补</h4><p><strong>数据清洗：</strong></p><ul><li>移除了不合理的值，例如超过 60%的身体脂肪百分比或负的骨矿物质含量，并用 NaN 替换。</li></ul><p><strong>特征工程：</strong></p><ul><li>创建了多种描述性的活动追踪器特征，并为白天和夜晚设置了不同的掩码。</li><li>使用 PCA 对活动追踪器数据进行降维保留了 15 个成分。</li><li>额外包含的特征基于年龄组均值进行归一化的值，以及其他看似合理的特征，如每日能量消耗与基础代谢率之间的差异。</li><li>对大部分特征应用了分位数分箱来处理噪声，效果出奇地好。</li></ul><p><strong>插补：</strong></p><ul><li>Lasso 用于特征插补，由于维度较高且存在噪声。</li><li>使用 Lasso 进行特征插补。对于每个目标列，使用缺失值少于 40%的特征训练模型，并基于训练好的模型插补这些特征中的缺失值。如果找不到可用于插补的有效特征（即缺失值少于 40%的特征），或者有效样本数量过少，解决方案则默认采用均值插补。</li></ul><h4 id="参数调优和特征选择"><a href="#参数调优和特征选择" class="headerlink" title="参数调优和特征选择"></a><strong>参数调优和特征选择</strong></h4><p>在比赛早期，很明显常规的参数调优与交叉验证设置会导致结果不稳定。<br>为解决这一问题：</p><ul><li>在参数调优过程中采用了重复分层 K 折交叉验证。重复次数为 10 到 20 次。计算成本更高，但得到了更稳健的结果。</li><li>基于特征重要性手动进行特征选择，将数据集减少到 39 个特征</li></ul><h3 id="第三名-JOBAYER-HOSSAIN"><a href="#第三名-JOBAYER-HOSSAIN" class="headerlink" title="第三名@JOBAYER HOSSAIN"></a>第三名@JOBAYER HOSSAIN</h3><p> <strong>交叉验证</strong><br>主要关注点之一是建立一个稳定可靠的 CV 框架。</p><p>在整个过程中，避免使用任何固定的随机种子。进行了 100 次 5 折分层 KFold 重复实验获得稳定的结果，而在 Optuna 超参数调优时使用了 20 次重复。<br>为了优化最终的 QWK 阈值，使用了所有这些重复实验的 OOF 预测。</p><p> <strong>模型</strong><br> LightGBM</p><p> <strong>特征工程</strong></p><ul><li>活动记录数据：</li><li>计算了 X、Y、Z 和 AngleZ 的标准差，以及 Elmo 的平均值。</li><li>使用 Elmo 表示的五组最长的非活跃和活跃连续时间特征。</li><li>将”light”列按从黄昏到直射阳光的范围分类，并统计每个类别的值计数。</li><li>仪器数据：</li><li>从公共笔记本开始，检查每个特征是否真的对模型有贡献。之后，根据实验添加了一些自定义特征。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">feature_engineering</span>(<span class="hljs-params">df</span>):<br><br>    <span class="hljs-keyword">for</span> col, (col_min, col_max) <span class="hljs-keyword">in</span> min_max_dict.items():<br>        df[col] = df[col].clip(lower=col_min, upper=col_max)<br><br>    bins = [<span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">18</span>, <span class="hljs-number">100</span>]<br>    labels = [<span class="hljs-string">&#x27;1 to 6&#x27;</span>, <span class="hljs-string">&#x27;7 to 12&#x27;</span>, <span class="hljs-string">&#x27;13 to 18&#x27;</span>, <span class="hljs-string">&#x27;19 to 100&#x27;</span>]<br>    df[<span class="hljs-string">&#x27;Age_Binned&#x27;</span>] = pd.cut(df[<span class="hljs-string">&#x27;Basic_Demos-Age&#x27;</span>], bins=bins, labels=labels, right=<span class="hljs-literal">True</span>)<br>    df[<span class="hljs-string">&#x27;Age_Sex&#x27;</span>] = df[<span class="hljs-string">&#x27;Age_Binned&#x27;</span>].astype(<span class="hljs-built_in">str</span>) + <span class="hljs-string">&#x27;_&#x27;</span> + df[<span class="hljs-string">&#x27;Basic_Demos-Sex&#x27;</span>].astype(<span class="hljs-built_in">str</span>)<br>    <br>    df[<span class="hljs-string">&#x27;BFP_BMI&#x27;</span>] = df[<span class="hljs-string">&#x27;BIA-BIA_Fat&#x27;</span>] / df[<span class="hljs-string">&#x27;BIA-BIA_BMI&#x27;</span>]<br>    df[<span class="hljs-string">&#x27;BFP_BMR&#x27;</span>] = df[<span class="hljs-string">&#x27;BIA-BIA_Fat&#x27;</span>] * df[<span class="hljs-string">&#x27;BIA-BIA_BMR&#x27;</span>]<br>    df[<span class="hljs-string">&#x27;BMR_Weight&#x27;</span>] = df[<span class="hljs-string">&#x27;BIA-BIA_BMR&#x27;</span>] / df[<span class="hljs-string">&#x27;Physical-Weight&#x27;</span>]<br>    <br>    df[<span class="hljs-string">&#x27;Muscle_to_Fat&#x27;</span>] = df[<span class="hljs-string">&#x27;BIA-BIA_SMM&#x27;</span>] / df[<span class="hljs-string">&#x27;BIA-BIA_FMI&#x27;</span>]<br>    df[<span class="hljs-string">&#x27;Hydration_Status&#x27;</span>] = df[<span class="hljs-string">&#x27;BIA-BIA_TBW&#x27;</span>] / df[<span class="hljs-string">&#x27;Physical-Weight&#x27;</span>]<br>    <br>    df[<span class="hljs-string">&#x27;PreInt_FGC_CU_PU&#x27;</span>] = df[<span class="hljs-string">&#x27;PreInt_EduHx-computerinternet_hoursday&#x27;</span>] * df[<span class="hljs-string">&#x27;FGC-FGC_CU&#x27;</span>] * df[<span class="hljs-string">&#x27;FGC-FGC_PU&#x27;</span>]<br>    df[<span class="hljs-string">&#x27;FGC_GSND_GSD_Age&#x27;</span>] = df[<span class="hljs-string">&#x27;FGC-FGC_GSND&#x27;</span>] * df[<span class="hljs-string">&#x27;FGC-FGC_GSD&#x27;</span>] * df[<span class="hljs-string">&#x27;Basic_Demos-Age&#x27;</span>]<br>    df[<span class="hljs-string">&#x27;SDS_Activity&#x27;</span>] = df[<span class="hljs-string">&#x27;BIA-BIA_Activity_Level_num&#x27;</span>] * df[<span class="hljs-string">&#x27;SDS-SDS_Total_T&#x27;</span>]<br>    <br>    df[<span class="hljs-string">&#x27;CGasync_Score_Normalized&#x27;</span>] = df[<span class="hljs-string">&#x27;CGAS-CGAS_Score&#x27;</span>] - df.groupby(<span class="hljs-string">&#x27;Basic_Demos-Enroll_Season&#x27;</span>)[<span class="hljs-string">&#x27;CGAS-CGAS_Score&#x27;</span>].transform(<span class="hljs-string">&#x27;mean&#x27;</span>)<br>    df[<span class="hljs-string">&#x27;Internet_Physical_Difference&#x27;</span>] = df[<span class="hljs-string">&#x27;PreInt_EduHx-computerinternet_hoursday&#x27;</span>] - df[<span class="hljs-string">&#x27;PAQ_A-PAQ_A_Total&#x27;</span>]<br>   <br>    df[df.select_dtypes(include=<span class="hljs-string">&#x27;object&#x27;</span>).columns] = df.select_dtypes(include=<span class="hljs-string">&#x27;object&#x27;</span>).astype(<span class="hljs-string">&#x27;category&#x27;</span>)<br>    <span class="hljs-keyword">return</span> df<br></code></pre></td></tr></table></figure><p> <strong>数据增强</strong></p><ul><li><p>NaN 增强：<br>最初在已有缺失值的列中随机填充 NaN。<br>后来在所有含有 NaN 的列中填充了 20%的 NaN 数据，并将这种增强数据与原始数据集合并。</p></li><li><p>高斯噪声和填充：<br>进行了简单填充，并向 20%的数据中添加了高斯噪声。这种增强数据随后与原始数据集合并。</p></li></ul><p> <strong>后处理</strong><br>使用’PCIAT-PCIAT_Total’这一列进行训练。应用了优化后的阈值来计算每个 100*5 模型的 sii，并取众数来生成最终预测。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DDPM加速</title>
    <link href="/2025/20250527/"/>
    <url>/2025/20250527/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>正如DDIM作者在其论文中所写，“从 DDPM 中采样 50k 个 32×32 大小的图像大约需要 20 小时，但在 Nvidia 2080 Ti GPU 上从 GAN 中完成这一操作不到一分钟。”</p><p>DDPM等扩散模型的慢速度一直被诟病。</p><p>在这篇博客中，将介绍一部分的加速模型。</p><span id="more"></span><h2 id="DDIM"><a href="#DDIM" class="headerlink" title="DDIM"></a>DDIM</h2><p> ICLR 2021。2010年10月发表。</p><p>让我们回忆一下DDPM。</p><p><img src="/2025/20250527/diffusion13.png"></p><p>延续符号：</p><p>$\bar \alpha_t&#x3D;\alpha_1\alpha_2…\alpha_t$</p><p>$a_t&#x3D;1-\beta_t$</p><p>$\tilde \beta_t&#x3D;\frac{1-\bar \alpha _ {t-1}}{1-\bar \alpha _ {t}}\beta_t$</p><p>有<br>$$<br>\begin{align}<br>x_0&amp;&#x3D;\frac{x_t-\sqrt{1-\bar\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}<br>\\<br>q(x _ {t-1}\mid x_t,x_0)&amp;&#x3D;N(\frac{\sqrt{\alpha _ {t-1}}\beta_tx_0+\sqrt{\alpha_t}(1-\bar\alpha _ {t-1})x_t}{1-\bar\alpha_t},\frac{1-\bar \alpha _ {t-1}}{1-\bar \alpha _ {t}}\beta_tI)<br>\\&amp;&#x3D;N(\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon\right),\frac{1-\bar \alpha _ {t-1}}{1-\bar \alpha _ {t}}\beta_tI)<br>\end{align}<br>$$</p><p>我们将方差分离开，（或者叫做重参数化），即：</p><p>$$<br>\begin{align}<br>q(x _ {t-1}\mid x_t,x_0)&amp;&#x3D;\frac{1}{\sqrt{\alpha_t}}\left(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon\right)+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\frac{1}{\sqrt{\alpha_t}}x_t-\frac{1}{\sqrt{\alpha_t}}\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\frac{\sqrt{\bar\alpha _ {t-1}}}{\sqrt{\bar\alpha _ {t}}} x_t<br>-\frac{1}{\sqrt{\alpha_t}}\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\sqrt{\bar\alpha _ {t-1}} \frac{x_t-\sqrt{1-\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}<br>+\sqrt{\bar\alpha _ {t-1}} \frac{\sqrt{1-\alpha_t}\epsilon}{\sqrt{\bar\alpha_t}}<br>-\frac{1}{\sqrt{\alpha_t}}\frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon<br>+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\sqrt{\bar\alpha _ {t-1}} x_0<br>+\frac{1}{\sqrt{\alpha_t}}\frac{1-\bar\alpha_t-(1-\alpha_t)}{\sqrt{1-\bar\alpha_t}}\epsilon<br>+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\sqrt{\bar\alpha _ {t-1}} x_0<br>+\frac{1}{\sqrt{\alpha_t}}\frac{\alpha_t-\bar\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon<br>+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\sqrt{\bar\alpha _ {t-1}} x_0<br>+\frac{1}{\sqrt{\alpha_t}}\frac{\alpha_t(1-\bar\alpha _ {t-1})}{\sqrt{1-\bar\alpha_t}}\epsilon<br>+\sqrt{\tilde \beta_t}N(0,1)<br>\\&amp;&#x3D;\sqrt{\bar\alpha _ {t-1}} x_0<br>+\sqrt{1-\alpha _ {t-1}-\tilde\beta_t}\epsilon<br>+\sqrt{\tilde \beta_t}N(0,1)<br>\end{align}<br>$$</p><p>我们干脆把$\sqrt{\tilde\beta_t}$定义为超参数$\sigma_t$，即$\sigma_t^2&#x3D;\eta \tilde\beta_t$。当$\eta&#x3D;0$为DDIM，当$\eta&#x3D;1$时为DDPM。</p><p>方差为0有个好处，就是它变为了确定性的，我们就可以像GAN那样进行插值。</p><p>虽然我们破坏了DDPM的$q(x _ {t-1}\mid x_t,x_0)$中原有的性质，但是我们并未破坏$q(x _ t\mid x_0)$的性质。这从推导过程中是可以看出来的。所以这样做也是可行的。</p><h2 id="IDDPM"><a href="#IDDPM" class="headerlink" title="IDDPM"></a>IDDPM</h2><p>2021年2月发表。</p><p>《Improved Denoising Diffusion Probabilistic Models》</p><p>我们同样对方差进行操作，不过我们让神经网络去学习它。我们预测一个参数v，并令：<br>$$<br>\sigma_t^2(x,t)&#x3D;exp(vlog\beta_t+(1-v)log\tilde\beta_t)<br>$$<br>同时我们也在损失函数中加入与这个项相关的。</p><h2 id="TDPM"><a href="#TDPM" class="headerlink" title="TDPM"></a>TDPM</h2><p>ICLR 2023。2022年2月发表。</p><p>《Truncated Diffusion Probabilistic Models and Diffusion-based Adversarial Auto-Encoders》</p><p><img src="/2025/20250527/tdpm.png"></p><p>作者选择不是被扩散到纯高斯噪声 xT，而是只扩散到一个中间的、噪声程度较低的时刻$T _ {trunc}$。</p><p>且$q(x _ {trunc}\mid x_0)$仍然遵循标准的扩散定义:<br>$$<br>X _ {trunc}&#x3D;\sqrt{\bar\alpha}x_0+\sqrt{1-\bar\alpha}\epsilon<br>$$<br>但是$q(X _ {trunc})$不再是高斯分布，所以作者使用GAN去生成它。</p><h2 id="渐进式蒸馏"><a href="#渐进式蒸馏" class="headerlink" title="渐进式蒸馏"></a>渐进式蒸馏</h2><p>另一个常用于加速&#x2F;量化的策略是知识蒸馏。</p><p>这篇正是这样做的。</p><p>ICLR2022，2022二月发表。</p><p>《Progressive Distillation for Fast Sampling of Diffusion Models》</p><p><img src="/2025/20250527/tea1.png"></p><p>使用学生模型去学习如何用<strong>一步</strong>来模拟教师模型<strong>多步</strong>（通常是两步）的去噪效果。</p><p><img src="/2025/20250527/tea.png"></p><h2 id="DPMSolver"><a href="#DPMSolver" class="headerlink" title="DPMSolver"></a>DPMSolver</h2><p>Neurips 2022。2022年6月发表。</p><p>DPMSolver由于涉及到更高维度，比如从ODE&#x2F;SDE的角度上俯瞰问题。</p><p>并非主要是模型上的改进，<strong>故我们不在这里做过多描述。</strong></p><p>具体而言，扩散模型可以写成ODE的形式，但是标准的一阶 ODE 求解器（如欧拉法，注意与上一章中的欧拉-丸山法作区分）在求解上述 ODE 时，为了保证精度，通常需要非常小的步长，导致采样步数很多（例如 1000 步）。</p><p>DPMSolver的核心思想来源于观察到扩散模型中的 ODE 具有特殊的<strong>半线性结构 (semi-linear structure)</strong>。</p><p>我们简要介绍一下一阶DPMSolver。</p><h3 id="欧拉法"><a href="#欧拉法" class="headerlink" title="欧拉法"></a>欧拉法</h3><p>欧拉法是最简单、最基础的 ODE 数值求解器。它的核心思想是用当前点的导数（即切线斜率）来线性外推到下一个点。</p><p>假设我们知道在时刻$t_i$的样本$x_i$，以及此时的导数$F(x_i,t_i)$。如果我们想前进一个时间步长$h&#x3D;t _ {i+1}-t_i$：<br>$$<br>x _ {i+1}\approx x_i+hF(x_i,t_i)<br>$$</p><h3 id="一阶-DPMSolver"><a href="#一阶-DPMSolver" class="headerlink" title="一阶 DPMSolver"></a>一阶 DPMSolver</h3><p>扩散模型的 ODE 通常可以被看作或转化为 $dx&#x2F;d\lambda&#x3D;LinearPart(\lambda,x)+NonLinearPart(\lambda,\epsilon_\theta(x,\lambda))$，其中$\lambda_t&#x3D;log(\bar\alpha_t&#x2F;\sigma_t^2)$。</p><p>我们定义为：</p><p>$$<br>\frac{d{x}}{d\lambda} &#x3D; F(\lambda){x} + G(\lambda){\epsilon}_\theta({x}, \lambda)<br>$$</p><p>其中：</p><ul><li>${x}$ 是当前的样本。</li><li>$\lambda$ 是重新参数化后的时间或噪声水平。</li><li>$F(\lambda){x}$ 是关于 ${x}$ 的线性部分。</li><li>$G(\lambda){\epsilon} _ \theta({x}, \lambda)$ 是非线性部分，其中 ${\epsilon} _ \theta({x}, \lambda)$ 是神经网络对噪声的预测。</li></ul><p>在从当前时间 $\lambda_i$ 到下一个时间 $\lambda _ {i+1}$ 的一个小子区间内，我们将非线性相关的部分（即神经网络的输出 ${\epsilon} _ \theta({x}, \lambda)$）近似为在该区间开始时的值，即 $\epsilon _ {\theta,i} &#x3D; {\epsilon}_\theta({x}_i, \lambda _ i)$，并将其视为常数。</p><p>这样，ODE 变为：</p><p>$$<br>\frac{d{x}}{d\lambda} &#x3D; F(\lambda){x} + G(\lambda){\epsilon} _ {\theta,i}<br>$$</p><p>这是一个关于 ${x}$ 的一阶线性非齐次微分方程。</p><p>代入定义的：<br>$$<br>\begin{align}<br>F(\lambda)&amp;&#x3D;\frac{1}{2}\frac{dlog\sigma_\lambda^2}{d\lambda}<br>\\<br>G(\lambda)&amp;&#x3D;-\frac{\sigma_\lambda}{2\alpha_\lambda}\frac{dlog\sigma_\lambda^2}{d\lambda}<br>\end{align}<br>$$</p><p>可得更新公式为：<br>$$<br>x _ {i+1}&#x3D;\frac{\sigma(\lambda _ {i+1})}{\sigma(\lambda _ {i})}x_I-\sigma(\lambda _ {i+1})(e^k-1)\epsilon_\theta(x_i,\lambda_i))<br>$$</p><h2 id="Consistency-Models"><a href="#Consistency-Models" class="headerlink" title="Consistency Models"></a>Consistency Models</h2><p>一作Yang Song。Ilya Sutskever通讯。</p><p>ICML 2023,2023 5月发表。</p><p>Consistency Models 的基石是<strong>概率流常微分方程 (Probability Flow Ordinary Differential Equation, PF ODE)</strong>。在扩散模型的连续时间视角下，存在一个 PF ODE，其轨迹可以将任何数据点平滑地转换到不同噪声水平 t 下的含噪版本，反之亦然。</p><p><strong>”一致性“定义为：</strong></p><p>对于任何沿着同一条轨迹的数据点$(x_t,t)$和$(x _ {t’},t’)$，通过一致性函数$f$的映射，都应该得到相同的轨迹起点，即原始数据$x_0$。<br>$$<br>f(x_t,t)&#x3D;x_0\quad \text{for all}\quad t\in[\epsilon,T]<br>$$<br>其中$\epsilon$是一个接近于0很小的正数，T是总的扩散时间。</p><p>对于同一轨迹上的任意两个点，我们期望：<br>$$<br>f(x_t,t)&#x3D;f(x _ {t’},t’)<br>$$<br><strong>边界条件：</strong></p><p>对于任何$f$,都有$f(x_\epsilon，\epsilon)&#x3D;x_\epsilon$</p><p>故$f$可设为：<br>$$<br>f_\theta (x,t)&#x3D;<br>\begin{cases}x\quad &amp;t&#x3D;\epsilon<br>\\<br>F_\theta(x,t)\quad &amp;t\in(\epsilon ,T]<br>\end{cases}<br>$$</p><p>或$f_\theta(x,t)&#x3D;c _ {skip}(t)x+c _ {out}F_\theta(x,t)$，其中c是可微函数，且$c _ {skip}(\epsilon)&#x3D;1$和$c _ {out}(\epsilon)&#x3D;0$。</p><p>有两种训练方法：</p><p><strong>（1）一致性蒸馏（Consistency Distillation ，CD）</strong></p><p>它依赖于一个已经训练好的扩散模型（教师模型）。一致性蒸馏损失定义为:<br>$$<br>\begin{aligned}<br> \mathcal{L}^N_\text{CD} (\theta, \theta^-; \phi) &amp;&#x3D; \mathbb{E}<br> [\lambda(t_n)d(f_\theta(x _ {t _ {n+1}}, t _ {n+1}), f _ {\theta^-}(\hat{x}^\phi _ {t_n}, t_n)]<br> \end{aligned}<br>$$<br>其中<br>$x\sim p _ {data}$，$n \sim \mathcal{U}[1, N-1]$，$x _ {t _ {n+1}}\sim N(x;t _ {n+1}^2I)$。<br>$\hat{x}^\phi _ {t_n} &#x3D; {x} _ {t _ {n+1}} - (t_n - t _ {n+1}) \Phi(x _ {t _ {n+1}}, t _ {n+1}; \phi)$，$\Phi$是在执行一步欧拉法，$\phi$代表教师模型。</p><p>$\theta^-$是$\theta$ 的移动平均。<br>d是距离函数满足$d(x,y)\ge 0$ ，$d(x,y)&#x3D;0$ 仅当$x&#x3D;y$。论文中考虑L1、L2、LPIPS损失。<br>$\lambda$是正权重函数，论文定义为1。</p><p>作者还发现设置stopgrad效果会更好，且稳定训练过程。</p><p>即$\theta^-\leftarrow stopgrad(\mu\theta^-+(1-\mu)\theta)$。</p><p><img src="/2025/20250527/cd.jpg"></p><p><strong>(2)一致性训练(Consistency Training)</strong></p><p><img src="/2025/20250527/ct.jpg"></p><p>在前面的CD中，使用预训练的score model来近似真实的分数函数$\nabla logp_t(x)$。而<br>$$<br>\nabla logp_t(x_t)&#x3D;-E\left[\frac{x_t-x}{t^2}\mid x_t\right]<br>$$</p><blockquote><p>若 $x \sim p _ {\text{data}}(x)$, $x_t \sim \mathcal{N}(x; t^2 I)$, $p_t(x_t) &#x3D; p _ {\text{data}}(x) \otimes \mathcal{N}(0, t^2 I)$，或者写作$p_t(x_t)&#x3D;\int p _ {\text{data}}(x)  \mathcal{N}(x_t;x, t^2 I)dx$。则有 $\nabla \log p_t(x_t) &#x3D; -\mathbb{E}[\frac{x_t - x}{t^2} | x_t]$。</p><p>证明：根据 $p_t(x_t)$的定义, 我们有$\nabla \log p_t(x_t) &#x3D; \nabla _ {x_t} \log \int p _ {\text{data}}(x) p(x_t | x) dx$, 其中 $p(x_t | x) &#x3D; \mathcal{N}(x_t; x, t^2 I)$.<br>$$<br>\begin{align}<br>\nabla \log p_t(x_t) &amp;&#x3D; \frac{\int p_{\text{data}}(x) \nabla_{x_t} p(x_t | x) dx}{\int p_{\text{data}}(x) p(x_t | x) dx} \\<br>&amp;&#x3D; \frac{\int p_{\text{data}}(x) p(x_t | x) \nabla_{x_t} \log p(x_t | x) dx}{\int p_{\text{data}}(x) p(x_t | x) dx} \\<br>&amp;&#x3D; \frac{\int p_{\text{data}}(x) p(x_t | x) \nabla_{x_t} \log p(x_t | x) dx}{p_t(x_t)} \\<br>&amp;&#x3D; \int \frac{p_{\text{data}}(x) p(x_t | x)}{p_t(x_t)} \nabla_{x_t} \log p(x_t | x) dx \\<br>&amp;\stackrel{(Bayes’rule)}{&#x3D;} \int p(x | x_t) \nabla_{x_t} \log p(x_t | x) dx \\<br>&amp;&#x3D; \mathbb{E}[\nabla_{x_t} \log p(x_t | x) | x_t] \\<br>&amp;&#x3D; -\mathbb{E}\left[\frac{x_t - x}{t^2} | x_t\right]<br>\end{align}<br>$$</p></blockquote><p>损失函数变为：<br>$$<br>\mathcal{L}^N_\text{CT} (\theta, \theta^-; \phi) &#x3D; \mathbb{E}<br>[\lambda(t_n)d(f_\theta({x} + t_{n+1} {z},;t_{n+1}), f_{\theta^-}({x} + t_n {z},;t_n)]<br>\text{ where }{z} \in \mathcal{N}({0}, {I})<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>朗之万动力学和diffusion</title>
    <link href="/2025/20250526/"/>
    <url>/2025/20250526/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>朗之万动力学是一种描述粒子在流体中运动的物理模型，它考虑了两种主要的力：<span id="more"></span></p><ol><li><strong>耗散力 (Dissipative Force)</strong>：也称为摩擦力或阻尼力。这种力与粒子的速度成正比，方向相反，模拟了粒子在运动时由于与周围流体分子碰撞而受到的能量损失。数学上通常表示为 −γv，其中 γ 是阻尼系数，v 是粒子的速度。</li><li><strong>随机力 (Random Force)</strong>：也称为噪声项。这种力模拟了流体中分子对粒子的随机碰撞，这些碰撞是无规则且快速变化的。它通常被建模为一个均值为零的高斯白噪声，其强度与系统的温度和阻尼系数有关。这个力是系统能量的来源，使得粒子能够克服耗散力并进行持续的随机运动（布朗运动）。</li></ol><p>粗略而言，朗之万动力学采样依赖于分数（score）函数。而Tweedie公式提供了估计分数函数的方法。</p><p>故我们先介绍Tweedie公式和一些基本概念。</p><h2 id="Tweedie公式和DDPM"><a href="#Tweedie公式和DDPM" class="headerlink" title="Tweedie公式和DDPM"></a>Tweedie公式和DDPM</h2><p>Tweedie公式为$E[x\mid y]&#x3D;y+\sigma^2\nabla log p(y)$。其中$\nabla logp(y)$被称为分数（score）。</p><blockquote><p>证明：</p><p>$$<br>\begin{align}<br>p(y)&amp;&#x3D;\int p(y\mid x)p(x)dx<br>\\\nabla p(y)&amp;&#x3D;\nabla \int p(y\mid x)p(x)dx<br>\\&amp;&#x3D; \int \nabla p(y\mid x)p(x)dx<br>\end{align}<br>$$<br>由$p(y\mid x ) \propto exp(-\frac{1}{2\sigma^2}||y-x||^2)$得，$\nabla p(y\mid  x)&#x3D;-\frac{1}{\sigma^2}(y-x)p(y\mid x)$</p><p>故<br>$$<br>\begin{align}<br>\nabla p(y)&amp;&#x3D; \int -\frac{1}{\sigma^2}(y-x)p(y\mid x)p(x)dx<br>\\&amp;&#x3D;-\frac{1}{\sigma^2}y\int p(y\mid x)p(x)dx+\frac{1}{\sigma^2}\int p(y\mid x)xp(x)dx<br>\\&amp;&#x3D;-\frac{1}{\sigma^2}yp(y)+\frac{1}{\sigma^2}E[x\mid y]p(y)<br>\end{align}<br>$$<br>而$\nabla log p(y)&#x3D;\frac{\nabla p(y)}{p(y)}$，两边同除$p(y)$即可得到。 </p></blockquote><p>沿用DDPM的$q(x_t\mid x_0) &#x3D; \mathcal{N}(x _ {t} ; \sqrt{\bar\alpha_t}x_0, \left(1 - \bar\alpha_t\right)\textbf{I})$，由Tweedie公式有：<br>$$<br>\begin{align}<br>\mathbb{E}\left[\mu _ {x_t}\mid x_t\right] &#x3D; x_t + (1 - \bar\alpha_t)\nabla _ {x_t}\log p(x_t)<br>\\<br>\therefore \sqrt{\bar\alpha_t}x_0 &#x3D; x_t + (1 - \bar\alpha_t)\nabla\log p(x_t)<br>\\<br>    \therefore x_0 &#x3D; \frac{x_t + (1 - \bar\alpha_t)\nabla\log p(x_t)}{\sqrt{\bar\alpha_t}}<br>\end{align}<br>$$<br>对比DDPM中的$x _ 0&#x3D;\frac{x _ t - \sqrt{1 - \bar \alpha _ t} \epsilon _ 0}{\sqrt{\bar\alpha _ t}}$，可以发现$\nabla \log p(x _ t) &#x3D; -\frac{1}{ \sqrt{1 - \bar \alpha _ t}}\epsilon _ 0$。</p><p>DDPM其实也是基于分数函数来去噪。</p><h2 id="去噪分数匹配"><a href="#去噪分数匹配" class="headerlink" title="去噪分数匹配"></a>去噪分数匹配</h2><p>我们不妨考虑以下分布，被称为能量函数：<br>$$<br>\begin{align}<br>    p _ {\boldsymbol{\theta}}(\boldsymbol{x}) &#x3D; \frac{1}{Z _ {\boldsymbol{\theta}}}e^{-f _ {\boldsymbol{\theta}}(\boldsymbol{x})}<br>\end{align}<br>$$<br>我们可以通过神经网络来对学习$\nabla \log p(x)$：<br>$$<br>\begin{align}<br>\nabla _ {x} \log p _ {\theta}(x)<br>&amp;&#x3D; \nabla _ {x}\log(\frac{1}{Z _ {\theta}}e^{-f _ {\theta}(x)})<br>\\<br>&amp;&#x3D; \nabla _ {x}\log\frac{1}{Z _ {\theta}} + \nabla _ {x}\log e^{-f _ {\theta}(x)}\\<br>&amp;&#x3D; -\nabla _ {x} f _ {\theta}(x)\\<br>&amp;&#x3D; s _ {\theta}(x)<br>\end{align}<br>$$<br>具体而言，即我们可以使用神经网络f进行建模，通过任意计算其负梯度来查询分数；或者，更常见的是，我们可以直接使用神经网络对s进行建模。<br>$$<br>\mathbb{E} _ {p(x)}\left[\left\lVert s _ {\theta}(x) - \nabla\log p({x})\right\rVert_2^2\right]<br>$$<br>但接下来的一个难点是，我们很难知道$\nabla\log p({x})$。</p><p>2005年Aapo Hyvarinen在他的《Estimation of Non-Normalized Statistical Models by Score Matching》中提出了：<br>$$<br>\mathbb{E} _ {p(x)}\left[tr(\nabla s_\theta(x) )+\frac{1}{2}\lVert s_\theta(x) \rVert^2\right]<br>$$<br>这种没有使用p的方法也被叫做Implicit Score Matching。但是这种方法随着应用场景的维度的升高，迹变得更加难求。</p><p>2010年Pascal Vincent在他的《A connection between score matching and denoising autoencoders》中提出了：<br>$$<br>\mathbb{E} _ {p(\tilde x,x)}\left[\frac{1}{2}\lVert s_\theta(\tilde x)- \frac{x-\tilde x}{\sigma^2}\rVert^2\right]<br>$$<br>其中$\tilde x$是加噪后的，或者是x是去噪后的。</p><p>逐渐有了现在去噪分数匹配的雏形，这样的形式就变得很好求了。</p><p>当我们知道score，那我们就可以进行采样了，下式被称为（未调整的）朗之万动力学采样（Unadjusted Langevin Algorithm）：<br>$$<br>{x} _ {i+1} \leftarrow {x}_i + c\nabla\log p({x}_i) + \sqrt{2c}{\epsilon},\quad i &#x3D; 0, 1, …, K<br>$$<br>$\epsilon$为噪声，以实现更好的多样性。</p><h2 id="朗之万动力学采样"><a href="#朗之万动力学采样" class="headerlink" title="朗之万动力学采样"></a>朗之万动力学采样</h2><p>这一章我们将证明上式是如何得到的。</p><p>一般的朗之万方程——对于一个质量为m的粒子，其位置为m，其动力学行为可以用以下随机微分方程描述：<br>$$<br>m\frac{d^2x}{dt^2}&#x3D;F(x)-\gamma\frac{dx}{dt}+\eta(t)<br>$$<br>当粒子非常小或者阻尼非常大时，惯性项$m\frac{d^2x}{dt^2}$可以忽略不计，这被称为<strong>过阻尼朗之万动力学 (Overdamped Langevin Dynamics)</strong> 或布朗动力学。</p><p>我们考虑过阻尼朗之万动力学方程：</p><p>$$<br>\frac{d{x}(t)}{dt} &#x3D; \frac{1}{\gamma}{F}({x}(t)) + \sqrt{\frac{2k_B T}{\gamma}}{\xi}(t)<br>$$</p><p>其中 ${F}({x}) &#x3D; -\nabla U({x})$ 是由势能 $U({x})$ 产生的力，${\xi}(t)$ 是均值为0、协方差为 $\langle \xi_i(t) \xi_j(t’) \rangle &#x3D; \delta _ {ij} \delta(t-t’)$ 的高斯白噪声。</p><p>为了简化，设定一个“有效的扩散系数” $D &#x3D; k_B T &#x2F; \gamma$，并将方程改写为：<br>$$<br>d{x}(t) &#x3D; \frac{1}{\gamma}{F}({x}(t)) dt + \sqrt{2D} d{W}(t)<br>$$</p><p>我们依旧使用采样的目标分布是 $p({x}) \propto e^{-U({x})&#x2F;(k_B T)}$，我们可以令 $U({x}) &#x3D; -\log p({x})$ 并简写为：</p><p>$$<br>d{x}(t) &#x3D; \nabla \log p({x}(t)) dt + \sqrt{2} d{W}(t)<br>$$</p><h3 id="平衡态"><a href="#平衡态" class="headerlink" title="平衡态"></a>平衡态</h3><p>福克-普朗克方程描述了在随机过程中概率密度函数 P(x,t) 如何随时间演化。我们采样的目标分布实际上是一个平稳解。</p><p>福克-普朗克方程描述了概率密度 $P({x}, t)$ 如何随时间演化。</p><p>对于一个由随机微分方程 $d{X}_t &#x3D; {b}({X}_t)dt + \sqrt{2{D}({X}_t)}d{W}_t$ 描述的系统，其福克-普朗克方程为：<br>$$<br>\frac{\partial P({x}, t)}{\partial t} &#x3D; -\sum_i \frac{\partial}{\partial x_i} [b_i({x}) P({x}, t)] + \sum _ {i,j} \frac{\partial^2}{\partial x_i \partial x_j} [D _ {ij}({x}) P({x}, t)]<br>$$<br>这里 ${b}({x})$ 是漂移向量，$D _ {ij}({x})$ 是扩散张量。</p><p>平稳解 $P_s({x})$ 是指不随时间变化的解，即 $\frac{\partial P_s({x})}{\partial t} &#x3D; 0$。所以，我们需要解以下偏微分方程：<br>$$<br>-\sum_i \frac{\partial}{\partial x_i} [b_i({x}) P_s({x})] + \sum _ {i,j} \frac{\partial^2}{\partial x_i \partial x_j} [D _ {ij}({x}) P_s({x})] &#x3D; 0<br>$$<br>这个方程通常可以写作 $\nabla \cdot [-{b}({x}) P_s({x}) + \nabla \cdot ({D}({x}) P_s({x}))] &#x3D; 0$。更清晰的写法是定义概率流 (probability current) ${J}(P_s)$:<br>$$<br>J_k(P_s) &#x3D; b_k({x}) P_s({x}) - \sum_j \frac{\partial}{\partial x_j} [D _ {kj}({x}) P_s({x})]<br>$$<br>则平稳的福克-普朗克方程变为散度为零：<br>$$<br>\sum_k \frac{\partial J_k(P_s)}{\partial x_k} &#x3D; \nabla \cdot {J}(P_s) &#x3D; 0<br>$$<br>寻找这个方程的通解可能非常困难，具体方法取决于漂移项 ${b}({x})$ 和扩散项 ${D}({x})$ 的形式以及边界条件。</p><p>为了简便，在这里我们只讨论一维情况。</p><p>在一维情况下，方程简化为：<br>$$<br>-\frac{d}{dx}[b(x) P_s(x)] + D \frac{d^2 P_s(x)}{dx^2} &#x3D; 0<br>$$<br>这可以写成：<br>$$<br>\frac{d}{dx} \left[ -b(x) P_s(x) + D \frac{dP_s(x)}{dx} \right] &#x3D; 0<br>$$<br>这意味着方括号内的项（即概率流 $J(x)$）是一个常数，记为 $J_0$：<br>$$<br>-b(x) P_s(x) + D \frac{dP_s(x)}{dx} &#x3D; J_0<br>$$<br>对于大多数物理系统，尤其是在无限区域或者有反射边界的情况下，我们期望平稳态下没有净的概率流动，即 $J_0 &#x3D; 0$。如果 $J_0 &#x3D; 0$，则：<br>$$<br>D \frac{dP_s(x)}{dx} &#x3D; b(x) P_s(x)<br>$$<br>这是一个可分离变量的一阶常微分方程：<br>$$<br>\frac{1}{P_s(x)} dP_s(x) &#x3D; \frac{b(x)}{D} dx<br>$$<br>两边积分：<br>$$<br>\ln P_s(x) &#x3D; \int \frac{b(x)}{D} dx + \text{const}<br>$$<br>所以，平稳解为：<br>$$<br>P_s(x) &#x3D; A \exp\left(\int \frac{b(x)}{D} dx\right)<br>$$<br>其中 $A$ 是归一化常数，通过 $\int P_s(x) dx &#x3D; 1$ 来确定。</p><p>这与我们假设的 $P_s({x}) \propto e^{-U({x})&#x2F;(k_B T)}$ 形式类似。</p><h3 id="离散化"><a href="#离散化" class="headerlink" title="离散化"></a>离散化</h3><p>欧拉-丸山法 (Euler-Maruyama method) 是离散化SDE最简单的方法之一。</p><p>考虑过阻尼朗之万方程的简化形式：</p><p>$$<br>d{x}(t) &#x3D;{b}({x}(t)) dt + \sigma({x}(t)) d{W}(t)<br>$$</p><p>欧拉-丸山法的离散化更新规则如下：<br>给定当前状态 ${x}_k$ 在时间 $t_k$，下一个状态 ${x} _ {k+1}$ 在时间 $t _ {k+1} &#x3D; t_k + \Delta t$ 可以近似为：<br>$$<br>{x} _ {k+1} &#x3D;{x}_k +{b}({x}_k) \Delta t + \sigma({x}_k) \sqrt{\Delta t}{Z}_k<br>$$</p><p>其中：</p><ul><li>$\Delta t$ 是离散的时间步长。</li><li>${Z}_k$ 是一个从标准多元正态分布（均值为0，协方差矩阵为单位矩阵 ${I}$）中抽取的随机向量。</li><li>$d{W}(t)$ 在离散时间步长 $\Delta t$ 内的增量 $\Delta{W}_k &#x3D;{W}(t _ {k+1}) -{W}(t_k)$ 服从均值为0、协方差矩阵为 $\Delta t \cdot{I}$ 的正态分布。因此，$\Delta{W}_k$ 可以表示为 $\sqrt{\Delta t}{Z}_k$。</li></ul><h2 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h2><p>在p(x)较小的低密度区域，模型可能很难学习，从而导致效果不佳：</p><p><img src="/2025/20250526/pitfalls.jpg"></p><p>一个可行的办法是加大噪声以遍历更多的空间。</p><h2 id="具有多重噪声扰动的生成模型"><a href="#具有多重噪声扰动的生成模型" class="headerlink" title="具有多重噪声扰动的生成模型"></a>具有多重噪声扰动的生成模型</h2><p>但我们如何选取噪声的大小呢？</p><p>yang song提出了同时使用多个尺度的噪声扰动。</p><p><img src="/2025/20250526/multi_scale.jpg"></p><p>定义一个逐渐增加的方差$\sigma_1&lt;\sigma_2&lt;…&lt;\sigma_L$，定义一个逐步扰动的数据分布序列：<br>$$<br>p _ {\sigma_t}({x}_t) &#x3D; \int p({x})\mathcal{N}({x}_t; {x}, \sigma_t^2\textbf{I})d{x}<br>$$</p><p>使用神经网络同时学习所有噪声级别的得分函数：<br>$$<br>\underset{\arg\min}, \sum _ {t&#x3D;1}^T\lambda(t)\mathbb{E} _ {p _ {\sigma_t}({x}_t)}\left[\left\lVert {s} _ ({x}, t) - \nabla\log p _ {\sigma_t}({x}_t)\right\rVert_2^2\right]<br>$$<br>其中通常定义$\lambda(i)&#x3D;\sigma_i^2$。</p><p>使用退火朗之万动力学采样作为生成过程，其中通过依次对每个 $t&#x3D;T,T-1…$运行朗之万动力学来生成样本。</p><p>song yang也提出来一些实用建议：</p><ul><li><p>将$\sigma$选择为等比数列，其中$\sigma_1$足够小， $\sigma_L$与所有训练数据点之间的最大成对距离数量级相同。L为数百或数千的数量级。</p></li><li><p>用 U-Net 跳跃连接参数化模型 </p></li><li><p>在测试时对权重应用指数移动平均</p></li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>参考于yang song的博客，更多的内容也可以在该处查看。</p><p><a href="http://yang-song.net/blog/2021/score/">http://yang-song.net/blog/2021/score/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>物理</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用奇异值抵御标签噪声的免训练方法</title>
    <link href="/2025/20250523/"/>
    <url>/2025/20250523/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>上交东南大学出品（arxiv)。</p><span id="more"></span><h2 id="事先观察"><a href="#事先观察" class="headerlink" title="事先观察"></a>事先观察</h2><p>作者首先在干净的 CIFAR-100 数据集上训练一个成熟的分类器PRODEN，该分类器使用 ResNet-34作为骨干网络。</p><p>以预定义的概率 p 随机翻转每个样本的标签，生成具有不同程度标注噪声的数据集。并还强制实施一个约束，即每个样本至少保留一个标签。</p><p>对矩阵 W 和 W 进行奇异值分解（SVD），提取它们对应的奇异值Σ和Σ’以及右奇异酉矩阵，分别称为 V 和 V。然后，我们分析随着不准确度增加，不同权重的奇异值变化情况，结果如下图所示。</p><p><img src="/2025/20250523/1.png"></p><p>可以观察到权重矩阵的前几个奇异值在标签不准确度的一定范围内没有显著差异。然而，一旦标签不准确度超过某个阈值，前几个奇异值会迅速下降。</p><p>作者又评估了由 V 中前 i 个奇异向量张成的子空间与由 V 中前 j 个奇异向量张成的子空间的相似性。我们基于格拉斯曼距离计算归一化子空间相似度，即<br>$$<br>\phi (V,V’,i,j)&#x3D;\frac{||V _ {:i}^TV _ {:j}^T||_F^2}{min(i,j)}\in[0,1]<br>$$<br>随着标签不准确性的增加，V 和 V‘的子空间之间的整体差异变得更加明显。但在一定范围内的标签不准确度，权重的主体子空间基本不受影响，甚至几乎完全相同。</p><p><img src="/2025/20250523/2.png"></p><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>考虑以下分类器（或损失）：<br>$$<br>min_W||XW-G||_F^2+\lambda||W||_F^2<br>$$<br>闭式解：<br>$$<br>W&#x3D;(X^TX+\lambda I)^{-1}X^TG&#x3D;K^{-1}X^TG<br>$$</p><blockquote><p>其实这就是个岭回归线性模型，可能不能代表所有的模型，但是可以作为一个启发。</p></blockquote><p>标签扰动可以表示为$Y&#x3D;G+M$，其中Y是噪声标签，G是groud truth，M是扰动。<br>$$<br>W’&#x3D;K^{-1}X^TY&#x3D;K^{-1}X^TG+K^{-1}X^TM&#x3D;W+\Delta W<br>$$<br>而<br>$$<br>||\Delta W||_F \le||K^{-1}X^T||_2||M||_F\le||K^{-1}||_2||X^T||_2||M||_F<br>$$<br>由于通常情况下 q ≪ n ，X 预期是满秩的。因此有：<br>$$<br>\begin{align}<br>||K^{-1}||_2&amp;&#x3D;||(X^TX+\lambda I)^{-1}||_2&#x3D;\frac{1}{\lambda _ {min}(X^TX)+\lambda}<br>\\<br>||X^T||_2&amp;&#x3D;||X||_2&#x3D;\sigma _ {max}(X)<br>\end{align}<br>$$<br>我们定义标签不准确的程度为 p，即$P(M _ {ij}\ne0)&#x3D;p$，则$||M||_F&#x3D;\sqrt{pnl}$，故有<br>$$<br>||\Delta W||_F\le \frac{\sigma _ {max}(X)\sqrt{nl}}{\lambda _ {min}(X^TX)+\lambda}\sqrt{p}<br>$$<br>根据Davis-Kahan 正弦定理，W 和 W’ 扰动后子空间之间的角度θ的正弦值被限制为$sin\theta\le\frac{||\Delta W||_2}{\delta}$，故$sin\theta\le \frac{\sigma _ {max}(X)\sqrt{nl}}{\delta \lambda _ {min}(X^TX)+\lambda}\sqrt{p}$。</p><p>故p比较小的情况下，角度也小。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>W 进行奇异值分解，选择前k个。</p><p>我们还可以再对剩余的再做一些处理。</p><p>假设$U_l&#x3D;[u _ {k+1},…,u _ {min(q,l)}]$,$V_l&#x3D;[v _ {k+1},…,v _ {min(q,l)}]$,$\Sigma_l&#x3D;diag(\sigma _ {k+1},…,\sigma _ {min(q,l)})$，则：<br>$$<br>W’&#x3D;W_k+U_l\Sigma_lV_l^T<br>$$<br>为了优化奇异值，我们使用训练数据重新训练它们以提取关键信息，这导致了以下优化问题：<br>$$<br>\begin{align}<br>min _ {\Sigma_l}||XW’-Y||^2_F<br>\\<br>s.t. \quad W’&#x3D;W_k+U_l\Sigma_lV_l^T<br>\end{align}<br>$$<br>上式重写为：<br>$$<br>min _ {\Sigma_l}||X(W_k+\sum _ {i&#x3D;k+1}^{min(q,l)}\sigma_iu_iv_i^T)-Y||^2_F<br>$$<br>我们可以直接求导求出来，（所以是免训练的）：<br>$$<br>\begin{align}<br>\sigma_j&amp;&#x3D;\frac{u_j^TX^T(Y-XW_k)v_j}{u_j^TX^TXu_j}<br>\\<br>\Sigma_l^\ast&amp;&#x3D;\frac{diag(U_l^TX^T(Y-XW_k)V_l)}{diag(U_l^TX^TXU_l)}<br>\end{align}<br>$$</p><h2 id="题外"><a href="#题外" class="headerlink" title="题外"></a>题外</h2><p>图神经网络中也有使用SVD来抵御对抗攻击的。比如WSDM ‘20的《All You Need Is Low (Rank): Defending Against Adversarial Attacks on Graphs》。</p><p><img src="/2025/20250523/lowrank.png"></p><p>作者发现，Nettack是一种高秩攻击，这是因为Nettack带来的对抗扰动只会影响图中少量的节点，攻击给图结构谱域带来的影响较小，故主要反映在rank较高的奇异值上。</p><p>作者同样截断前k个特征值大的作为SVD的近似。但没有对余下的SVD做处理。</p><p>作者分析了，说这种近似可能检测不到大于节点度数&gt;$\sigma_r^2-2$的，作者进一步证明了。<br>$$<br>Pr(X\ge\sigma_r^2)\approx\frac{\zeta(\alpha,\sigma_r^2)-\zeta(\alpha,d_{max}+1)}{\zeta(\alpha,d_{min})-\zeta(\alpha,d_{max}+1)}&lt;\tau<br>$$<br>其中$\zeta(\alpha,x)&#x3D;\sum_{k&#x3D;0}^\infty(k+x)^{-\alpha}$，$\alpha\approx 1+|D_G|\left[\sum_{d_i\in D_G} log \frac{d_I}{d_{min-\frac{1}{2}}}\right]$，$D_G&#x3D;\left\{d_v^G|v\in V,d_v^G\ge d_{min}\right\}$。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>5%&gt;100%-Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks</title>
    <link href="/2025/20250522/"/>
    <url>/2025/20250522/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>算是adapter的改进。5%&gt;100%指的是微调（参数只有5%）大于全量。</p><span id="more"></span><h2 id="adapter类架构"><a href="#adapter类架构" class="headerlink" title="adapter类架构"></a>adapter类架构</h2><p>adapter：用于NLP</p><p><a href="https://arxiv.org/abs/1902.00751">https://arxiv.org/abs/1902.00751</a></p><p><img src="/2025/20250522/adapter.png"></p><p><img src="/2025/20250522/adapter1.png"></p><p>adaptformer：用于CV</p><p><a href="https://arxiv.org/abs/2205.13535">https://arxiv.org/abs/2205.13535</a></p><p><img src="/2025/20250522/adaptformer.png"></p><p><img src="/2025/20250522/adaptformer1.png"></p><h2 id="论文架构"><a href="#论文架构" class="headerlink" title="论文架构"></a>论文架构</h2><p><img src="/2025/20250522/arch.png"></p><p>典型的线性adapter在应用于视觉任务时会遇到两个问题。首先，固定层参数无法微调以匹配新任务的分布，导致传递给adapter的特征分布存在偏差。</p><p>作者使 Mona 能够调整输入分布以及来自固定层的输入比例。具体来说，作者在 Mona 的顶部添加了一个范数层和两个可学习权重，以调整输入分布。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="/2025/20250522/miou.png"></p><p><img src="/2025/20250522/res.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>模型参数与标签无关的模型</title>
    <link href="/2025/20250521/"/>
    <url>/2025/20250521/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我们可以减轻标签的作用吗？</p><p>完全舍弃标签？这也太疯狂了，而且显然这样会学不到什么。所以我们可以折中一下，使模型为$\hat y&#x3D;f(S,y)$，即S是训练后的、与标签无关的。有点类似于之前提过的PMLP，我们在训练中不使用邻接矩阵，但是测试的时候加上。</p><p>听起来可能依旧荒谬，但回想一下线性模型，对于$\hat{y}&#x3D;Xw$，最佳权重可以写成$w&#x3D;(X^TX)^{-1}Xy$，即可以写成Sy的形式，且S与y无关。</p><span id="more"></span><p>在此基础上，我们能不能扩展到更多模型？</p><p>这正是《Supervised Models Can Generalize Also When Trained on Random Labels 》要做的。</p><p>我们先规定一些符号：</p><p>y  训练样本的标签（我们不会提及测试样本的标签）</p><p>上下标s代表样本内&#x2F;训练样本</p><p>上下标o代表样本外&#x2F;测试样本</p><p>f是预测</p><p>我们会对所有的样本进行归一化。</p><h2 id="不依赖y的二阶样本矩匹配（SSMM）"><a href="#不依赖y的二阶样本矩匹配（SSMM）" class="headerlink" title="不依赖y的二阶样本矩匹配（SSMM）"></a>不依赖y的二阶样本矩匹配（SSMM）</h2><p>将模型表述为“平滑器”（smoother），即 f&#x3D;Sy，其中 <code>S</code> 是平滑矩阵</p><p>作者假设，一个能够很好泛化的模型，其样本外预测 ($f^o$) 的分布应该与真实观测数据 (y) 的分布大致相同 。</p><p>故最小化:<br>$$<br>|\frac{1}{n}\sum^n y_i^2-\frac{1}{n_o}\sum^{n_o} (\hat f_i^o)^2|&#x3D;|y^T(\frac{1}{n}I_n-\frac{1}{n_o}S_o^TS_o^T)y|&#x3D;|y^TAy|<br>$$<br>有两种方法来让这条式子y-free：</p><p>（1）使用随机标签 $y_R$ (基于迹半范数)</p><p>如果 $y_R$ 的元素不相关，均值为0，方差为 $\sigma_y^2$，那么最小化$|y_R^TAy_R|$的期望就等价于最小化$|E_{y_R}(y_R^TAy_R)|&#x3D;|Tr(A)|\sigma_y^2$</p><p>（2）利用不等式<br>$$<br>|y^TAy|\leq||A|| _ 2\cdot||y||^2 _ 2\leq||A|| _ {*}\cdot||y||^2 _ 2<br>$$<br>其中*代表核范数。</p><h2 id="对于神经网络"><a href="#对于神经网络" class="headerlink" title="对于神经网络"></a>对于神经网络</h2><p>先前已有使用NTK，来生成$f&#x3D;S(\theta)y$的形式，但是$S(\theta)$与y有关</p><p>即<br>$$<br>S_{k+1}&#x3D;S_k+\gamma(S_k-S_{k-1})+\eta K_{k+1}(I_n-S_k^s)<br>$$<br>其中$K_{k+1}$是一个广义的、随时间（训练迭代次数）变化的 NTK 。对于平方损失，它就是标准的 NTK；对于交叉熵损失，它会乘以一个依赖于当前预测的矩阵。</p><p>为了y-free，我们依旧采用不使用真实的标签 y，而是使用一个随机生成（采样）的标签&#x2F;响应向量的方法。</p><p>使用随机标签训练神经网络时，需要一个标准来决定何时停止训练。使用 Frobenius 范数版本的 y-free SSMM 来决定训练的时机。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MCMC之前尘后事</title>
    <link href="/2025/20250520/"/>
    <url>/2025/20250520/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>MCMC即马尔可夫链蒙特卡罗（Markov Chain Monte Carlo, MCMC），是一类在统计学和机器学习等领域广泛应用的强大算法。</p><span id="more"></span><h2 id="前尘"><a href="#前尘" class="headerlink" title="前尘"></a>前尘</h2><p>最早的源于Metropolis等人发表于《Journal of Chemical Physics》的主要针对以下形式的积分：<br>$$<br>J&#x3D;\int F(\theta)exp(-E(\theta)&#x2F;kT)d\theta &#x2F; \int exp(-E(\theta)&#x2F;kT)d\theta<br>$$<br>$\theta$表示$R^2$上一组N个粒子（particles），能量E为：<br>$$<br>E(\theta)&#x3D;\frac{1}{2}\sum _ {i&#x3D;1}^N\sum _ {j&#x3D;1,j\ne i}^NV(d _ {ij})<br>$$<br>其中V是势函数，d是粒子i和j在$\theta$中的欧几里得距离。玻尔兹曼分布$exp(-E(\theta))$。</p><p>考虑到问题的维度很大，即使是标准的蒙特卡洛技术也无法正确地近似$J$。</p><p>因为对于粒子系统（在 2N 的正方形中均匀分布）的随机构型的大多数实现，exp{−E(θ)&#x2F;kT }非常小。为了提高蒙特卡洛方法的效率，Metropolis 等人（1953 年）提出了对 N 个粒子的随机游走修改，即<br>$$<br>\begin{align}<br>x_i’&#x3D;x_i+\sigma\xi _ {1i}<br>\\<br>y_i’&#x3D;y_i+\sigma\xi _ {2i}<br>\end{align}<br>$$<br>$\xi _ {1i}、\xi _ {2i}$都是均匀分布U(-1,1)。</p><p>然后计算新配置（configuration）和旧配置之间的能量差∆E，并以概率接受新配置。<br>$$<br>min(1,exp(-\Delta E&#x2F;KT))<br>$$<br>否则，先前配置将被复制，即在随机游走 τ 步的 F (θ) 的最终平均中，其计数器增加一 。但Metropolis 等人一次移动一个粒子，而不是一起移动所有粒子，这使得初始算法看起来像是一种原始的吉布斯采样器。</p><h2 id="基本采样方法"><a href="#基本采样方法" class="headerlink" title="基本采样方法"></a>基本采样方法</h2><p>像变分贝叶斯推断是基于确定性近似的推断⽅法，考虑基于数值采样的近似推断⽅法，也被称为蒙特卡罗方法。</p><p>MCMC方法巧妙地将两个核心概念结合起来：<strong>蒙特卡罗方法</strong>和<strong>马尔可夫链</strong>。</p><ul><li><p><strong>蒙特卡罗方法</strong>：这是一种通过大量随机抽样来估计复杂问题数值解的方法。</p></li><li><p><strong>马尔可夫链</strong>：一个马尔可夫链是一个随机过程，其中未来的状态仅依赖于当前状态，而与过去的状态无关（即“无记忆性”）。在MCMC中，我们构造一个特殊的马尔可夫链，使其<strong>平稳分布</strong>恰好是我们想要抽样的目标概率分布 $\pi(x)$。这意味着，当马尔可夫链运行足够长的时间后，它所产生的样本将近似服从目标分布 $\pi(x)$。</p></li></ul><p>在介绍MCMC之前，我们先介绍一些基本概念和基本方法。</p><p>我们希望解决的基本的问题涉及到关于⼀个概率分布p(z)寻找某个函数f(z)的期望。即:<br>$$<br>E[f]&#x3D;\int f(z)p(z)dz<br>$$<br>采样⽅法背后的⼀般思想是得到从概率分布p(z)中独⽴抽取的⼀组变量$z^{(l)}$。这使得期望可以通过有限和的⽅式计算，即<br>$$<br>\hat f&#x3D;\frac{1}{L}\sum f(z^{(l)})<br>$$<br>且估计的精度不依赖于z的维度，且原则上数量较少也能达到较高的精度。</p><blockquote><p><strong>hoeffding不等式：</strong></p><p>一系列随机有界独立变量，$z_1…z_n$，$z_i\in[a,b]$，对所有的i，有：<br>$$<br>P(\frac{1}{n}\sum(z_i-E(z_i))\ge t)\le exp(-\frac{2nt^2}{(b-a)^2})<br>$$</p></blockquote><h3 id="基本采样"><a href="#基本采样" class="headerlink" title="基本采样"></a>基本采样</h3><p>我们考虑如何从简单的⾮均匀分布中⽣成随机数，假定我们已经有了⼀个均匀分布的随机数的来源。假设<em>z</em>在区间(0*,* 1)上均匀分布，我们使⽤某个函数f(·)对z的值进⾏变换，即y&#x3D;f(z)。y上的概率分布为:<br>$$<br>p(y)&#x3D;p(z)\left|\frac{dz}{dy}\right|<br>$$<br>我们的⽬标是选择⼀个函数<em>f</em>(<em>z</em>)使得产⽣出的<em>y</em>值具有某种所需的具体的分布形式p(y)，对上式进行积分有：<br>$$<br>z&#x3D;h(y)&#x3D;\int _ {-\infty}^y p(\hat y)d\hat y<br>$$<br>它是p(y)的不定积分。因此，$y&#x3D;h^-1(z)$，因此我们必须使⽤⼀个函数来对这个均匀分布的随机数进⾏变换</p><p>假如指数分布（exponential distribution）$p(y)&#x3D;\lambda exp(-\lambda y)$，其中$0\le y&lt;\infty$，故有<br>$$<br>\begin{align}<br>z&amp;&#x3D;h(y)&#x3D;\int_0^y p(y)dy<br>\\<br>&amp;&#x3D;\int_0^y \lambda exp(-\lambda y)dy<br>\\<br>&amp;&#x3D;1-exp(-\lambda y)<br>\end{align}<br>$$</p><p>故我们均匀生成z，并通过$y&#x3D;-\lambda^{-1}ln(1-z)$来生成指数分布。</p><p>另外统计学中还有一个Box-Muller⽅法⽤于⽣成⾼斯概率分布的样本。</p><p>但是我们这种方法只能用于⼀些⾮常有限的简单的概率分布可⾏，因此我们必须寻找⼀些更加⼀般的⽅法。</p><h3 id="拒绝采样"><a href="#拒绝采样" class="headerlink" title="拒绝采样"></a>拒绝采样</h3><p>为了应⽤拒绝采样⽅法，我们需要⼀些简单的概率分布q(z)，有时被称为提议分布（proposal distribution）。满足对所有的z，都要$kq(z)\ge \tilde p(z)$，函数kq(z)被称为⽐较函数。</p><p>⾸先，我们从概率分布q(z)中⽣成⼀个数z0。</p><p>接下来，我们在区间[0, kq(z0)]上的均匀分布中⽣成⼀个数u0。这对随机数在函数kq(z)的曲线下⽅是均匀分布。</p><p>最后，如果$u_0&gt;\tilde{p}(z_0)$，那么样本被拒绝，否则u0被保留。因此，如果它位于下图的灰⾊阴影部分，它就会被拒绝。这样，剩余的点对在曲线$\tilde{p}(z)$下⽅是均匀分布的，因此对应的z值服从概率分布p(z)。</p><p><img src="/2025/20250520/reject.jpg"></p><p>在某些情况下，找到满足对所有的z，都要$kq(z)\ge \tilde p(z)$的q是困难的，所以又有可调节的拒绝采样。</p><p>拒绝采样在⼀维或⼆维空间中是⼀个有⽤的⽅法，但是它不适⽤于⾼维空间。</p><h3 id="重要采样"><a href="#重要采样" class="headerlink" title="重要采样"></a>重要采样</h3><p>与拒绝采样的情形相同，重要采样基于的是对提议分布q(z)的使⽤，我们很容易从提议分布中采样。之后，我们可以通过q(z)中的样本{$z^{(l)}$}的有限和的形式来表⽰期望：<br>$$<br>\begin{align}<br>E(f)&amp;&#x3D;\int f(z)p(z)dz<br>\\<br>&amp;&#x3D;\int f(z)\frac{p(z)}{q(z)}q(z)dz<br>\\<br>&amp;\approx\frac{1}{L}\sum^L f(z^{(l)})\frac{p(^{(l)})}{q(^{(l)})}<br>\end{align}<br>$$<br>其中$r_l&#x3D;\frac{p(^{(l)})}{q(^{(l)})}$​被称为重要性权重（importance weights），修正了由于从错误的概率分布中采样引⼊的偏差。注意，与拒绝采样不同，所有⽣成的样本都被保留。</p><p>常见的情形是，概率分布p(z)的计算结果没有归⼀化，即$p(z)&#x3D;\tilde p(z)&#x2F;Z_p$，但$Z_p$未知。类似地，我们可能希望使⽤重要采样分布$q(z)&#x3D;\tilde q(z)&#x2F;Z_q$，它具有相同的性质。于是我们有：<br>$$<br>\begin{align}<br>E(f)&amp;&#x3D;\int f(z)p(z)dz<br>\\<br>&amp;&#x3D;\frac{Z_q}{Z_p}\int f(z)\frac{\tilde p(z)}{\tilde q(z)}q(z)dz<br>\\<br>&amp;\approx\frac{1}{L}\sum^L f(z^{(l)})\tilde r_l<br>\end{align}<br>$$<br>我们可以计算比值Zp&#x2F;Zq：<br>$$<br>\frac{Z_p}{Z_q}&#x3D;\frac{1}{Z_q}\int \tilde p(z)dz&#x3D;\int \frac{\tilde p(z)}{\tilde q(z)}q(z)dz\approx \frac{1}{L}\sum \tilde r_l<br>$$<br>故<br>$$<br>E(f)\approx\sum w_lf(z^{(l)})<br>$$<br>其中<br>$$<br>w_l&#x3D;\frac{\tilde r_l}{\sum_m \tilde r_m}&#x3D;\frac{\frac{\tilde p^{(l)}}{q(z^{(l)})}}{\frac{\sum_m \tilde p(z^{(m)})}{q(z^{(m)})}}<br>$$</p><p>伪代码：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs text">Function Importance_Sampling_Normalized(N, f, tilde_p, q_sampler, tilde_q):<br>  // 初始化<br>  sum_weighted_f_numerator = 0.0<br>  sum_weights_denominator = 0.0<br><br>  // 抽样和计算权重阶段<br>  For i from 1 to N:<br>    1. // 从提议分布 q(x) 中抽取一个样本<br>       x_i = q_sampler()<br><br>    2. // 计算该样本在目标分布下的未归一化值<br>       tilde_p_value_at_x_i = tilde_p(x_i)<br><br>    3. // 计算该样本在提议分布下的未归一化值<br>       tilde_q_value_at_x_i = tilde_q(x_i)<br><br>    4. // 安全检查：避免除以零<br>       If tilde_q_value_at_x_i == 0:<br>         // 如果 tilde_p_value_at_x_i 也为零，则此样本贡献为0，可以跳过或赋权重0<br>         // 如果 tilde_p_value_at_x_i 不为零，这是一个严重问题，说明 q(x) 在 p(x) 有值的地方取了0<br>         // 实践中需要更稳健的错误处理或确保 q 的选择是合适的<br>         raw_weight_i = 0.0 // 简单处理：赋权重0<br>         // 也可以打印警告信息: print &quot;Warning: tilde_q(x_i) is zero!&quot;<br>       Else:<br>         // 计算原始（未归一化的）重要性权重<br>         raw_weight_i = tilde_p_value_at_x_i / tilde_q_value_at_x_i<br>       End If<br><br>    5. // 累加分子项：f(x_i) * 原始权重<br>       sum_weighted_f_numerator = sum_weighted_f_numerator + f(x_i) * raw_weight_i<br><br>    6. // 累加分母项：原始权重之和<br>       sum_weights_denominator = sum_weights_denominator + raw_weight_i<br>  End For<br><br>  // 计算最终估计值<br>  If sum_weights_denominator == 0:<br>    // 如果所有权重都为零，可能表示 p 和 q 的重叠非常差，或者 N 太小<br>    // 或者 f(x_i) * raw_weight_i 始终为0<br>    print &quot;Warning: Sum of weights is zero. Estimate might be unreliable.&quot;<br>    estimate_E_p_f = 0.0 // 或返回 NaN 或根据具体情况处理<br>  Else:<br>    estimate_E_p_f = sum_weighted_f_numerator / sum_weights_denominator<br>  End If<br><br>  Return estimate_E_p_f<br>End Function<br></code></pre></td></tr></table></figure><h2 id="MCMC"><a href="#MCMC" class="headerlink" title="MCMC"></a>MCMC</h2><h3 id="基本方法"><a href="#基本方法" class="headerlink" title="基本方法"></a>基本方法</h3><p>与拒绝采样和重要采样相同，我们再⼀次从提议分布中采样。但是这次我们记录下当前状态$z^{(\tau)}$，以及依赖于这个当前状态的提议分布$z^{(\tau)}$，从⽽样本序列$z^{(1)}$,$z^{(2)}$, . . .组成了⼀个马尔科夫链。与之前⼀样，如果我们有$p(z)&#x3D;\frac{\tilde p(z)}{Z_p}$，那么我们会假定对于任意的值z都可以计算$\tilde p(z)$，虽然$Z_p$的值可能位置。提议分布本⾝被选择为⾜够简单，从⽽直接采样很容易。在算法的每次迭代中，我们从提议分布中⽣成⼀个候选样本$z^{*}$，然后根据⼀个恰当的准则接受这个样本。</p><p>在基本的Metropolis算法中，我们假定提议分布是对称的，即$q(z_A|z_B)&#x3D;q(z_B|z_A)$，则候选的样本被接受的概率为：<br>$$<br>A(z^{\ast},z^{(\tau)})&#x3D;min(1,\frac{\tilde p(z^{\ast})}{\tilde p(z^{(\tau)})})<br>$$<br>若候选样本被接受，则$z^{(\tau+1)}&#x3D;z^{*}$，否则候选样本点被丢弃，再设置$z^{\tau+1}&#x3D;z^{\tau}$，然后从概率分布$q(z|z^{(\tau+1)})$​中再次抽取⼀个候选样本。与拒绝采样不同，那⾥拒绝的样本被简单地丢弃。在Metropolis算法中，当⼀个候选点被拒绝时，前⼀个样本点会被包含到最终的样本的列表中，从⽽产⽣了样本点的多个副本。</p><p>使⽤Metropolis算法从⼀个⾼斯分布中采样的简单例⼦，我们可以看出采样过程是连着的，这也就是链。</p><p><img src="/2025/20250520/mcmc.jpg"></p><p>比如我们从二维高斯分布中采样<code>N = multivariate_normal(mean=mu, cov=S)</code>，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">X = np.mgrid[-<span class="hljs-number">3</span>:<span class="hljs-number">3</span>:<span class="hljs-number">0.05</span>, -<span class="hljs-number">3</span>:<span class="hljs-number">3</span>:<span class="hljs-number">0.05</span>]<br>Z = np.apply_along_axis(N.pdf, <span class="hljs-number">0</span>, X)<br>obs = N.rvs(<span class="hljs-number">500</span>)<br>c = <span class="hljs-number">0.1</span><br>z = obs.mean(axis=<span class="hljs-number">0</span>)<br>samples = []<br>nrounds = <span class="hljs-number">0</span><br>nsamples = <span class="hljs-number">310</span><br><span class="hljs-keyword">while</span> <span class="hljs-built_in">len</span>(samples) &lt; nsamples:<br>    nrounds += <span class="hljs-number">1</span><br>    q = multivariate_normal(z, cov=c)<span class="hljs-comment"># 定义提议分布</span><br>    z_star = q.rvs()<br><br>    A = <span class="hljs-built_in">min</span>(<span class="hljs-number">1</span>, N.pdf(z_star) / N.pdf(z))<br>    <span class="hljs-keyword">if</span> A &gt; rand():<br>        samples.append(z_star)<br>        z = z_star<br>samples = np.c_[samples]<br></code></pre></td></tr></table></figure><p>但这个方法会受到随机游⾛⾏为的影响。</p><p>比如我们考虑⼀个由整数组成的状态空间z，概率为：<br>$$<br>\begin{align}<br>p(z^{(\tau+1)}&#x3D;z^{(\tau)})&#x3D;0.5<br>\\<br>p(z^{(\tau+1)}&#x3D;z^{(\tau)}+1)&#x3D;0.5<br>\\<br>p(z^{(\tau+1)}&#x3D;z^{(\tau)}-1)&#x3D;0.5<br>\end{align}<br>$$<br>则根据对称性,$E[z^{(\tau)}]&#x3D;0$。且</p><p>$$<br>\begin{align}<br>E_{\tau} [l(z^{(\tau)})^2] &amp;&#x3D; 0.5 \cdot E_{\tau-1}[l(z^{(\tau-1)})^2] + 0.25 \cdot E_{\tau-1}[l(z^{(\tau-1)}+1)^2] + 0.25 \cdot E_{\tau-1}[l(z^{(\tau-1)}-1)^2]<br>\\&amp;&#x3D; E_{\tau-1}[l(z^{(\tau-1)})^2] + 0.5<br>\end{align}<br>$$</p><p>故$E[(z^{(\tau)})^2]&#x3D;\frac{\tau}{2}$。</p><p>可以看出随机游⾛在探索状态空间时是很低效的。所以设计马尔科夫链蒙特卡罗⽅法的⼀个中⼼⽬标就是避免随机游⾛⾏为。</p><h3 id="马尔科夫链"><a href="#马尔科夫链" class="headerlink" title="马尔科夫链"></a>马尔科夫链</h3><p>我们可以按照下面的方式具体化一个马尔可夫链：给定初始变量的概率分布 $p(z^{(0)})$，以及后续变量的条件概率，用转移概率 (transition probability) $T_m(z^{(m)}, z^{(m+1)}) \equiv p(z^{(m+1)} | z^{(m)})$ 的形式表示。</p><p>如果对于所有的 $m$，转移概率都相同，那么这个马尔可夫链被称为同质的 (homogeneous)。</p><p>对于一个特定的变量，边缘概率可以根据前一个变量的边缘概率用链式乘积的方式表示出来，形式为</p><p>$$p(z^{(m+1)}) &#x3D; \sum _ {z^{(m)}} p(z^{(m+1)} | z^{(m)}) p(z^{(m)}) $$</p><p>对于一个概率分布来说，如果马尔可夫链中的每一步都让这个概率分布保持不变，那么我们说这个概率分布关于这个马尔可夫链是不变的，或者静止的。因此，对于一个转移概率为 $T(z’, z)$ 的同质的马尔可夫链来说，如果</p><p>$$\sum _ {z’} T(z’, z) p^\ast(z’) &#x3D; p^\ast(z) $$</p><p>那么概率分布 $p^\ast(z)$ 是不变的。</p><p>确保所求的概率分布 $p(z)$ 不变的一个充分（非必要）条件是令转移概率满足细节平衡 (detailed balance) 性质，定义为</p><p>$$p^\ast(z) T(z, z’) &#x3D; p^\ast(z’) T(z’, z)$$</p><p>对特定的概率分布 $p^\ast(z)$ 成立。</p><p>很容易看到，满足关于特定概率分布的细节平衡性质的转移概率会使得那个概率分布具有不变性，因为</p><p>$$\sum _ {z’} p^\ast(z’) T(z’, z) &#x3D; \sum _ {z’} p^\ast(z) T(z, z’) &#x3D; p^\ast(z) \sum _ {z’} T(z, z’) &#x3D; p^\ast(z)$$</p><p>满足细节平衡性质的马尔可夫链被称为可翻转的 (reversible)。</p><p>我们的目标是使用马尔可夫链从一个给定的概率分布中采样。如果我们构造一个马尔可夫链使得所求的概率分布是不变的，那么我们就可以达到这个目标。然而，我们还要求对于 $m \to \infty$，概率分布 $p(z^{(m)})$ 收敛于所求的不变的概率分布 $p^\ast(z)$，与初始概率分布 $p(z^{(0)})$ 无关，这种性质被称为各态历经性 (ergodicity)。这个不变的概率分布被称为均衡 (equilibrium) 分布。很明显，一个具有各态历经性的马尔可夫链只能有唯一的一个均衡分布。可以证明，同质的马尔可夫链具有各态历经性，只需对不变的概率分布和转移概率做出较弱的限制即可。</p><h3 id="Metropolis-Hastings算法"><a href="#Metropolis-Hastings算法" class="headerlink" title="Metropolis-Hastings算法"></a>Metropolis-Hastings算法</h3><p>这种情形下，提议分布不再是参数的⼀个对称函数。<br>$$<br>A_k(z^{\ast},z^{(\tau)})&#x3D;min(1,\frac{\tilde p(z^{\ast})q_k(z^{(\tau)}|z^{\ast})}{\tilde p(z^{(\tau)})q_k(z^{\ast}|z^{\tau})})<br>$$</p><p>我们可以证明该定义的马尔科夫链是⼀个不变的概率分布，我们可以先证细节平衡，再利用充分性：<br>$$<br>\begin{align}<br>p(z)q_k(z’|z)A_k(z’,z)&amp;&#x3D;min(p(z)q_k(z’|z),p(z’)q_k(z|z’))<br>\\<br>&amp;&#x3D;min(p(z’)q_k(z|z’),p(z)q_k(z’|z))<br>\\<br>&amp;&#x3D;p(z’)q_k(z|z’)A_k(z,z’)<br>\end{align}<br>$$<br>然而，如果概率分布在不同的⽅向上的差异⾮常⼤，那么Metropolis-Hastings算法的收敛速度会⾮常慢。故有了以下改进。</p><h3 id="吉布斯采样"><a href="#吉布斯采样" class="headerlink" title="吉布斯采样"></a>吉布斯采样</h3><p>例如，对于一个三维随机向量 $(x_1, x_2, x_3)$，吉布斯采样的迭代步骤如下：</p><ol><li>从 $p(x_1 | x_2^{(t)}, x_3^{(t)})$ 中抽取 $x_1^{(t+1)}$</li><li>从 $p(x_2 | x_1^{(t+1)}, x_3^{(t)})$ 中抽取 $x_2^{(t+1)}$</li><li>从 $p(x_3 | x_1^{(t+1)}, x_2^{(t+1)})$ 中抽取 $x_3^{(t+1)}$</li></ol><p>为了证明这个步骤能够从所需的概率分布中采样，我们⾸先注意到对于吉布斯采样的每个步骤来说，概率分布p(z)是不变的，因此对于整个马尔科夫链来说也是不变的。这是由于当我们从$p(z_i|z _ {\backslash  i})$中采样时，边缘概率分布$p(z _ {\backslash i})$显然是不变的，因为$z _ {\backslash i }$的值是不变的。并且，根据定义，对于每个步骤中来⾃正确条件概率分布$p(z_i|z _ {\backslash  i})$的样本，条件概率分布都是不变的。由于条件概率分布和边缘概率分布共同确定的联合概率分布，因此我们看到联合概率分布本⾝是不变的。</p><p>为了让吉布斯采样能够从正确的概率分布中得到样本，第⼆个需要满⾜的要求为各态历经性。各态历经性的⼀个充分条件是没有条件概率分布处处为零。如果这个要求满⾜，那么z空间中的任意⼀点都可以从其他的任意⼀点经过有限步骤达到，这些步骤中每次对⼀个变量进⾏更新。如果这个要求没有满⾜，即某些条件概率分布为零，那么在这种情况下应⽤吉布斯采样时，必须显式地证明各态历经性。</p><p>吉布斯采样是MH算法的一个特例。考虑一个Metropolis-Hastings采样的步骤，它涉及到变量 $z_k$，同时保持剩余的变量 $z _ {\setminus k}$ 不变，并且对于这种情形来说，从 $z$ 到 $z^\ast$ 的转移概率为 $q _ k(z^\ast | z) &#x3D; p(z^\ast_k | z _ {\setminus k})$。我们注意到 $z^\ast  _ {\setminus k} &#x3D; z _ {\setminus k}$，因为在采样的步骤中，向量的各个元素都不改变。并且，$p(z) &#x3D; p(z_k | z _ {\setminus k})p(z _ {\setminus k})$。因此，确定Metropolis-Hastings算法中的接受概率的因子为</p><p>$$<br>A(z^\ast, z) &#x3D; \frac{p(z^\ast)q_k(z | z^\ast)}{p(z)q_k(z^\ast | z)} &#x3D; \frac{p(z^\ast_k | z^\ast _ {\setminus k})p(z^\ast _ {\setminus k})p(z_k | z^\ast _ {\setminus k})}{p(z_k | z _ {\setminus k})p(z _ {\setminus k})p(z^\ast_k | z _ {\setminus k})} &#x3D; 1<br>$$</p><p>注意我们用到了$z _ {\backslash  k}^\ast&#x3D;z _ {\backslash  k}$。</p><p>我们也可以证明吉布斯采样是细节平衡的。<br>$$<br>\begin{align}<br>p^\ast(z) T(z, z’) &amp;&#x3D;p(z_1^\tau,z_2^\tau,…,z_M^\tau)p(z_j^{\tau+1}|z_{\backslash j}^\tau)<br>\\&amp;&#x3D;p(z_j^\tau|z_{\backslash j}^\tau)p(z_{\backslash j}^\tau)p(z_j^{\tau+1}|z_{\backslash j}^\tau)<br>\\&amp;&#x3D;p(z_j^\tau|z_{\backslash j}^{\tau+1})p(z_{\backslash j}^{\tau+1})p(z_j^{\tau+1}|z_{\backslash j}^{\tau+1})<br>\\&amp;&#x3D;p(z_j^{\tau}|z_{\backslash j}^{\tau+1})p(z_1^{\tau+1},z_2^{\tau+1},…,z_M^{\tau+1})<br>\\&amp;&#x3D; p^\ast(z’) T(z’, z)<br>\end{align}<br>$$<br><strong>代码：</strong></p><p>比如我们对$p(x,y)&#x3D;exp(-x^2y^2)$，则：<br>$$<br>\begin{align}<br>X|Y&#x3D;y\sim N(0,\frac{1}{2y^2})<br>\\<br>Y|X&#x3D;x\sim N(0,\frac{1}{2x^2})<br>\end{align}<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">x, y = <span class="hljs-number">0.01</span>, <span class="hljs-number">0.01</span><br>samples = [(x, y)]<br><br>seed(<span class="hljs-number">31</span>)<br><span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1_000</span>):<br>    sigma_x = np.sqrt(<span class="hljs-number">1</span> / (<span class="hljs-number">2</span> * y ** <span class="hljs-number">2</span>))<br>    x_giv_y = norm(loc=<span class="hljs-number">0</span>, scale=sigma_x)<br>    x = x_giv_y.rvs()<br>    samples.append([x, y])<br>    <br>    sigma_y = np.sqrt(<span class="hljs-number">1</span> / (<span class="hljs-number">2</span> * x ** <span class="hljs-number">2</span>))<br>    y_giv_x = norm(loc=<span class="hljs-number">0</span>, scale=sigma_y)<br>    y = y_giv_x.rvs()<br>    samples.append([x, y])<br>    <br>samples = np.r_[samples]<br></code></pre></td></tr></table></figure><h2 id="更多"><a href="#更多" class="headerlink" title="更多"></a>更多</h2><p>此外，还有切⽚采样、源于模拟哈密顿动⼒学的混合蒙特卡罗⽅法等，我们在这里不再赘述。</p><p>我们也可以结合神经网络来改进MCMC。比如可以使用神经网络（例如，归一化流 Normalizing Flows）来学习复杂的、自适应的提议分布，即训练一个归一化流模型来学习并逼近目标后验分布，然后利用这个训练好的流模型作为 MCMC 算法中的提议分布。像今年新出的《Can Transformers Learn Full Bayesian Inference in Context?》</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>MCMC的核心步骤</strong>可以概括为：</p><ol><li><strong>构造马尔可夫链</strong>：设计一个状态转移机制（通常由一个转移核或转移概率矩阵 $P$ 定义），使得该马尔可夫链的平稳分布是目标分布 $\pi(x)$。一个关键的条件是<strong>细致平稳条件（Detailed Balance Condition）</strong>，即对于任意两个状态 $i$ 和 $j$，满足 $\pi(i)P(i,j) &#x3D; \pi(j)P(j,i)$。满足细致平稳条件的马尔可夫链，其平稳分布就是 $\pi(x)$。</li><li><strong>从初始状态开始迭代采样</strong>：选择一个初始状态 $x^{(0)}$，然后根据构造的马尔可夫链的转移规则，不断从当前状态转移到下一个状态，生成一系列样本 $x^{(1)}, x^{(2)}, \ldots, x^{(N)}$。</li><li><strong>达到平稳分布后收集样本</strong>：由于马尔可夫链需要一段时间才能达到平稳分布，通常会舍弃初始阶段产生的一批样本（称为“老化期”或“burn-in period”）。之后产生的样本则被认为是来自目标分布 $\pi(x)$ 的近似独立同分布样本（尽管它们在链中是相关的）。</li><li><strong>利用样本进行估计</strong>：使用收集到的有效样本来进行蒙特卡罗积分，例如估计某个函数 $f(x)$ 在目标分布 $\pi(x)$ 下的期望值：$E[f(x)] \approx \frac{1}{M} \sum _ {i&#x3D;1}^{M} f(x^{(i)})$，其中 $M$ 是有效样本的数量。</li></ol><p>有多种具体的MCMC算法：</p><ul><li><p><strong>Metropolis-Hastings (MH) 算法</strong>：这是一个非常通用和基础的MCMC算法。它通过一个“提议分布”（proposal distribution）$q(x’|x)$ 来生成候选样本 $x’$，然后根据一个接受概率 $A(x, x’)$ 来决定是否接受这个新样本。接受概率通常定义为：<br>  $$A(x, x’) &#x3D; \min\left(1, \frac{\pi(x’)q(x|x’)}{\pi(x)q(x’|x)}\right)$$<br>  如果提议分布是对称的，即 $q(x’|x) &#x3D; q(x|x’)$，则MH算法简化为Metropolis算法。MH算法的优点在于它只需要计算目标分布 $\pi(x)$ 的比例，而不需要知道其归一化常数。</p></li><li><p><strong>吉布斯采样 (Gibbs Sampling)</strong>：这是MH算法的一个特例，特别适用于处理高维联合分布。当直接从联合分布抽样困难，但从每个变量的<strong>全条件概率分布 (full conditional probability distribution)</strong> 抽样相对容易时，吉布斯采样非常有效。其基本思想是，轮流对每个变量进行抽样，抽样时固定其他所有变量的当前值。例如，对于一个三维随机向量 $(x_1, x_2, x_3)$，吉布斯采样的迭代步骤如下：</p><ol><li>从 $p(x_1 | x_2^{(t)}, x_3^{(t)})$ 中抽取 $x_1^{(t+1)}$</li><li>从 $p(x_2 | x_1^{(t+1)}, x_3^{(t)})$ 中抽取 $x_2^{(t+1)}$</li><li>从 $p(x_3 | x_1^{(t+1)}, x_2^{(t+1)})$ 中抽取 $x_3^{(t+1)}$<br>  吉布斯采样的接受概率恒为1，因此所有提议的样本都会被接受。</li></ol></li></ul><h2 id="MCMC的收敛性诊断"><a href="#MCMC的收敛性诊断" class="headerlink" title="MCMC的收敛性诊断"></a>MCMC的收敛性诊断</h2><p>评估MCMC算法是否收敛到目标平稳分布至关重要，因为基于未收敛的链进行的推断可能是错误的。常用的收敛诊断方法包括：</p><ul><li><strong>迹图 (Trace Plots)</strong>：绘制参数样本值随迭代次数变化的图形。如果链已收敛，迹图应该看起来像一个稳定的水平带状区域，没有明显的趋势或周期性模式。</li><li><strong>密度图 (Density Plots)</strong>：绘制样本的经验概率密度图。对于从不同初始点开始的多条链，如果它们都已收敛，其密度图应该相似。</li><li><strong>自相关图 (Autocorrelation Plots)</strong>：显示样本序列中不同滞后期的相关性。理想情况下，自相关性应随着滞后的增加而迅速下降。高自相关意味着链的混合较差，需要更多样本。</li><li><strong>Gelman-Rubin 统计量 (R-hat)</strong>：通过比较多条从不同初始点并行运行的马尔可夫链的链内方差和链间方差来评估收敛性。R-hat值接近1表明链已收敛。通常认为小于1.1或1.2的值表示可接受的收敛。</li><li><strong>有效样本量 (Effective Sample Size, ESS)</strong>：考虑到样本间的自相关性，ESS估计了等效的独立样本数量。ESS较低表明需要运行更长的链或改进采样器。</li><li><strong>接受率 (Acceptance Rate)</strong> (主要针对MH算法)：指提议样本被接受的比例。过高或过低的接受率都可能表明采样效率低下。对于某些类型的MH算法，存在理论上的最优接受率范围（例如，对于高斯提议分布和高斯目标分布，最优接受率约为0.234）。</li></ul><p>实践中，通常会结合使用多种诊断方法来综合判断MCMC算法的收敛性。在确认收敛后，还需要丢弃“老化期”的样本，并可能对剩余样本进行“减薄”处理，以获得用于后续分析的近似独立的样本。</p><p>总而言之，MCMC是一套功能强大的统计计算工具，它为从复杂概率分布中抽样提供了可行的解决方案，极大地推动了贝叶斯统计和相关领域的发展与应用。理解其原理、掌握常用算法并关注其收敛性是有效运用MCMC方法的关键。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data</p><p>[2]PRML</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>概率论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>18岁的AI天文学家</title>
    <link href="/2025/20250518/"/>
    <url>/2025/20250518/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>“18岁美国高中生Matteo Paz利用AI分析NASA的2000亿条数据，发现150万个隐藏天体”（来自公众号）。</p><p>该项目与其中一篇论文相关——《<a href="https://arxiv.org/pdf/2409.15499">A Submillisecond Fourier and Wavelet-based Model to Extract Variable Candidates from the NEOWISE Single-exposure Database</a>》，发表于《The Astronomical Journal》（天文学其中一个重要的期刊）</p><span id="more"></span><h2 id="阅读前介绍"><a href="#阅读前介绍" class="headerlink" title="阅读前介绍"></a>阅读前介绍</h2><p>我们先用一句话来概括：</p><p><strong>根据光度数据（光变曲线）对星系进行四分类。</strong></p><h2 id="研究背景与意义"><a href="#研究背景与意义" class="headerlink" title="研究背景与意义"></a>研究背景与意义</h2><p>论文将VARnet应用于近地天体广域红外巡天探测器（NEOWISE）单次曝光数据库，该数据库在10.5年间记录了近2000亿次红外源的出现信息 。研究旨在设计一个流程，从NEOWISE数据中提取可变候选体，以验证VARnet的有效性，并为未来对整个NEOWISE数据集进行变异性调查提供方法论 。</p><h2 id="WISE空间望远镜与NEOWISE数据"><a href="#WISE空间望远镜与NEOWISE数据" class="headerlink" title="WISE空间望远镜与NEOWISE数据"></a>WISE空间望远镜与NEOWISE数据</h2><p>广域红外巡天探测器（WISE）是一个轨道空间望远镜观测站，最初在2009年至2010年间以四波段红外巡天方式运行 。在固态氢制冷剂耗尽后，它曾短暂作为双波段巡天运行 。经过2011年至2013年12月的休眠期后，该任务以NEOWISE的形式重新启动，持续在其两个最短波长波段收集数据 。由于其长时间尺度和红外灵敏度，WISE为时域天文学研究提供了独特的机会 。</p><p>NEOWISE-R单次曝光数据库包含自2013年望远镜重新激活以来每次单次曝光中提取的光度数据 。每次观测（apparition）包含天体的赤经（RA）、赤纬（Dec）、修正儒略日（mjd）以及W1和W2波段的星等数据 。然而，数据库中的数据没有结构，与单个源相关的观测可能分散在数据库中，并且还包含由宇宙射线、条纹曝光或星云造成的噪声观测 。这为收集单个天文物体的数据带来了挑战 。</p><h2 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h2><p>作者将通量变化将恒星分为四种非常广泛的类型：静止型（Nulls）、瞬变型（Transients）、凌星型（Transits）、动型（Pulsating Variables）。</p><p>作者也自己声明，这种有限分类法对于变源的全面调查和统计分析并<strong>不十分有用</strong>。</p><h3 id="数据聚类与预处理："><a href="#数据聚类与预处理：" class="headerlink" title="数据聚类与预处理："></a><strong>数据聚类与预处理</strong>：</h3><p>由于望远镜精度的限制和噪声，同一源的中心在每次通过时可能会略有不同，从而形成一个近似二维高斯分布的点云 。研究人员使用基于密度的噪声应用空间聚类（DBSCAN）算法对单次曝光源目录中的观测进行空间聚类，以收集光变曲线数据 。DBSCAN能够稳健地处理不同形状的簇，不需要预先指定簇的数量，并且可以滤除噪声点 。</p><h3 id="数据预处理："><a href="#数据预处理：" class="headerlink" title="数据预处理："></a><strong>数据预处理</strong>：</h3><p>为了构建适合神经网络分析的高质量矩阵表示，研究选择了对异常和事件较不敏感的W1波段作为亮度特征 。星等值被转换为绝对通量读数，减去中位数，并使用四分位距进行标准化 。然后使用反正弦双曲函数（arcsinh）将数据压缩到接近[−1,1]的范围 。时间戳（mjd）也被归一化到[0,1]的范围 。测量的预期误差（wlsigmpro）也经过类似的缩放处理并作为模型的特征之一 。最终，每个时间点的数据被表示为一个包含亮度、不确定性和时间戳的三维向量 。</p><h3 id="数据生成："><a href="#数据生成：" class="headerlink" title="数据生成："></a><strong>数据生成</strong>：</h3><p>由于已知瞬变源的数量有限，研究人员制定了数学模型来为四种光变类型（静止、瞬变、脉动、凌星）生成模拟的WISE光变曲线，用于训练VARnet 。这些模拟光变曲线的生成考虑了源的基本亮度、噪声水平以及WISE的采样节奏 。</p><h4 id="基本函数"><a href="#基本函数" class="headerlink" title="基本函数"></a><strong>基本函数</strong></h4><p>在数据生成之前。我们定义一些基本操作。</p><p><strong>平均或基础亮度</strong>：随机选择一个星等值（范围在6到16等之间，并倾向于选择更暗的星等以模拟天空中的真实分布），然后将其转换为通量值，作为源的基础亮度 ω。</p><p><strong>不确定度（噪声标准差）</strong>：随机选择一个在 10−4 到 10−1 之间的值作为不确定度 σω，这个值与源的绝对亮度无关，以覆盖各种观测条件（例如，银道面背景光较强导致亮星误差增大，或特定星等下光度测量流程变化导致误差异常） 。</p><p><strong>缩放不确定度</strong>：NEOWISE数据库中的 <code>wlsigflux</code> 误差值并不直接等于真实亮度值周围的实际离散程度的一个标准差，因此在合成器中使用的不确定度值需要进行缩放以匹配数据库中的值。研究发现，将 $σ_ω$ 乘以一个在0.4到0.6之间随机选择的实数，可以获得最佳性能 。</p><p><img src="/2025/20250518/1.jpg"></p><h4 id="静止型（Nulls）"><a href="#静止型（Nulls）" class="headerlink" title="静止型（Nulls）"></a>静止型（Nulls）</h4><p>简单地选择一个基础通量值，并以该值为均值、特定标准差进行正态分布采样 。</p><p><img src="/2025/20250518/2.jpg"></p><h4 id="瞬变型（Transients）："><a href="#瞬变型（Transients）：" class="headerlink" title="瞬变型（Transients）："></a>瞬变型（Transients）：</h4><p>设计了一个基于形态学而非天体物理现象的光度随时间变化的函数模型，能够模拟新星、超新星以及某些类型的瞬变YSO活动等事件 。</p><p><img src="/2025/20250518/3.jpg"></p><p><img src="/2025/20250518/31.jpg"></p><h4 id="凌星型（Transits）："><a href="#凌星型（Transits）：" class="headerlink" title="凌星型（Transits）："></a>凌星型（Transits）：</h4><p>采用梯形模型来近似凌星光变曲线，考虑了凌星深度、次凌星深度和周期等因素 。</p><p><img src="/2025/20250518/4.jpg"></p><h4 id="动型（Pulsating-Variables）："><a href="#动型（Pulsating-Variables）：" class="headerlink" title="动型（Pulsating Variables）："></a>动型（Pulsating Variables）：</h4><p>通过创建一个表示一个周期的离散网格，在某些点上添加不同高度的狄拉克δ脉冲，然后用高斯滤波器进行离散卷积来生成各种波形 。</p><p><img src="/2025/20250518/51.jpg"></p><p><img src="/2025/20250518/5.jpg"></p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2025/20250518/model1.jpg"></p><p><img src="/2025/20250518/model2.png"></p><p>模型除了卷积以外还有小波变换和FFT。</p><h3 id="小波变换"><a href="#小波变换" class="headerlink" title="小波变换"></a>小波变换</h3><p>这部分没有额外的改进。</p><p>即一维离散小波变换就是把信号分别通过低通滤波器和高通滤波器把原始信号分解为原信号的近似系数和原信号的细节系数两个部分，这也被叫做mallat 算法。</p><p><img src="/2025/20250518/ImgTrans_diswav1.png"></p><h2 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h2><p>作者觉得DFT时间复杂度太高了，所以进行了优化。<br>我们称y为a的离散傅里叶变换，其中$w&#x3D;e^{-i\frac{2\pi}{n}}$：</p><p><img src="/2025/20250518/fft1.png"></p><p>我们取对数，令$z&#x3D;ln(w)$:</p><p><img src="/2025/20250518/fft2.png"></p><p>我们可以写成外积的形式$uv^T$:<br>$$<br>\begin{align}<br>u_j&#x3D;j,0\leq j&lt; N\\<br>v_j&#x3D;zj,0\leq j&lt; N<br>\end{align}<br>$$<br>故我们可以给出：<br>$$<br>\mathcal{F}(k)&#x3D;\frac{1}{N}(exp(uv^T)\vec{a})_k<br>$$<br>我们可以直接将u作为模型的参数引入，其维度作为超参数。我们将此超参数命名为 samples，并通过以下方式初始化 FEFT：<br>$$<br>\begin{align}<br>a_k\in R^{samples}\\<br>a_k&#x3D;\frac{k}{samples}\\<br>u_k&#x3D;a_k(N-1)<br>\end{align}<br>$$</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="/2025/20250518/final_4class_conf.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>天文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph Neural Networks with Learnable Structural and Positional Representations</title>
    <link href="/2025/20250517/"/>
    <url>/2025/20250517/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该论文在拉普拉斯PE的基础上，新的可学习节点位置编码（PE），用于图嵌入。</p><p>（ICLR 2022）</p><span id="more"></span><h2 id="为什么要使用位置编码"><a href="#为什么要使用位置编码" class="headerlink" title="为什么要使用位置编码"></a>为什么要使用位置编码</h2><p>比如单靠信息传播，下图的结构并不能很好学习：</p><p><img src="/2025/20250517/pe.jpg"></p><p><strong>解决方案：</strong></p><ul><li>堆叠多层</li></ul><p>由于过度压缩现象，它可能对远距离节点是不够的。 </p><ul><li>应用高阶</li></ul><p>K 阶 WL-GNN 在扩展中比 MP-GNN 计算上更昂贵，即使对于中等规模的图也是如此。 </p><ul><li>考虑节点的位置编码（PE）（以及边的位置编码）</li></ul><p>比如像graphormer（第一个Graph transfomrer），采用了三种位置编码：</p><p>（1）Centrality Encoding</p><p>即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">node_feature = (<br>    node_feature<br>    + nn.Embedding()(in_degree)<br>    + nn.Embedding()(out_degree)<br>)<br></code></pre></td></tr></table></figure><p>（2）Spatial Encoding</p><p>给定一个合理的距离度量 ϕ(v_i, v_j), 根据两个节点(v_i, v_j)之间的距离，为其分配相应的编码向量。距离度量 ϕ(⋅) 的选择多种多样，对于一般性的图数据可以选择无权或带权的最短路径，而对于特别的图数据则可以有针对性的选择距离度量，例如物流节点之间的最大流量，化学分子 3D 结构中原子之间的欧氏距离等等。</p><p>为了不失一般性，Graphormer 在实验中采取了无权的最短路径作为空间编码的距离度量。</p><p>（3）Edge Encoding</p><p>在计算两个节点之间的相关性时，作者对这两个节点最短路径上的连边特征进行加权求和作为注意力偏置，其中权重是可学习的。</p><hr><p>另外简单的还有比如Laplacian Eigenvectors as PE，但由于拉普拉斯矩阵的特征向量 v 和其相反向量 −v 都描述了相同的结构信息（因为它们对应同一个特征值）。当我们将这些特征向量用作节点的位置编码时，就会出现不唯一性。它可能会将这些实际上表示相同结构信息的编码视为完全不同的输入，从而影响学习效率和泛化能力。</p><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>官方代码：<a href="https://github.com/vijaydwivedi75/gnn-lspe">https://github.com/vijaydwivedi75/gnn-lspe</a></p><p>写得有些混乱，我们不如直接看dgl的源码：<a href="https://www.dgl.ai/dgl_docs/_modules/dgl/transforms/functional.html#random_walk_pe">https://www.dgl.ai/dgl_docs/_modules/dgl/transforms/functional.html#random_walk_pe</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">random_walk_pe</span>(<span class="hljs-params">g, k, eweight_name=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">r&quot;&quot;&quot;Random Walk Positional Encoding, as introduced in</span><br><span class="hljs-string">    `Graph Neural Networks with Learnable Structural and Positional Representations</span><br><span class="hljs-string">    &lt;https://arxiv.org/abs/2110.07875&gt;`__</span><br><span class="hljs-string"></span><br><span class="hljs-string">    This function computes the random walk positional encodings as landing probabilities</span><br><span class="hljs-string">    from 1-step to k-step, starting from each node to itself.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    g : DGLGraph</span><br><span class="hljs-string">        The input graph. Must be homogeneous.</span><br><span class="hljs-string">    k : int</span><br><span class="hljs-string">        The number of random walk steps. The paper found the best value to be 16 and 20</span><br><span class="hljs-string">        for two experiments.</span><br><span class="hljs-string">    eweight_name : str, optional</span><br><span class="hljs-string">        The name to retrieve the edge weights. Default: None, not using the edge weights.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    Tensor</span><br><span class="hljs-string">        The random walk positional encodings of shape :math:`(N, k)`, where :math:`N` is the</span><br><span class="hljs-string">        number of nodes in the input graph.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Example</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    &gt;&gt;&gt; import dgl</span><br><span class="hljs-string">    &gt;&gt;&gt; g = dgl.graph(([0,1,1], [1,1,0]))</span><br><span class="hljs-string">    &gt;&gt;&gt; dgl.random_walk_pe(g, 2)</span><br><span class="hljs-string">    tensor([[0.0000, 0.5000],</span><br><span class="hljs-string">            [0.5000, 0.7500]])</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    N = g.num_nodes()  <span class="hljs-comment"># number of nodes</span><br>    M = g.num_edges()  <span class="hljs-comment"># number of edges</span><br>    A = g.adj_external(scipy_fmt=<span class="hljs-string">&quot;csr&quot;</span>)  <span class="hljs-comment"># adjacency matrix</span><br>    <span class="hljs-keyword">if</span> eweight_name <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># add edge weights if required</span><br>        W = sparse.csr_matrix(<br>            (g.edata[eweight_name].squeeze(), g.find_edges(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(M)))),<br>            shape=(N, N),<br>        )<br>        A = A.multiply(W)<br>    <span class="hljs-comment"># 1-step transition probability</span><br>    <span class="hljs-keyword">if</span> version.parse(scipy.__version__) &lt; version.parse(<span class="hljs-string">&quot;1.11.0&quot;</span>):<br>        RW = np.array(A / (A.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) + <span class="hljs-number">1e-30</span>))<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Sparse matrix divided by a dense array returns a sparse matrix in</span><br>        <span class="hljs-comment"># scipy since 1.11.0.</span><br>        RW = (A / (A.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>) + <span class="hljs-number">1e-30</span>)).toarray()<br><br>    <span class="hljs-comment"># Iterate for k steps</span><br>    PE = [F.astype(F.tensor(np.array(RW.diagonal())), F.float32)]<br>    RW_power = RW<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k - <span class="hljs-number">1</span>):<br>        RW_power = RW_power @ RW<br>        PE.append(F.astype(F.tensor(np.array(RW_power.diagonal())), F.float32))<br>    PE = F.stack(PE, dim=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">return</span> PE<br></code></pre></td></tr></table></figure><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs text">g = dgl.graph(([0,1,1], [1,1,0])) 边: 0-&gt;1, 1-&gt;1 (自环), 1-&gt;0<br>对于这个 g，邻接矩阵 A (假设 MockDGLGraph 行为类似):<br>[[0, 1],<br> [1, 1]]<br><br>N = 2, M = 3 (在 DGL 示例中，尽管 MockDGLGraph 可能根据其初始化方式不同地计算唯一边)<br>对于 g = MockDGLGraph(([0,1,1], [1,1,0])):<br>A = [[0, 1],<br>     [1, 1]] (节点0连接到1；节点1连接到0和1(自环))<br><br>A.sum(1):<br>第0行和 (节点0): 0 + 1 = 1<br>第1行和 (节点1): 1 + 1 = 2<br>所以, A.sum(1) = [[1], [2]] (作为列向量用于广播)<br><br>RW = A / A.sum(1):<br>RW_row0 = [0/1, 1/1] = [0, 1]<br>RW_row1 = [1/2, 1/2] = [0.5, 0.5]<br>RW = [[0.0, 1.0],<br>      [0.5, 0.5]]<br><br>初始 PE (1步返回概率):<br>RW.diagonal() = [0.0, 0.5]<br>PE = [[0.0, 0.5]] (概念上，在堆叠之前)<br><br>k = 2, 所以循环运行 k-1 = 1 次。<br>迭代 1 (计算2步概率):<br>RW_power = RW @ RW<br>RW_power = [[0.0, 1.0],  @  [[0.0, 1.0],<br>            [0.5, 0.5]]     [0.5, 0.5]]<br><br>RW_power[0,0] = (0.0 * 0.0) + (1.0 * 0.5) = 0.5<br>RW_power[0,1] = (0.0 * 1.0) + (1.0 * 0.5) = 0.5<br>RW_power[1,0] = (0.5 * 0.0) + (0.5 * 0.5) = 0.25<br>RW_power[1,1] = (0.5 * 1.0) + (0.5 * 0.5) = 0.5 + 0.25 = 0.75<br><br>RW_power (RW^2) = [[0.5 , 0.5 ],<br>                   [0.25, 0.75]]<br><br>RW_power (RW^2) 的对角线: [0.5, 0.75]<br>PE 列表变为: [[0.0, 0.5], [0.5, 0.75]] (概念上)<br><br>F.stack(PE, dim=-1):<br>堆叠 [0.0, 0.5] 和 [0.5, 0.75]<br>最终的 PE 张量:<br>[[0.0, 0.5 ],  &lt;- 节点0: P(1步返回), P(2步返回)<br> [0.5, 0.75]]  &lt;- 节点1: P(1步返回), P(2步返回)<br><br>这与示例输出匹配:<br>tensor([[0.0000, 0.5000],<br>        [0.5000, 0.7500]])<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>（几乎涵盖一切的）表示学习统一框架</title>
    <link href="/2025/20250515/"/>
    <url>/2025/20250515/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>I-Con （<a href="https://arxiv.org/abs/2504.16929">I-Con: A Unifying Framework for Representation Learning</a> ，ICLR 2025）是第一个将监督学习、对比学习、聚类和降维目标统一到一个损失函数下的框架，从kmeans、TSNE到SimCLR、CLIP都能统一到一起。</p><span id="more"></span><p><img src="/2025/20250515/I-Con.jpg"></p><p><img src="/2025/20250515/model.jpg"></p><p><img src="/2025/20250515/detail.jpg"></p><p>我们首先定义 $x _ {i}\in\mathbb{R}^{d}$ 代表高维数据点，  $\phi_{i}\in\mathbb{R}^{m}$ 代表低维嵌，其中 $m\ll d$。</p><h2 id="统一SNE"><a href="#统一SNE" class="headerlink" title="统一SNE"></a>统一SNE</h2><p>基于SNE的定义，其目标分布（监督部分），由高维空间中的领域分布描述：</p><p>$$<br>p_{\theta}(j|i)&#x3D;\frac{exp(-||x _ {i}-x _ {j}||^{2}&#x2F;2\sigma_{i}^{2})}{\sum _ {k\ne i}exp(-||x _ {i}-x _ {k}||^{2}&#x2F;2\sigma_{i}^{2})}<br>$$</p><p>学习的低维领域分布是：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(-||\phi_{i}-\phi_{j}||^{2})}{\sum _ {k\ne i}exp(-||\phi_{i}-\phi_{k}||^{2})}<br>$$</p><p>目标是使它们之间的KL散度最小化：</p><p>$$<br>\mathcal{L}&#x3D;\sum _ {i}D_{KL}(p_{\theta}(\cdot|i)||q_{\phi}(\cdot|i))&#x3D;\sum _ {i}\sum _ {j}p_{\theta}(j|i)log\frac{p_{\theta}(j|i)}{q_{\phi}(j|i)}<br>$$</p><h2 id="统一tSNE"><a href="#统一tSNE" class="headerlink" title="统一tSNE"></a>统一tSNE</h2><p>tSNE和SNE类似，不过低维分布变为了具有一个自由度的student t分布。</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{(1+||\phi_{i}-\phi_{j}||^{2})^{-1}}{\sum _ {k\ne i}(1+||\phi_{i}-\phi_{k}||^{2})^{-1}}<br>$$</p><h2 id="统一PCA"><a href="#统一PCA" class="headerlink" title="统一PCA"></a>统一PCA</h2><p><strong>引理1.</strong> <em>Let $X:&#x3D;{x _ {i}} _ {i&#x3D;1}^{n}$， then the following cohesion variance loss</em></p><p>$$<br>L_{cohesion-var}&#x3D;\frac{1}{n}\sum _ {ij}w_{ij}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}-2Var(X)<br>$$</p><p><em>is an instance of I-Con in the special case $w_{ij}&#x3D;p(j|i)$ and $q_{\phi}$ is Gaussian as with a large width as $\sigma\rightarrow\infty$</em> </p><blockquote><p>证明：By using AM-GM inequality， we have [cite: 269]</p><p>$\frac{1}{n}\sum _ {k&#x3D;1}^{n}e^{-z_{k}}\ge(\Pi_{k&#x3D;1}^{n}e^{-z_{k}})^{\frac{1}{n}}\Rightarrow\frac{1}{n}\sum _ {k&#x3D;1}^{n}e^{-z_{k}}\ge(e^{-\sum _ {k&#x3D;1}^{n}z_{k}})^{\frac{1}{n}}$</p><p>which implies that </p><p>$log\sum _ {k&#x3D;1}^{n}e^{-z_{k}}-logn\ge log(e^{-\sum _ {k&#x3D;1}^{n}z_{k}})^{\frac{1}{n}}\Rightarrow log\sum _ {k&#x3D;1}^{n}e^{-z_{k}}\ge-\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}+log(n)$</p><p>Alternatively， this can be written as </p><p>$-log\sum _ {k&#x3D;1}^{n}e^{-z_{k}}\le\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}-log(n)$</p><p>Now assume that we have a Gaussian Kernel $q_{\phi}$ [cite: 269]</p><p>$q_{\phi}(j|i)&#x3D;\frac{exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}&#x2F;\sigma^{2})}{\sum _ {k\ne i}exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2}&#x2F;\sigma^{2})}$</p><p>Therefore， given the inequality of exp-sum that we showed above， we have [cite: 269]</p><p>$logq_{\phi}(j|i)&#x3D;-\frac{||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}}{\sigma^{2}}-log\sum _ {k\ne i}exp(-\frac{||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2}}{\sigma^{2}})$<br>$\le-\frac{1}{\sigma^{2}}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}+\frac{1}{n\sigma^{2}}\sum _ {k\ne i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2}-log(n)$<br>$&#x3D;-\frac{1}{\sigma^{2}}(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}+\frac{1}{n}\sum _ {k\ne i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2})-log(n)$</p><p>Therefore， the cross entropy $H(p_{\theta}，q_{\phi})$ is bounded by [cite: 269]</p><p>$H(p_{\theta}，q_{\phi})&#x3D;-\frac{1}{n}\sum _ {i}\sum _ {j}p(j|i)logq_{\phi}(j|i)$</p><p>$\le\frac{1}{n}\sum _ {i}\sum _ {j}p(j|i)(\frac{1}{\sigma^{2}}(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}+\frac{1}{n}\sum _ {k\ne i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2})-log(n))$<br>$&#x3D;\frac{1}{\sigma^{2}}(\frac{1}{n}\sum _ {ij}p(j|i)||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}-\frac{1}{n^{2}}\sum _ {ijk}p(j|i)||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2})-log(n)$<br>$&#x3D;\frac{1}{\sigma^{2}}(\frac{1}{n}\sum _ {ij}p(j|i)||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}-2Var(X))+log(n)$<br>$&#x3D;\frac{1}{\sigma^{2}}(\frac{1}{n}\sum _ {ij}p(j|i)||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}-2Var(X))+log(n)$<br>$&#x3D;\frac{1}{\sigma^{2}}\mathcal{L} _ {cohesion-var}+log(n)$</p><p>On the other hand， the L.H.S. can be upper bounded by using second order bound $e^{-z}\le1-z+z^{2}&#x2F;2$， which implies that [cite: 270]</p><p>$-log\sum _ {k&#x3D;1}^{n}e^{-z_{k}}\ge log(1-\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}+\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}^{2})-log(n)$ [cite: 270]</p><p>— PAGE 19 —</p><p>On the other hand， $log(1+u)\ge u-u^{2}&#x2F;2$， therefore， [cite: 271]</p><p>$-log\sum _ {k&#x3D;1}^{n}e^{-z_{k}}\ge(1-\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}+\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}^{2})-\frac{1}{2}(1-\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}+\frac{1}{n}\sum _ {k&#x3D;1}^{n}z_{k}^{2})^{2}-log(n)$</p><p>Therefore， in the limit $\sigma\rightarrow\infty$ the bounds become tighter and the I-Con loss approaches the cohesion variance loss. [cite: 271] $\Pi$ [cite: 272]</p></blockquote><p>根据引理1， 当 $p_{j|i}&#x3D;1[i&#x3D;j]$ 我们有</p><p>$$<br>\begin{align}<br>\mathcal{L}&amp;&#x3D;\frac{1}{n}\sum _ {ij}p_{j|i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}-2Var(X)<br>\\<br>&amp;&#x3D;\frac{1}{n}\sum _ {i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {i})||^{2}-2Var(X)<br>\\<br>&amp;&#x3D;-2Var(X)<br>\end{align}<br>$$</p><p>故最小化 L 等于最大化方差，这与PCA的目标一致。如果我们限制 $f _ {\phi}$为一个线性投影映射，则等同于PCA。</p><h2 id="统一InfoNCE"><a href="#统一InfoNCE" class="headerlink" title="统一InfoNCE"></a>统一InfoNCE</h2><p>InfoNCE 旨在最大化正样本对之间的相似度，同时最小化负样本对之间的相似度，在学习的特征空间中。在 I-Con 框架中，这可以解释为最小化两个分布之间的散度：原始空间中的邻域分布和学习到的嵌入空间中的分布。</p><p>$$<br>p_{\theta}(j|i)&#x3D;\begin{cases}\frac{1}{k}，x_j为x_i的k \  positive \ views\\ 0\end{cases}<br>$$</p><p>其中 k 是$x _ {i}$ 的positive pairs数量。</p><p> $q_{\phi}(j|i)$ 基于 $f _ {\phi}(x _ {i})$ 和 $f _ {\phi}(x _ {j})$之间相似度，并约束在单位范数上 $(||f _ {\phi}(x _ {i})||&#x3D;1)$ 。使用一个带温度系数的高斯核为：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {j})&#x2F;\tau)}{\sum _ {k\ne i}exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {k})&#x2F;\tau)}<br>$$</p><p> $||f _ {\phi}(x _ {i})||&#x3D;1$， $f _ {\phi}(x _ {i})$ 和 $f _ {\phi}(x _ {j})$ 之间的欧氏距离为 $2-2(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {j}))$</p><p>而<br>$$<br>\mathcal{L} _ {InfoNCE}&#x3D;-\sum _ {i}log\frac{exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {i}^{+})&#x2F;\tau)}{\sum _ {k}exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {k})&#x2F;\tau)}<br>$$</p><p>故<br>$$<br>\mathcal{L} _ {InfoNCE}\propto\sum _ {i}\sum _ {j}p_{\theta}(j|i)logq_{\phi}(j|i)&#x3D;H(p_{\theta}，q_{\phi})<br>$$<br>H表示交叉熵，因为 $p_{\theta}(j|i)$ 是固定的，所以最小化交叉熵等价于最小化KL散度。</p><h2 id="统一VICReg"><a href="#统一VICReg" class="headerlink" title="统一VICReg"></a>统一VICReg</h2><p>根据引理 1，我们知道任何凝聚方差形式的损失都是I-Con 的一个实例。</p><p>$$<br>\mathcal{L}&#x3D;\frac{1}{n}\sum _ {ij}p_{j|i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}-2Var(X)<br>$$</p><p>若我们选择 $p_{j|i}$ 作为正样本对的指示器， i and $i^{+}$， 则：</p><p>$$<br>\mathcal{L}&#x3D;\frac{1}{n}\sum _ {i}||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {i^{+}})||^{2}-2Var(X)<br>$$</p><p>这是没有协方差项且不变性-方差项比例为 1:2 的 VICReg 损失。观察到 VICReg 没有负样本，因为它对所有点施加相等的排斥力。这相当于在嵌入上的条件高斯分布中取 $\sigma\rightarrow\infty$ 。</p><h2 id="统一Triplet-Loss"><a href="#统一Triplet-Loss" class="headerlink" title="统一Triplet Loss"></a>统一Triplet Loss</h2><p>$$<br>P_{\theta}(j|i) &#x3D; \begin{cases} 1 &amp; \text{if } x _ {j} \text{ is among the k positive views of } x _ {i}<br>\\<br>0 &amp; \text{otherwise，} \end{cases}<br>$$</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(-\frac{||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2}}{\sigma^{2}})}{\sum _ {k\ne i}exp(-\frac{||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2}}{\sigma^{2}})^{2}}<br>$$<br>特别仅考虑两个邻居的特殊情况下，一个是正视图，一个是负视图。</p><p>为了简化，我们设置 $\sigma&#x3D;1$ 。<br>$$<br>\mathcal{L}&#x3D;-\frac{1}{N}\sum _ {i}\sum _ {j}q_{\phi}(j|i)log\frac{exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2})}{\sum _ {k\ne i}exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2})}<br>$$<br>在锚点 $x _ {i}$ 恰好有一个正例 $x _ {i}^{+}$ 和一个负例 $x _ {i}^{-}$ 的情况下，分母简化为：<br>$$<br>\sum _ {k\ne i}exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2})&#x3D;exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {i}^{+})||^{2})+exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {i}^{-})||^{2})<br>$$<br>将 $d_{i}^{+}&#x3D;||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {i}^{+})||^{2}$ 和 $d_{i}^{-}&#x3D;||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {i}^{-})||^{2}$代入损失函数，我们得到：<br>$$<br>\begin{align}<br>\mathcal{L}&amp;&#x3D;-\frac{1}{N}\sum _ {i}log\frac{exp(-d_{i}^{+})}{exp(-d_{i}^{+})+exp(-d_{i}^{-})}<br>\\<br>&amp;&#x3D;-\frac{1}{N}\sum _ {i}log(\frac{1}{1+exp(d_{i}^{-}-d_{i}^{+})})<br>\\<br>&amp;&#x3D;\frac{1}{N}\sum _ {i}log(1+exp(d_{i}^{+}-d_{i}^{-}))<br>\end{align}<br>$$<br>我们知道：</p><p>$$<br>max(z，0)\le log(1+exp(z))\le max(z，0)+log(2)<br>$$</p><p>则有：</p><p>$$<br>\frac{1}{N}\sum _ {i}max(d_{i}^{+}-d_{i}^{-}，0)\le\mathcal{L}\le\frac{1}{N}\sum _ {i}max(d_{i}^{+}-d_{i}^{-}，0)+log(2)<br>$$</p><p>左侧是 Triplet loss $\mathcal{L} _ {Triplet}&#x3D;\frac{1}{N}\sum _ {i}max(d_{i}^{+}-d_{i}^{-}，0)$，故有以下bounds:</p><p>$$<br>\mathcal{L}-log(2)\le\mathcal{L} _ {Triplet}\le\mathcal{L}<br>$$</p><p>对于一般的 $\sigma$， 不等式界限为：</p><p>$$<br>\mathcal{L} _ {\sigma}-\sigma^{2}log(2)\le\mathcal{L} _ {Triplet}\le\mathcal{L} _ {\sigma}<br>$$</p><p>其中</p><p>$$<br>\mathcal{L} _ {\sigma}&#x3D;-\frac{\sigma^{2}}{N}\sum _ {i}\sum _ {j}q_{\phi}(j|i)log\frac{exp(-\frac{||f _ {\sigma}(x _ {i})-f _ {\phi}(x _ {j})||^{2}}{\sigma^{2}})}{\sum _ {k\ne i}exp(-\frac{||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2}}{\sigma^{2}})}<br>$$</p><p>当 $\sigma\to 0$， $\mathcal{L} _ {Triplet}$ 接近于 $\mathcal{L} _ {\sigma}$ </p><h2 id="统一有监督对比损失"><a href="#统一有监督对比损失" class="headerlink" title="统一有监督对比损失"></a>统一有监督对比损失</h2><p>从统一InfoNCE的式子来看，定义：<br>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {j})&#x2F;\tau)}{\sum _ {k\ne i}exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {k})&#x2F;\tau)}<br>$$</p><p>$$<br>p_{\theta}(j|i)&#x3D;\frac{1}{K_{i}-1}1[i \text{ and } j \text{ share the same label}]<br>$$</p><p>其中 $f _ {\phi}$ 是映射到 deep feature space 的， $K_{i}$ 是类i样本数。</p><h2 id="统一X-Sample对比损失"><a href="#统一X-Sample对比损失" class="headerlink" title="统一X-Sample对比损失"></a>统一X-Sample对比损失</h2><p>考虑以下p在相应特征（例如图像的caption embeddings）上的分布</p><p>$$<br>\frac{exp(g _ {\theta}(x _ {i})\cdot g _ {\theta}(x _ {j}))}{\sum _ {k\ne i}exp(g _ {\theta}(x _ {i})\cdot_{\theta}(x _ {k}))}<br>$$<br>和大多数特征学习方法类似，学习到的分布是关于学习嵌入的余弦距离的高斯分布</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {j}))}{\sum _ {k\ne i}exp(f _ {\phi}(x _ {i})\cdot f _ {\phi}(x _ {k}))}<br>$$</p><h2 id="统一CMC和CLIP"><a href="#统一CMC和CLIP" class="headerlink" title="统一CMC和CLIP"></a>统一CMC和CLIP</h2><p>CMC（Contrastive Multiview Coding） 和 CLIP 的关键区别在于它们优化了跨不同模态的alignment， $p_{\theta}(j|i)$ 可以表示为</p><p>$$<br>p_{\theta}(j|i)&#x3D;\frac{1}{Z}1[i \text{ and } j \text{ are positive pairs and } V_{i}\ne V_{j}]<br>$$</p><p>其中 $V_{i}$ and $V_{j}$ 表示 $x _ {i}$ and $x _ {j}$的模态集。在这里 $p_{\theta}(j|i)$ 对从不同模态中抽取的正样本对赋予均匀概率。这个学习到的分布 $q_{\phi}(j|i)$是基于深度特征之间的高斯相似度，但条件是来自相反模态集的点。因此，学习到的分布定义为：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {j})||^{2})}{\sum _ {k\in V_{j}}exp(-||f _ {\phi}(x _ {i})-f _ {\phi}(x _ {k})||^{2})}<br>$$</p><h2 id="统一交叉熵"><a href="#统一交叉熵" class="headerlink" title="统一交叉熵"></a>统一交叉熵</h2><p>交叉熵可以看作是CMC的特例，其中一个视角对应数据点特征，另一个对应类别logit。</p><h2 id="统一Harmonic-Loss"><a href="#统一Harmonic-Loss" class="headerlink" title="统一Harmonic Loss"></a>统一Harmonic Loss</h2><p>这相当于从交叉熵中的 $q(j|i)$ 高斯分布移动到student-t分布，类似于从 SNE 移动到 t-SNE。更具体地说，令V 为数据点集，C为类原型集， $\phi_{i}$ 为第 i 类的学习到的类原型， n为谐波损失度。</p><p>考虑以下p这是一个数据标签指示器（ data-label indicator）。</p><p>$$<br>p(j|i)&#x3D;1[i \text{ belongs to class } j]<br>$$</p><p>和以下q，这是一个2n-1自由度的student-t分布：</p><p>$$<br>lim_{\sigma\rightarrow0}\frac{(1+||f _ {\phi}(x _ {i})-\phi_{j}||^{2}&#x2F;((2n-1)\sigma^{2}))^{-n}}{\sum _ {k\in C}(1+||f _ {\phi}(x _ {i})-\phi_{k}||^{2}&#x2F;((2n-1)\sigma^{2}))^{-n}}<br>$$</p><p>它可以写成：</p><p>$$<br>lim_{\sigma\rightarrow0}\frac{(((2n-1)\sigma^{2})+||f _ {\phi}(x _ {i})-\phi_{j}||^{2})^{-n}}{\sum _ {k\in C}((2n-1)\sigma^{2})+||f _ {\phi}(x _ {i})-\phi_{k}||^{2}&#x2F;)^{-n}}<br>$$</p><p>当 $\sigma\rightarrow\infty$ 损失函数为$\mathcal{L}&#x3D;\sum _ {i\in C}\frac{(||f _ {\phi}(x _ {i})-\phi_{j}||^{2})^{-n}}{\sum _ {k\in C}(||f _ {\phi}(x _ {i})-\phi_{k}||^{2}&#x2F;)^{-n}}$</p><h2 id="统一MLM"><a href="#统一MLM" class="headerlink" title="统一MLM"></a>统一MLM</h2><p>在掩码语言建模（MLM）中，目标是为被掩码的词j预测其周围的上下文$x_i$。这种设置自然地符合 I-Con 框架。</p><p>目标分布 $p_{\theta}(j|i)$ 是关于上下文i和tokens j的经验分布，定义为：</p><p>$$<br>p_{\theta}(j|i)&#x3D;\frac{1}{Z}\#[\text{Context } i \text{ precedes token } j]<br>$$</p><p>其中 $\#[\text{Context } i \text{ precedes token } j]$ 表示在训练语料库中tokensj与上下文x_i之后出现的次数， Z是一个归一化常数，以确保 $\sum _ {j}p_{\theta}(j|i)&#x3D;1$。</p><p> $q_{\phi}(j|i)$ 使用神经网络输出的tokens预测logits进行建模，它被定义为上下文嵌入 $f _ {\phi}(x _ {i})$ 和 token 嵌入 $\phi_{j}$ 的点积的softmax：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\frac{exp(f _ {\phi}(x _ {i})\cdot\phi_{j})}{\sum _ {k\in\mathcal{V}}exp(f _ {\phi}(x _ {i})\cdot\phi_{k})}<br>$$</p><p>其中 $f _ {\phi}(x _ {i})$ 是模型生成的上下文 $x _ {i}$ 的嵌入， $\phi_{j}$ 是token j的嵌入，V是所有可能tokens的词汇表。</p><p>MLM 损失旨在最小化目标分布p和学习分布q之间的交叉熵：</p><p>$$<br>\mathcal{L} _ {MLM}&#x3D;-\sum _ {i}\sum _ {j}p_{\theta}(j|i)logq_{\phi}(j|i)&#x3D;H(p_{\theta}，q_{\phi})<br>$$</p><p>由于在实践中，对于每个 $x _ {i}$ 仅考虑真实的masked token $j_{i}^{*}$ ，目标分布简化为：</p><p>$$<br>p_{\theta}(j|i)&#x3D;\delta_{j，j_{i}^{*}}<br>$$</p><p>其中 $\delta_{j.j_{i}^{<em>}}$ 是 Kronecker delta function， 当 $j&#x3D;j_{i}^{</em>}$ 时等于1，否则为0</p><p>代入损失函数，MLM损失变为：</p><p>$$<br>\mathcal{L} _ {MLM}&#x3D;-\sum _ {i}logq_{\phi}(j_{i}^{*}|x _ {i})<br>$$</p><h2 id="统一Kmeans"><a href="#统一Kmeans" class="headerlink" title="统一Kmeans"></a>统一Kmeans</h2><p>经典Kmeans的定义：<br>$$<br>\mathcal{L} _ {k-Means}&#x3D;\sum _ {i&#x3D;1}^{N}\sum _ {c&#x3D;1}^{m}1(c^{(i)}&#x3D;c)||x _ {i}-\mu_{c}||^{2}<br>$$</p><p> $c^{(i)}$代表聚类分配：<br>$$<br>c^{(i)}&#x3D;argmin_{c}||x _ {i}-\mu_{c}||^{2}<br>$$</p><h3 id="Probabilistic-K-means-松弛"><a href="#Probabilistic-K-means-松弛" class="headerlink" title="Probabilistic K-means 松弛"></a>Probabilistic K-means 松弛</h3><p>在probabilistic K-means， $x _ {i}$ 属于c类的概率为 $\phi_{ic}$。</p><p>$$<br>\mathcal{L} _ {Prob-k-Means}&#x3D;\sum _ {i&#x3D;1}^{N}\sum _ {c&#x3D;1}^{m}\phi_{ic}||x _ {i}-\mu_{c}||^{2}<br>$$</p><p>为了能融入到我们的框架中，我们尽量把它融入到对比学习那一套中。</p><h3 id="似然-K-means-的对比公式"><a href="#似然-K-means-的对比公式" class="headerlink" title="似然 K-means 的对比公式"></a>似然 K-means 的对比公式</h3><p>定义条件概率为 $q_{\phi}(j|i)$ ：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\sum _ {c&#x3D;1}^{m}\frac{\phi_{ic}\phi_{jc}}{\sum _ {k&#x3D;1}^{N}\phi_{kc}}<br>$$</p><p>我们可以将probabilistic K-means 重新表述为对比损失：</p><p>$$<br>\mathcal{L}&#x3D;-\sum _ {i，j}(x _ {i}\cdot x _ {j})q_{\phi}(j|i)<br>$$</p><p>证明：</p><p>$$<br>\mathcal{L} _ {Prob-k-Means}&#x3D;\sum _ {i&#x3D;1}^{N}\sum _ {c&#x3D;1}^{m}\phi_{ic}||x _ {i}-\mu_{c}||^{2}<br>$$</p><p>展开：</p><p>$$<br>\mathcal{L} _ {Prob\cdot k\cdot Means}&#x3D;\sum _ {c&#x3D;1}^{m}(\sum _ {i&#x3D;1}^{N}\phi_{ic})||\mu_{c}||^{2}-2\sum _ {c&#x3D;1}^{m}(\sum _ {i&#x3D;1}^{N}\phi_{ic}x _ {i})\cdot\mu_{c}+\sum _ {i&#x3D;1}^{N}||x _ {i}||^{2}<br>$$</p><p>使该损失最小化的聚类中心由下式给出：</p><p>$$<br>\mu_{c}&#x3D;\frac{\sum _ {i&#x3D;1}^{N}\phi_{ic}x _ {i}}{\sum _ {i&#x3D;1}^{N}\phi_{ic}}<br>$$</p><p>代回损失函数的式子，得到：</p><p>$$<br>\mathcal{L}&#x3D;-\sum _ {i，j}(x _ {i}\cdot x _ {j})q_{\phi}(j|i)<br>$$</p><p>备选损失函数：</p><p>$$<br>\mathcal{L}&#x3D;-\sum _ {i，j}||x _ {i}-x _ {j}||^{2}q_{\phi}(j|i)<br>$$</p><p>在${\phi_{ic}}$的最小化下，它会产生相同的最佳聚类分配。</p><p>证明：在损失函数中展开平方范数得到：</p><p>$$<br>\begin{align}<br>\mathcal{L}&amp;&#x3D;-\sum _ {i，j}(||x _ {i}||^{2}-2x _ {i}\cdot x _ {j}+||x _ {j}||^{2})q_{\phi}(j|i)$<br>\\<br>&amp;&#x3D;2(-\sum _ {i，j}x _ {i}\cdot x _ {j}q_{\phi}(j|i))<br>\end{align}<br>$$</p><h3 id="PROBABILISTIC-K-MEANS-作为-I-Con-方法"><a href="#PROBABILISTIC-K-MEANS-作为-I-Con-方法" class="headerlink" title="PROBABILISTIC K-MEANS 作为 I-Con 方法"></a>PROBABILISTIC K-MEANS 作为 I-Con 方法</h3><p>令 $q_{\phi}(j|i)$ 表示从$x _ {i}$的聚类中随机选择 $x _ {j}$ 的概率：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\sum _ {c&#x3D;1}^{m}\frac{\phi_{ic}\phi_{jc}}{\sum _ {k&#x3D;1}^{N}\phi_{kc}}<br>$$</p><p>定义：</p><p>$$<br>p_{\theta}(j|i)&#x3D;\frac{exp(-||x _ {i}-x _ {j}||^{2}&#x2F;2\sigma^{2})}{\sum _ {k}exp(-||x _ {i}-x _ {k}||^{2}&#x2F;2\sigma^{2})}<br>$$</p><p>$$<br>q_{\phi}(j|i)&#x3D;\sum _ {c&#x3D;1}^{m}\frac{\phi_{ic}\phi_{jc}}{\sum _ {k&#x3D;1}^{N}\phi_{kc}}<br>$$</p><p>则</p><p>$$<br>\mathcal{L} _ {c\cdot SNE}&#x3D;\sum _ {i}D_{KL}(q_{\phi}(\cdot|i)||p_{\theta}(\cdot|i))<br>$$<br>$$<br>\mathcal{L} _ {c\cdot SNE}&#x3D;\frac{1}{2\sigma^{2}}\mathcal{L} _ {Pmb\cdot k\cdot Means}-\sum _ {i}H(q_{\phi}(\cdot|i))<br>$$</p><p>证明：假设 $2\sigma^{2}&#x3D;1$。记 $\sum _ {k}exp(-||x _ {i}-x _ {k}||^{2})$ 为 $Z_{i}$，则有：</p><p>$$<br>logp_{\theta}(j|i)&#x3D;-||x _ {i}-x _ {j}||^{2}-logZ_{i}<br>$$</p><p>令 $\mathcal{L} _ {i}$ 为 $-\sum _ {j}||x _ {i}-x _ {j}||^{2}q_{\phi}(j|i)$ ，则：</p><p>$$<br>\begin{align}<br>\mathcal{L} _ {i}&amp;&#x3D;-\sum _ {j}||x _ {i}-x _ {j}||^{2}q_{\phi}(j|i)<br>\\&amp;&#x3D;\sum _ {j}(log(p_{\theta}(j|i))+log(Z_{i}))q_{\phi}(j|i)<br>\\&amp;&#x3D;\sum _ {j}q_{\phi}(j|i)log(p_{\theta}(j|i))+\sum _ {j}q_{\phi}(j|i)log(Z_{i})<br>\\&amp;&#x3D;\sum _ {j}q_{\phi}(j|i)log(p_{\theta}(j|i))+log(Z_{i})<br>\\&amp;&#x3D;H(q_{\phi}(\cdot|i)，p_{\theta}(\cdot|i))+log(Z_{i})<br>\\&amp;&#x3D;D_{KL}(q_{\phi}(\cdot|i)||p_{\theta}(\cdot|i))+H(q_{\phi}(\cdot|i))+log(Z_{i})<br>\end{align}<br>$$</p><p>故：<br>$$<br>\begin{align}<br>\mathcal{L} _ {Prob-KMeans}&amp;&#x3D;-\sum _ {i}\sum _ {j}||x _ {i}-x _ {j}||^{2}q_{\phi}(j|i)&#x3D;\sum _ {i}\mathcal{L} _ {i}<br>\\&amp;&#x3D;\sum _ {i}D_{KL}(q_{\phi}(\cdot|i)||p_{\theta}(\cdot|i))+H(q_{\phi}(\cdot|i))+log(Z_{i})<br>\\&amp;&#x3D;\mathcal{L} _ {c.SNE}+\sum _ {i}H(q_{\phi}(\cdot|i))+constant<br>\end{align}<br>$$<br>因此， </p><p>$$<br>\mathcal{L} _ {c\cdot SNE}&#x3D;\mathcal{L} _ {Prob-KMeans}-\sum _ {i}H(q_{\phi}(\cdot|i))<br>$$</p><p>如果我们允许 $\sigma$ 为任何值：</p><p>$$<br>\mathcal{L} _ {c\cdot SNE}&#x3D;\frac{1}{2\sigma^{2}}\mathcal{L} _ {Prob\cdot KMeans}-\sum _ {i}H(q_{\phi}(\cdot|i))<br>$$</p><p>注意上述关系仅差一个加性常数。这意味着最小化具有熵正则化的对比概率 K-means 损失，等价于最小化KL 散度之和。</p><h2 id="统一谱聚类"><a href="#统一谱聚类" class="headerlink" title="统一谱聚类"></a>统一谱聚类</h2><p>谱聚类通过首先使用归一化拉普拉斯矩阵的前 k 个特征向量将数据嵌入到低维空间中，扩展了这个想法。亲和矩阵 A是通过相似性度量（例如径向基函数核）构建的，并编码了数据点之间的分配概率。在这个转换下，谱聚类是投影嵌入上的 I-Con 的一个实例。</p><h2 id="统一Normalized-Cuts"><a href="#统一Normalized-Cuts" class="headerlink" title="统一Normalized Cuts"></a>统一Normalized Cuts</h2><p>其损失函数定义为：</p><p>$$<br>\mathcal{L} _ {NomCus}&#x3D;\sum _ {c&#x3D;1}^{m}\frac{cut(A_{c}，\overline{A} _ {c})}{vol(A_{c})}<br>$$</p><p>其中 $A_{c}$ 为对应于聚类c的数据子集， $\overline{A} _ {c}$ 是其补集，  $cut(A_{c}，\overline{A_{c}})$ 表示 $A_{c}$ 和 $\overline{A} _ {c}$之间的边权重之和，而 $vol(A_{c})$ 是聚类 $A_{c}$ 的总体积&#x2F;边权重之和。</p><p>类似于 K-Means，通过以软分配的对比方式重新表述这一点，学习到的分布可以表示为概率簇分配 $\phi_{ic}&#x3D;p(c|x _ {i})$ ：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\sum _ {c&#x3D;1}^{m}\frac{\phi_{ic}\phi_{jc}d_{j}}{\sum _ {k&#x3D;1}^{N}\phi_{kc}d_{k}}<br>$$</p><p>其中d为节点的度数。</p><h2 id="统一互信息聚类"><a href="#统一互信息聚类" class="headerlink" title="统一互信息聚类"></a>统一互信息聚类</h2><p>鉴于 SimCLR、K-Means 和 I-Con 框架之间已建立的联系，这一结果自然而然地得出。具体来说，目标分布 $p_{\theta}(j|i)$ 是观测到的正对的一个均匀分布：</p><p>$$<br>P_{\theta}(j|i) &#x3D; \begin{cases} 1 &amp; \text{if } x _ {j} \text{ is among the k positive views of } x _ {i} \\ 0 &amp; \text{otherwise.} \end{cases}<br>$$</p><p>另一方面，学习到的嵌入 $\phi_{i}$ 表示 $x _ {i}$ 被分配到聚类中的概率。因此，类似于 K-Means 连接的分析，学习到的分布被建模为：</p><p>$$<br>q_{\phi}(j|i)&#x3D;\sum _ {c&#x3D;1}^{m}\frac{\phi_{ic}\phi_{jc}}{\sum _ {k&#x3D;1}^{N}\phi_{kc}}<br>$$</p><h2 id="统一变分法"><a href="#统一变分法" class="headerlink" title="统一变分法"></a>统一变分法</h2><p>互信息的变分界限被广泛探索，并与 InfoNCE 等损失函数联系起来，其中最小化 InfoNCE 最大化互信息下界（Oord 等人，2018；Poole 等人，2019）。证明通常从重写互信息开始：</p><p>$$<br>I(X;Y)&#x3D;\mathbb{E} _ {p(x，y)}[log\frac{q(x|y)}{p(x)}]+\mathbb{E} _ {p(y)}[D_{KL}(p(x|y)||q(x|y))]<br>$$</p><p>我们一般假设 $p(x _ {i})&#x3D;\frac{1}{N}$ ，则 $p(x，y)&#x3D;\frac{1}{N}p(x|y)$。互信息下界变为<br>$$<br>\begin{align}<br>I(X;Y)&amp;\geq \mathbb{E} _ {p(x，y)}[logq(x|y)]-\mathbb{E} _ {p(x，y)}[log p(x)]<br>\\&amp;&#x3D;\mathbb{E} _ {p(x，y)}[logq(x|y)]+log(N)<br>\\&amp;&#x3D;\frac{1}{N}\sum _ {x，y\in\mathcal{X}\times\mathcal{X}}p(x|y)logq(x|y)+log(N)<br>\\&amp;&#x3D;\frac{1}{N}\sum _ {y\in\mathcal{X}}\sum _ {x\in\mathcal{X}}p(x|y)logq(x|y)+log(N)<br>\\&amp;&#x3D;-H(p(x|y)，q(x|y))+log(N)<br>\end{align}<br>$$</p><p>因此，最大化两个分布之间的交叉熵会最大化样本之间的互信息。</p><p>变分贝叶斯中的近似是通过最小化变分分布和真实后验之间的 KL散度来实现的：</p><p>$$<br>KL(q_{\phi}(z)||p(z|x))&#x3D;\mathbb{E} _ {q_{\phi}(z)}[log\frac{q_{\phi}(z)}{p(z|x)}]<br>$$</p><p>优化目标（被称为ELBO）：</p><p>$$<br>ELBO&#x3D;\mathbb{E} _ {q_{\varphi}(z)}[logp(x，z)]-\mathbb{E} _ {q_{\varphi}(z)}[logq_{\phi}(z)]<br>$$</p><p>最大化 ELBO 等价于最小化 KL 散度，从而确保 $q_{\phi}(z)$ 接近 $p(z|x)$。VB 可以通过在变量和分布之间进行特定映射，在 I-Con 框架内进行描述。让i对应数据点x，j对应潜变量z，我们可以将监督分布 $p_{\theta}(z|x)$ 设置为真实后验$p(z|x)$。我们可以定义学习到的分布 $q_{\phi}(z|x)$ 与x无关，即 $q_{\phi}(z|x)&#x3D;q_{\phi}(z)$。</p><p>则I-Con loss 简化为：</p><p>$$<br>\mathcal{L}(\phi)&#x3D;\int_{x\in\mathcal{X}}KL(p(z|x)||q_{\phi}(z))dx&#x3D;\mathbb{E} _ {p(x)}[KL(p(z|x)||q_{\phi}(z))]<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随机块模型</title>
    <link href="/2025/20250513/"/>
    <url>/2025/20250513/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在笔者之前研究图神经网络的时候，一直苦于难以使用数学去之间建模图数据。但似乎这一切有了转机，笔者最近发现了一种特殊的分析方法来分析GNN，对此这就不得不读了。</p><span id="more"></span><p>像之前提到的《Joint Graph Rewiring and Feature Denoising via Spectral Resonance》（ICLR2025）、上篇的《Is homophily a necessity for graph neural networks? 》ICLR2022、<a href="https://arxiv.org/abs/2207.11311">《Understanding Non-linearity in Graph Neural Networks from the Bayesian-Inference Perspective》</a>(NIPS2022)、<a href="https://arxiv.org/abs/2304.14274">《When Do Graph Neural Networks Help with Node Classification? Investigating the Impact of Homophily Principle on Node Distinguishability》</a>（NIPS2023）都使用到了stochastic block model或contextual stochastic block model（SBM&#x2F;CSBM)。</p><h2 id="SBM"><a href="#SBM" class="headerlink" title="SBM"></a>SBM</h2><p>SBM 的基本参数包括：节点数 n、将节点划分为r个社区的分区集合 {C1,…,Cr}，以及一个对称的 r×r边概率矩阵 P。<br> 对于任意两个节点 $u∈C_i，v\in C_j$，在 SBM 中它们以概率 $P_{ij}$ 相连。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_sbm</span>(<span class="hljs-params">n, sizes, p_matrix, seed=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Generate a graph from the Stochastic Block Model (SBM).</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters:</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    n : int</span><br><span class="hljs-string">        Total number of nodes.</span><br><span class="hljs-string">    sizes : list of int</span><br><span class="hljs-string">        Sizes of each community (sum(sizes) == n).</span><br><span class="hljs-string">    p_matrix : 2D array-like of shape (r, r)</span><br><span class="hljs-string">        Connection probability matrix between communities.</span><br><span class="hljs-string">    seed : int, optional</span><br><span class="hljs-string">        Random seed for reproducibility.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    G : networkx.Graph</span><br><span class="hljs-string">        Generated SBM graph with node attribute &#x27;block&#x27; indicating community.</span><br><span class="hljs-string">    labels : np.ndarray of shape (n,)</span><br><span class="hljs-string">        Community label for each node.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> seed <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        np.random.seed(seed)<br>    r = <span class="hljs-built_in">len</span>(sizes)<br>    <span class="hljs-keyword">assert</span> p_matrix.shape == (r, r), <span class="hljs-string">&quot;p_matrix must be r x r&quot;</span><br><br>    <span class="hljs-comment"># Assign block labels</span><br>    labels = np.hstack([[i] * size <span class="hljs-keyword">for</span> i, size <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sizes)])<br><br>    <span class="hljs-comment"># Initialize adjacency matrix</span><br>    A = np.zeros((n, n), dtype=<span class="hljs-built_in">int</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, n):<br>            pi = labels[i]<br>            pj = labels[j]<br>            <span class="hljs-keyword">if</span> np.random.rand() &lt; p_matrix[pi, pj]:<br>                A[i, j] = A[j, i] = <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># Build graph</span><br>    G = nx.from_numpy_array(A)<br>    <span class="hljs-comment"># Assign block attribute</span><br>    <span class="hljs-keyword">for</span> idx, lbl <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):<br>        G.nodes[idx][<span class="hljs-string">&#x27;block&#x27;</span>] = <span class="hljs-built_in">int</span>(lbl)<br><br>    <span class="hljs-keyword">return</span> G, labels<br></code></pre></td></tr></table></figure><h2 id="CSBM"><a href="#CSBM" class="headerlink" title="CSBM"></a>CSBM</h2><p>cSBM 在 SBM 基础上额外定义：</p><ul><li>对每个节点 i，从与其社区i相关的属性分布（如高维高斯分布）中抽样特征向量$x_u$。</li><li>边的生成仍遵循 SBM 的 $P_{ij}$；节点属性生成由社区-条件分布控制。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_contextual_sbm</span>(<span class="hljs-params">n, pi, p_matrix, theta, feature_dist=<span class="hljs-string">&#x27;gaussian&#x27;</span>, seed=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Generate a graph with attributes from the Contextual SBM (cSBM).</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Parameters:</span><br><span class="hljs-string">    ----------</span><br><span class="hljs-string">    n : int</span><br><span class="hljs-string">        Total number of nodes.</span><br><span class="hljs-string">    pi : list or np.ndarray of shape (r,)</span><br><span class="hljs-string">        Mixing proportions for each of r communities.</span><br><span class="hljs-string">    p_matrix : 2D array-like of shape (r, r)</span><br><span class="hljs-string">        Connection probability matrix between communities.</span><br><span class="hljs-string">    theta : list of dict</span><br><span class="hljs-string">        Parameters for each community&#x27;s feature distribution.</span><br><span class="hljs-string">        For Gaussian: &#123;&#x27;mean&#x27;: array, &#x27;cov&#x27;: array&#125;</span><br><span class="hljs-string">    feature_dist : str</span><br><span class="hljs-string">        Distribution type (&#x27;gaussian&#x27;).</span><br><span class="hljs-string">    seed : int, optional</span><br><span class="hljs-string">        Random seed for reproducibility.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">    -------</span><br><span class="hljs-string">    G : networkx.Graph</span><br><span class="hljs-string">        Generated cSBM graph with node attributes &#x27;block&#x27; and &#x27;features&#x27;.</span><br><span class="hljs-string">    labels : np.ndarray of shape (n,)</span><br><span class="hljs-string">        Community label for each node.</span><br><span class="hljs-string">    X : np.ndarray of shape (n, d)</span><br><span class="hljs-string">        Feature matrix.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> seed <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        np.random.seed(seed)<br>    r = <span class="hljs-built_in">len</span>(pi)<br>    <span class="hljs-keyword">assert</span> p_matrix.shape == (r, r), <span class="hljs-string">&quot;p_matrix must be r x r&quot;</span><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(theta) == r, <span class="hljs-string">&quot;theta must have one entry per community&quot;</span><br><br>    <span class="hljs-comment"># Sample community labels</span><br>    labels = np.random.choice(r, size=n, p=pi)<br>    <span class="hljs-comment"># Generate features</span><br>    <span class="hljs-comment"># Assume Gaussian mixture</span><br>    d = theta[<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;mean&#x27;</span>].shape[<span class="hljs-number">0</span>]<br>    X = np.zeros((n, d))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        z = labels[i]<br>        params = theta[z]<br>        X[i] = np.random.multivariate_normal(mean=params[<span class="hljs-string">&#x27;mean&#x27;</span>], cov=params[<span class="hljs-string">&#x27;cov&#x27;</span>])<br><br>    <span class="hljs-comment"># Generate adjacency</span><br>    A = np.zeros((n, n), dtype=<span class="hljs-built_in">int</span>)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, n):<br>            <span class="hljs-keyword">if</span> np.random.rand() &lt; p_matrix[labels[i], labels[j]]:<br>                A[i, j] = A[j, i] = <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># Build graph</span><br>    G = nx.from_numpy_array(A)<br>    <span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>        G.nodes[idx][<span class="hljs-string">&#x27;block&#x27;</span>] = <span class="hljs-built_in">int</span>(labels[idx])<br>        G.nodes[idx][<span class="hljs-string">&#x27;features&#x27;</span>] = X[idx]<br><br>    <span class="hljs-keyword">return</span> G, labels, X<br></code></pre></td></tr></table></figure><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>形式确实很简单，有点类似回归中解释是高斯分布。</p><p>我们可以来看上篇的《Is homophily a necessity for graph neural networks? 》是怎么使用的。</p><p>我们考虑一个图 G，其中每个节点 i 具有特征 x∈ R 标签 y。</p><p>我们假设</p><p>(1) 节点 i 的特征从特征分布 F 中采样，即 x∼ F，μ(F)表示其均值；</p><p> (2) x 的维度之间相互独立； </p><p>(3) X 中的特征被一个正标量 B 所限制，即 max|X[i, j]| ≤ B；</p><p>(4) 对于节点 i，其邻居的标签从邻居分布 D 中独立采样。采样重复 deg(i)次以采样 deg(i)个邻居的标签。</p><p><strong>定理1：</strong> 单个GCN操作激活前的输出的期望是：<br>$$<br>E[h_i]&#x3D;W(E_{c\sim D_{y_i},x\sim F_c}[x])<br>$$<br>以及对于任何 t &gt; 0，观测值与其期望的距离大于 t 的概率是有界的：<br>$$<br>P(||h_i-E[h_i]||_2\ge t)\le 2l\cdot exp(-\frac{deg(i)t^2}{2\rho^2(W)B^2l})<br>$$</p><p>其中$\rho$代表最大奇异值，$l$代表特征维度。</p><p>对于CSBM模型$G\sim CSBM(\mu_1,\mu_2,p,q)$，我们可以认为邻域分布$D_{c1}&#x3D;[\frac{p}{p+q},\frac{q}{p+q}],D_{c2}&#x3D;[\frac{q}{p+q},\frac{p}{p+q}]$。</p><p>基于领域分布，对于$i\in C_1$：<br>$$<br>h_i\sim N(\frac{p\mu_1+q\mu_2}{p+q},\frac{I}{\sqrt{deg(i)}})<br>$$<br>根据高斯分布的性质，很容易看出定理1成立。</p><p><strong>命题1：</strong> $(E_{c1}[x_i],E_{c2}[x_i])$和$(E_{c1}[h_i],E_{c2}[h_i])$具有相同的中点m，且$E_{c1}[x_i]-E_{c2}[x_i]$和$E_{c1}[h_i]-E_{c2}[h_i]$具有相同的方向w。$m&#x3D;\frac{\mu_1+\mu_2}{2},w&#x3D;\frac{\mu_1-\mu_2}{||\mu_1-\mu_2||_2}$。</p><p><strong>定理2：</strong> 图符合CSBM，则对于任意节点i，当deg(i)&gt;(p+q)&#x2F;(p-q)时，由决策边界 P 定义的线性分类器误分类$h_i$的概率低于误分类$x_i$的概率。</p><p><strong>证明：</strong> </p><p>我们仅对来自类$C_1$的节点进行证明。因为对称性，其他的证明相同。</p><p>对于节点$i\in C_1$，<br>$$<br>\begin{align}<br>P(x_i \ \text{is mis-classfied})&#x3D;P(w^Tx_i+b\le0)<br>\\<br>P(h_i \ \text{is mis-classfied})&#x3D;P(w^Th_i+b\le0)<br>\end{align}<br>$$</p><p>w和$b&#x3D;-w^T(\mu_1+\mu_2)&#x2F;2$是决策边界P的参数。</p><p>我们有：<br>$$<br>P(w^Th_i+b\le 0)&#x3D;P(w^T\sqrt{deg(i)}h_i+\sqrt{deg(i)}b\le 0)<br>$$<br>令$h_i’&#x3D;\sqrt{deg(i)}h_i$，故：<br>$$<br>h_i’&#x3D;\sqrt{deg(i)}h_i\sim N(\frac{\sqrt{deg(i)}(p\mu_1+q\mu_2)}{p+q},I)<br>$$<br>$x_i$和$h_i’$具有相同的方差。</p><p>为了比较误分类概率，我们只需比较期望值到决策边界的距离，具体而言：<br>$$<br>\begin{align}<br>dis_{x_i}&amp;&#x3D;\frac{||\mu_1-\mu_2|| _ 2}{2}<br>\\<br>dis_{h_i’}&amp;&#x3D;\frac{\sqrt{deg(i)}|p-q|}{p+q}\frac{||\mu_1-\mu_2||_2}{2}<br>\end{align}<br>$$<br>得证。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>同质性对图神经网络是必需的吗？</title>
    <link href="/2025/20250512/"/>
    <url>/2025/20250512/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>出自<a href="https://arxiv.org/abs/2106.06134">《Is homophily a necessity for graph neural networks? 》ICLR, 2022.</a></p><p>GNNs 通常被认为由于同质性假设（“物以类聚”）而工作良好，但在异质性图中（不同节点相连）泛化能力较差。</p><p>这篇论文则认为，如果图中的同一类的结点具有相似的邻居的分布, 则 Homophily 不是必须的。</p><span id="more"></span><h2 id="同质性假设"><a href="#同质性假设" class="headerlink" title="同质性假设"></a>同质性假设</h2><p>同质性可以分为：</p><p>标签同质性：即同一类别的节点更易相连</p><p>特征同质性：具有相似特征向量的节点，比特征差异很大的节点，更有可能通过一条边相连。</p><p>业界一致认为同质性是GNN的必要前提。</p><p>比如像《A Unified View on Graph Neural Networks as Graph Signal Denoising》(CIKM), 2021.。甚至认为这些GNN（GCN、GAT、PPAP等）其实都是以下问题的形式：<br>$$<br>argmin_F L&#x3D;||F-S||_F^2+c*tr(F^TLF)<br>$$<br>我们在这里只讲一下GCN。</p><p>$$<br>\nabla L&#x3D;2||F-S||+2cLF<br>\mathop{&#x3D;}^{F&#x3D;S}2cLS<br>$$</p><p>如果我们采用梯度下降求解：<br>$$<br>\begin{align}<br>F&amp;&#x3D;S-\lambda (2cLS)&#x3D;(1-2\lambda c)S+2\lambda cAS<br>\\<br>&amp;\mathop{&#x3D;}^{\lambda&#x3D;1&#x2F;2c,S&#x3D;X’}AX’<br>\end{align}<br>$$<br>即GCN的aggregation。</p><h2 id="我们真的需要同质性吗"><a href="#我们真的需要同质性吗" class="headerlink" title="我们真的需要同质性吗"></a>我们真的需要同质性吗</h2><p>论文首先举了个这个例子，可以发现这个图中所有蓝色节点将具有1的表示，而橙色节点将具有0的表示。</p><p><img src="/2025/20250512/x1.png"></p><p>如果图中的同一类的结点具有相似的邻居的分布, 则 Homophily 不是必须的，比如：</p><p><img src="/2025/20250512/1603215-20231031155025693-1474591626.png"></p><p>严谨地，论文同样从contextual stochastic block model(CSBM)入手，证明了</p><p><img src="/2025/20250512/theo.jpg"></p><p>$CBSM(\mu_1,\mu_2,p,q)$表示两个社区，若节点i和j属于同一社区，则它们之间有边的概率为$P(A_{ij}&#x3D;1)&#x3D;p$，若节点i和j属于不同社区，则它们之间有边的概率为$P(A_{ij}&#x3D;1)&#x3D;q$，</p><p>同质性为$\frac{p}{p+q}$。</p><p>这句话的意思是，比如在极端情况下，p&#x3D;9q或q&#x3D;9p，则$（p+q）^2&#x2F;(p-q)^2&#x3D;1.23$，只要节点度数大于1（这个条件很容易达到），是可以被区分的，故没有同质性并不会影响GCN的性能。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fourier Position Embedding</title>
    <link href="/2025/20250511/"/>
    <url>/2025/20250511/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Fourier Position Embedding（FOPE)出自ICML2025，对当前LLM常用的ROPE进行了改进，认为ROPE默认“每一维只存在单一频率的语义”是过于理想的，所以提出了增加更多的频率（比如基于傅里叶）的方法。</p><span id="more"></span><h2 id="旋转位置编码"><a href="#旋转位置编码" class="headerlink" title="旋转位置编码"></a>旋转位置编码</h2><p>旋转式位置编码（Rotary Position Embedding，RoPE）由苏神提出，<a href="https://kexue.fm/archives/8265">在其博客中也有介绍</a>。</p><p>我们也可以重温一下。</p><p>我们想要一个又有绝对位置，又有相对位置的编码。苏神假定存在恒等关系：<br>$$<br>&lt;f(q,m),f(k,n)&gt;&#x3D;g(q,k,m-n)<br>$$<br>苏神找到了以下f：$f(q,m)&#x3D;qe^{im\theta}$。</p><p><img src="/2025/20250511/rope1.png"></p><p>我们可以把它扩展到更多维：</p><p><img src="/2025/20250511/rope2.png"></p><p>注意到<code>[[A,O][O,B]]*[[C,O][O,D]]=[[AC,O][O,BD]]</code></p><p>所以这样拼接，原本的性质是不会被破坏的。</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>ROPE看上去已经很不错了，但是它会有什么问题呢？</p><p><strong>线性层会混杂频率：</strong></p><p>线性层WX由于是对所有元素进行操作，自然而然会产生混杂。</p><p><strong>激活函数会混杂频率：</strong></p><p>作者以非线性函数和泰勒展开的角度，假设非线性函数可以展开为$\sum ax^n$，这样会有交互项。比如$(sin\theta+cos\theta)^2&#x3D;1+sin2\theta$，这会引入一个新频率。</p><blockquote><p>我感觉这里怪怪的</p></blockquote><p> <strong>时域截断：</strong></p><p>即由于时域截断，也会产生新的频率。</p><p>即正常的应为只在0处有值，但经过时域截断后，会变为：</p><p><img src="/2025/20250511/sinxx.png"></p><p>其他地方也会有频率掺杂进来了。</p><p>（比较经典的信号与系统知识。）</p><h2 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h2><p>① 既然每一维中不可避免的混杂其他频率的分量，那就干脆在一开始就把每一维都建模成一个傅里叶级数（Fourier Series）。即使这样的建模不会避免频谱破坏，FoPE却可以在每一维中分解出更多频率的信息（利用三角函数的正交性），从而让更多的频率分量保持周期延拓性；</p><p>② 既然极低频的分量周期过长，会导致这些频率分量的周期特性无法被学习到，那就将他们裁剪成频率为0的直流分量。考虑到直流分量的良好性质（既可以看作周期无限短，又可以看作周期无限长），</p><p>第一点听起来有点晦涩，我们直接来看代码。</p><p>代码就像照妖镜，不管故事讲得多么动听，它总能抓住背后的核心本质。</p><p>代码：<a href="https://github.com/TsinghuaC3I/Fourier-Position-Embedding">https://github.com/TsinghuaC3I/Fourier-Position-Embedding</a></p><p><strong>ROPE：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_rotary_pos_emb</span>(<span class="hljs-params">self, sin, cos, x, inverse=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-comment"># 先把最后一半维度右移（rotate-half）</span><br>    <span class="hljs-comment"># x.shape = [..., 2*(dim//2)]</span><br>    rot = <span class="hljs-variable language_">self</span>.rotate_half(x)<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inverse:<br>        <span class="hljs-keyword">return</span> x * cos + rot * sin<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> x * cos - rot * sin<br><br></code></pre></td></tr></table></figure><p><strong>FOPE：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply_rotary_pos_emb</span>(<span class="hljs-params">self, sin, cos, x, inverse=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-comment"># 1) 先把 sin/cos 从 (…, input_dim*2) 投影到 (…, output_dim)</span><br>    <span class="hljs-comment">#    支持两种系数：sin_coef/cos_coef 或者 一个 fourier_coef</span><br>    fourier_sin = torch.einsum(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.input_shape&#125;</span>, <span class="hljs-subst">&#123;self.coef_shape&#125;</span> -&gt; <span class="hljs-subst">&#123;self.output_shape&#125;</span>&quot;</span>,<br>                               sin, <span class="hljs-variable language_">self</span>.sin_coef)<br>    fourier_cos = torch.einsum(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;self.input_shape&#125;</span>, <span class="hljs-subst">&#123;self.coef_shape&#125;</span> -&gt; <span class="hljs-subst">&#123;self.output_shape&#125;</span>&quot;</span>,<br>                               cos, <span class="hljs-variable language_">self</span>.cos_coef)<br>    <span class="hljs-comment"># 2) pad 到原始 head_dim，拼接成 (…, head_dim)</span><br>    fourier_sin = torch.cat((fourier_sin, fourier_sin), dim=-<span class="hljs-number">1</span>)<br>    fourier_cos = torch.cat((fourier_cos, fourier_cos), dim=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 3) 再做 rotate-half + 加权</span><br>    rot = <span class="hljs-variable language_">self</span>.rotate_half(x)<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> inverse:<br>        <span class="hljs-keyword">return</span> x * fourier_cos + rot * fourier_sin<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> x * fourier_cos - rot * fourier_sin<br><br></code></pre></td></tr></table></figure><p>这里采用的是einsum，其实相当于做了个线性层变换(<code>nn.Linear(input_dim, output_dim, bias=False</code>)。</p><h2 id="原作回应"><a href="#原作回应" class="headerlink" title="原作回应"></a>原作回应</h2><p>ROPE作者和FOPE的作者也有<a href="https://www.kexue.fm/archives/10862">讨论</a>。苏神给出的回应是”基于<a href="https://kexue.fm/archives/9403%EF%BC%8C%E7%90%86%E8%AE%BA%E4%B8%8ARoPE%E7%9A%84%E5%BD%93%E5%89%8D%E5%BD%A2%E5%BC%8F%E6%98%AF%E5%AE%8C%E5%A4%87%E7%9A%84%E3%80%82%E2%80%9C">https://kexue.fm/archives/9403，理论上RoPE的当前形式是完备的。“</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost中的base_score</title>
    <link href="/2025/20250503/"/>
    <url>/2025/20250503/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文主要源自与@broccoli beef的讨论和kaggle的TPS竞赛经历。笔者率先指出了<code>base_score</code>的差异，但@broccoli beef给出了更多有价值的意见和内容。</p><span id="more"></span><p>与LGBM和Catboost不同，XGBoost有一个令人忽视的参数<code>base_score</code>。</p><p>在多场的kaggle中，这个参数都起着不可忽视的作用，且让笔者在kaggle TPS05E3保持优势。</p><p><code>base_score</code> 可以看作是第0轮的预测 $F_0(x_i)&#x3D;C$。我们希望找到一个最优的常数初始预测，所以也有说法说，只要迭代次数够大，base_score基本无影响。</p><p>kaggle的XGBoost默认版本为2.0.3，但最新的稳定版本是3.0.0。不同版本之间处理<code>base_score</code>出现了不同。</p><p><a href="https://github.com/dmlc/xgboost/pull/10298">此PR</a>将简单目标（逻辑回归、泊松、伽马、泰迪）的截距初始化修改为使用它们的闭式最优解（即：使目标函数最小化的那个数字）而不是非最优的一步牛顿法。</p><p>比如说在二分类&#x2F;logloss中：</p><p><strong>3.0.0版本前：</strong></p><p>最优的一步牛顿法，我们在z&#x3D;0处泰勒展开：<br>$$<br>L(y,z)\approx L(y,0)+\sum_i g_i(0)z_i+\frac{1}{2}\sum_i h_i(0)z_i^2<br>$$<br>根据牛顿法：<br>$$<br>F_{k+1}&#x3D;F(k)-\frac{\sum grad(F_k)}{\sum hess(F_k)}<br>$$<br><code>base_score</code> $b&#x3D;l^{-1}(-\frac{\sum_i g_i(0)}{\sum_i h_i(0)})$，其中$l$是GLM中的连接函数，$l&#x3D;\sigma^{-1}$.<br>$$<br>g&#x3D;\sigma(z)-y,h&#x3D;\sigma(z)(1-\sigma(z))<br>$$<br>故<br>$$<br>b&#x3D;\sigma\left(-\frac{\sum_i (0.5-y_i)}{\sum_i (0.5*0.5)}\right)&#x3D;\sigma\left(\frac{\sum_i y_i-0.5N)}{0.25N}\right)&#x3D;\sigma(4\bar{y}-2)<br>$$<br><strong>3.0.0版本：</strong></p><p>最优闭式解也就是导数为0的点。<br>$$<br>\begin{align}<br>\frac{\partial}{\partial_{p}}\sum_i -(y_ilog(p)+(1-y)log(1-p))<br>&amp;&#x3D;\sum_i-\left(\frac{y_i}{p}-\frac{1-y_i}{1-p}\right)<br>\\&amp;&#x3D;\sum_i-\frac{y_i-p}{p(1-p)}<br>\\&amp;&#x3D;0<br>\end{align}<br>$$<br>故<br>$$<br>p&#x3D;\frac{\sum y }{N}&#x3D;\bar{y}<br>$$<br><strong>注意</strong>，二分类情况下是在logit空间操作的。所以如果自定义损失函数，比如$\bar{y}$需要写成$log(\frac{\bar{y}}{1-\bar{y}})$</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://github.com/dmlc/xgboost/pull/10298">https://github.com/dmlc/xgboost/pull/10298</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>力场启发的扩散模型</title>
    <link href="/2025/20250502/"/>
    <url>/2025/20250502/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将介绍两篇论文，巧妙地通过力场的思想去构建扩散模型。出品自MIT，作者包括KAN一作刘子鸣等，（但对于KAN那篇，个人认为噱头大于实用性。）</p><span id="more"></span><p>《Poisson Flow Generative Models》NIPS22</p><p>《PFGM++: Unlocking the Potential of Physics-Inspired Generative Models》ICCML2023</p><h2 id="论文引入"><a href="#论文引入" class="headerlink" title="论文引入"></a>论文引入</h2><p>前两篇分别介绍了DDPM和BFN，它们尽管采用了不同的（加噪）方式，但围绕着同一个主题，从一个熟悉的分布到数据分布的相互转换。</p><p>PFGM也是如此，但不同的是，它采用的不是高斯分布而是物理上的性质。</p><p><strong>即在超平面中由（几乎）任何分布生成的泊松场在足够远的距离处会产生均匀的角分布。</strong></p><p><strong>或者通俗地讲，无穷远处的多源引力场，等价于位于质心、质量叠加的质点引力场。</strong></p><p><img src="/2025/20250502/vis.gif"></p><p><img src="/2025/20250502/67aa869a17b5fe3c0c0037fc_heart_evolution.png"></p><p>如果我们知道由分布产生的电场（即泊松场），那么我们可以从半球上均匀采样的点开始，在反向时间中运行动力学（沿着电场线运动），以恢复原始数据分布。</p><p>电场的力可以写为<br>$$<br>F(x,y)&#x3D;\frac{-1}{S_{N-1}(1)}\frac{x-y}{||x-y||^N}<br>$$<br>其中$S_{}(1)$是d维单位超球面的表面积，这个式子是d维poisson方程的格林函数的梯度，所以也被称为poisson场。<br>$$<br>G(x,y)&#x3D;\frac{1}{(N-2)S_{N-1}(1)}\frac{1}{||x-y||^{N-2}}<br>$$</p><h2 id="模式坍塌"><a href="#模式坍塌" class="headerlink" title="模式坍塌"></a>模式坍塌</h2><p>在将这一特性使用进我们的模型之前，我们要解决一个重要的事情——模式坍塌。</p><p>即，比如3维的情况下，球壳内部场强为0。这时候就让我们无法到达理想的样本。</p><p>作者想了一个妙招，即增加一维，尽管我在d维下可能有一些数据点达不到，但是我可以增加一维通过额外的维度绕过去以采样我想要的数据点。记新增的维度为z。</p><h2 id="定律应用"><a href="#定律应用" class="headerlink" title="定律应用"></a>定律应用</h2><p>但是我们的数据是d维的，所以论文采用狄拉克函数以映射会d维。</p><p><img src="/2025/20250502/67aa869a303690b7b2cfb604_augmented_data_dist.png"></p><p>我们计算分布产生的场：</p><p><img src="/2025/20250502/67aa869a67b9850804f4ff22_empirical_field-1.png"></p><p>我们实际上并不关心场强度，只关心方向。（相当于我们只关心正确的路径，不关心速度和加速度，速度和加速度我们可以自己配置。）</p><p>为了更好地训练，我们对场进行规范化：</p><p><img src="/2025/20250502/67aa869a45b5729b89cc0541_normalized_empirical_field.png"></p><p>伪代码如下：</p><p><img src="/2025/20250502/67aa869ad6eb718bfcf03131_algo.png"></p><p>可以发现，伪代码还有个perturb，看上去很诡异，让人摸不着头脑。</p><p>苏神在<a href="https://kexue.fm/archives/9370#%E9%97%AE%E9%A2%98%E9%87%8D%E6%8B%BE">它的博客</a>中给出了这一点是如何得到的。</p><h2 id="定义起点和终点"><a href="#定义起点和终点" class="headerlink" title="定义起点和终点"></a>定义起点和终点</h2><p>场强定义为$E&#x3D;\frac{dx}{dt}$，则我们可以反向常微分方程以采样$d\hat x&#x3D;v(\hat x)dt$</p><p>我们定义为终点为到达z&#x3D;0。<br>$$<br>d(x,z)&#x3D;(\frac{dx}{dt}\frac{dt}{dz}dz,dz)&#x3D;(v(x)_xv(x)_z^{-1},1)dz<br>$$<br>在半球上均匀采样并使用得到的 z 分量作为常微分方程的起始点，这很容易。但是存在一个小问题。许多常微分方程求解器要求整个批次具有相同的初始值。位于半球上的点具有许多不同的 z 值，这带来了一些小障碍。</p><p>为绕过这个问题，将半球上的均匀分布投影到与半球顶部重合的超平面 zmax 上。由于泊松场在 x 趋向于无穷大时是纯径向的，因此投影是一个简单的径向投影。下面是一个 2D 示意图，其中红色线条连接球面上的紫色点到绿色线上的投影点。</p><p><img src="/2025/20250502/67aa869a0c216dd7c0070adf_radial_proj.png"></p><p>如红线所框住位置，几乎是平行的。</p><p><img src="/2025/20250502/radio.png"></p><p>初始分布可写为：</p><p><img src="/2025/20250502/67aa869a45b5729b89cc0544_image-33.png"></p><p>此处推导（来自苏神）：</p><p><img src="/2025/20250502/td.png"></p><p>我们换成超球坐标即有</p><p><img src="/2025/20250502/67aa869a189eca76c7f4e844_image-34.png"></p><p>即$dV&#x3D;r^{N-1}dr\times d\Omega$</p><h2 id="进一步改进"><a href="#进一步改进" class="headerlink" title="进一步改进"></a>进一步改进</h2><p>一个很自然的想法是从增加一维到任意维度，给模型更多多样性的空间。</p><p>PFGM++就是这样做的，还证明了当D接近无穷的时候则是DDPM。</p><p><img src="/2025/20250502/muid.png"></p><p>我们继续沿用PFGM的算法，$\hat x&#x3D;[x,z]^T$，其中$z\in R^D$，则N+D维空间中的场为：<br>$$<br>E(\hat x)&#x3D;\int \frac{1}{S_{N+D-1}(1)}\frac{\hat x-\hat y}{||\hat x-\hat y||^{N+D}}p(\hat y)d\hat y<br>$$<br>但由于这时候z是多维的，我们要把它转换为一维。</p><p>我们注意到，由于对称性：</p><p><img src="/2025/20250502/sym.png"></p><p>所以我们可以使用离超柱面r的距离r来替代z（类似柱坐标系）。$\sum z_i^2&#x3D;r^2$。</p><p>我们重新记为$\hat x&#x3D;[x,r]^T$，这时候r就是1维的了。</p><p><img src="/2025/20250502/++1.png"></p><p><img src="/2025/20250502/++2.png"></p><p><img src="/2025/20250502/++3.png"></p><p>并且在$r_{max}\to \infty $，</p><p><img src="/2025/20250502/++4.png"></p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>如果我们继续沿用PFGM的，则为：</p><p><img src="/2025/20250502/++5.png"></p><p>作者认为，继续沿用会有较大的开销，且是有偏估计。所以换了一种。</p><p><img src="/2025/20250502/++6.png"></p><p>$p_r(x)$代表一种前向过程，令其为：</p><p><img src="/2025/20250502/++7.png"></p><p><img src="/2025/20250502/++8.png"></p><p><img src="/2025/20250502/++9.png"></p><p>我们可以归一化以保证稳定。</p><p><img src="/2025/20250502/++10.png"></p><p><img src="/2025/20250502/++11.png"></p><h3 id="如何和DDPM等模型联系起来"><a href="#如何和DDPM等模型联系起来" class="headerlink" title="如何和DDPM等模型联系起来"></a>如何和DDPM等模型联系起来</h3><p><img src="/2025/20250502/gauss.png"></p><p>可以看出其余有高斯噪声的形式。</p><h2 id="额外阅读"><a href="#额外阅读" class="headerlink" title="额外阅读"></a>额外阅读</h2><p><a href="https://kexue.fm/archives/9370">https://kexue.fm/archives/9370</a></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models">https://www.assemblyai.com/blog/an-introduction-to-poisson-flow-generative-models</a></p><p><a href="https://kexue.fm/archives/9305">https://kexue.fm/archives/9305</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>物理</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>贝叶斯来加噪：Bayesian Flow Networks</title>
    <link href="/2025/20250501/"/>
    <url>/2025/20250501/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Bayesian Flow Networks是由Alex Graves提出的，使用贝叶斯技巧关于生成的模型算法。</p><p>2023年，我就看到了这篇论文，但由于篇幅过长、数学推导较多难以理解、无闲暇时间，最终拖到两年后的今天，才决定动手写写自己的理解。</p><p>此文写法并不算太好，据闻youtube上也有说此文晦涩的评论。文中Alice&amp;Bob发生和接收的意图从“趣味”入手的案例反而加难了初学者对论文的理解。</p><p>本文将抛开”发生和接收“，更言简意赅地介绍该算法。</p><span id="more"></span><p>作者将数据分为连续数据、离散数据，在连续数据上又进一步分为离散化数据，即是连续但我们把它离散化，比如位图，也有点类似分箱。</p><p>为了简便，我们目前只讨论连续数据。<strong>（也许后续会更新更多。）</strong></p><h2 id="作者背景"><a href="#作者背景" class="headerlink" title="作者背景"></a>作者背景</h2><p>艾力克斯·格雷夫斯（英语：Alex Graves）是一名计算机科学家。在DeepMind担任研究科学家之前，他在爱丁堡大学获得理论物理学学士学位，并在IDSIA的于尔根·施密德胡伯指导下获得了人工智能博士学位。他还曾在慕尼黑工业大学的施密德胡伯和多伦多大学的hinton手下做过博士后。</p><p> 在IDSIA，格雷夫斯通过一种称为CTCloss的新方法训练LSTM。格雷夫斯也是神经图灵机（Neural Turing machine）和密切相关的可微分神经计算机的创造者。</p><p>以上来自维基百科，可以看出作者具有很强的物理数学背景，论文中也涉及了大量数学和贝叶斯。</p><p>尽管这篇论文只发表于arxiv上，但其后续的《Protein sequence modelling with Bayesian flow networks》发表于Nature Communications，ICLR24Oral亦有一篇以Bayesian flow networks为基础，即清华大学提出的《Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks》。</p><h2 id="论文引入"><a href="#论文引入" class="headerlink" title="论文引入"></a>论文引入</h2><p>神经网络在生成模型中可以视为一种大号的函数拟合器。在DDPM中NN充当拟合噪声的角色。</p><p>类似上篇的DDPM，同样涉及加噪、一次性扩散、找到神经网络运用的场景。</p><h2 id="加噪设定"><a href="#加噪设定" class="headerlink" title="加噪设定"></a>加噪设定</h2><p>与DDPM不同，DDPM的加噪是直接加上一个高斯噪声，BFN则不同，它是通过一个改变方差（倒数称为精度）的方式。</p><p>听上去好像很抽象，但是我们注意到DDPM中从x0到xt：<br>$$<br>q(x_t|x_o)&#x3D;N(x_t;\sqrt{\overline a_t }x_o,(1-\overline a_t)I)<br>$$<br>$$<br>x_t&#x3D;\sqrt{\overline a_t}x_0+\sqrt{1-\overline a_t}\varepsilon,\varepsilon \sim N(0,I)<br>$$<br>DDPM中的均值和方差是随着时间发生改变的。</p><p>在BFN中，我们是通过精度来进行加噪，即$\rho$。也就是说DDPM中加噪体现在均值和方差都发生了改变，那么我们干脆直接改变方差和均值以达到抽象而非具体的“加噪”的效果。</p><p>当然我们也不是乱改变的，我们要保持从高斯噪声到数据分布的过程。有什么算法是类似这样的过程呢？贝叶斯推断！<strong>当我们假定先验分布是高斯噪声，那么观测值只要足够多，我们是能得到接近实际数据分布的后验分布的。中间的分布也蕴含了信息，那么我们就把它作为加噪了的数据分布。</strong></p><p>天才想法！</p><p>我们令时间$t\in [0,1]$，让$\alpha(t)&gt;0$，令accuracy schedule为$\beta(t)&#x3D;\int_{t’&#x3D;0}^t\alpha(t’)dt’$。则$\beta(0)&#x3D;0,\frac{d\beta(t)}{dt}&#x3D;\alpha(t)$。<br>$$<br>\rho_t&#x3D;\rho_0+\int_{t’&#x3D;0}^t\alpha(t’)dt’&#x3D;1+\beta(t) \tag{1}<br>$$</p><h2 id="一次性扩散"><a href="#一次性扩散" class="headerlink" title="一次性扩散"></a>一次性扩散</h2><p>一次性扩散的意思就是给定时间t，我们能求出初始和时间t的加噪数据关系，而不用一步一步去加噪。</p><p>DDPM蕴含了无限次加高斯噪声则最后会变成高斯噪声的原理，由高斯分布的性质可知，这蕴含了数据分布也是一种高斯分布的意思。</p><p>BFN也是采用了这一思想，它从某个初始的高斯分布出发，从数据样本中抽取，不断使用贝叶斯更新以获得t时间的分布。</p><h3 id="初始"><a href="#初始" class="headerlink" title="初始"></a>初始</h3><p>BFN假定先验分布（或初始分布）为最简单的高斯分布。<br>$$<br>\theta &#x3D;{\mu,\rho}&#x3D;{0,1}\tag{2}<br>$$<br>$$<br>p(x|\theta)&#x3D;N(x|\mu,\rho^{-1}I)&#x3D;N(x|0,I)<br>$$</p><h3 id="扩散"><a href="#扩散" class="headerlink" title="扩散"></a>扩散</h3><p>我们可以复习一些贝叶斯推断的知识。</p><p>假定高斯先验，观测值x来自似然，则后验也为高斯分布。（称为共轭性）</p><p>即假定高斯分布$N(\mu_a,\rho_a^{-1})$，似然y来自$N(x,\alpha^{-1})$，则后验分布$N(\mu_b,\rho_a^{-1})$满足：<br>$$<br>\rho_b&#x3D;\rho_a+\alpha<br>$$<br>$$<br>\mu_b&#x3D;\frac{\mu_a\rho_a+y\alpha}{\rho_a+\alpha}&#x3D;\frac{\mu_a\rho_a+y\alpha}{\rho_b}<br>$$<br>而<br>$$<br>\mu_b&#x3D;\frac{\mu_a\rho_a+y\alpha}{\rho_b}&#x3D;\frac{\alpha}{\rho_b} N(\mu_a,\rho_a^{-1})+\frac{\mu_a\rho_a}{\rho_b}\sim N(\frac{\alpha x+\mu_a\rho_a}{\rho_b},\frac{\alpha}{\rho_b^2} I)<br>\tag{4}<br>$$<br>我们可以扩展到更多步。</p><p>即<br>$$<br>\mu_{i}\sim N(\frac{(\alpha_a+\alpha_b) x+\mu_{i-2}\rho_{i-2}}{\rho_i},\frac{(\alpha_a+\alpha_b)}{\rho_i^2} I) \tag{5}<br>$$</p><p>我们可以类似地把两次加和扩展到n次加和。这在文中也被称为Additive Accuracies。</p><h3 id="开始加噪"><a href="#开始加噪" class="headerlink" title="开始加噪"></a>开始加噪</h3><p>作者说，输入分布的期望熵要线性地随时间减小，这代表数据信息要<strong>以恒定速率注入</strong>到输入分布中。</p><p>我们知道期望熵即多维正态分布的联合熵$H(t)&#x3D;\frac{1}{2}ln((2\pi e)^D|\Sigma|)$。</p><p>而$|\Sigma|&#x3D;(\sigma_t^2)^D&#x3D;(\frac{1}{1+\beta(t)})^D$，故$H(t)&#x3D;\frac{D}{2}ln(\frac{2\pi e}{1+\beta(t)})$</p><p>“<strong>恒定速率注入</strong>”则通过H(t)是t的单调递减函数来实现，即</p><p>$$<br>\begin{align}<br>H(t)&amp;&#x3D;(1-t)H(0)+tH(1)\\<br>ln(\frac{1}{1+\beta(t)})&amp;&#x3D;t\cdot ln(\frac{1}{1+\beta(1)})\\<br>\frac{1}{1+\beta(t)}&amp;&#x3D;(\frac{1}{1+\beta(1)})^t\\<br>\frac{1}{1+\beta(t)}&amp;&#x3D;(\sigma_1^2)^t\\<br>则\ \beta(t)&amp;&#x3D;\sigma_1^{-2t}-1\\<br>\alpha(t)&amp;&#x3D;\frac{-2ln(\sigma_1)}{\sigma_1^{2t}} \tag{6}<br>\end{align}<br>$$</p><p>其中$\sigma_1$是我们要主动去设定的一个超参。</p><h3 id="扩散结果"><a href="#扩散结果" class="headerlink" title="扩散结果"></a>扩散结果</h3><p>我们将上述结果应用于（1）（5)式则有，并注意到（2）式的$\mu&#x3D;0$则有<br>$$<br>\begin{align}<br>\mu&amp;\sim N(\frac{\beta(t)x}{1+\beta(t)},\frac{\beta(t)}{(1+\beta(t))^2}I)\\<br>&amp;&#x3D;N(\gamma(t)x,\gamma(t)(1-\gamma(t))I)<br>\end{align}<br>$$<br>其中<br>$$<br>\gamma(t)&#x3D;\frac{\beta(t)}{1+\beta(t)}&#x3D;1-\sigma_1^{2t}<br>$$<br>同样使用运用于DDPM的重参数化则有，<br>$$<br>\begin{align}<br>\mu\sim N(\gamma(t)x,\gamma(t)(1-\gamma(t))I)\\<br>\mu&#x3D; \gamma(t)x+\sqrt{\gamma(t)(1-\gamma(t))}\varepsilon,\varepsilon\sim N(0,I)\\<br>x&#x3D;\frac{\mu}{\gamma(t)}-\sqrt{\frac{1-\gamma(t)}{\gamma(t)}}\varepsilon<br>\end{align}<br>$$<br>类似DDPM的，我们同样使用神经网络去预测这个噪声$\varepsilon$，最终预测对x的预测为$\hat x$。</p><p>以上部分的伪代码是</p><p><img src="/2025/20250501/pcode1.png"></p><p>类似地，采样代码为</p><p><img src="/2025/20250501/pcode4.png"></p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>至此，连续数据的所有环节已经介绍完了，还剩下损失函数没有介绍。</p><p>可能有人觉得，不如我们直接（像DDPM一样）对x使用MSE来作为损失函数。</p><p>但笔者以生成mnist的例子做了尝试，至少给MSE加一个系数（所有epochs都一样)的效果其实是不好的。</p><p>论文中提到的损失函数是必要的，其实也不复杂，本质上是在MSE前面加一个动态系数。但系数是有一系列数学公式来推导的，类似target encoding中的处理，也不是随心所欲去定义一个参数。</p><p>在此之前，我们先回忆一下两个多维正态分布之间的 KL 散度，</p><blockquote><p>若$p\sim N(\mu_1,\Sigma_1)$，$q\sim N(\mu_2,\Sigma_2)$，则$D_{KL}(p||q)&#x3D;\frac{(\mu_1-\mu_2)^T\Sigma^{-1}(\mu_1-\mu_2)}{2}$</p></blockquote><h3 id="离散时间损失"><a href="#离散时间损失" class="headerlink" title="离散时间损失"></a>离散时间损失</h3><p>我们通过KL散度来定义。损失函数$L^n(x)$为：<br>$$<br>\begin{align}<br>D_{KL}(N(x,\alpha_i^{-1}I)||N(\hat x,\alpha_i^{-1}I))&amp;&#x3D;\frac{\alpha_i}{2}||x-\hat x||^2\\<br>&amp;&#x3D;\frac{\beta(t_i)-\beta(t_{i-1})}{2}||x-\hat x||^2\\<br>&amp;&#x3D;\frac{\sigma_1^{-2i&#x2F;n}(1-\sigma_1^{2&#x2F;n})}{2}||x-\hat x||^2<br>\end{align}<br>$$<br>伪代码如下，可以发现其实前面还有个n，这是为什么呢？作者是从整体定义损失函数的，也就是n步，但是我们实际上（或者DDPM实际上）只对一步进行拟合，那么我们要获得完整的损失函数，我们需要乘上一个n去近似它。</p><p><img src="/2025/20250501/pcode2.png"></p><h3 id="连续时间损失"><a href="#连续时间损失" class="headerlink" title="连续时间损失"></a>连续时间损失</h3><p>$$<br>\begin{align}<br>L^\infty(x)&#x3D;\frac{\alpha_i}{2}||x-\hat x||^2<br>\mathop{&#x3D;}^{由(6)}&#x3D;\frac{-ln(\sigma_1)}{\sigma_1^{2t}}||x-\hat x||^2<br>\end{align}<br>$$</p><p><img src="/2025/20250501/pcode3.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>BFN的实验效果实际上也比DDPM更好，论文中的实验跳过了DDPM，直接和其他更高级的DDPM++等作比较，都取得了更好的结果。</p><p>尽管论文使用了更复杂的一些数学公式，一些信息论的知识，贝叶斯的知识啦，但个人认为还是没有脱离DDPM的框架。</p><p>其中的创新性还是值得我们思考。比如抽象地去定义噪声，比如如何去使用贝叶斯推断等。</p><p>至于论文号称（或他人声称）的“从分布建模而不是从数据建模”、“这体现了生成即智能”，我觉得更多只是华丽的叙事。</p><h2 id="扩展阅读"><a href="#扩展阅读" class="headerlink" title="扩展阅读"></a>扩展阅读</h2><p>如果想要更详近的诠释（比如离散化和离散数据的处理），可以阅读知乎上的一系列文章，写得还是比较详细，且遵循论文的结构来叙述。</p><p><a href="https://zhuanlan.zhihu.com/p/659111986">Bayesian Flow Networks (一)：生成即压缩！结合贝叶斯统计与深度学习的生成模型 —— 贝叶斯流网络 - CW不要無聊的風格的文章 - 知乎</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>信息论</tag>
      
      <tag>贝叶斯</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Diffusion Model</title>
    <link href="/2025/20250427/"/>
    <url>/2025/20250427/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Difussion模型是当今生成领域火热的模型，过往的GAN和VAE已然退出前排。</p><p>本文主要围绕<a href="https://arxiv.org/abs/2006.11239">《Denoising Diffusion Probabilistic Models》</a> (DDPM) 展开。</p><span id="more"></span><h2 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h2><p>Diffusion模型本质上是加噪和去噪的过程，通过加噪将一张图片转为纯噪声，通过去噪将一张噪声转为有价值的图片。</p><p>这种原理其实很显然，比如像GAN和VAE，从特定分布中抽取并生成，image &#x3D; f(noise)。Diffusion也是这样，只要我们能通过一个g(image)&#x3D;noise，那么我们就能从g(image)&#x3D;noise中得到image &#x3D; f(noise)。但由于神经网络的复杂性（一个黑盒子很难找到它的反函数），所以Diffusion做的处理是，加噪使用简单的操作，去噪再使用神经网络，而不是两者都用神经网络。（这样做也更简单。）</p><p><img src="/2025/20250427/diffusion1.png"></p><p><img src="/2025/20250427/diffusion2.png"></p><p>另一方面，由于直接预测会很难，所以内部实际上是预测残差。</p><p>这有点类似GBDT，不断地去拟合残差，且不是一次拟合完。多次逐一地去拟合。</p><p><img src="/2025/20250427/diffusion3.png"></p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/2025/20250427/diffusion4.png"></p><p>对于训练的第五行：</p><p><img src="/2025/20250427/diffusion5.png"></p><p>画红框的地方是这样来的：</p><p><img src="/2025/20250427/diffusion6.png"></p><p><img src="/2025/20250427/diffusion7.png"></p><p><img src="/2025/20250427/diffusion8.png"></p><p>对于sampling的第四行：</p><p><img src="/2025/20250427/diffusion9.png"><br><img src="/2025/20250427/diffusion10.png"><br><img src="/2025/20250427/diffusion11.png"><br><img src="/2025/20250427/diffusion12.png"><br>即<br><img src="/2025/20250427/diffusion13.png"><br>对于KL散度，有以下公式。<br><img src="/2025/20250427/diffusion14.png"><br>但是实际上方差我们假定是固定的，所以只需要均值越靠近越好。也就是相当于使用去噪神经网络直接预测左下角那一坨。<br><img src="/2025/20250427/diffusion15.png"><br><img src="/2025/20250427/diffusion16.png"><br><img src="/2025/20250427/diffusion17.png"><br>另外，注意到第四行最右边还有一项加噪声，这是为了多样性。正如李宏毅课上所说的，实际上的模型也并不是每次取最优解。<br><img src="/2025/20250427/diffusion18.png"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1.<a href="https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php">李宏毅的机器学习课程</a></p><p>2.<a href="https://kexue.fm/archives/9152">苏神的生成扩散模型漫谈一系列文章</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>生成模型</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>消息传递与特征变换分离的图神经网络</title>
    <link href="/2025/20250426/"/>
    <url>/2025/20250426/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在研究图神经网络的鲁棒性的时候，发现消息传递和特征变化分离开会更有鲁棒性，但未找到是否有前人做过。后来我才发现这正是PPNP的结构。</p><h2 id="PPNP"><a href="#PPNP" class="headerlink" title="PPNP"></a>PPNP</h2><p>PPNP（personalized propagation of neural predictions）出自ICLR2019的《 Predict then Propagate: Graph Neural Networks meet Personalized PageRank》，简单来说就是先使用神经网络进行特征变换，最后使用pagerank来消息传播。</p><span id="more"></span><p><img src="/2025/20250426/ppnp.png"></p><p><img src="/2025/20250426/ppnp1.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">PPNPNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, hidden=<span class="hljs-number">64</span>, K=<span class="hljs-number">10</span>, alpha=<span class="hljs-number">0.1</span></span>):<br>        <span class="hljs-built_in">super</span>(PPNPNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.lin1 = torch.nn.Linear(in_channels, hidden)<br>        <span class="hljs-variable language_">self</span>.lin2 = torch.nn.Linear(hidden, out_channels)<br>        <span class="hljs-variable language_">self</span>.K = K<br>        <span class="hljs-variable language_">self</span>.alpha = alpha<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, edge_index</span>):<br>        <span class="hljs-comment"># MLP head</span><br>        x = F.relu(<span class="hljs-variable language_">self</span>.lin1(x))<br>        x = F.dropout(x, p=<span class="hljs-number">0.5</span>, training=<span class="hljs-variable language_">self</span>.training)<br>        z = <span class="hljs-variable language_">self</span>.lin2(x)  <span class="hljs-comment"># initial predictions</span><br><br>        <span class="hljs-comment"># Build normalized adjacency with self-loops</span><br>        edge_index, _ = add_self_loops(edge_index, num_nodes=z.size(<span class="hljs-number">0</span>))<br>        row, col = edge_index<br>        deg = degree(col, z.size(<span class="hljs-number">0</span>), dtype=z.dtype)<br>        deg_inv_sqrt = deg.<span class="hljs-built_in">pow</span>(-<span class="hljs-number">0.5</span>)<br>        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]<br>        adj = torch.sparse.FloatTensor(<br>            torch.vstack([row, col]),<br>            norm,<br>            torch.Size([z.size(<span class="hljs-number">0</span>), z.size(<span class="hljs-number">0</span>)])<br>        )<br><br>        <span class="hljs-comment"># Personalized PageRank propagation</span><br>        h = z<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.K):<br>            h = <span class="hljs-variable language_">self</span>.alpha * z + (<span class="hljs-number">1</span> - <span class="hljs-variable language_">self</span>.alpha) * torch.sparse.mm(adj, h)<br>        <span class="hljs-keyword">return</span> h<br></code></pre></td></tr></table></figure><p>由于有求逆步骤，会导致开销较大，又有以下近似版本。</p><p><img src="/2025/20250426/appnp.png"></p><h2 id="PMLP"><a href="#PMLP" class="headerlink" title="PMLP"></a>PMLP</h2><p>我的想法实际上是连训练的过程中都不用邻接矩阵。</p><p>这实际上与ICLR2023的《GRAPH NEURAL NETWORKS ARE INHERENTLY GOOD GENERALIZERS: INSIGHTS BY BRIDGING GNNS AND MLPS》不谋而合。</p><p>论文中主要是从NTK的角度研究问题，模型创新性较低。</p><blockquote><p>审稿人所说：<strong>Q1: “PMLP 可能不是一个新颖的方法，它可能是 APPNP 从转导到归纳设置的扩展版本。”</strong></p><p>作者回答：确实，从模型架构的角度来看，APPNP 论文[1]中提出的用于消融研究的变体可以大致看作是 PMLP 的特殊情况（即具有 APPNP 式架构、个性化 PageRank MP 方案和残差连接）。然而，这并不削弱我们的主要贡献，因为之前尚未确定本文的核心方面，即经验发现以及理论理解。</p><p>[1]的主要贡献在于提出了 APPNP 作为新的特定 GNN 模型，并展示了其在先前 GNN 模型上的经验优势。相比之下，我们引入了 PMLP 作为一类新的模型架构，适用于大量 GNN，更重要的是，PMLP 用于分析目的，基于此，我们识别出一种普遍现象，该现象在 MP 实例化、GNN 架构、超参数等方面是一致的。除了这一新的经验发现之外，我们还利用 PMLP 作为显微镜，揭示 GNN 在节点级任务中成功的主要原因是它们在推理中使用的 MP 操作带来的固有泛化能力，这也得到了我们理论分析的证实。</p><p>关于归纳设置，请特别注意，我们并非刻意与专注于归纳设置的先前工作不同，而是为了实现公平比较（即通过使用归纳设置确保 MLP、GNN 和 PMLP 能够访问相同的训练数据信息），并使我们的结果合理且有意义。为了帮助在演示中更清晰地展示我们的整体框架，我们在第 1.1 节中增加了更多讨论，以更好地定位这项工作与先前技术的关系。</p></blockquote><p>论文也只理论研究了说PMLP为什么会比MLP更好，而没有涉及到GNN。更多只是实验。</p><p>对于MLP：</p><p><img src="/2025/20250426/mlp.png"></p><p>对于PMLP：</p><p><img src="/2025/20250426/pmlp.png"></p><p>两者对比，尽管在外推情况下二者都会变为线性模型，但是可以发现PMLP的参数更多，也许会学到一个更好更平滑的表示。</p><blockquote><p>正如审稿人所说：<strong>Q2: “定理 4 和 5 似乎适用于 PMLP 和原始 GNN。因此，仍然存在一个差距需要解释为什么 PMLPs 能够实现接近 GNN 的性能。”</strong></p><p>作者回答：确实，定理 4 和 5 的目的是为了揭示“图神经网络天生具有泛化能力”的理论洞察，并解释我们关于 GNNs&#x2F;PMLPs 具有优越泛化能力的实证发现，这种泛化能力源于用于推理的 GNN 架构本身，而不是严格回答“为什么用 MLP 权重替换 GNN 权重不会损害准确性”。PMLPs 和 GNNs 之间接近的性能实际上作为实证证据，使得我们的理论结果更加有力：如果权重对泛化很重要，那么在分析中“不考虑权重看起来如何”时，我们的理论结果将不那么有说服力。<br>尽管如此，我们确实认为理解 PMLPs 和 GNNs 之间紧密性能背后的理论基础很重要。作为第一个揭示这一现象的工作，我们也探讨了 PMLPs 在哪些情况下优于或劣于 GNNs，并进行了相应的讨论（第 3.2 节和第 5 节），这涉及到噪声边、自连接、模型表达性和未标记节点的作用。将这些因素全部纳入分析可能足以满足 GNN 理论中的另一项工作。因此，我们将这个问题留给未来的研究。我们对第 4 节开头和第 5 节的“当前局限性和展望”进行了修改，以使这一点更加清晰。</p></blockquote><p>看上去算法并不创新，理论并不完善，还能得到审稿人多个8分，这个还是让我挺震惊的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Target Encoding</title>
    <link href="/2025/20250425/"/>
    <url>/2025/20250425/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Target Encoding又名mean encoding。正如其名，实际上是把符合条件的y的均值作为这个条件的所有样本的新特征。</p><p>在kaggle的S5E2和这个月正在进行的S5E4等比赛都获得了耀眼的表现。</p><p>最简单的形式是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df.groupby(<span class="hljs-string">&quot;xxx&quot;</span>).mean()<br></code></pre></td></tr></table></figure><span id="more"></span><p>为了避免数据泄露，Target Encoding往往采用的是$(1-\alpha)y_{类内}+\alpha y_{all}$。我们当然可以手动指定$\alpha$，但更好的选择是我们通过一系列数学方法来确定，这也就引出了各种的target encoder。</p><p>python上有多个库提供了“target encoder”，<strong>但它们实际上并不是完全同一个东西。</strong></p><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.TargetEncoder.html">sklearn</a>中的相当于<a href="https://contrib.scikit-learn.org/category_encoders/">category_encoders</a>中的JamesSteinEncoder，<a href="https://docs.rapids.ai/api/cuml/stable/api/#cuml.preprocessing.TargetEncoder.TargetEncoder">cuml</a>中的相当于<a href="https://contrib.scikit-learn.org/category_encoders/">category_encoders</a>中的M-estimate Encoder。JamesSteinEncoder事实是可以当做一种特殊的M-estimate Encoder，所以我们先从JamesSteinEncoder介绍起。</p><h2 id="JamesSteinEncoder"><a href="#JamesSteinEncoder" class="headerlink" title="JamesSteinEncoder"></a>JamesSteinEncoder</h2><p>$$<br>\alpha&#x3D;\frac{Var(y_i)}{Var(y_i)+Var(y)}&#x3D;\frac{\sigma^2&#x2F;n_i}{\sigma^2&#x2F;n_i+\tau^2}<br>$$</p><p><a href="https://chris-said.io/2017/05/03/empirical-bayes-for-multiple-sample-sizes/">Chris Said</a>认为，如果你熟悉James-Stein 估计量的话，可以得到（注意我们要减去一个自由度）：<br>$$<br>\begin{align}<br>\alpha&amp;&#x3D;\frac{(k-3)\hat\sigma^2}{\sum(xi-\bar X)^2}<br>\\<br>&amp;\approx \frac{(k-1)\hat\sigma^2}{\sum(xi-\bar X)^2}<br>\\<br>&amp;&#x3D; \frac{\hat\sigma^2}{\sum(xi-\bar X)^2&#x2F;(k-1)}<br>\\<br>&amp;&#x3D;\frac{Var(y_i)}{Var(y_i)+Var(y)}<br>\end{align}<br>$$<br>（唉，怎么k-3&#x3D;k-1了）<del>我们实际上是天文学家，它们都是同一数量级的。</del></p><p>它也可以从贝叶斯中得出：</p><p>比如假定高斯分布$N(\mu,\tau^2)$，似然y来自$N(xi,\sigma_i^2&#x2F;n_i)$，则后验分布$N(\mu_b,)$满足：<br>$$<br>\mu_b&#x3D;\frac{\frac{x_in_i}{\sigma_i^2}+\frac{\mu}{\tau^2}}{\frac{1}{\tau^2}+\frac{n_i}{\sigma_i^2}}<br>$$<br>我们化简即可得到。</p><p>我们也可以假定先验为$N(0,\tau^2)$，则后验分布为$N(\frac{\tau^2}{\sigma^2&#x2F;n+\tau^2},)$</p><p>故贝叶斯估计量为$\frac{\tau^2}{\sigma^2&#x2F;n+\tau^2} \bar X$，则有$\frac{1}{||\mathbf{X}||_2^2} \sim \frac{1}{\tau^2 + \sigma^2} \cdot \text{inverse-}\chi^2_p$。<br>$$<br>\begin{align} \mathbb{E}[\hat{\alpha}^{(JS)}] &amp;&#x3D; \mathbb{E}\left[\left( 1 - \frac{(p - 2) \sigma^2}{||\mathbf{X}||_2^2} \right)\right] \\ &amp;&#x3D; 1 - \mathbb{E}\left[\left(\frac{(p - 2) \sigma^2}{||\mathbf{X}||_2^2} \right)\right] \\ &amp;&#x3D; 1 - (p - 2) \sigma^2 \mathbb{E}\left[\frac{1}{||\mathbf{X}||_2^2} \right] \\ &amp;&#x3D; 1 - (p - 2) \sigma^2 \frac{1}{(\tau^2 + \sigma^2) (p - 2)} \\ &amp;&#x3D; 1 -  \frac{\sigma^2}{\tau^2 + \sigma^2} \\ &amp;&#x3D; \frac{\tau^2}{\tau^2 + \sigma^2} \ \end{align}<br>$$<br>然而对于$\alpha$，一个一个计算类内方差可能会有较大的复杂度，我们可以假定全部方差都相同，比如<a href="https://www.kaggle.com/code/mmotoki/hierarchical-bayesian-target-encoding">kaggle-ASHRAE - Great Energy Predictor III比赛第一名所用的那样</a>。我们也假设各类符合相同分布，这也被称为<a href="https://chris-said.io/2017/05/03/empirical-bayes-for-multiple-sample-sizes/">pooled</a>。</p><h2 id="M-estimate-Encoder"><a href="#M-estimate-Encoder" class="headerlink" title="M-estimate Encoder"></a>M-estimate Encoder</h2><p>我们知道JamesSteinEncoder中$\alpha&#x3D;\frac{\sigma^2&#x2F;n_i}{\sigma^2&#x2F;n_i+\tau^2}&#x3D;\frac{\sigma^2&#x2F;\tau^2}{\sigma^2&#x2F;\tau^2+n_i}$，那么我们干脆令$\sigma^2&#x2F;\tau^2$为自定义参数m，这也就是M-estimate Encoder。</p><h2 id="避免过拟合"><a href="#避免过拟合" class="headerlink" title="避免过拟合"></a>避免过拟合</h2><p>显然地，target encoding直接利用了目标列的信息，这难免会过拟合，有以下几种方法可以避免过拟合，交叉验证、加噪声。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料:"></a>参考资料:</h2><p><a href="https://www.econometrics.blog/post/not-quite-the-james-stein-estimator/">https://www.econometrics.blog/post/not-quite-the-james-stein-estimator/</a></p><p><a href="https://chris-said.io/2017/05/03/empirical-bayes-for-multiple-sample-sizes/">https://chris-said.io/2017/05/03/empirical-bayes-for-multiple-sample-sizes/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ICLR25 Oral 若干（感兴趣）论文解析</title>
    <link href="/2025/20250424/"/>
    <url>/2025/20250424/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ICLR25这几天在新加坡进行，借着兴致，选择了若干篇感兴趣的文章进行分享。本着能复现的原则，会尽量选择有代码或者实现容易的论文。</p><h2 id="Joint-Graph-Rewiring-and-Feature-Denoising-via-Spectral-Resonance"><a href="#Joint-Graph-Rewiring-and-Feature-Denoising-via-Spectral-Resonance" class="headerlink" title="Joint Graph Rewiring and Feature Denoising via Spectral Resonance"></a>Joint Graph Rewiring and Feature Denoising via Spectral Resonance</h2><p><strong>论文地址:</strong> <a href="https://openreview.net/forum?id=zBbZ2vdLzH">https://openreview.net/forum?id=zBbZ2vdLzH</a></p><p><strong>代码:</strong> <a href="https://github.com/jlinki/JDR">https://github.com/jlinki/JDR</a></p><p><strong>摘要:</strong> 在从图数据中学习时，图和节点特征都会为节点标签提供噪声信息。在本文中，我们提出了一种算法来联合去噪特征和重连图（JDR），这提高了下游节点分类图神经网络（GNN）的性能。JDR 通过对齐图和特征矩阵的主谱空间来工作。它以处理具有多个类别和不同同质或异质程度图的方式，近似解决了相关的非凸优化问题。我们在一个风格化的环境中对 JDR 进行了理论证明，并表明它在广泛的合成和真实世界节点分类任务上始终优于现有的重连方法。</p><p>该文中的算法尽管也是重构边和特征，但是与图结构学习不同，它是应对内部自身携带的噪声，而非外部攻击产生的扰动。</p><p>算法并不难，但是为何可以这样做的篇幅较大。</p><p><img src="/2025/20250424/jdr.jpg"></p><p>论文先从一种图生成模型（cSBM)入手，人工生成的图会更好分析。</p><p><img src="/2025/20250424/jdrdef1.jpg"></p><p>定义邻接矩阵为$A^c&#x3D;\frac{\lambda}{N}yy^T+\frac{1}{\sqrt{N}}O_A$，特征为$X &#x3D; \sqrt{\frac{\mu}{N}} y \zeta^T+\frac{1}{F}O_X$</p><span id="more"></span><h2 id="Standard-Gaussian-Process-is-All-You-Need-for-High-Dimensional-Bayesian-Optimization"><a href="#Standard-Gaussian-Process-is-All-You-Need-for-High-Dimensional-Bayesian-Optimization" class="headerlink" title="Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization"></a>Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization</h2><p><strong>论文地址:</strong> <a href="https://openreview.net/forum?id=1CLzLXSFNn">https://openreview.net/forum?id=1CLzLXSFNn</a></p><p><strong>代码:</strong>  <a href="https://github.com/XZT008/Standard-GP-is-all-you-need-for-HDBO">https://github.com/XZT008/Standard-GP-is-all-you-need-for-HDBO</a></p><p><strong>摘要:</strong> 长期以来，人们普遍认为，标准高斯过程（GP）的贝叶斯优化（BO）——称为标准 BO——在高维优化问题中表现不佳。虽然这种看法似乎合理，但它既缺乏稳健的实证证据，也缺乏理论上的证明。为了填补这一空白，我们进行了一项系统性的研究。首先，通过在十二个基准上的全面评估，我们发现，虽然流行的平方指数（SE）核往往导致性能不佳，但使用 Matern 核可以使标准 BO 持续取得顶级结果，经常超越专门为高维优化设计的方法。其次，我们的理论分析表明，SE 核的失败主要源于长度尺度参数的不当初始化，这些参数在实践中常用，但可能导致训练中的梯度消失。我们提供了一个概率界限来描述这个问题，表明 Matern 核对这一问题不太敏感，并且可以稳健地处理更高的维度。 第三，我们提出了一种简单且鲁棒的初始化策略，该策略显著提高了 SE 核的性能，使其接近最先进的方法，而无需额外的先验或正则化。我们证明了另一个概率界限，展示了如何使用我们的方法有效地缓解梯度消失问题。我们的发现主张重新评估标准 BO 在高维设置中的潜力。</p><h2 id="TimeMixer-A-General-Time-Series-Pattern-Machine-for-Universal-Predictive-Analysis"><a href="#TimeMixer-A-General-Time-Series-Pattern-Machine-for-Universal-Predictive-Analysis" class="headerlink" title="TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis"></a>TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis</h2><p><strong>论文地址:</strong> <a href="https://openreview.net/forum?id=1CLzLXSFNn">https://openreview.net/forum?id=1CLzLXSFNn</a></p><p><strong>代码:</strong> 无</p><p><strong>摘要:</strong> 时间序列分析在众多应用中扮演着关键角色，支持预测、分类、异常检测和插补等任务。在这项工作中，我们提出了时间序列模式机（TSPM），这是一种通过强大的表示和模式提取能力在广泛的时间序列任务中表现出色的模型。传统的时序模型往往难以捕捉普遍模式，限制了它们在多样化任务中的有效性。为了解决这个问题，我们在时间域中定义了多个尺度，在频域中定义了各种分辨率，采用各种混合策略来提取复杂、任务自适应的时间序列模式。具体来说，我们引入了 TimeMixer++，这是一种通用的 TSPM，使用（1）多分辨率时间成像（MRTI）、（2）时间图像分解（TID）、（3）多尺度混合（MCM）和（4）多分辨率混合（MRM）来提取全面的时序模式。MRTI 将多尺度时间序列转换为多分辨率时间图像，捕捉时间和频率域中的模式。 TID 利用双轴注意力提取季节性和趋势模式，而 MCM 按层次将这些模式在各个尺度上聚合。MRM 自适应地整合所有分辨率下的表示。TimeMixer++在 8 个时间序列分析任务中实现了最先进的性能，持续超越通用和特定任务的模型。我们的工作标志着向下一代 TSPM 迈出的一个有希望的步伐，为时间序列分析领域的进一步发展铺平了道路。</p><p><img src="/2025/20250424/framework.png"></p><h2 id="DIFFERENTIAL-TRANSFORMER"><a href="#DIFFERENTIAL-TRANSFORMER" class="headerlink" title="DIFFERENTIAL TRANSFORMER"></a>DIFFERENTIAL TRANSFORMER</h2><p><strong>论文地址:</strong> <a href="https://openreview.net/forum?id=GMwRl2e9Y1">https://openreview.net/forum?id=GMwRl2e9Y1</a></p><p><strong>代码:</strong> <a href="https://github.com/nanowell/Differential-Transformer-PyTorch/blob/main/DiffTransformer.py">https://github.com/nanowell/Differential-Transformer-PyTorch/blob/main/DiffTransformer.py</a> </p><p>官方: <a href="https://github.com/microsoft/unilm/tree/master/Diff-Transformer">https://github.com/microsoft/unilm/tree/master/Diff-Transformer</a></p><p><strong>摘要:</strong> Transformer 倾向于过度分配注意力到无关的上下文。在本工作中，我们引入了 DTransformer，它放大了对相关上下文的注意力，同时消除噪声。具体来说，差分注意力机制通过计算两个独立的 softmax 注意力图之间的差异来计算注意力分数。减法消除了噪声，促进了稀疏注意力模式的出现。在语言模型上的实验结果表明，DTransformer 在各种扩大模型规模和训练标记的设置中优于 Transformer。更有趣的是，它在实际应用中提供了显著的优势，例如长上下文建模、关键信息检索、幻觉缓解、上下文学习和激活异常的减少。通过减少对无关上下文的干扰，DTransformer 可以缓解问答和文本摘要中的幻觉。对于上下文学习，DTransformer 不仅提高了准确性，而且对顺序排列的鲁棒性更强，这被认为是一个长期存在的鲁棒性问题。这些结果将 DTransformer 定位为一种高度有效且具有前景的架构，以推进大型语言模型的发展。</p><p><strong>论文介绍:</strong> 本质上是在transformer的基础上改为SwiGLU+differential attention，比较简单，论文主要在实验上。论文附录还证明了梯度与传统的transformer类似，所以也说明了至少能保证稳定性。</p><p><img src="/2025/20250424/datt1.jpg"></p><p><img src="/2025/20250424/datt.jpg"></p><p><img src="/2025/20250424/code_highlight.png"></p><h2 id="Restructuring-Vector-Quantization-with-the-Rotation-Trick"><a href="#Restructuring-Vector-Quantization-with-the-Rotation-Trick" class="headerlink" title="Restructuring Vector Quantization with the Rotation Trick"></a>Restructuring Vector Quantization with the Rotation Trick</h2><p><strong>论文地址:</strong> <a href="https://openreview.net/forum?id=GMwRl2e9Y1">https://openreview.net/forum?id=GMwRl2e9Y1</a></p><p><strong>代码:</strong> 空</p><p><strong>摘要:</strong> 向量量化变分自编码器（VQ-VAEs）旨在将连续输入压缩到离散潜在空间，并以最小失真重建。它们通过维护一组向量（通常称为码本）并量化每个编码器输出到码本中最近的向量来操作。然而，由于向量量化不可微分，梯度流向编码器是通过向量量化层“绕过”而不是“穿过”的直通近似。这种近似可能是不理想的，因为向量量化操作的所有信息都丢失了。在这项工作中，我们提出了一种通过 VQ-VAEs 的向量量化层传播梯度的方法。我们通过旋转和缩放线性变换将每个编码器输出平滑地转换为相应的码本向量，该变换在反向传播期间被视为常数。 因此，编码器输出与码本向量之间的相对幅度和角度被编码到梯度中，随着它通过向量量化层传播回编码器。在 11 种不同的 VQ-VAE 训练范式下，我们发现这种重构提高了重建指标、码本利用率和量化误差。</p><h2 id="LLM-SR-Scientific-Equation-Discovery-and-Symbolic-Regression-via-Programming-with-LLMs"><a href="#LLM-SR-Scientific-Equation-Discovery-and-Symbolic-Regression-via-Programming-with-LLMs" class="headerlink" title="LLM-SR: Scientific Equation Discovery and Symbolic Regression via Programming with LLMs"></a>LLM-SR: Scientific Equation Discovery and Symbolic Regression via Programming with LLMs</h2><p><strong>论文地址:</strong> <a href="https://arxiv.org/abs/2404.18400">https://arxiv.org/abs/2404.18400</a></p><p><strong>代码:</strong> <a href="https://github.com/deep-symbolic-mathematics/LLM-SR/">https://github.com/deep-symbolic-mathematics/LLM-SR/</a></p><p><strong>摘要:</strong> 在本文中，我们介绍了 LLM-SR，这是一种利用大型语言模型（LLMs）优势进行科学方程发现和符号回归的新方法。LLM-SR 结合了 LLMs 的科学技术知识和代码生成能力，以及进化搜索，从数据中发现准确且可解释的方程。该方法将方程表示为程序框架，允许在特定领域先验的指导下进行灵活的假设生成。在物理学、生物学和材料科学等领域的自定义基准问题上的实验表明，LLM-SR 的性能优于最先进的符号回归方法，尤其是在领域外泛化方面。论文还突出了常见基准的局限性，并提出了新的、具有挑战性的数据集，用于评估基于 LLM 的方程发现方法。</p><p><img src="/2025/20250424/llm_sr1.jpg"></p><p><img src="/2025/20250424/llm_sr2.png"></p><h3 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1. 问题定义"></a>1. 问题定义</h3><p>符号回归（Symbolic Regression, SR）的目标是从观测数据集 D&#x3D;{(xi,yi)}i&#x3D;1nD &#x3D; {(x_i, y_i)}_{i&#x3D;1}^nD&#x3D;{(xi,yi)}i&#x3D;1n 中，自动发现一个符号化表达式 f<del>(x)\tilde f(x)f</del>(x) 来近似未知映射 $f: \mathbb{R}^d \to \mathbb{R}$，使得</p><p>$\tilde f(x_i) \approx y_i,\quad \forall i$,</p><p>且该表达式要具有良好的可解释性和对未见数据的泛化能力。传统 SR 方法多以表达式树或上下文无关文法表示候选方程，利用遗传编程等演化算法在组合空间中搜索，但往往忽略领域先验且搜索效率低下。LLM-SR 则将方程视作可执行程序，借助大模型的科学知识和强大代码生成能力进行搜索与优化.</p><hr><h3 id="2-表达式表示"><a href="#2-表达式表示" class="headerlink" title="2. 表达式表示"></a>2. 表达式表示</h3><ul><li><p><strong>程序化骨架 (Skeleton)</strong><br> 每个候选表达式被表示为一个 Python 函数骨架: </p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs ruby"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">x, params</span>):<br>    <span class="hljs-comment"># 由 LLM 生成的数学运算骨架，内部用 params[i] 占位</span><br>    <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><p>其中 <code>params</code> 是一个待优化的参数向量（长度上限设为 10），用于填充骨架中的数值常数和系数.</p></li><li><p><strong>搜索空间</strong><br> 该程序化表示极大扩展了表达式的表达能力，但也带来了庞大的搜索空间。LLM-SR 通过引入领域先验、逐步迭代优化以及经验缓冲机制，有效聚焦搜索到高质量区域。</p></li></ul><hr><h3 id="3-假说生成（Hypothesis-Generation）"><a href="#3-假说生成（Hypothesis-Generation）" class="headerlink" title="3. 假说生成（Hypothesis Generation）"></a>3. 假说生成（Hypothesis Generation）</h3><ol><li><p><strong>Prompt 结构</strong>: 每轮迭代构造给 LLM 的提示包含四部分</p><ul><li><strong>Instruction</strong>: 如何补全函数体，比如“考虑输入变量间的物理含义”</li><li><strong>Problem Specification</strong>: 简要问题描述及变量含义</li><li><strong>Evaluation &amp; Optimization Function</strong>: 定义如何根据数据评估候选方程</li><li><strong>Experience Demonstration</strong>: 若干（通常 k&#x3D;2）高分骨架示例，以 in-context 形式给出.</li></ul></li><li><p><strong>采样策略</strong>: </p><ul><li><p>使用温度采样（temperature &#x3D; 0.8）生成一批 b&#x3D;4 个骨架: </p><p>fi∼πθ(⋅∣prompt)</p></li><li><p>丢弃无法执行或超时（30 s）&#x2F;超内存（2 GB）的骨架，保证候选有效性.</p></li></ul></li></ol><hr><h3 id="4-参数优化与评估（Hypothesis-Optimization-Assessment）"><a href="#4-参数优化与评估（Hypothesis-Optimization-Assessment）" class="headerlink" title="4. 参数优化与评估（Hypothesis Optimization &amp; Assessment）"></a>4. 参数优化与评估（Hypothesis Optimization &amp; Assessment）</h3><p>对每个合法骨架 f，将占位参数 <code>params</code> 优化以最小化均方误差 (MSE): </p><ol><li><p><strong>优化方法</strong></p><ul><li><strong>numpy + BFGS</strong>（Fletcher, 1987）: 适用于参数较少的情况</li><li><strong>torch + Adam</strong>（Kingma &amp; Ba, 2014）: 适用于包含可微张量运算的骨架.</li></ul></li><li><p><strong>评分</strong><br> 优化后计算预测值 $\hat y&#x3D;f(x, params∗)$，得分</p><p>$s&#x3D;-\mathrm{MSE}(\hat y,,y)$.</p><p>负 MSE 越大说明拟合越好。</p></li></ol><hr><h3 id="5-经验管理与迭代（Experience-Management）"><a href="#5-经验管理与迭代（Experience-Management）" class="headerlink" title="5. 经验管理与迭代（Experience Management）"></a>5. 经验管理与迭代（Experience Management）</h3><p>为了平衡探索与利用，并避免陷入局部最优，LLM-SR 维护一个“多岛”经验缓冲区: </p><ol><li><strong>多岛模型</strong><ul><li>m 个独立“岛”，每岛内存储若干 (f,s) 对，初始仅包含最简单线性骨架示例。</li><li>若新骨架在其来源岛上取得更好分数，则加入该岛.</li></ul></li><li><strong>周期性重置</strong><ul><li>每隔若干迭代（约 4 h）丢弃表现最差的 m&#x2F;2 个岛中的所有骨架，用表现较好岛的骨架重初始化，保持多样性。</li></ul></li><li><strong>示例采样</strong><ul><li>构造下一轮 prompt 时，<strong>先</strong>均匀选岛，<strong>再</strong>在岛内按两阶段采样: <ol><li><strong>簇级别 Boltzmann 采样</strong>（偏好高分簇）</li><li><strong>程序级别长度惩罚采样</strong>（偏好更简洁骨架）</li></ol></li><li>最终选出 kkk 个示例提供给 LLM，指导其生成下轮骨架</li></ul></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AUC损失</title>
    <link href="/2025/20250321/"/>
    <url>/2025/20250321/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在最新的TPS中，是以AUC为指标，故有了直接优化AUC的想法。（最终它帮助我获得了rank 18&#x2F;4381)。</p><span id="more"></span><h2 id="AUC直接优化"><a href="#AUC直接优化" class="headerlink" title="AUC直接优化"></a>AUC直接优化</h2><p>根据定义<br>$$<br>AUC&#x3D;E_{x^+\sim P+,x^-\sim P-}[1(f(x+)-f(x-)&gt;)]<br>$$<br>其中1为指示函数。</p><p>AUC的不可导是由于指示函数的存在，故我们需要替代函数。其实它类似与0-1损失，所以我们也可以采取类似的替代函数，比如SVM中对hinge loss，比如逻辑回归中的logistic loss。</p><h2 id="什么样的替代函数是好的"><a href="#什么样的替代函数是好的" class="headerlink" title="什么样的替代函数是好的"></a>什么样的替代函数是好的</h2><p>但是我们也不是随心所欲地去选替代函数，</p><p>高尉、周志华的一篇<a href="https://arxiv.org/abs/1208.0645">《On the Consistency of AUC Pairwise Optimization》</a>进一步探索了替代函数的一致性（指优化该替代损失函数的期望风险是否收敛到贝叶斯风险），提供了新的 AUC 一致性充分条件，并指出了校准性对于 AUC 一致性是必要的但不足够的。</p><p>论文指出充分条件：<strong>若替代函数是凸函数，且0处导小于0，则它与AUC一致。</strong></p><p>值得注意到是，hinge loss 并不具有一致性，而q-norm loss才具有一致性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AUC指标的公榜探测次数</title>
    <link href="/2025/20250319/"/>
    <url>/2025/20250319/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（本文主要源自与@broccoli beef的讨论。）</p><p>对于AUC指标，公榜探测需要多少次能得到？</p><p>我们可以提前给个结论，一个粗糙的用来估计的界限是N*H(p)&#x2F;2log2(N)。</p><span id="more"></span><hr><p>对于AUC，总所周知的定义是：</p><p>$$\mathrm{AUC}&#x3D;E(1(f(x^+)&gt;f(x^-)))$$</p><p>但是常被忽视的定义是：</p><p>$$\mathrm{AUC}&#x3D;\frac{\sum R_{pos} -\frac{n_{pos}*(n_{pos}+1)}{2}}{n_{pos}n_{neg}}$$</p><p>其中$R_{pos}$是正样本的排名。</p><p>故我们可以不断提交笔记本获得公榜来获取某个样本的排名，并可得知该样本前后的样本数，正负样本数可以通过排行榜的差值来获取。即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Before: m pos  || rank i  || n pos<br>After: m pos   ||   n pos   ||   rank i<br></code></pre></td></tr></table></figure><p>类似于我们可以大幅降低复杂度，比如在正负样本3:1的情况下150次提交可以把$\binom{N}{\frac{3}{4}N}$降为$\binom{\frac{1}{150}N}{\frac{1}{150}\frac{3}{4}N}^{150}$。</p><p>但是这种方法还是太暴力和粗糙。</p><p>还有没有更快的呢？有的有的。</p><p><strong>更快的方法：</strong></p><p>根据AUC的后一个定义，我们发现其是线性的。实际上这是一个混合整数规划（Mixed Integer Programming, MIP）。</p><p>我们可以使用ortools来求解，比如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python">N=<span class="hljs-number">146</span><br>p=<span class="hljs-number">113</span><br>n=<span class="hljs-number">33</span><br><span class="hljs-keyword">from</span> ortools.sat.python <span class="hljs-keyword">import</span> cp_model<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SolutionCallback</span>(cp_model.CpSolverSolutionCallback):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.solutions = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">on_solution_callback</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>.solutions.append([<span class="hljs-variable language_">self</span>.Value(x[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)])<br><br>model = cp_model.CpModel()<br>x = [model.NewIntVar(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-string">F&#x27;x[<span class="hljs-subst">&#123;i&#125;</span>]&#x27;</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N)]<br><br>model.Add(<span class="hljs-built_in">sum</span>(x)==p)<br><br><span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>    y_pred = preds[m]<br>    r = pd.Series(y_pred).rank().values<br>    model.Add(<span class="hljs-built_in">sum</span>(x[i]*<span class="hljs-built_in">int</span>(np.around(r[i]*<span class="hljs-number">2</span>)) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(N))==<span class="hljs-built_in">int</span>(np.around(scores[m]*n*p*<span class="hljs-number">2</span>))+p*(p+<span class="hljs-number">1</span>))<br><br>solver = cp_model.CpSolver()<br>s = SolutionCallback()<br>solver.SearchForAllSolutions(model, s)<br></code></pre></td></tr></table></figure><p><strong>另一个更重要的问题是，我们大概需要多少个提交呢？</strong></p><p>我们可以使用信息论粗糙地估计一下。</p><p>总信息量是$log_2\binom{N}{\frac{3}{4}N}\approx N*H(p)$，其中H是熵。</p><p>如果我们使用斯特林公式，即$ln(n!)\approx nlnn−n+\frac{1}{2}ln(2\pi n)$则：</p><p>$$<br>log_2\binom{N}{\frac{3}{4}N}\approx N*H(p)-\frac{1}{2}log_2(2\pi Np(1-p))<br>$$</p><p>每次探测大概有O(N^2)的范围，大概能提供$2log_2N$的信息量.</p><p>$$k\approx\frac{N*H(p)}{2log_2N}$$</p><p>对于3月的TPS数据集，公榜为146，正负样本比例为3:1，k~8，和现实的实践是一样的。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>kaggle</tag>
      
      <tag>信息论</tag>
      
      <tag>运筹学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>两篇大气相关的论文阅读</title>
    <link href="/2025/20250315/"/>
    <url>/2025/20250315/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在kaggle上讨论时，兰州大学的muqingyu博士推荐的两篇论文。</p><span id="more"></span><h2 id="Dust-Accelerates-the-Life-Cycle-of-High-Clouds-Unveiled-Through-Strongly-Constrained-Meteorology"><a href="#Dust-Accelerates-the-Life-Cycle-of-High-Clouds-Unveiled-Through-Strongly-Constrained-Meteorology" class="headerlink" title="Dust Accelerates the Life Cycle of High Clouds Unveiled Through Strongly-Constrained Meteorology"></a>Dust Accelerates the Life Cycle of High Clouds Unveiled Through Strongly-Constrained Meteorology</h2><p><a href="https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2024GL109998">https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2024GL109998</a></p><p>muqingyu四作。</p><p>具体而言，PCA后取第一个分量，发现具有较好的解释性。</p><p><img src="/2025/20250315/grl68161-fig-0001-m.png"></p><p>PC1 与高云量之间的关系。(a) 从 CERES 数据中提取的 2010-2020 年间的全球高云量。(b) 青藏高原上高云量的分布。(c) 从 ERA5 再分析数据中提取的同一时期气象条件的主模态。(d) 在青藏高原上，气象条件主模态与高云量之间的相关系数分布，所有网格点都通过了 99% 的显著性检验，因此我们整个空间部分的相关系数是显著的。</p><p>论文其余内容是在这基础上做的气象分析。</p><h2 id="Short-term-Precipitation-Forecasting-in-The-Netherlands-An-Application-of-Convolutional-LSTM-neural-networks-to-weather-radar-data"><a href="#Short-term-Precipitation-Forecasting-in-The-Netherlands-An-Application-of-Convolutional-LSTM-neural-networks-to-weather-radar-data" class="headerlink" title="Short-term Precipitation Forecasting in The Netherlands: An Application of Convolutional LSTM neural networks to weather radar data"></a>Short-term Precipitation Forecasting in The Netherlands: An Application of Convolutional LSTM neural networks to weather radar data</h2><p><img src="/2025/20250315/figure2.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>kaggle</tag>
      
      <tag>人工智能</tag>
      
      <tag>大气</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Muon优化器</title>
    <link href="/2025/20250224/"/>
    <url>/2025/20250224/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>近日，Moonshot开源了改进版 Muon 优化算法及用 Muon 训练的SOTA级的MoE小模型。开启了Muon在大模型应用的局面。也许新的优化器时代即将到来！</p><p>PS：像谷歌23年提出的Lion（Evo<strong>L</strong>ved S<strong>i</strong>gn M<strong>o</strong>me<strong>n</strong>tum）优化器也号称比AdamW好，但是缺乏在大模型上的成功实验，大多数人还是选择Adam&#x2F;AdamW。</p><span id="more"></span><p>苏神和官方作者Keller Jordan都写了一些博客来介绍这个内容。</p><h2 id="初步介绍"><a href="#初步介绍" class="headerlink" title="初步介绍"></a>初步介绍</h2><p><img src="/2025/20250224/muon_algo.png"></p><p>其中NewtonSchulz5被定义为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Pytorch code</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">newtonschulz5</span>(<span class="hljs-params">G, steps=<span class="hljs-number">5</span>, eps=<span class="hljs-number">1e-7</span></span>):<br>    <span class="hljs-keyword">assert</span> G.ndim == <span class="hljs-number">2</span><br>    a, b, c = (<span class="hljs-number">3.4445</span>, -<span class="hljs-number">4.7750</span>, <span class="hljs-number">2.0315</span>)<br>    X = G.bfloat16()<br>    X /= (X.norm() + eps)<br>    <span class="hljs-keyword">if</span> G.size(<span class="hljs-number">0</span>) &gt; G.size(<span class="hljs-number">1</span>):<br>        X = X.T<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):<br>        A = X @ X.T<br>        B = b * A + c * A @ A<br>        X = a * X + B @ X<br>    <span class="hljs-keyword">if</span> G.size(<span class="hljs-number">0</span>) &gt; G.size(<span class="hljs-number">1</span>):<br>        X = X.T<br>    <span class="hljs-keyword">return</span> X<br></code></pre></td></tr></table></figure><p>Muon取得了以下经验结果。</p><ul><li>研究者将训练速度记录提高到在 CIFAR-10 上达到 94%准确率的 2.6 A100-秒，从 3.3 秒提升。</li><li>将训练速度记录提升至 FineWeb（一个被称为 NanoGPT 速度跑的竞争性任务）上的 3.28 验证损失，提高了 1.35 倍。</li><li>持续在扩展到 774M 和 1.5B 参数时展示训练速度提升。</li><li>训练了一个 1.5B 参数的 Transformer，在 HellaSwag 上达到 GPT-2 XL 级别的性能，耗时 10 个 8xH100 小时。使用 AdamW 达到相同结果需要 13.3 小时。</li></ul><h3 id="什么是NewtonSchulz5"><a href="#什么是NewtonSchulz5" class="headerlink" title="什么是NewtonSchulz5"></a>什么是NewtonSchulz5</h3><p>我们初看，可能很疑惑，怎么突然出现了NewtonSchulz5，这是怎么来的？</p><p>事实上，该算法和msign（<a href="https://en.wikipedia.org/wiki/Matrix_sign_function">矩阵符号函数</a>）有关。<strong>Newton-schulz就是用来求（或近似）msign(M)的。</strong> 在这里可以认为二者就是同个东西。</p><p>对于msign，我们有以下求法：</p><p><strong>（1）SVD</strong></p><p>有以下性质，当m&#x3D;n&#x3D;r时，<br>$$<br>msign(M)&#x3D;argmin_{O^TO&#x3D;I}||M-O||^2_F<br>$$</p><p>证明：</p><p>$||M-O||^2_F&#x3D;||M||^2_F+||O||^2_F-2\left&lt;M,O\right&gt; _ F$</p><p>$&#x3D;||M||^2_F+n-2Tr(MO^T)$<br>$&#x3D;||M||^2_F+n-2Tr(U\Sigma V^TO^T)$<br>$&#x3D;||M||^2_F+n-2Tr(\Sigma V^TO^TU)$<br>$&#x3D;||M||^2_F+n-2\sum_i^n \Sigma_{i.i}(V^TO^TU)_{i,i})$</p><p>最小值时$O&#x3D;UV^T$</p><p>进一步的，<br>$$<br>U, \Sigma, V^{\top}&#x3D;S V D ( M ) \quad\Rightarrow\quad\mathrm{m s i g n} ( M )&#x3D;U_{[ : ,: r ]} V_{[ :, : r ]}^{\top}<br>$$<br>$U\in R^{n\times n},\Sigma\in R^{n\times m},V\in R^{m\times m}$，r是M的秩。</p><p>SVD易于理解，但太慢了。</p><p><strong>（2）耦合牛顿迭代（Coupled Newton iteration）</strong></p><p>Shampoo中用于执行逆四次方根，并且可以轻松地适应进行正交化。</p><p>但它在至少 float32 精度下运行以避免数值不稳定性，这使得它在现代 GPU 上运行缓慢。</p><p><strong>（3）NewtonSchulz5</strong></p><p>可以在bfloat16 中稳定运行</p><p>$$<br>\mathrm{m s i g n} ( M )&#x3D;( M M^{\top} )^{-1 &#x2F; 2} M&#x3D;M ( M^{\top} M )^{-1 &#x2F; 2}<br>$$<br>我们也可以联想一下标量中的sign函数，sgn(x)和$x(x^2)^{-\frac{1}{2}}&#x3D;\frac{x}{\sqrt{x^2}}$是一样的。</p><p>我们对$(M^TM)^{-\frac{1}{2}}$在$M^TM&#x3D;I$进行展开到二阶，有<br>$$<br>msign(M)&#x3D;M(M^TM)^{-1&#x2F;2}\approx \frac{15}{8}M-\frac{5}{4}M(M^TM)+\frac{3}{8}M(M^TM)^2<br>$$<br>然而泰勒展开有可能会有误差，既然我们得到了这样的形式，我们干脆假设$msign(M)\approx aM+bM(M^TM)+cM(M^TM)^2$</p><p>作者进一步优化得到(a,b,c)&#x3D;(3.4445, -4.7750, 2.0315)。</p><p>作者也尝试用了使用三阶和七阶多项式（注意现在是五阶），但并没有达到更好的效果。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><p><a href="https://kellerjordan.github.io/posts/muon/">Muon: An optimizer for hidden layers in neural networks</a></p></li><li><p><a href="https://kexue.fm/archives/10592">Muon优化器赏析：从向量到矩阵的本质跨越</a></p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MoBA vs NSA</title>
    <link href="/2025/20250222/"/>
    <url>/2025/20250222/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Kimi公开了他们处理长文的秘密了。团队提出了MoBA (Mixture of Block Attention) ，解决了传统注意力机制在处理长文本时的效率问题。</p><p>DeepSeek 发布了一篇新论文，提出了一种改进版的注意力机制 NSA（Native Sparse Attention），加上还有创始人兼 CEO 梁文锋亲自参与。</p><span id="more"></span><p>由于MoBA比NSA更简单，于是我们循序渐进先介绍NSA。</p><p>这两个都主要对KV进行优化。</p><h2 id="NSA"><a href="#NSA" class="headerlink" title="NSA"></a>NSA</h2><p><img src="/2025/20250222/NSA.png"></p><p>其中定义了三种注意力：压缩（cmp)、选择（sle)、滑窗（win），并最后使用门控来简单汇聚，$g^{cmp}o^{cmp}+g^{sle}o^{sle}+g^{win}o^{win}$。</p><h3 id="压缩注意力"><a href="#压缩注意力" class="headerlink" title="压缩注意力"></a>压缩注意力</h3><p>压缩注意力的本质是将一段序列的KV压成一个KV。</p><p><img src="/2025/20250222/cmp.jpg"></p><h3 id="选择注意力"><a href="#选择注意力" class="headerlink" title="选择注意力"></a>选择注意力</h3><p>论文选择将这部分和压缩注意力结合起来。</p><p>即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">p_slc = p_cmp.<span class="hljs-built_in">sum</span>(dim = <span class="hljs-number">1</span>) <span class="hljs-comment"># 在head维度上进行合并</span><br><span class="hljs-built_in">print</span>(p_cmp.shape) <span class="hljs-comment"># torch.Size([1, 4, 32, 4])</span><br><span class="hljs-built_in">print</span>(p_slc.shape) <span class="hljs-comment"># torch.Size([1, 32, 4])</span><br>select_top_k = <span class="hljs-number">2</span><br>_, idx = torch.topk(p_slc, dim = <span class="hljs-number">2</span>, k = select_top_k)<br><span class="hljs-built_in">print</span>(idx[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,:]) <span class="hljs-comment"># [3,0] 即 q0注意到第3片段和第0片段</span><br>idx.shape <span class="hljs-comment"># [1, 32, 2] : batch_size, q_len, top_k</span><br></code></pre></td></tr></table></figure><h3 id="滑窗注意力"><a href="#滑窗注意力" class="headerlink" title="滑窗注意力"></a>滑窗注意力</h3><p>这部分是用来捕捉临近的kv片段。其实很简单。<br>$$<br>\tilde{K} _ {t} ^{w i n}&#x3D;{\bf k} _ {t-w : t}, \tilde{V} _ {t} ^{w i n}&#x3D;{\bf v} _ {t-w : t}<br>$$<br>值得注意的是，这部分可能会跨block。</p><h3 id="汇聚"><a href="#汇聚" class="headerlink" title="汇聚"></a>汇聚</h3><p>前面我们已经介绍过$g^{cmp}o^{cmp}+g^{sle}o^{sle}+g^{win}o^{win}$</p><p>它实际上使用了 MLP和 sigmoid，即</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">W_gated = torch.randn(dim, <span class="hljs-number">3</span>) <span class="hljs-comment"># mlp, dim-&gt;3: cmp, slc, win</span><br>gate = X @ W_gated<br>gate = F.sigmoid(gate) <span class="hljs-comment"># sigmoid activation</span><br><span class="hljs-built_in">print</span>(gate.shape) <span class="hljs-comment"># 1, 32, 3 , bs, q_len, gated</span><br></code></pre></td></tr></table></figure><h2 id="MoBA"><a href="#MoBA" class="headerlink" title="MoBA"></a>MoBA</h2><p>也正如苏神在知乎上所说，“如果读者对比过NSA和MoBA，估计都有种MoBA是NSA的简化版的感觉：NSA用了MLP来压缩block，MoBA直接用Mean Pooling；NSA用另一块压缩的Attention来学block select，MoBA直接去掉了这部分。不得不说，NSA的设计是更符合一般人的想法，如果由我自己独立来设计MoBA，估计最终形式会更像NSA，因为MoBA这种极致简化的做法则更需要一点勇气（以及长时间的尝试）。”</p><p>相比NSA，MoBA更简单，block select相当于更精细的内容，但MoBA把这部分去掉了。Mean Pooling代替MLP减少了许多参数量，这让我想到了global average pooling 中指出：”One advantage of global average pooling over the fully connected layers is that it is more native to the convolution structure. Another advantage is that there is no parameter to optimize in the global average pooling, thus overfitting is avoided at this layer.“</p><p><img src="/2025/20250222/MOBA.png"></p><p>同样地，MoBA也把KV分为若干个block。</p><p>操作也很简单和明显，我们可以看出它具体做了什么，图中的内容无需再赘述。</p><p>此外，保持自回归语言模型的因果关系很重要。</p><p>MoBA设置了不能关注未来块，另外，整个块的平均池化可能会无意包含来自未来标记的信息。未来解决这些问题，它强制要求每个token必须路由到当前块，并应用因果掩码。</p><p>MoBA也有更多的灵活性，它的参数与full attention参数相比数量不变，不增不减。这一特性启发我们进行全注意力与 MoBA 之间的平滑过渡。具体来说，在初始化阶段，每个注意力层可以选择全注意力或 MoBA，如果需要，这个选择可以在训练过程中动态更改。</p><p>当然它并不是可免训练的、即插即用的，作者指出：“MoBA没有参数是不是拿来就可以在现有模型上用? MoBA 不是一个免训练 sparse attention，虽然是无额外参数的，但是依然需要对现有模型进行Continue Training。训练中关注Trailing token loss下降情况，或者直接关注 longctx 相关 bmk 涨点情况即可“</p><p><strong>代码:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">moba_attn_varlen_naive</span>(<span class="hljs-params"></span><br><span class="hljs-params">    q: torch.Tensor,</span><br><span class="hljs-params">    k: torch.Tensor,</span><br><span class="hljs-params">    v: torch.Tensor,</span><br><span class="hljs-params">    cu_seqlens: torch.Tensor,</span><br><span class="hljs-params">    max_seqlen: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    moba_chunk_size: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params">    moba_topk: <span class="hljs-built_in">int</span>,</span><br><span class="hljs-params"></span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;Implement the moba brute-force setting for reference</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        q (torch.Tensor): [seqlen, head, head_dim]</span><br><span class="hljs-string">        k (torch.Tensor): [seqlen, head, head_dim]</span><br><span class="hljs-string">        v (torch.Tensor): [seqlen, head, head_dim]</span><br><span class="hljs-string">        cu_seqlens (torch.Tensor): the cumulative sequence length tensor, same definition in flash attn</span><br><span class="hljs-string">        max_seqlen (int): the max sequence length of the batch, same definition in flash attn</span><br><span class="hljs-string"></span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        attn_output (torch.Tensor): [seqlen, head, head_dim]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># qkv shape = [ S, H, D ]</span><br>    batch = cu_seqlens.numel() - <span class="hljs-number">1</span><br>    softmax_scale = q.shape[-<span class="hljs-number">1</span>] ** (-<span class="hljs-number">0.5</span>)<br><br>    o = torch.zeros_like(q)<br>    <span class="hljs-keyword">for</span> batch_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch):<br>        batch_start = cu_seqlens[batch_idx].item()<br>        batch_end = cu_seqlens[batch_idx + <span class="hljs-number">1</span>].item()<br>        <span class="hljs-comment"># get qkv of this batch</span><br>        q_ = q[batch_start:batch_end]<br>        k_ = k[batch_start:batch_end]<br>        v_ = v[batch_start:batch_end]<br>        o_ = o[batch_start:batch_end]<br>        <span class="hljs-comment"># calc key gate weight</span><br>        key_gate_weight = []<br>        batch_size = batch_end - batch_start<br>        num_block = math.ceil(batch_size / moba_chunk_size)<br>        <span class="hljs-keyword">for</span> block_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, num_block):<br>            block_start = block_idx * moba_chunk_size<br>            block_end = <span class="hljs-built_in">min</span>(batch_size, block_start + moba_chunk_size)<br>            key_gate_weight.append(k_[block_start:block_end].mean(dim=<span class="hljs-number">0</span>, keepdim=<span class="hljs-literal">True</span>))<br>        key_gate_weight = torch.cat(key_gate_weight, dim=<span class="hljs-number">0</span>)  <span class="hljs-comment"># [ N, H, D ]</span><br>        <span class="hljs-comment"># calc &amp; mask gate</span><br>        <span class="hljs-comment"># use fp32 to avoid precision issue in bf16</span><br>        q_ = q_.<span class="hljs-built_in">type</span>(torch.float32)<br>        key_gate_weight = key_gate_weight.<span class="hljs-built_in">type</span>(torch.float32)<br>        gate = torch.einsum(<span class="hljs-string">&quot;shd,nhd-&gt;hsn&quot;</span>, q_, key_gate_weight)  <span class="hljs-comment"># [ H, S, N ]</span><br>        key_gate_weight = key_gate_weight.type_as(k)<br>        q_ = q_.type_as(k)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_block):<br>            <span class="hljs-comment"># select the future Qs that can attend to KV chunk i</span><br>            gate[:, : (i + <span class="hljs-number">1</span>) * moba_chunk_size, i] = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>)<br>            gate[:, i * moba_chunk_size : (i + <span class="hljs-number">1</span>) * moba_chunk_size, i] = <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)<br>        <span class="hljs-comment"># gate_top_k_idx = gate_top_k_val = [ H S K ]</span><br>        gate_top_k_val, gate_top_k_idx = torch.topk(<br>            gate, k=<span class="hljs-built_in">min</span>(moba_topk, num_block), dim=-<span class="hljs-number">1</span>, largest=<span class="hljs-literal">True</span>, <span class="hljs-built_in">sorted</span>=<span class="hljs-literal">False</span><br>        )<br>        gate_top_k_val, _ = gate_top_k_val.<span class="hljs-built_in">min</span>(dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># [ H, S ]</span><br>        need_attend = gate &gt;= gate_top_k_val.unsqueeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># add gate_idx_mask in case of there is cornercases of same topk val been selected</span><br>        gate_idx_mask = torch.zeros(<br>            need_attend.shape, dtype=torch.<span class="hljs-built_in">bool</span>, device=q.device<br>        )<br>        gate_idx_mask = gate_idx_mask.scatter_(dim=-<span class="hljs-number">1</span>, index=gate_top_k_idx, value=<span class="hljs-literal">True</span>)<br>        need_attend = torch.logical_and(need_attend, gate_idx_mask)<br>        gate[need_attend] = <span class="hljs-number">0</span><br>        gate[~need_attend] = -<span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)<br>        gate = gate.repeat_interleave(moba_chunk_size, dim=-<span class="hljs-number">1</span>)[<br>            :, :, :batch_size<br>        ]  <span class="hljs-comment"># [ H, S, S ]</span><br>        gate.masked_fill_(<br>            torch.ones_like(gate, dtype=torch.<span class="hljs-built_in">bool</span>).tril().logical_not(), -<span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;inf&quot;</span>)<br>        )<br><br>        <span class="hljs-comment"># calc qk = qk^t</span><br>        q_ = q_.<span class="hljs-built_in">type</span>(torch.float32)<br>        k_ = k_.<span class="hljs-built_in">type</span>(torch.float32)<br>        v_ = v_.<span class="hljs-built_in">type</span>(torch.float32)<br>        qk = torch.einsum(<span class="hljs-string">&quot;xhd,yhd-&gt;hxy&quot;</span>, q_, k_)<br>        <span class="hljs-comment"># mask</span><br>        qk += gate<br>        qk *= softmax_scale<br>        <span class="hljs-comment"># calc o</span><br>        p = qk.softmax(dim=-<span class="hljs-number">1</span>)<br>        o_ += torch.einsum(<span class="hljs-string">&quot;hxy,yhd-&gt;xhd&quot;</span>, p, v_)<br>        o = o.type_as(q)<br><br>    <span class="hljs-keyword">return</span> o<br></code></pre></td></tr></table></figure><p>作者也介绍了一些<a href="https://www.zhihu.com/question/12696635711/answer/105709816990">心路历程</a>，也值得一看。</p><h2 id="MoBA-VS-NSA"><a href="#MoBA-VS-NSA" class="headerlink" title="MoBA VS NSA"></a>MoBA VS NSA</h2><p>它们都有和flash attention比较，也都达到了100%的大海捞针测试。</p><p>其中有一个有趣的是</p><p><img src="/2025/20250222/moba.jpg"></p><p>MoBA的损失曲线一开始不如full attention，但后续逐渐重合。</p><p><img src="/2025/20250222/nsa1.png"></p><p>而NSA则全面优于full attention。</p><p>另外，如果全用MoBA可能会有问题，所以后续需要加上几层（苏神说一层就足够）full attention。而NSA则没有这一问题。</p><h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/24841366485">【手撕NSA】DeepSeek新作-原生稀疏注意力-超长文(附代码)</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Group Relative Policy Optimization</title>
    <link href="/2025/20250220/"/>
    <url>/2025/20250220/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Group Relative Policy Optimization（GRPO）起自deepseekmath，在deepseek-R1中也大放光彩。</p><p>看到复旦某组开源了一个简单的仅~200行的关于GRPO的项目simple_GRPO，故决定学习并写写。</p><span id="more"></span><p>GRPO是PPO的改进。</p><p><img src="/2025/20250220/PPOVSGRPO.png"></p><p>具体流程图：</p><p><img src="/2025/20250220/GRPO.png"></p><p>流程看上去有点复杂，我们可以直接来看代码。</p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>源代码用100行代码实现了<a href="https://github.com/lsdefine/simple_GRPO/blob/main/ref_server.py">ref_server.py</a>，用了分布并行运行reference model。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">system_prompt = <span class="hljs-string">&quot;&quot;&quot;You are a helpful assistant. A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The Assistant first thinks about the reasoning process in the mind and then provides the user with the answer. ... &quot;&quot;&quot;</span><br></code></pre></td></tr></table></figure><h2 id="奖励"><a href="#奖励" class="headerlink" title="奖励"></a>奖励</h2><ul><li>提取回答的正确性，匹配奖励为1，否则为-1。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_correct</span>(<span class="hljs-params">item, answer</span>):<br>    pattern = <span class="hljs-string">r&#x27;\d+\.\d+|\d+/\d+|\d+&#x27;</span><br>    nums = re.findall(pattern, answer)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(nums) == <span class="hljs-number">0</span>: <span class="hljs-keyword">return</span> -<span class="hljs-number">1.0</span><br>    lastnum = nums[-<span class="hljs-number">1</span>]<br>    ans = parse(lastnum, extraction_config=[ExprExtractionConfig()])<br>    ground_truth = parse(item[<span class="hljs-string">&quot;A&quot;</span>], extraction_config=[ExprExtractionConfig()])<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> verify(ans, ground_truth) <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><ul><li>如果回答符合指定格式，则奖励为 <code>1.25</code>，否则为 <code>-1</code>。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_format</span>(<span class="hljs-params">item, answer</span>):<br>    pattern = <span class="hljs-string">r&quot;^&lt;think&gt;.*?&lt;/think&gt;&lt;answer&gt;.*?&lt;/answer&gt;$&quot;</span><br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.25</span> <span class="hljs-keyword">if</span> re.<span class="hljs-keyword">match</span>(pattern, answer, re.DOTALL | re.VERBOSE) <span class="hljs-keyword">else</span> -<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h2 id="loss-部分"><a href="#loss-部分" class="headerlink" title="loss 部分"></a>loss 部分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">GRPO_step</span>(<span class="hljs-params">batch</span>):<br>    prompt_length = batch[<span class="hljs-string">&#x27;plen&#x27;</span>]<br>    inputs = batch[<span class="hljs-string">&#x27;inputs&#x27;</span>].to(engine.device)<br>    rewards = batch[<span class="hljs-string">&#x27;rewards&#x27;</span>].to(engine.device)<br><br>    logits = engine(inputs).logits<br>    logits = logits[:, :-<span class="hljs-number">1</span>, :]  <span class="hljs-comment"># (B, L-1, V), exclude the last logit: it corresponds to the next token pred</span><br>    input_ids = inputs[:, <span class="hljs-number">1</span>:]  <span class="hljs-comment"># (B, L-1), exclude the first input ID since we don&#x27;t have logits for it</span><br>    <br>    per_token_logps = get_per_token_logps(logits, input_ids)<br>    per_token_logps = per_token_logps[:,prompt_length-<span class="hljs-number">1</span>:]<br>    ref_per_token_logps = batch[<span class="hljs-string">&#x27;refs&#x27;</span>].to(per_token_logps.device)<br><br>    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - <span class="hljs-number">1</span><br>    completion_mask = (inputs[:, prompt_length:] != tokenizer.pad_token_id).<span class="hljs-built_in">int</span>()<br><br>    mean_grouped_rewards = rewards.view(-<span class="hljs-number">1</span>, num_pre_Q).mean(dim=<span class="hljs-number">1</span>)<br>    std_grouped_rewards = rewards.view(-<span class="hljs-number">1</span>, num_pre_Q).std(dim=<span class="hljs-number">1</span>)<br>    mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(num_pre_Q, dim=<span class="hljs-number">0</span>)<br>    std_grouped_rewards = std_grouped_rewards.repeat_interleave(num_pre_Q, dim=<span class="hljs-number">0</span>)<br>    advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + <span class="hljs-number">1e-4</span>)<br><br>    per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(<span class="hljs-number">1</span>)<br>    per_token_loss = -(per_token_loss - beta * per_token_kl)<br>    loss = ((per_token_loss * completion_mask).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>) / completion_mask.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)).mean()<br>    <span class="hljs-keyword">return</span> loss<br><br></code></pre></td></tr></table></figure><p><img src="/2025/20250220/GRPOloss.jpg"></p><h3 id="无偏小方差KL"><a href="#无偏小方差KL" class="headerlink" title="无偏小方差KL"></a>无偏小方差KL</h3><p>这里使用的并不是传统的KL，而是使用了一个无偏小方差KL，即$KL&#x3D;rlogr-(r-1)$</p><p>传统的KL：<br>$$<br>K L [ q, p ]&#x3D;\sum_{x} q ( x ) \operatorname{log} \frac{q ( x )} {p ( x )}&#x3D;E_{x \sim q} [ \operatorname{log} \frac{q ( x )} {p ( x )} ]<br>$$<br>如何构建一个好的估计？<strong>一个好的估计量是无偏的（它有正确的均值）并且具有低方差。</strong></p><p><strong>一个无偏估计量k1</strong>是$k1&#x3D;log\frac{q(x)}{p(x)}&#x3D;-log r$，但他有高方差，因为他对于因为它对于一半的样本是负的，而 KL 总是正的。</p><p><strong>另一个低方差但存在偏差的估计量k2</strong>是$k2&#x3D;\frac{1}{2}(log\frac{q(x)}{p(x)})^2&#x3D;\frac{1}{2}(logr)^2$。</p><p>它的期望可以从<a href="https://en.wikipedia.org/wiki/F-divergence">f-divergence</a>上考虑，其定义为凸函数f的$D_{f} ( p, q )&#x3D;E_{x \sim q} [ f ( \frac{p ( x )} {q ( x )} ) ] $。</p><p>而<br>$$<br>D_{f} ( p_{0}, p_{\theta} )&#x3D;\frac{f^{\prime\prime} ( 1 )} {2} \theta^{T} F \theta+O ( \theta^{3} )<br>$$<br>其中F是$p_\theta&#x3D;p_0$处的Fisher information matrix。</p><p>$E_{q} [ k_{2} ]&#x3D;E_{q} [ \frac{1} {2} ( \operatorname{log} r )^{2} ]$，而$\frac{1}{2}(log(x))^2$和传统KL对应的$-log(x)$具有相同的$f’’(1)&#x3D;1$。</p><p>另一个无偏但方差低的KL散度就是$rlogr-(r-1)$。为了降低方差，我们可以在k1的基础上再加入一个期望为0但与k1负相关的量，而$r-1$正是这个量。</p><p>另外，<a href="https://github.com/huggingface/trl/pull/2806">@ingambe在这里</a>尝试了把KL项去掉了，但也取得不错的效果。<a href="https://arxiv.org/pdf/2405.14734">SimPO</a>也删去了reference model，且取得了比DPO更好的结果。<br>这也给了我们一个反思：<strong>既然GRPO在PPO的基础上去除掉了value model，那reference model也是必要的吗？我们可以完全去掉它吗？</strong></p><h3 id="优势函数"><a href="#优势函数" class="headerlink" title="优势函数"></a>优势函数</h3><p>由于GRPO比PPO少了value model，所以GRPO采用了一个更简单的方式来代替它的功能。<br>$\hat{A}_{i,t}&#x3D;\frac{r_i-mean(\text{r})}{std(\text{r})}$<br>相当于用“多次模拟成绩平均值”代替价值函数。</p><h3 id="实际上无clip"><a href="#实际上无clip" class="headerlink" title="实际上无clip"></a>实际上无clip</h3><p><img src="/2025/20250220/GRPOloss.jpg"></p><p>loss函数还有一个clip，用来约束新旧策略每次迭代之间不要偏移得太远。</p><p>但实际是代码实现上GRPO 的损失没有 clip，也直接假设ratio&#x3D;1，（ TRL 源码也是这样做的。）</p><p>对应代码中的<code> torch.exp(per_token_logps - per_token_logps.detach())</code>，exp内恒为0，但有梯度，且$e^0&#x3D;1$。</p><p>这是因为只update一次，如deepseekmath中所说。</p><blockquote><p>“The policy model only has a single update following each exploration stage”——deepseekmath中4.2. Training and Evaluating DeepSeekMath-RL</p></blockquote><p>所以old policy和new policy无变化。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://github.com/lsdefine/simple_GRPO">github项目: simple_GRPO</a></li><li><a href="https://arxiv.org/pdf/2402.03300">《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》</a></li><li><a href="http://joschu.net/blog/kl-approx.html">Approximating KL Divergence</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>火烧金阁寺</title>
    <link href="/2025/20250217/"/>
    <url>/2025/20250217/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>金阁寺是三岛由纪夫所著。</p><p>我一向对日本文学除侦探小说以外的书籍不太感兴趣，诸如比较著名的村上春树，我也阅读较少。觉得它们阴沉和晦涩。</p><p>我接触三岛由纪夫的相关书籍，可以追溯到高中搬宿舍时从高中学长“继承”而来的《晓寺》。那边草草阅读了一些，就结束了对三岛由纪夫的接触。</p><p>看这本三岛由纪夫的《金阁寺》源自fabel的《风吹草动》。由林夕作词，其中写道“火烧金阁寺，是哪一位比我痴”，“分于金阁寺，大有超生的意思”。</p><span id="more"></span><p>这部《金阁寺》讲述了沟口走向浑浊的世界，走向金阁寺的黑暗。由美走向恶，由神圣走向亵渎。沟口也渐渐地变得暗黑，令人憎恶。</p><p>但对于这场故事，可以以书中的一句话一言以蔽之：“如今想想，这桩不幸的恋爱其实是我不幸的心灵造就的。我天生有一副黯淡的心灵，从来没有体验过欢乐明朗的感情。”</p><p> “我”的朋友选择了自我毁灭，而“我”选择了与金阁寺共同毁灭。</p><p>那熊熊燃烧的烈焰，既是毁灭，也是一场献祭。他焚烧的不仅仅是美的象征——金阁寺，更是自己内心那个被“美”所囚禁的、无法得到救赎的灵魂。</p><p>连灵魂烧光都不怕，再发觉瓦砾很浪漫。</p><p>《晓寺》拗口而晦涩，但是这部《金阁寺》反而更易懂且笔锋尖锐和深入。</p><h2 id="林夕之金阁寺"><a href="#林夕之金阁寺" class="headerlink" title="林夕之金阁寺"></a>林夕之金阁寺</h2><p>既然缘起林夕，就不得不提林夕与金阁寺，他们的关系可以追述到2013年于KKBOX的专栏【非關音樂】<a href="http://cantonpopsong.blogspot.com/2013/11/kk-box.html">《拿下了你這感情包袱 ，或者反而相信愛》</a> 。</p><p>全文如下：（原文繁体，已转为简体）</p><p>亲爱的，当我们爱到透明，别误会，不是把你当透明，而是，当我们朝夕长期相看，看著见著，渐渐把感情的现象，看透了背后的本质，爱变得清晰了，却也变稀薄了。当中因果关系，大概谁都不愿意考究。</p><blockquote><p>林夕给许廷铿写过一首《爱到透明》:</p><p>从盲目爱到透明 就当我透明<br>其实我衷心高兴 你忠于本性<br>你也许每晚要给感动<br>自自然熟到感觉过剩<br>我退出只因功成 无谓反省<br>你只想搬走这布景<br>谈情像一起堆砌著模型<br>完成了想有什么反应</p></blockquote><p>当那样天天共处，有你在就有我在，应该算是爱著的时候，一切都是理所当然，习以为常的－就像长在脚趾上的甲，没有凸显出来，就好好隐藏在鞋子里，直到把脚尖弄痛了。</p><p>之前，日子赶日子，话赶话，赶忙到只觉得好受与不好受，没有爱与不爱那么严重。生活是沈重的，而感觉越来越轻，但又长了厚厚的茧，我们忙著打理一个共处的窝，多于经营关系。或者所有由聚而散的故事，都免不了这段剧情的线索，只顾著修理坏掉的电器，顾不上修剪过长的脚甲。</p><p>亲爱的，我忽然不想过著顾店般的日子，琐碎麻烦，连一个人去一趟京都，看看金阁寺的静美，都显得不太合适－我不是嫌弃有你在旁，这何尝不曾是我梦寐以求的画面，只是有时候会很想静静一个人看天赏地，而无需有熟人听我说感受、或给我当评述员，你也会偶然这样吧。</p><p>当我自诩是个重感情的人，觉得不好意思把你当包袱看待时，才又第一次怀疑，感情只像趾甲，更行更远还生，爱情呢？</p><p>于是就分了、松了、清空了。</p><p>曾经有个朋友，在失恋后表现失措，听了半杯水的寓言后，立时就没事了，痛得真快，真痛快。半杯水是半满还是半空，我并不关心这问题的答案，因为如今我像个清空了杯，水都泼光了，什么包袱都没有了。人说，空杯才容得下活水，空腹才吃得下美食，空，才容纳到福气，诸如此类。</p><blockquote><p>水的暗喻让我想到了黄伟文所写的《多喝水》：</p><p>若这天 讲到他 还是死穴<br>爱共仇 不去解决 还会作孽<br>为时未晚 怨毒每天清洁<br>饮多杯水 冲开心底那 蝴蝶结</p></blockquote><p>而我的空啊，我看著个空瓶子，玻璃般透明，却看到了爱确实存在过。不是因为什么失去才懂得珍惜，而是能把感情当作包袱看待，正因为当中有爱情，否则，如同一张坐久了的沙发，谁会起了那个劲去换掉？就一直坐下去好了。</p><p>因为我们都不够懒，才会有今天的结果吧。因为不能再这样下去，才证明我爱你、因为不能再忍受，才发现爱的感受、因为怀疑，才重新相信，这信念，可算悲欣交集。</p><p>「你是千堆雪，我是长街，怕日出一到，彼此瓦解。」这没什么可怕，谁是雪谁是街也好，爱情需要个实践的地方，如果这是一个家，烦心事就多了，要符合的条件就多了。条件即是因缘，因缘际会，家就似家，条件消失了，家不像家；如春暖雪融，家的前身，原来是条荒街。之前能冰封三尺，自非一日之寒，因为冬寒，我们有相依取暖的必要与条件，反而风和日丽就……这样的真相，很容易理解，亦很难消化，再排泄成养分。</p><blockquote><p>《临崖勒马》：</p><p>就快被破坏沦落家不似家</p><p>若嫩芽突然全部腐化</p><p>浑浊浓雾弥漫难淡化</p><p>太可怕 这代价</p></blockquote><p>这天，我轻身上路，来到京都金阁寺，没有包袱也不再是谁的包袱，心静如初雪无声。亲爱的，这种释然，竟让我流完快乐的眼泪，再微笑得有点伤感。</p><h2 id="摘抄"><a href="#摘抄" class="headerlink" title="摘抄"></a>摘抄</h2><h3 id="《金阁寺》"><a href="#《金阁寺》" class="headerlink" title="《金阁寺》"></a><strong>《金阁寺》</strong></h3><ul><li>我想当艺术家，又过于傲慢，做一名暴君或大艺术家吧，但仅仅停留于幻想，丝毫不愿意着手干一点儿实际的事情。</li><li>我唯一的自豪，就是不被人理解，所以未曾有过一次让人理解我的冲动的表现。我认为，自己命中注定不为他人所注意。孤独越来越肥硕，简直就像一头猪。</li><li>本来，我的生活和外界几乎无缘，所以幻想着一旦闯入外界，一切都会变得轻而易举、迎刃而解了。</li><li>此时，我感到自己化作了一块顽石，意志和欲望，一切都变成了石头。同我的内心毫无关系，外界确乎再次存在于我的周围。</li><li>没等我参与，现实就横在眼前，而且带着从未见过的重负。这毫无意义的浩大的黑暗的现实，不由分说，迎头向我压迫过来。</li><li>我诅咒有为子快死。几个月后，我的诅咒实现了。自那之后，我确信诅咒是很灵验的。</li></ul><h3 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a><strong>第四章</strong></h3><ul><li>这时，我体内产生了异样的冲动，如同满心重要的话语被口吃妨碍着一时难于迸发，这冲动只是在喉咙管里燃烧。我想寻求解脱。</li><li>我们处于同世界对立的状态，这种可怕的不满需要通过世界和我们某一方的变化获得治愈。</li><li>我们突然变得残暴是在一瞬之间，就像春日和煦的午后，坐在悉心修剪的草地上，朦胧眺望由树叶间漏泄下来的阳光，那种一眨眼的工夫。</li></ul><h3 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a><strong>第五章</strong></h3><ul><li>柏木暗示于我并当着我的面表演的人生，生存和毁灭只具有同一种意义。这种人生既缺少自然，那么，也缺少金阁一般的结构之美，可以说只是一种痛楚的痉挛而已。</li><li>“未知的人生是难以忍受的。”</li><li>就越发证明他对人生的一片忠诚</li></ul><h3 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a><strong>第六章</strong></h3><ul><li>自少年时代起，不为人所理解成为我唯一的骄傲。</li><li>鹤川暴死之后，我未曾接触过生这个东西。过了很久，才开始接触个别非薄命的、更加黯淡的生，也就是只要继续活着就不断伤害他人的生的律动，并由此而受到鼓舞。</li></ul><h3 id="第七章"><a href="#第七章" class="headerlink" title="第七章"></a><strong>第七章</strong></h3><ul><li>我若多少有些人的感情，就无法不期待对方对我也要有相应的感情，不管是爱还是憎。</li><li>我一任心脏怦怦地跳动，在这之前，我从未怀着希望等待过什么，这回的作孽只是期待着老师的憎恨。然而，我又幻想着人与人之间互相理解的戏剧性的热情洋溢的场面。</li><li>“我想逃避周围的一切。我的周围臭气熏天，都是些无能的气息……老师也无能，非常无能。这我很清楚。”</li><li>这里才是真正的内日本的海！是我一切不幸和灰暗思想的源泉，是我所有的丑行和力量的源泉！</li></ul><h3 id="第八章"><a href="#第八章" class="headerlink" title="第八章"></a><strong>第八章</strong></h3><ul><li><p>打过来的手指乱了，指头的力量丧失了，指尖如细冰粒落在了脸上。</p></li><li><p>“你净给我出难题啊！干了这等事，今后就不能在寺里待下去了，你想过没有？其他还有种种……”老师说了半截，大概顾忌着在场的柏木，不再说下去了，“钱我替你还，你回去吧。”</p></li><li><p><strong>2025&#x2F;02&#x2F;17 发表想法</strong></p><p>回归主题，这桩不幸的经历其实是“我”不幸的心灵造就的。“我”天生有一副黯淡的心灵，从来没有体验过欢乐明朗的感情。 一个选择了自我毁灭，一个选择了与金阁寺共同毁灭。</p><blockquote><p>原文：“如今想想，这桩不幸的恋爱其实是我不幸的心灵造就的。我天生有一副黯淡的心灵，从来没有体验过欢乐明朗的感情。”</p></blockquote></li><li><p>“如今想想，这桩不幸的恋爱其实是我不幸的心灵造就的。我天生有一副黯淡的心灵，从来没有体验过欢乐明朗的感情。”</p></li><li><p>“忍耐生命有没有别的办法可想呢？”“没有。剩下的要么发狂，要么死去。”</p></li></ul><h3 id="第九章"><a href="#第九章" class="headerlink" title="第九章"></a><strong>第九章</strong></h3><ul><li>于是，一种被所有人抛弃的感觉袭击着我。</li></ul><h3 id="第十章"><a href="#第十章" class="headerlink" title="第十章"></a><strong>第十章</strong></h3><ul><li>我的内心和外界之间这把生锈的锁顺利地打开了，内心和外界打通了，风从那里自由地吹过。</li><li>从这里看不见金阁的外形，只能看见翻滚的烟雾和冲天的火光。林木之间飘扬着众多的火粉，金阁的上空像遍洒着金沙子。我紧抱膝头，</li></ul><h3 id="译后记"><a href="#译后记" class="headerlink" title="译后记"></a><strong>译后记</strong></h3><ul><li>故而，我译《金阁寺》，满心如秋风般悲凉。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GELU函数以其近似</title>
    <link href="/2025/20250216/"/>
    <url>/2025/20250216/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>GELU，即Gaussian Error Linear Unit，在论文《Gaussian Error Linear Units (GELUs)》提出，被广泛运用于各大LLM中。</p><span id="more"></span><h2 id="GELU函数"><a href="#GELU函数" class="headerlink" title="GELU函数"></a>GELU函数</h2><p>正如名字所说，他与高斯分布&#x2F;正态分布有关。<br>$$<br>\begin{equation}\text{GELU}(x)&#x3D;x \Phi(x)\end{equation}<br>$$<br>其中<br>$$<br>\begin{equation}\Phi(x)&#x3D;\int_{-\infty}^x \frac{e^{-t^2&#x2F;2}}{\sqrt{2\pi}}dt&#x3D;\frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]\end{equation}<br>$$</p><p><img src="/2025/20250216/gelu.png"></p><p>由于erf并不好求，故有以下近似。</p><h2 id="近似1"><a href="#近似1" class="headerlink" title="近似1"></a>近似1</h2><p>$$<br>\begin{equation}x\Phi(x)\approx x\sigma(1.702 x)\label{eq:x-sigma}\end{equation}<br>$$</p><p>$x\sigma(\beta x)$其实也被叫做swish，当$\beta&#x3D;1$时，叫做SiLU</p><p>Swish由Google brain于2017年提出。</p><p>他遍历了所有激活函数，并得出最好的是Swish。</p><p>定义的激活函数如下：</p><p><img src="/2025/20250216/swish.jpg"></p><p>其中</p><ul><li><p>Unary functions:<br>$x - x, |x|, x^2, x^3, \sqrt{x}, \beta x, x + \beta, \log(|x| + t), exp(x) \sin(x), cos(x)$,<br>$sinh(x), cosh(x), tanh(x), sinh^{-1}(x), tan^{-1}(x), sinc(x), max(x,0), min(x,0), \sigma(x)$,<br>$\log(1 + exp(x)), exp(-x^2), erf(x), \beta$</p></li><li><p>Binary functions:<br>$x_1 + x_2, x_1 \times x_2, x_1 - x_2, \frac{x_1}{x_2 + 1}, max(x_1, x_2), min(x_1, x_2), \sigma(x_1) \cdot x_2$,<br>$exp(-\beta (x_1 - x_2)^2), exp(-\beta |x_1 - x_2|), \beta x_1 + (1 - \beta)x_2$</p></li></ul><p>也有一篇论文从数学的角度上求解最佳的$\beta$：<a href="https://core.ac.uk/download/pdf/41787448.pdf">《A logistic approximation to the cumulative normal distribution》</a></p><h2 id="近似2"><a href="#近似2" class="headerlink" title="近似2"></a>近似2</h2><p>$$<br>\begin{equation}x\Phi(x)\approx \frac{1}{2} x \left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715 x^3\right)\right)\right]\label{eq:x-phi}\end{equation}<br>$$</p><p>可以追述到1977年的<a href="https://www.jstor.org/stable/2346872">《Approximations to the Cumulative Normal Function and its Inverse for Use on a Pocket Calculator》</a></p><p>它基于1963年Tocher提出的：<br>$$<br>F ( x )&#x3D;\int_{-\infty}^{x} ! {\frac{1} {\sqrt{( 2 \pi)}}} , e^{-\frac{1}{2} u^{2}} d u \simeq e^{2 k x} &#x2F; ( 1+e^{2 k x} )&#x3D;\frac{1}{2}(1+tanh(kx)), \ \ k&#x3D;\sqrt{( 2 &#x2F; \pi)}<br>$$<br>在其论文中，进一步对$e^{2 k x} &#x2F; ( 1+e^{2 k x} )$改进为$e^{2 k_1 x(1+k_2x^2)} &#x2F; ( 1+e^{2 k_1 x(1+k_2x^2)} )$，并得到以上的近似。。</p><p>但论文实际并没有给出具体使用什么方法求解出最佳系数。</p><p>PS：<a href="https://spaces.ac.cn/archives/7309">苏神</a>对以上进一步使用程序的寻找最佳系数。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mixture of Experts(MoE)</title>
    <link href="/2025/20250215/"/>
    <url>/2025/20250215/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>MOE是当前比较火的技术之一。比如Mistral、当前最火的deepseek都用到了这一技术。</p><p>MOE具有预训练速度更快，推理速度更快的性质。但泛化能力不足，对显存需求比较高。</p><span id="more"></span><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>MOE起源于1991的《Adaptive mixture of local experts》</p><p>原论文和BP比，且有着不错的性能。</p><p>从其论文图像就可以看出MOE的雏形和如今很像。</p><p><img src="/2025/20250215/begin.png"></p><h2 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h2><p>在LLM潮浪之前，谷歌多个团队都有着几篇惊艳的论文。</p><h3 id="2013年的《Learning-Factored-Representations-in-a-Deep-Mixture-of-Experts》"><a href="#2013年的《Learning-Factored-Representations-in-a-Deep-Mixture-of-Experts》" class="headerlink" title="2013年的《Learning Factored Representations in a Deep Mixture of Experts》"></a>2013年的《Learning Factored Representations in a Deep Mixture of Experts》</h3><p>作者有Ilya Sutskever。</p><p>架构图：</p><p><img src="/2025/20250215/DMOE.png"></p><p>可以看出在原始的基础上还更多考虑了深度。</p><p>作者发现，Deep Mix of Experts 自动学习在第一层培养位置相关（“where”）的专家，在第二层自动学习培养特定于类（“what”）的专家。</p><h3 id="2017年的《Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer》"><a href="#2017年的《Outrageously-Large-Neural-Networks-The-Sparsely-Gated-Mixture-of-Experts-Layer》" class="headerlink" title="2017年的《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》"></a>2017年的《Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer》</h3><p>作者有Jeff Dean和Hinton。</p><p>架构图:</p><p><img src="/2025/20250215/jeffMOE.png"></p><p>稀疏门控函数选择两个专家来执行计算。他们的输出由门控网络的输出调制。类似于后面的Top2 MOE。</p><h3 id="2020年的《GShard-Scaling-Giant-Models-with-Conditional-Computation-and-Automatic-Sharding》"><a href="#2020年的《GShard-Scaling-Giant-Models-with-Conditional-Computation-and-Automatic-Sharding》" class="headerlink" title="2020年的《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》"></a>2020年的《GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding》</h3><p>架构图也是类似的：</p><p><img src="/2025/20250215/gshard.png"></p><p>在前人的基础上增加了负载均衡、Auxiliary loss、为了能在多个GPU上并行运行的切片等相关操作。</p><h2 id="基础版本MOE"><a href="#基础版本MOE" class="headerlink" title="基础版本MOE"></a>基础版本MOE</h2><p>MOE是FFN的替代品，其中的Expert 一般是一个 FeadFoward Network，FFN。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicExpert</span>(nn.Module):<br>    <span class="hljs-comment"># 一个 Expert 可以是一个最简单的， linear 层即可</span><br>    <span class="hljs-comment"># 也可以是 MLP 层</span><br>    <span class="hljs-comment"># 也可以是 更复杂的 MLP 层（active function 设置为 swiglu）</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, feature_in, feature_out</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.linear = nn.Linear(feature_in, feature_out)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.linear(x)<br></code></pre></td></tr></table></figure><p><img src="/2025/20250215/basic-moe-model.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicMOE</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, feature_in, feature_out, expert_number</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.experts = nn.ModuleList(<br>            [<br>                BasicExpert(feature_in, feature_out) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(expert_number)<br>            ]<br>        )<br>        <span class="hljs-comment"># gate 就是选一个 expert </span><br>        <span class="hljs-variable language_">self</span>.gate = nn.Linear(feature_in, expert_number)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x 的 shape 是 （batch, feature_in)</span><br>        expert_weight = <span class="hljs-variable language_">self</span>.gate(x)  <span class="hljs-comment"># shape 是 (batch, expert_number)</span><br>        expert_out_list = [<br>            expert(x).unsqueeze(<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> expert <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.experts<br>        ]  <span class="hljs-comment"># 里面每一个元素的 shape 是： (batch, ) ??</span><br><br>        <span class="hljs-comment"># concat 起来 (batch, expert_number, feature_out)</span><br>        expert_output = torch.cat(expert_out_list, dim=<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># print(expert_output.size())</span><br><br>        expert_weight = expert_weight.unsqueeze(<span class="hljs-number">1</span>) <span class="hljs-comment"># (batch, 1, expert_nuber)</span><br><br>        <span class="hljs-comment"># expert_weight * expert_out_list</span><br>        output = expert_weight @ expert_output  <span class="hljs-comment"># (batch, 1, feature_out)</span><br>        <br>        <span class="hljs-keyword">return</span> output.squeeze()<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_basic_moe</span>():<br>    x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br><br>    basic_moe = BasicMOE(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>)<br>    out = basic_moe(x)<br>    <span class="hljs-built_in">print</span>(out)<br><br><br>test_basic_moe()<br></code></pre></td></tr></table></figure><h2 id="SparseMoE-（大模型训练使用）"><a href="#SparseMoE-（大模型训练使用）" class="headerlink" title="SparseMoE （大模型训练使用）"></a>SparseMoE （大模型训练使用）</h2><p>比较有名的是switch transformer。</p><p>编码器解码器结构</p><p>1.6万亿参数的Moe, 2048个专家</p><p>网络结构升级</p><p>预训练速度为T5-XXL的4倍</p><p><img src="/2025/20250215/switch-transformers-moe-model.png"></p><p>和 Basic 区别是，MOE 选择 topK 个专家，然后对这 topK 个专家的输出进行加权求和，并且把输入样本变成了大模型中真实的输入 Shape，(batch, seq_len, hidden_dim)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-comment"># 主要参考自 mistral MOE 的实现</span><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MOERouter</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_dim, expert_number, top_k</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.gate = nn.Linear(hidden_dim, expert_number)<br>        <span class="hljs-variable language_">self</span>.expert_number = expert_number<br>        <span class="hljs-variable language_">self</span>.top_k = top_k<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):<br>        <span class="hljs-comment"># 计算路由logits</span><br>        router_logits = <span class="hljs-variable language_">self</span>.gate(hidden_states)  <span class="hljs-comment"># shape is (b * s, expert_number)</span><br>        <br>        <span class="hljs-comment"># 计算专家经过softmax之后的概率</span><br>        routing_probs = F.softmax(router_logits, dim=-<span class="hljs-number">1</span>, dtype=torch.<span class="hljs-built_in">float</span>)<br>        <br>        <span class="hljs-comment"># 计算topk的专家的输出</span><br>        router_weights, selected_experts = torch.topk(<br>            routing_probs, <span class="hljs-variable language_">self</span>.top_k, dim=-<span class="hljs-number">1</span><br>        )  <span class="hljs-comment"># shape都是 (b * s, top_k)</span><br>        <br>        <span class="hljs-comment"># 专家权重归一化</span><br>         router_weights = router_weights / router_weights.<span class="hljs-built_in">sum</span>(dim=-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        router_weights = router_weights.to(hidden_states.dtype)<br>        <br>        <span class="hljs-comment"># 生成专家掩码</span><br>        expert_mask = F.one_hot(<br>            selected_experts,<br>            num_classes=<span class="hljs-variable language_">self</span>.expert_number<br>        )  <span class="hljs-comment"># shape是 (b * s, top_k, expert_number)</span><br>        expert_mask = expert_mask.permute(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>)  <span class="hljs-comment"># (expert_number, top_k, b * s)</span><br>        <br>        <span class="hljs-keyword">return</span> router_logits, router_weights, selected_experts, expert_mask<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MOEConfig</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params"></span><br><span class="hljs-params">            self, </span><br><span class="hljs-params">            hidden_dim, </span><br><span class="hljs-params">            expert_number, </span><br><span class="hljs-params">            top_k, </span><br><span class="hljs-params">            shared_experts_number=<span class="hljs-number">2</span>,</span><br><span class="hljs-params">        </span>):<br>        <span class="hljs-variable language_">self</span>.hidden_dim = hidden_dim<br>        <span class="hljs-variable language_">self</span>.expert_number = expert_number<br>        <span class="hljs-variable language_">self</span>.top_k = top_k<br>        <span class="hljs-variable language_">self</span>.shared_experts_number = shared_experts_number<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SparseMOE</span>(nn.Module):<br>    <span class="hljs-comment"># 稀疏 MOE 模型，这里每一个 token 都会过 topk 个专家，得到对应token 的 hidden_embeddings</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-variable language_">self</span>.hidden_dim = config.hidden_dim<br><br>        <span class="hljs-variable language_">self</span>.expert_number = config.expert_number<br>        <span class="hljs-variable language_">self</span>.top_k = config.top_k<br><br>        <span class="hljs-variable language_">self</span>.experts = nn.ModuleList(<br>            [<br>                BasicExpert(<span class="hljs-variable language_">self</span>.hidden_dim, <span class="hljs-variable language_">self</span>.hidden_dim) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.expert_number)<br>            ]<br>        )<br><br>        <span class="hljs-variable language_">self</span>.router = MOERouter(<span class="hljs-variable language_">self</span>.hidden_dim, <span class="hljs-variable language_">self</span>.expert_number, <span class="hljs-variable language_">self</span>.top_k)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x shape is (b, s, hidden_dim)</span><br>        batch_size, seq_len, hidden_dim = x.size()<br><br>        <span class="hljs-comment"># 合并前两个维度，因为不是 Sample 维度了，而是 token 维度</span><br>        hidden_states = x.view(-<span class="hljs-number">1</span>, hidden_dim) <span class="hljs-comment"># shape is(b * s, hidden_dim)</span><br><br>        router_logits, router_weights, selected_experts_indices, expert_mask = <span class="hljs-variable language_">self</span>.router(hidden_states)<br>        <span class="hljs-comment"># 其中 selected_experts_indices shape 是 (b * s, top_k)</span><br>        <span class="hljs-comment"># 其中 expert_mask shape 是 (expert_number, top_k, b * s)</span><br>        <br>        final_hidden_states = torch.zeros(<br>            (batch_size * seq_len, hidden_dim),<br>            dtype=hidden_states.dtype,<br>            device=hidden_states.device<br>        )<br><br>        <span class="hljs-keyword">for</span> expert_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.expert_number):<br>            expert_layer = <span class="hljs-variable language_">self</span>.experts[expert_idx]<br>            <span class="hljs-comment"># expert_mask[expert_idx] shape 是 (top_k, b * s)</span><br>            idx, top_x = torch.where(expert_mask[expert_idx]) <br>            <span class="hljs-comment"># idx 和 top_x 都是一维 tensor</span><br>            <span class="hljs-comment"># idx 的值是 0 或 1, 表示这个 token 是作为当前专家的 top1 还是 top2</span><br>            <span class="hljs-comment"># top_x 的值是 token 在 batch*seq_len 中的位置索引</span><br>            <span class="hljs-comment"># 例如对于 batch_size=2, seq_len=4 的输入:</span><br>            <span class="hljs-comment"># top_x 的值范围是 0-7, 表示在展平后的 8 个 token 中的位置</span><br>            <span class="hljs-comment"># idx 的值是 0/1, 表示这个 token 把当前专家作为其 top1/top2 专家</span><br><br>            <span class="hljs-comment"># hidden_states 的 shape 是 (b * s, hidden_dim)</span><br>            <span class="hljs-comment"># 需要取到 top_x 对应的 hidden_states</span><br>            current_state = hidden_states.unsqueeze(<br>                <span class="hljs-number">0</span><br>            )[:, top_x, :].reshape(-<span class="hljs-number">1</span>, hidden_dim) <span class="hljs-comment"># （selected_token_number, hidden_dim）</span><br><br>            <span class="hljs-comment"># router_weight 的 shape 是 (b * s, top_k)</span><br>            current_hidden_states = expert_layer(<br>                current_state<br>            ) * router_weights[top_x, idx].unsqueeze(-<span class="hljs-number">1</span>)  <span class="hljs-comment"># （selected_token_number, 1） 这里有广播</span><br><br>            <span class="hljs-comment"># 把当前专家的输出加到 final_hidden_states 中</span><br>            <span class="hljs-comment"># 方式1 的写法性能更好，并且方式1容易出现</span><br>            final_hidden_states.index_add_(<span class="hljs-number">0</span>, top_x, current_hidden_states.to(hidden_states.dtype))<br>            <span class="hljs-comment"># 方式2</span><br>            <span class="hljs-comment"># final_hidden_states[top_x] += current_hidden_states.to(hidden_states.dtype)</span><br>            <span class="hljs-comment"># 方式1 的写法性能更差，并且方式1容易出现错误，+= 操作在处理重复索引时需要多次读写内存，可能会导致竞争条件</span><br><br>        <span class="hljs-comment"># 把 final_hidden_states 还原到原来的 shape</span><br>        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)<br><br>        <span class="hljs-keyword">return</span> final_hidden_states, router_logits <span class="hljs-comment"># shape 是 (b * s, expert_number)</span><br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_token_level_moe</span>():<br>    x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">16</span>)<br>    config = MOEConfig(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    token_level_moe = SparseMOE(config)<br>    out = token_level_moe(x)<br>    <span class="hljs-built_in">print</span>(out[<span class="hljs-number">0</span>].shape, out[<span class="hljs-number">1</span>].shape)<br><br><br>test_token_level_moe()<br></code></pre></td></tr></table></figure><h3 id="数学理解"><a href="#数学理解" class="headerlink" title="数学理解"></a>数学理解</h3><p>苏神对这一类型的MOE做了一个偏数学的分析。</p><p><strong>1、一个常规的Dense模型FFN，可以等价改写为n个Expert向量v1,v2,⋯,vn之和；</strong></p><p>FFN可以写为<br>$$<br>\begin{equation}\boldsymbol{y} &#x3D; f(\boldsymbol{x}\boldsymbol{W}^{(A)})\boldsymbol{W}^{(B)}\end{equation}<br>$$<br>即</p><p><img src="/2025/20250215/math1.png"></p><p><strong>2、为了节省计算量，我们试图挑出k个向量求和来逼近原本的n个向量之和；</strong></p><p>MOE要解决的问题是：</p><p><strong>能否只挑k个向量的和来逼近n个向量的和呢？这样就可以将计算量降低到k&#x2F;n了。</strong></p><p><strong>3、转化为数学问题求解后，我们发现挑选规则是模长最大的k个向量；</strong></p><p>写成数学公式是</p><p><img src="/2025/20250215/math2.png"></p><p>记$\gamma_i &#x3D; 1 - \lambda_i$，则</p><p><img src="/2025/20250215/math3.png"></p><p>在这里苏神强行地假设了$v_i$两两正交。（我觉得这个可以和<strong>高维空间任取两个向量几乎正交的性质</strong>结合起来）</p><p>则上式最优解显然就是让模长$||v_i||$最小的$n-k$个$\gamma_i$等于1。</p><p><strong>4、直接去算n个Expert的模长然后选k个实际上是不省计算量的，所以要重新设计Expert；</strong></p><p><strong>5、将$v_i$归一化得到$e_i$，然后用另外的小模型（Router）预测模长$p_i$，最终的Expert为$p_ie_i$；</strong></p><p>归一化即$\boldsymbol{e}_i &#x3D; \boldsymbol{v}_i&#x2F; \Vert\boldsymbol{v}_i\Vert$</p><p>我们也不一定用L2 Normalize，也可以gamma参数恒等于1的RMS Norm等等等。</p><p>PS：当实际上当前主流的MOE都没有归一化操作，苏神也在他博客的评论区指出。</p><p><strong>6、此时，我们就可以先算全体$p_i$，挑出k个后才去计算$e_i$，达到节省计算量的目的。</strong></p><h2 id="ShareExpert-SparseMoE-（deepseek-版本）"><a href="#ShareExpert-SparseMoE-（deepseek-版本）" class="headerlink" title="ShareExpert SparseMoE （deepseek 版本）"></a>ShareExpert SparseMoE （deepseek 版本）</h2><p>和前面的sparsemoe不同的是，有一个shared experts 的模型，是所有 token 共享的。</p><p><img src="/2025/20250215/deepseek-v3-model-architecture.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ShareExpertMOE</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        <span class="hljs-variable language_">self</span>.moe_model = SparseMOE(config)<br>        <span class="hljs-variable language_">self</span>.shared_experts = nn.ModuleList(<br>            [<br>                BasicExpert(<br>                    config.hidden_dim, config.hidden_dim<br>                ) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.shared_experts_number)<br>            ]<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x shape 是 (b, s, hidden_dim)</span><br>        <span class="hljs-comment"># 首先过 moe 模型</span><br>        sparse_moe_out, router_logits = <span class="hljs-variable language_">self</span>.moe_model(x)<br>        <br>        <span class="hljs-comment"># 针对的还是 x 的每一个 </span><br>        <span class="hljs-comment"># 然后过 shared experts</span><br>        shared_experts_out = [<br>            expert(x) <span class="hljs-keyword">for</span> expert <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.shared_experts<br>        ] <span class="hljs-comment"># 每一个 expert 的输出 shape 是 (b, s, hidden_dim)</span><br>        <br>        shared_experts_out = torch.stack(<br>            shared_experts_out, dim=<span class="hljs-number">0</span><br>        ).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br>        <br>        <span class="hljs-comment"># 把 sparse_moe_out 和 shared_experts_out 加起来</span><br>        <span class="hljs-keyword">return</span> sparse_moe_out + shared_experts_out, router_logits<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_share_expert_moe</span>():<br>    x = torch.rand(<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">16</span>)<br>    config = MOEConfig(<span class="hljs-number">16</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)<br>    share_expert_moe = ShareExpertMOE(config)<br>    out = share_expert_moe(x)<br>    <span class="hljs-built_in">print</span>(out[<span class="hljs-number">0</span>].shape, out[<span class="hljs-number">1</span>].shape)<br><br><br>test_share_expert_moe()<br></code></pre></td></tr></table></figure><h2 id="模型训练测试代码"><a href="#模型训练测试代码" class="headerlink" title="模型训练测试代码"></a>模型训练测试代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">switch_load_balancing_loss</span>(<span class="hljs-params">router_logits: torch.Tensor, num_experts: <span class="hljs-built_in">int</span></span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    计算 Switch Transformers 的负载均衡损失</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        router_logits: shape [batch_size * sequence_length, num_experts]</span><br><span class="hljs-string">        num_experts: 专家数量</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        total_loss: 总损失 = auxiliary_loss + z_loss</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算路由概率</span><br>    router_probs = torch.softmax(router_logits, dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># [b*s, num_experts]</span><br>    <br>    <span class="hljs-comment"># 获取每个token的最优专家</span><br>    _, selected_experts = torch.topk(router_probs, k=<span class="hljs-number">2</span>, dim=-<span class="hljs-number">1</span>) <br>    <br>    <span class="hljs-comment"># 创建one-hot矩阵表示选中的专家</span><br>    mask = torch.nn.functional.one_hot(selected_experts, num_experts).<span class="hljs-built_in">float</span>() <br>    <br>    <span class="hljs-comment"># 计算每个专家的期望负载 (理想情况下应该是 1/num_experts)</span><br>    expected_load = torch.ones_like(router_probs) / num_experts<br>    <br>    <span class="hljs-comment"># 计算实际负载 (每个专家处理的token数量除以总token数量)</span><br>    <span class="hljs-comment"># 在batch维度上计算平均值</span><br>    actual_load = mask.mean(dim=<span class="hljs-number">0</span>)<br>    <br>    <span class="hljs-comment"># 计算auxiliary loss</span><br>    <span class="hljs-comment"># 这会惩罚负载分布与期望负载的差异</span><br>    aux_loss = torch.<span class="hljs-built_in">sum</span>(actual_load * router_probs.mean(dim=<span class="hljs-number">0</span>)) * num_experts<br>    <br>    <span class="hljs-comment"># 计算z_loss (可选)</span><br>    <span class="hljs-comment"># 这会惩罚过大的路由logits</span><br>    z_loss = torch.mean(torch.square(router_logits))<br>    z_loss_weight = <span class="hljs-number">0.001</span>  <span class="hljs-comment"># 可调整的超参数</span><br>    <br>    <span class="hljs-comment"># 总损失</span><br>    total_loss = aux_loss + z_loss * z_loss_weight<br>    <br>    <span class="hljs-keyword">return</span> total_loss<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">test_moe_training</span>():<br>    <span class="hljs-comment"># Create a simple dataset</span><br>    batch_size = <span class="hljs-number">32</span><br>    seq_len = <span class="hljs-number">16</span><br>    hidden_dim = <span class="hljs-number">32</span><br>    num_batches = <span class="hljs-number">100</span><br>    <br>    <span class="hljs-comment"># Initialize model and optimizer</span><br>    config = MOEConfig(hidden_dim=hidden_dim, <br>                      expert_number=<span class="hljs-number">4</span>,<br>                      top_k=<span class="hljs-number">2</span>,<br>                      shared_experts_number=<span class="hljs-number">2</span>)<br>    model = ShareExpertMOE(config)<br>    optimizer = torch.optim.Adam(model.parameters(), lr=<span class="hljs-number">0.001</span>)<br>    <br>    <span class="hljs-comment"># Training loop</span><br>    model.train()<br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):<br>        <span class="hljs-comment"># Generate random input data</span><br>        x = torch.randn(batch_size, seq_len, hidden_dim)<br>        target = torch.randn(batch_size, seq_len, hidden_dim)<br>        <br>        <span class="hljs-comment"># Forward pass</span><br>        output, router_logits = model(x)<br><br>        <span class="hljs-comment"># Compute losses</span><br>        <span class="hljs-comment"># MSE loss for prediction</span><br>        mse_loss = F.mse_loss(output, target)<br>        <br>        aux_loss = switch_load_balancing_loss(router_logits, config.expert_number)<br>        <span class="hljs-comment"># Combined loss</span><br>        total_loss = mse_loss + <span class="hljs-number">0.01</span> * aux_loss<br>        <br>        <span class="hljs-comment"># Backward pass and optimize</span><br>        optimizer.zero_grad()<br>        total_loss.backward()<br>        optimizer.step()<br>        <br>        <span class="hljs-keyword">if</span> batch % <span class="hljs-number">10</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Batch <span class="hljs-subst">&#123;batch&#125;</span>, Loss: <span class="hljs-subst">&#123;total_loss.item():<span class="hljs-number">.4</span>f&#125;</span> &quot;</span><br>                  <span class="hljs-string">f&quot;(MSE: <span class="hljs-subst">&#123;mse_loss.item():<span class="hljs-number">.4</span>f&#125;</span>, Aux: <span class="hljs-subst">&#123;aux_loss.item():<span class="hljs-number">.4</span>f&#125;</span>)&quot;</span>)<br><br><span class="hljs-comment"># Run the training test</span><br>test_moe_training()<br></code></pre></td></tr></table></figure><h2 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h2><p>（1）冻结MOE，这个更好，和全量更新相当。</p><p>（2）冻结除MOE以外的，类似于迁移学习。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li><a href="https://bruceyuan.com/llms-zero-to-hero/the-way-of-moe-model-evolution.html">LLM MOE的进化之路，从普通简化 MOE，到 sparse moe，再到 deepseek 使用的 share_expert sparse moe</a></li><li><a href="https://kexue.fm/archives/10699">MoE环游记：1、从几何意义出发</a></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>TPSS5E1 复盘</title>
    <link href="/2025/20250210/"/>
    <url>/2025/20250210/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://www.kaggle.com/competitions/playground-series-s5e1">预测贴纸销量 | Kaggle — Forecasting Sticker Sales | Kaggle</a>是我参加最久的一次TPS（<strong>2025&#x2F;04&#x2F;01更新：</strong> 3月又全力参加了一次，排名18&#x2F;4381，排名仍达不到拿swag的名次，但成为唯二的在shakeup中留存的top选手也算差强人意。不得不说第2的chiris是真的强），但成绩不够理想，只拿到了27&#x2F;2722，其中一个原因是一直参考@<a href="https://www.kaggle.com/cabaxiom">Cabaxiom</a>的线性回归笔记本，但是其中年份product存在计算错误的问题。</p><span id="more"></span><hr><p>之前TPS也有个<a href="https://www.kaggle.com/competitions/playground-series-s3e19">类似的比赛</a>，可以说是之前的翻版，所以很多solution也是参考了过去的思路。</p><h2 id="之前的第一名方案"><a href="#之前的第一名方案" class="headerlink" title="之前的第一名方案"></a>之前的第一名方案</h2><p><a href="https://www.kaggle.com/code/ivyzang/1st-place-solution-less-is-more">https://www.kaggle.com/code/ivyzang/1st-place-solution-less-is-more</a></p><h3 id="EDA"><a href="#EDA" class="headerlink" title="EDA"></a>EDA</h3><p>在检查商店后，发现每个商店都显示相似的销售额分布，仅差一个标量常数。</p><p><img src="/2025/20250210/less1.png"></p><p>在每年的每个国家中，num_sold 都遵循着一致的规律，尽管不同国家之间的年度变化幅度有所不同。</p><p><img src="/2025/20250210/less2.png"></p><p>使用归一化GDP后，发现仍有显然的趋势，因此可以使用 GDP 作为外部预测因子。</p><p><img src="/2025/20250210/less3.png"></p><p>产品具有周期性。</p><p><img src="/2025/20250210/less4.png"></p><p>疫情的影响。</p><p><img src="/2025/20250210/less5.png"></p><p>周一至周四的分布相同，而周五至周日的分布不同。</p><p><img src="/2025/20250210/less6.png"></p><p>具有节假日效应（即节假日前后消费更高）。且具有后续的“圣诞节和元旦的节假日效应可能会出现覆盖或影响的情况”，作者故新建了一个新年因子。</p><h3 id="第一轮分析"><a href="#第一轮分析" class="headerlink" title="第一轮分析"></a>第一轮分析</h3><p>乘性线性模型：<br>$$<br>\begin{equation} \text{sold} &#x3D; \text{GDP}(\text{country}, \text{year}) \times \text{const}(\text{store}) \times \left[ \text{sine&#x2F;cosine waves}(\text{product}) + \text{holiday} + \text{weekday} + \text{covid} \right] \end{equation}<br>$$</p><p>因为是乘法所以取对数，并利用$lnx\sim x-1$，对除了GDP以外的元素做此操作。</p><p>检查残差，作者发现：年末奇怪的直线和一些奇怪的持续水平线</p><p><img src="/2025/20250210/less7.png"></p><p>检查发现是产品’Using LLMs to Win Friends and Influence People’引起的。</p><p>继续分析数据，作者观察到 Kagglazon 的”num_sold”呈现出良好的正弦波。然而，对于阿根廷和爱沙尼亚这两个国家，Kaggle Learn 和 Kaggle Store 的”num_sold”则表现为直线。</p><p><img src="/2025/20250210/less8.png"></p><p>这引发了这样一个问题：这些商店是否每天都在销售相同数量的产品？作者继续深入挖掘数据。</p><p><img src="/2025/20250210/less9.png"></p><p>作者检查直线，发现：下面的代码从时间序列数据中识别潜在的候选国家特定假日。通过计算残差并根据国家和一年中的日期进行分组，它使用 z 分数检测显著偏差。这些偏差可能表明假日或特殊事件会影响销售模式</p><table><thead><tr><th align="left">country 国家</th><th align="left">dayfix 日修正</th><th align="left">residual 残差</th><th align="left">z_score 标准分数</th><th align="left">date 日期</th></tr></thead><tbody><tr><td align="left">Argentina 阿根廷</td><td align="left">363</td><td align="left">-0.068320</td><td align="left">-3.576392</td><td align="left">2017-12-29</td></tr><tr><td align="left"></td><td align="left">364</td><td align="left">-0.105399</td><td align="left">-5.517371</td><td align="left">2017-12-30</td></tr><tr><td align="left"></td><td align="left">365</td><td align="left">-0.097595</td><td align="left">-5.108857</td><td align="left">2017-12-31</td></tr><tr><td align="left"></td><td align="left">366</td><td align="left">-0.084645</td><td align="left">-4.430960</td><td align="left">2018-01-01</td></tr><tr><td align="left">Canada 加拿大</td><td align="left">53</td><td align="left">0.060064</td><td align="left">3.144173</td><td align="left">2017-02-22</td></tr><tr><td align="left"></td><td align="left">54</td><td align="left">0.062321</td><td align="left">3.262322</td><td align="left">2017-02-23</td></tr><tr><td align="left">Estonia 爱沙尼亚</td><td align="left">365</td><td align="left">-0.058432</td><td align="left">-3.058758</td><td align="left">2017-12-31</td></tr><tr><td align="left"></td><td align="left">366</td><td align="left">-0.062565</td><td align="left">-3.275110</td><td align="left">2018-01-01</td></tr><tr><td align="left">Japan 日本</td><td align="left">1</td><td align="left">0.119266</td><td align="left">6.243256</td><td align="left">2017-01-01</td></tr><tr><td align="left"></td><td align="left">2</td><td align="left">0.058107</td><td align="left">3.041753</td><td align="left">2017-01-02</td></tr><tr><td align="left"></td><td align="left">361</td><td align="left">0.084893</td><td align="left">4.443952</td><td align="left">2017-12-27</td></tr><tr><td align="left"></td><td align="left">362</td><td align="left">0.160301</td><td align="left">8.391348</td><td align="left">2017-12-28</td></tr><tr><td align="left"></td><td align="left">363</td><td align="left">0.245581</td><td align="left">12.855538</td><td align="left">2017-12-29</td></tr><tr><td align="left"></td><td align="left">364</td><td align="left">0.298412</td><td align="left">15.621061</td><td align="left">2017-12-30</td></tr><tr><td align="left"></td><td align="left">365</td><td align="left">0.285922</td><td align="left">14.967284</td><td align="left">2017-12-31</td></tr><tr><td align="left"></td><td align="left">366</td><td align="left">0.233630</td><td align="left">12.229903</td><td align="left">2018-01-01</td></tr><tr><td align="left">Spain 西班牙</td><td align="left">363</td><td align="left">-0.073186</td><td align="left">-3.831102</td><td align="left">2017-12-29</td></tr><tr><td align="left"></td><td align="left">364</td><td align="left">-0.107250</td><td align="left">-5.614233</td><td align="left">2017-12-30</td></tr><tr><td align="left"></td><td align="left">365</td><td align="left">-0.111582</td><td align="left">-5.841026</td><td align="left">2017-12-31</td></tr><tr><td align="left"></td><td align="left">366</td><td align="left">-0.086679</td><td align="left">-4.537431</td><td align="left">2018-01-01</td></tr></tbody></table><p>作者发现这一些天中有一些不存在假期效应，故需要从假期中移除。还发现日本不在圣诞节和新年（除 2017 年外）庆祝假期。这就是为什么在年末会有很大的残差。</p><p>另外作者还发现使用三角函数拟合周期时，只需要 sin_1&#x2F;cos_1 波，而对于其他产品，只需要组合 sin_0.5 和 cos_0.5。</p><h3 id="第二轮分析"><a href="#第二轮分析" class="headerlink" title="第二轮分析"></a>第二轮分析</h3><p>通过第一轮，作者在这轮做了以下处理：</p><ol><li>删除一些假日。移除加拿大的一些“无效”节假日（2,4,5,8,10月）。移除日本2018-12-24的特定假日。</li><li>新年和圣诞节排除日本</li><li>调整产品周期性特征</li></ol><p>再次进行残差分析，发现仍有一些日期的残差较大。</p><p>观察到数据后，发现加拿大和爱沙尼亚在12月26日和27日有节假日。模型已经单独处理了“大”的新年假期，为避免多重共线性，打算从普通节假日列表中移除1月1日和12月25日。</p><p>假期效应用高斯分布拟合。</p><p><img src="/2025/20250210/holiday.png"></p><h3 id="第三轮分析"><a href="#第三轮分析" class="headerlink" title="第三轮分析"></a>第三轮分析</h3><ol><li>从普通节假日列表中移除圣诞节和元旦（因为已由特殊日期特征处理）。</li><li>单独处理加拿大和爱沙尼亚12月26日的节假日。</li><li>使用高斯曲线拟合的权重来创建节假日特征（代替原来的10个0&#x2F;1指示变量）</li></ol><h3 id="第四轮分析"><a href="#第四轮分析" class="headerlink" title="第四轮分析"></a>第四轮分析</h3><p>绘制岭回归系数随正则化的变化，以找到最优的 alpha。</p><p>尝试移除 <code>year_2020</code> 参数，增加 <code>year_2019_Dec</code> 参数（最终发现不加这两个参数的版本在公开排行榜更好）。</p><p>尝试添加CCI（消费者信心指数）数据，但未改善CV分数。</p><h3 id="最终提交"><a href="#最终提交" class="headerlink" title="最终提交"></a>最终提交</h3><h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><p><strong>日期特征</strong>: 与之前类似。</p><p><strong>节假日特征 (<code>df_holidays</code>)</strong>:</p><ul><li>从<code>holidays</code>库获取节假日。</li><li>移除了加拿大的一些“无效”节假日。</li><li>移除了所有国家（阿根廷、加拿大、爱沙尼亚、西班牙）的1月1日和12月25日（因为它们由<code>special_date_columns</code>处理）。</li><li>移除了加拿大和爱沙尼亚的12月26日（因为它们由<code>holiday_1226</code>特殊特征处理）。</li><li>移除了日本2018-12-24的假日。</li></ul><p><strong>GDP特征</strong>: 之前类似。</p><p><strong>高斯加权节假日特征 (<code>holiday</code>, <code>holiday_1226</code>)</strong>:</p><ul><li><code>holiday_diff = [np.exp(-(i - 4.5) ** 2 / 8.5) for i in range(11)]</code>: </li><li>创建一个单一的<code>holiday</code>特征。对于<code>df_holidays</code>中的每个节假日，其影响会根据<code>holiday_diff</code>的权重分布到节假日当天及之后的10天。</li><li><code>special_date_columns</code>: 创建12月25-31日和1月1-10日的指示变量（日本特殊处理）。</li><li><code>holiday_1226</code>: 为爱沙尼亚和加拿大12月26日的节假日创建一个高斯加权特征。</li></ul><p><strong>周期性特征 (<code>product_year_columns</code>)</strong>: 与第二轮类似，根据不同产品的特性选择 <code>sin/cos_1</code> 或 <code>sin/cos_0.5</code>。</p><p><strong>COVID特征 (<code>featured_month_columns</code>)</strong>: 2020年3-10月的月度指示变量。</p><p><strong>其他分类特征</strong>: <code>week_columns</code> (周五、六、日)，<code>store_columns</code>，<code>product_columns</code> (均为独热编码，去掉第一类)。</p><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>使用<code>Pipeline</code>包含<code>StandardScaler</code>（标准化）和<code>Ridge(alpha=150, tol=0.00001, max_iter=10000)</code>（岭回归）。<code>alpha=150</code>是正则化强度，通过调优选择。</p><h4 id="后处理"><a href="#后处理" class="headerlink" title="后处理"></a>后处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">result_2.loc[(result_2[<span class="hljs-string">&#x27;year&#x27;</span>] == <span class="hljs-number">2022</span>)&amp;(result_2[<span class="hljs-string">&#x27;country&#x27;</span>] == <span class="hljs-string">&#x27;Argentina&#x27;</span>), <span class="hljs-string">&#x27;predict_exp&#x27;</span>] *= <span class="hljs-number">3.372</span><br>result_2.loc[(result_2[<span class="hljs-string">&#x27;year&#x27;</span>] == <span class="hljs-number">2022</span>)&amp;(result_2[<span class="hljs-string">&#x27;country&#x27;</span>] == <span class="hljs-string">&#x27;Spain&#x27;</span>), <span class="hljs-string">&#x27;predict_exp&#x27;</span>] *= <span class="hljs-number">1.6</span><br>result_2.loc[(result_2[<span class="hljs-string">&#x27;year&#x27;</span>] == <span class="hljs-number">2022</span>)&amp;(result_2[<span class="hljs-string">&#x27;country&#x27;</span>] == <span class="hljs-string">&#x27;Japan&#x27;</span>), <span class="hljs-string">&#x27;predict_exp&#x27;</span>] *= <span class="hljs-number">1.394</span><br>result_2.loc[(result_2[<span class="hljs-string">&#x27;year&#x27;</span>] == <span class="hljs-number">2022</span>)&amp;(result_2[<span class="hljs-string">&#x27;country&#x27;</span>] == <span class="hljs-string">&#x27;Estonia&#x27;</span>), <span class="hljs-string">&#x27;predict_exp&#x27;</span>] *= <span class="hljs-number">1.651</span><br>result_2.loc[(result_2[<span class="hljs-string">&#x27;year&#x27;</span>] == <span class="hljs-number">2022</span>)&amp;(result_2[<span class="hljs-string">&#x27;country&#x27;</span>] == <span class="hljs-string">&#x27;Canada&#x27;</span>), <span class="hljs-string">&#x27;predict_exp&#x27;</span>] *= <span class="hljs-number">0.850</span><br>result_2 = result_2.loc[result_2[<span class="hljs-string">&#x27;date&#x27;</span>] &gt;= dt.datetime(<span class="hljs-number">2022</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), [<span class="hljs-string">&#x27;id&#x27;</span>, <span class="hljs-string">&#x27;predict_exp&#x27;</span>]]<br></code></pre></td></tr></table></figure><h2 id="数据介绍"><a href="#数据介绍" class="headerlink" title="数据介绍"></a>数据介绍</h2><p>共五列分为日期（天为单位）、country、store、product、num_sold(目标值)。</p><p>2010-2016是训练集</p><p>2017是测试集的公榜</p><p>2018-2019是测试集的私榜</p><h2 id="EDA-1"><a href="#EDA-1" class="headerlink" title="EDA"></a>EDA</h2><p>@BROCCOLI BEEF指出，product具有周期性、store具有不变性、GDP和销售额具有较强相关性。</p><p><img src="/2025/20250210/product.png"></p><p><img src="/2025/20250210/store.png"></p><p><img src="/2025/20250210/gdp.png"></p><p>@Konstantin Dmitriev发现GDP在Kenya不符合，我和@BROCCOLI BEEF发现原因可能是因为$num_{sold}&#x3D;a*GDP+bias$，而我们忽视了bias。</p><p><img src="/2025/20250210/gdp_kenya.png"></p><p>由上，我顺带分析了所有的<a href="https://data.worldbank.org/indicator?tab=featured">514个世界银行特殊指标</a>，其中GDP具有最高的R2，这也说明GDP已经足够了。</p><p><img src="/2025/20250210/allbank.png"></p><p>我发现，每周周日具有最多的销售额，且每周具有周期性。</p><p><img src="/2025/20250210/sunday.png"></p><p>@BROCCOLI BEEF指出了节假日效应：</p><p><img src="/2025/20250210/inbox_1048539_0951cf95b72d72e0ff3275e22dcb96ac_Capture.png"></p><p>并分析了节假日效应大概的持续时间：</p><p><img src="/2025/20250210/inbox_1048539_9d65b667a572ec2e70fba2464f5c71c1_Capture.png"></p><p>我指出圣诞节和元旦的节假日效应可能会出现覆盖或影响的情况：<br>（事实上，在比赛中，我并没有完整读完上届的第一名的solution，但是得到了不谋而合的结论）</p><p><img src="/2025/20250210/nochris.jpg"></p><p><img src="/2025/20250210/chris.jpg"></p><p>接下来，我将介绍前6的solutions（solution类似的则只叙述最高排名的solution）</p><p>其中“圣诞节和元旦的节假日效应可能会出现覆盖或影响的情况”这个性质，在前6名中，只被第一名所使用。也许这就是在高分者算法大致类似的情况下，第一名没有更多复杂的操作但能优于其他人的原因之一。</p><h2 id="第一名-by-George-Koussa"><a href="#第一名-by-George-Koussa" class="headerlink" title="第一名 by George Koussa"></a>第一名 by George Koussa</h2><p>类似第三名的乘性线性模型</p><p>闰年特征</p><p>正弦余弦特征</p><p>是否在节假日的十天内特征：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;near_holiday&#x27;</span>] = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> country <span class="hljs-keyword">in</span> df[<span class="hljs-string">&#x27;country&#x27;</span>].unique():<br>    days = [day <span class="hljs-keyword">for</span> day <span class="hljs-keyword">in</span> holidays.CountryHoliday(country, years=df[<span class="hljs-string">&#x27;year&#x27;</span>].unique())]<br>    <span class="hljs-keyword">for</span> day <span class="hljs-keyword">in</span> days:<br>        df.loc[(df.country == country) &amp; (df[<span class="hljs-string">&#x27;date&#x27;</span>].dt.date &lt; day + dt.timedelta(days=<span class="hljs-number">10</span>)) &amp; (df[<span class="hljs-string">&#x27;date&#x27;</span>].dt.date &gt; day - dt.timedelta(days=<span class="hljs-number">10</span>)), <span class="hljs-string">&#x27;near_holiday&#x27;</span>] = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>gdp采用bias</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;gdp_factor&#x27;</span>] =  (-<span class="hljs-number">17643.346899</span>+<span class="hljs-number">85.42355636</span>*df[<span class="hljs-string">&#x27;gdp&#x27;</span>]) / <span class="hljs-number">365</span><br></code></pre></td></tr></table></figure><p>商店因子排除加拿大和肯尼亚进行考虑</p><p>通过ridge来预测未来因子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">df[<span class="hljs-string">&#x27;ratio&#x27;</span>] = df[<span class="hljs-string">&#x27;store_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;gdp_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;product_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;day_of_week_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;sincos_factor&#x27;</span>]<br>df[<span class="hljs-string">&#x27;total&#x27;</span>] = df[<span class="hljs-string">&#x27;num_sold&#x27;</span>] / df[<span class="hljs-string">&#x27;ratio&#x27;</span>]<br><span class="hljs-comment"># ... (模型拟合和绘图代码) ...</span><br>model = Ridge(alpha=<span class="hljs-number">0.1</span>) <span class="hljs-comment"># 使用岭回归</span><br>model.fit(X, y)<br>df[<span class="hljs-string">&#x27;trend_factor&#x27;</span>] = model.predict(df[<span class="hljs-string">&#x27;n_day&#x27;</span>].to_numpy().reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>df.loc[df[<span class="hljs-string">&#x27;date&#x27;</span>] &lt; dt.datetime(<span class="hljs-number">2013</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), <span class="hljs-string">&#x27;trend_factor&#x27;</span>] = <span class="hljs-number">1</span> <span class="hljs-comment"># 2013年前趋势因子设为1</span><br><span class="hljs-comment"># ... (可视化) ...</span><br>df[<span class="hljs-string">&#x27;ratio&#x27;</span>] = df[<span class="hljs-string">&#x27;store_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;gdp_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;product_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;day_of_week_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;sincos_factor&#x27;</span>] * df[<span class="hljs-string">&#x27;trend_factor&#x27;</span>]<br>df[<span class="hljs-string">&#x27;total&#x27;</span>] = df[<span class="hljs-string">&#x27;num_sold&#x27;</span>] / df[<span class="hljs-string">&#x27;ratio&#x27;</span>]<br></code></pre></td></tr></table></figure><p>新年因子：为新年设置一个节假日因子，以应对“圣诞节和元旦的节假日效应可能会出现覆盖或影响的情况”。</p><h2 id="第二名-by-Chris-Deotte"><a href="#第二名-by-Chris-Deotte" class="headerlink" title="第二名 by Chris Deotte"></a>第二名 by Chris Deotte</h2><h3 id="后处理-1"><a href="#后处理-1" class="headerlink" title="后处理"></a>后处理</h3><p>对于未来，我们会抱有更好的态度。</p><p>所以Chris采用了两种策略，第一种是全乘以1.06，第二种是乘上1.06 + slope * (year - 2017)。</p><p><img src="/2025/20250210/future.png"></p><h3 id="模型一"><a href="#模型一" class="headerlink" title="模型一"></a>模型一</h3><p>Transformer，大致代码为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">build_model</span>():<br>        <br>    <span class="hljs-comment"># INPUT </span><br>    inp = tf.keras.Input(shape=(LEN,<span class="hljs-number">2</span>))<br>    <br>    <span class="hljs-comment"># POSITIONAL ENCODING</span><br>    x = layers.Dense(feat_dim)(inp)<br>    p = positional_encoding(<span class="hljs-number">1440</span>,feat_dim)<br>    x = x + p<br>    <br>    <span class="hljs-comment"># THREE BLOCKS of WAVENET and TRANSFORMER</span><br>    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_blocks):<br>        skip = x<br>        x = wave_block(x, feat_dim, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>)<br>        x = wave_block(x, feat_dim, <span class="hljs-number">3</span>, <span class="hljs-number">12</span>)<br>        x = TransformerBlock(embed_dim, feat_dim, num_heads, ff_dim, dropout_rate)(x)<br>        x = <span class="hljs-number">0.9</span>*x + <span class="hljs-number">0.1</span>*skip <br><br>    <span class="hljs-comment"># HEAD</span><br>    x = tf.keras.layers.GlobalAveragePooling1D()(x)<br>    x = tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">&#x27;relu&#x27;</span>)(x)<br>    x = tf.keras.layers.Dense(<span class="hljs-number">32</span>,activation=<span class="hljs-string">&#x27;linear&#x27;</span>, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)(x)<br>    <br>    <span class="hljs-comment"># COMPILE MODEL</span><br>    model = tf.keras.Model(inputs=inp, outputs=x)<br>    opt = tf.keras.optimizers.Adam(learning_rate = <span class="hljs-number">1e-3</span>)<br>    loss = tf.keras.losses.MeanSquaredError()<br>    model.<span class="hljs-built_in">compile</span>(loss=loss, optimizer = opt)<br>    <br>    <span class="hljs-keyword">return</span> model<br></code></pre></td></tr></table></figure><p>另外：</p><ul><li>在所有5个产品上训练1个模型（15个时期余弦调度）</li><li>添加30个布尔特征用于30个节假日</li><li>使用第一次预测（2017年，2018年）作为伪标签来训练第二次预测</li><li>使用第二次预测（2017年、2018年、2019年）作为伪标签来训练第三次预测</li><li>使用用不同种子训练的5个模型的中位数（用于第一次、第二次、第三次预测）</li><li>提交第三轮预测</li></ul><blockquote><p>事实上，我另外设计了一套大卷积核神经网络和类似FITS的傅里叶网络优于Chris的transformer模型，不过可惜没有像chris做那么多操作，使得最终分数并不是很好。</p></blockquote><h3 id="模型二"><a href="#模型二" class="headerlink" title="模型二"></a>模型二</h3><p>类似第三名的乘性线性模型</p><h3 id="ensemble"><a href="#ensemble" class="headerlink" title="ensemble"></a>ensemble</h3><p>通过在transformer上叠加线性回归来预测残差</p><h2 id="第三名-by-Konstantin-Dmitriev"><a href="#第三名-by-Konstantin-Dmitriev" class="headerlink" title="第三名 by Konstantin Dmitriev"></a>第三名 by Konstantin Dmitriev</h2><p>假期效应，同样使用高斯曲线来描述：<br>$$<br>H(t)&#x3D;exp(-\frac{(d-d_h-d_0)^2}{2\sigma_0^2})<br>$$</p><h3 id="后处理-2"><a href="#后处理-2" class="headerlink" title="后处理"></a>后处理</h3><p><img src="/2025/20250210/inbox_4308868_04bdd76209c6db4a5d94e6679101c685_pgs501.png"></p><p>使用$trend(d)&#x3D;1+s*ReLU(d-d1)$来描述，其中s和$d_1$是斜率和位移参数。</p><h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><p>乘性线性模型（部分地方使用Lasso，而不是Ridge）</p><blockquote><p>我曾在论坛率先指出Lasso在某些地方会有更好的结果。</p></blockquote><p>引入了day-of-year的因子</p><p>使用<code>scipy.optimize.minimize</code> 来优化MAPE</p><p>给每个国家都训练一个模型，具体而言：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># %% [code] &#123;&quot;execution&quot;:&#123;...&#125;&#125;</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimization_pipeline2</span>(<span class="hljs-params">df_train, df_test, cols, countries_groups, initial_guess_struct, optimization_steps_config</span>):<br>    x_params_list = [] <span class="hljs-comment"># 存储每个国家组优化后的参数</span><br>    df1, df2 = df_train.copy(), df_test.copy() <span class="hljs-comment"># 使用副本操作</span><br>    df1[<span class="hljs-string">&#x27;prediction&#x27;</span>] = <span class="hljs-number">0</span> <span class="hljs-comment"># 初始化预测列</span><br>    df2[<span class="hljs-string">&#x27;prediction&#x27;</span>] = <span class="hljs-number">0</span><br>    <br>    <span class="hljs-comment"># 遍历国家组</span><br>    <span class="hljs-keyword">for</span> countries_subset <span class="hljs-keyword">in</span> countries_groups:<br>        <span class="hljs-comment"># 如果测试集没有该国家组的数据，则跳过</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(df2[df2.country.isin(countries_subset)]) == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;\tDo optimization on the countries: &#x27;</span> + <span class="hljs-string">&#x27;, &#x27;</span>.join(countries_subset))<br>        <br>        <span class="hljs-comment"># 为当前国家子集运行一级优化流程</span><br>        res, (train_score, test_score) = optimization_pipeline(<br>            df1[df1.country.isin(countries_subset)],  <span class="hljs-comment"># 该国家子集的训练数据</span><br>            df2[df2.country.isin(countries_subset)],  <span class="hljs-comment"># 该国家子集的测试数据</span><br>            cols,                                     <span class="hljs-comment"># 特征列</span><br>            initial_guess_struct,                     <span class="hljs-comment"># 初始参数猜测 (结构化)</span><br>            optimization_steps_config                 <span class="hljs-comment"># 优化步骤配置</span><br>        )<br>        <br>        <span class="hljs-comment"># 存储该国家组优化后的参数</span><br>        x_params_list.append(res.x)<br><br>        <span class="hljs-comment"># 为该国家组在训练集和测试集上生成并存储预测</span><br>        df1.loc[df1.country.isin(countries_subset), <span class="hljs-string">&#x27;prediction&#x27;</span>] = predict(res.x, df1[df1.country.isin(countries_subset)][cols].to_numpy(), <span class="hljs-literal">True</span>)<br>        df2.loc[df2.country.isin(countries_subset), <span class="hljs-string">&#x27;prediction&#x27;</span>] = predict(res.x, df2[df2.country.isin(countries_subset)][cols].to_numpy(), <span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 整合结果</span><br>    res_final = &#123;<span class="hljs-string">&#x27;x&#x27;</span>: x_params_list&#125; <span class="hljs-comment"># 存储参数集列表 (每个国家组一个)</span><br>    <span class="hljs-comment"># 使用合并的预测计算完整训练集上的总体MAPE</span><br>    train_score_overall = mean_absolute_percentage_error(df1[<span class="hljs-string">&#x27;num_sold&#x27;</span>], df1[<span class="hljs-string">&#x27;prediction&#x27;</span>])<br>    <span class="hljs-comment"># 计算完整测试集上的总体MAPE (如果有效)</span><br>    test_score_overall = <span class="hljs-literal">None</span> <span class="hljs-keyword">if</span> pd.isna(df2[<span class="hljs-string">&#x27;num_sold&#x27;</span>]).<span class="hljs-built_in">any</span>() <span class="hljs-keyword">else</span> mean_absolute_percentage_error(df2[<span class="hljs-string">&#x27;num_sold&#x27;</span>], df2[<span class="hljs-string">&#x27;prediction&#x27;</span>])<br><br>    <span class="hljs-keyword">return</span> res_final, (train_score_overall, test_score_overall)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">predict2</span>(<span class="hljs-params">coef_list, df, cols, countries_groups, <span class="hljs-built_in">round</span>=<span class="hljs-literal">False</span></span>):<br>    df1 = df.copy()<br>    df1[<span class="hljs-string">&#x27;prediction&#x27;</span>] = <span class="hljs-number">0.0</span> <span class="hljs-comment"># 初始化预测列</span><br>    <span class="hljs-comment"># 遍历参数集 (coef_list) 和对应的国家组</span><br>    <span class="hljs-keyword">for</span> x_params, countries_subset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(coef_list, countries_groups):<br>        <span class="hljs-comment"># 使用其特定参数为当前国家组生成预测</span><br>        df1.loc[df1.country.isin(countries_subset), <span class="hljs-string">&#x27;prediction&#x27;</span>] = predict(x_params, df1[df1.country.isin(countries_subset)][cols].to_numpy(), <span class="hljs-built_in">round</span>)<br>    <span class="hljs-keyword">return</span> df1[<span class="hljs-string">&#x27;prediction&#x27;</span>] <span class="hljs-comment"># 返回整合的预测序列</span><br></code></pre></td></tr></table></figure><h2 id="第六名-by-Pascal-Terpstra"><a href="#第六名-by-Pascal-Terpstra" class="headerlink" title="第六名 by Pascal Terpstra"></a>第六名 by Pascal Terpstra</h2><p>第一个模型是@kdmitrie 发布的公共笔记本的改编。第二个模型使用的是乘性线性回归模型，第三个模型是@cdeotte 发布的 transformer。</p><p>对三者进行ensemble。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ARIMA</title>
    <link href="/2025/20250115/"/>
    <url>/2025/20250115/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ARIMA 模型，全称为<strong>自回归整合移动平均模型 (Autoregressive Integrated Moving Average model)</strong>。</p><span id="more"></span><p>ARIMA 模型通常表示为 $ARIMA(p, d, q)$，其中：</p><ul><li><strong>p (自回归阶数 - Autoregressive Order)</strong>: 模型中包含的滞后观测值的数量，即当前值与多少个过去值相关。</li><li><strong>d (差分阶数 - Degree of Differencing)</strong>: 使时间序列达到平稳状态所需的差分次数。</li><li><strong>q (移动平均阶数 - Moving Average Order)</strong>: 模型中包含的滞后预测误差项的数量，即当前值的预测误差与多少个过去的预测误差相关。</li></ul><h2 id="AR"><a href="#AR" class="headerlink" title="AR"></a>AR</h2><p>自回归 (AR) 部分的思想是，时间序列中当前的值 $Y_t$ 可以用其过去若干个时刻的值的线性组合来表示。</p><p>一个典型的例子，即使用过去几天的平均作为预测。</p><p>我们可以扩展为一个线性模型。对于一个 $p$ 阶的自回归模型，记作 $AR(p)$，其数学表达式为：</p><p>$$<br>Y_t &#x3D; c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \varepsilon_t<br>$$</p><p>其中：</p><ul><li>$Y_t$ 是时间序列在时刻 $t$ 的值。</li><li>$c$ 是一个常数项。</li><li>$\phi_1, \phi_2, \dots, \phi_p$ 是自回归系数，表示过去值对当前值的影响程度。</li><li>$Y_{t-1}, Y_{t-2}, \dots, Y_{t-p}$ 是过去 $p$ 个时刻的时间序列值。</li><li>$\varepsilon_t$ 是在时刻 $t$ 的白噪声误差项，通常假定其均值为0，方差恒定，且相互独立。</li></ul><h3 id="差分"><a href="#差分" class="headerlink" title="差分"></a>差分</h3><p>差分 (I) 部分指的是通过<strong>差分 (Differencing)</strong> 来处理时间序列中的非平稳性。</p><p>可能听起来很玄乎。我们可以通过一个例子来理解。</p><p>比如对三角函数：</p><p>$$<br>\begin{align}<br>sin(x)&amp;: Amplitude&#x3D;1, T&#x3D;2\pi<br>\\sin(x)-sin(x-0.2)&amp;: Amplitude&#x3D;2sin(\frac{1}{10}), T&#x3D;2\pi<br>\\(sin(x+0.2)-sin(x))-(sin(x)-sin(x-0.2))&amp;: Amplitude&#x3D;2^2sin^2(\frac{1}{10}), T&#x3D;2\pi<br>\end{align}<br>$$</p><p><img src="/2025/20250115/sinx.jpg"></p><p>多次差分可以接近于平稳序列。</p><p>参数 $d$ 表示使原始序列达到平稳状态所需的差分次数。如果序列已经是平稳的，则 $d&#x3D;0$，模型就变成了 ARMA(p,q) 模型。</p><h2 id="MA"><a href="#MA" class="headerlink" title="MA"></a>MA</h2><p>移动平均 部分与 AR 部分不同，它不是直接使用过去观测值的线性组合，而是使用过去预测误差的线性组合来预测当前值。一个 $q$ 阶的移动平均模型，记作 $MA(q)$，其数学表达式为：</p><p>$Y_t &#x3D; \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \dots + \theta_q \varepsilon_{t-q}$</p><p>其中：</p><ul><li>$Y_t$ 是时间序列在时刻 $t$ 的值。</li><li>$\mu$ 是序列的均值。</li><li>$\varepsilon_t$ 是当前的白噪声误差项。</li><li>$\theta_1, \theta_2, \dots, \theta_q$ 是移动平均系数，表示过去的预测误差对当前值的影响程度。</li><li>$\varepsilon_{t-1}, \varepsilon_{t-2}, \dots, \varepsilon_{t-q}$ 是过去 $q$ 个时刻的预测误差项。</li></ul><p><strong>简单来说，MA 模型假设当前值受到过去随机冲击或预测误差的影响。</strong> 例如，如果昨天的预测过于乐观（即 $\varepsilon_{t-1}$ 为负），模型可能会在今天的预测中进行调整。参数 $q$ 的选择决定了模型考虑多少个历史预测误差项。</p><h2 id="滞后算子"><a href="#滞后算子" class="headerlink" title="滞后算子"></a>滞后算子</h2><p>将 AR、I、MA 三个部分结合起来，就构成了 $ARIMA(p,d,q)$ 模型。如果一个时间序列 $Y_t$ 经过 $d$ 次差分后得到平稳序列 $W_t &#x3D; \Delta^d Y_t$，并且 $W_t$ 服从一个 $ARMA(p,q)$ 模型，那么原始序列 $Y_t$ 就服从 $ARIMA(p,d,q)$ 模型。</p><p>$W_t$ 的 $ARMA(p,q)$ 模型可以表示为：<br>$W_t &#x3D; c + \phi_1 W_{t-1} + \dots + \phi_p W_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \dots + \theta_q \varepsilon_{t-q}$</p><p>或者更紧凑地使用 $B$ (其中 $B^k Y_t &#x3D; Y_{t-k}$):<br>$(1 - \phi_1 B - \dots - \phi_p B^p)(1-B)^d Y_t &#x3D; c + (1 + \theta_1 B + \dots + \theta_q B^q)\varepsilon_t$</p><p>其中 $\Phi(B) &#x3D; (1 - \phi_1 B - \dots - \phi_p B^p)$ 是自回归算子多项式，$\Theta(B) &#x3D; (1 + \theta_1 B + \dots + \theta_q B^q)$ 是移动平均算子多项式。</p><h2 id="如何确定模型的阶数-p-d-q"><a href="#如何确定模型的阶数-p-d-q" class="headerlink" title="如何确定模型的阶数 (p, d, q)"></a>如何确定模型的阶数 (p, d, q)</h2><p>这是 ARIMA 建模中最关键且最具挑战性的一步，通常遵循 Box-Jenkins 方法论：</p><h3 id="a-确定差分阶数-d"><a href="#a-确定差分阶数-d" class="headerlink" title="a. 确定差分阶数 $d$:"></a>a. 确定差分阶数 $d$:</h3><ul><li><strong>单位根检验 (Unit Root Tests)</strong>:<ul><li><strong>ADF 检验 (Augmented Dickey-Fuller test)</strong>: 原假设是序列存在单位根 (非平稳)，备择假设是序列不存在单位根 (平稳)。如果 p-value 小于显著性水平 (如0.05)，则拒绝原假设，认为序列平稳。</li><li><strong>KPSS 检验 (Kwiatkowski-Phillips-Schmidt-Shin test)</strong>: 原假设是序列平稳，备择假设是序列非平稳。</li></ul></li><li><strong>自相关函数 (ACF) 图</strong>: 对于非平稳序列，ACF 图通常会缓慢衰减，而不是快速截尾。<br> 通常进行一阶差分，然后再次检验平稳性。如果仍不平稳，可以尝试二阶差分。但一般情况下， $d$ 的取值不会超过 2。过度差分会导致模型引入不必要的复杂性。</li></ul><h3 id="b-确定自回归阶数-p-和移动平均阶数-q"><a href="#b-确定自回归阶数-p-和移动平均阶数-q" class="headerlink" title="b. 确定自回归阶数 $p$ 和移动平均阶数 $q$:"></a>b. 确定自回归阶数 $p$ 和移动平均阶数 $q$:</h3><p>   在序列平稳化 (即确定了 $d$ 之后，对差分后的序列 $W_t$) 后，使用<strong>自相关函数 (ACF)</strong> 和<strong>偏自相关函数 (PACF)</strong> 图来初步判断 $p$ 和 $q$ 的值。</p><ul><li><strong>ACF (Autocorrelation Function)</strong>: 度量序列与其自身滞后值之间的相关性。</li><li><strong>PACF (Partial Autocorrelation Function)</strong>: 度量在控制了中间滞后项影响后，序列与其自身滞后值之间的相关性。</li></ul><p>   常见的判断规则：</p><table><thead><tr><th>模型类型</th><th>ACF 图特征</th><th>PACF 图特征</th></tr></thead><tbody><tr><td><strong>AR(p) 模型</strong></td><td>拖尾 (geometrically decaying or damped sine wave)</td><td>$p$ 阶后截尾 (cuts off after lag p)</td></tr><tr><td><strong>MA(q) 模型</strong></td><td>$q$ 阶后截尾 (cuts off after lag q)</td><td>拖尾 (geometrically decaying or damped sine wave)</td></tr><tr><td><strong>ARMA(p,q) 模型</strong></td><td>拖尾</td><td>拖尾</td></tr></tbody></table><ul><li><strong>截尾 (Cuts off)</strong>: 指相关系数在某个滞后阶数后突然变为0或在零附近小幅波动 (在置信区间内)。</li><li><strong>拖尾 (Tails off)</strong>: 指相关系数逐渐衰减至0，而不是突然截断。</li></ul><p>   如果 ACF 和 PACF 都显示拖尾，则可能是一个 ARMA(p,q) 模型，此时 $p$ 和 $q$ 的具体值可能需要尝试不同的组合，并结合信息准则 (如 AIC, BIC) 来选择。</p><h2 id="ARIMA-模型的建模步骤-Box-Jenkins-方法论"><a href="#ARIMA-模型的建模步骤-Box-Jenkins-方法论" class="headerlink" title="ARIMA 模型的建模步骤 (Box-Jenkins 方法论)"></a>ARIMA 模型的建模步骤 (Box-Jenkins 方法论)</h2><ol><li><p><strong>模型识别 (Identification)</strong>:</p><ul><li>绘制时间序列图，检查平稳性。</li><li>进行单位根检验 (如 ADF 检验) 确定差分阶数 $d$。</li><li>对差分后的平稳序列绘制 ACF 和 PACF 图，初步判断 $p$ 和 $q$ 的可能取值。</li></ul></li><li><p><strong>参数估计 (Estimation)</strong>:</p><ul><li>根据识别出的 $p, d, q$ 值，估计模型中的参数 ($\phi_i, \theta_j, c$)。常用的估计方法是最大似然估计 (MLE) 或最小二乘估计。</li></ul></li><li><p><strong>模型诊断 (Diagnostic Checking)</strong>:</p><ul><li><strong>残差分析</strong>: 检验模型的残差序列是否为白噪声。<ul><li>绘制残差的 ACF 和 PACF 图，看是否存在显著的自相关。理想情况下，残差的 ACF 和 PACF 图应该在所有滞后阶数上都没有显著的相关性。</li><li>进行 Ljung-Box 检验 (或 Box-Pierce 检验)，原假设是残差序列为白噪声。如果 p-value 较大，则不能拒绝原假设，说明模型拟合良好。</li></ul></li><li><strong>参数显著性检验</strong>: 检验估计出的参数是否显著不为零 (通常看 t 统计量或 p-value)。不显著的参数可以考虑从模型中剔除。</li><li><strong>信息准则</strong>: 比较不同阶数组合 (p,q) 的模型，选择 AIC (Akaike Information Criterion) 或 BIC (Bayesian Information Criterion) 值最小的模型。这些准则在模型拟合优度和模型复杂度之间进行权衡。</li></ul></li><li><p><strong>模型预测 (Forecasting)</strong>:</p><ul><li>如果模型通过了诊断检验，就可以用来对未来的时间序列值进行预测。可以生成点预测和区间预测。</li></ul></li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://otexts.com/fpp3cn/">https://otexts.com/fpp3cn/</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>时间序列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用更少的参数建模时间序列</title>
    <link href="/2025/20250110/"/>
    <url>/2025/20250110/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>先前的Dlinear已经足够简单，且击败了一众transformer模型。</p><p>我们还能使用更少的参数吗？Dlinear使用了两个线性网络，分别周期和残差，我们能只用一个吗？</p><p>这也就是FITS所做的，我们直接在傅里叶域上做神经网络，这样能实现了周期和残差的同时建模。</p><span id="more"></span><h2 id="FITS"><a href="#FITS" class="headerlink" title="FITS"></a>FITS</h2><p>FITS是DLinear的后续之作，依旧保持着简单有效的方法。获得ICLR2024。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p><img src="/2025/20250110/fits.jpg"></p><p>具体算法其实很清晰。</p><ul><li>首先对于长度为L的序列，作者首先进行了RIN归一化，目的是为了使序列均值为0，然后使用傅立叶变换rFFT把时域信息转到频域（复数域）。</li><li>然后，使用低通滤波器（LPF）将高频分量过滤掉，这部分在代码中是通过一个cut_freq参数来确定的。这样的好处在于能够去掉噪声，减少模型参数量。</li><li>之后，通过图中紫色部分的Complex-valued Linear Layer，这部分是一个上采样（线性层），相当于对频率特征做了线性变换，其长度取决于pred_len和seq_len的比例。</li><li>最后，将新的频率特征进行零pad，使用傅立叶逆变换irFFT转回时域。</li></ul><p>简而言之，其实就是在频域上的线性层。</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> models.NLinear <span class="hljs-keyword">as</span> DLinear<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br><br>    <span class="hljs-comment"># FITS: Frequency Interpolation Time Series Forecasting</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, configs</span>):<br>        <span class="hljs-built_in">super</span>(Model, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.seq_len = configs.seq_len<br>        <span class="hljs-variable language_">self</span>.pred_len = configs.pred_len<br>        <span class="hljs-variable language_">self</span>.individual = configs.individual<br>        <span class="hljs-variable language_">self</span>.channels = configs.enc_in<br><br>        <span class="hljs-variable language_">self</span>.dominance_freq=configs.cut_freq <span class="hljs-comment"># 720/24</span><br>        <span class="hljs-variable language_">self</span>.length_ratio = (<span class="hljs-variable language_">self</span>.seq_len + <span class="hljs-variable language_">self</span>.pred_len)/<span class="hljs-variable language_">self</span>.seq_len<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.individual:<br>            <span class="hljs-variable language_">self</span>.freq_upsampler = nn.ModuleList()<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.channels):<br>                <span class="hljs-variable language_">self</span>.freq_upsampler.append(nn.Linear(<span class="hljs-variable language_">self</span>.dominance_freq, <span class="hljs-built_in">int</span>(<span class="hljs-variable language_">self</span>.dominance_freq*<span class="hljs-variable language_">self</span>.length_ratio)).to(torch.cfloat))<br><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.freq_upsampler = nn.Linear(<span class="hljs-variable language_">self</span>.dominance_freq, <span class="hljs-built_in">int</span>(<span class="hljs-variable language_">self</span>.dominance_freq*<span class="hljs-variable language_">self</span>.length_ratio)).to(torch.cfloat) <span class="hljs-comment"># complex layer for frequency upcampling]</span><br>        <span class="hljs-comment"># configs.pred_len=configs.seq_len+configs.pred_len</span><br>        <span class="hljs-comment"># #self.Dlinear=DLinear.Model(configs)</span><br>        <span class="hljs-comment"># configs.pred_len=self.pred_len</span><br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># RIN</span><br>        x_mean = torch.mean(x, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        x = x - x_mean<br>        x_var=torch.var(x, dim=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)+ <span class="hljs-number">1e-5</span><br>        <span class="hljs-comment"># print(x_var)</span><br>        x = x / torch.sqrt(x_var)<br><br>        low_specx = torch.fft.rfft(x, dim=<span class="hljs-number">1</span>)<br>        low_specx[:,<span class="hljs-variable language_">self</span>.dominance_freq:]=<span class="hljs-number">0</span> <span class="hljs-comment"># LPF</span><br>        low_specx = low_specx[:,<span class="hljs-number">0</span>:<span class="hljs-variable language_">self</span>.dominance_freq,:] <span class="hljs-comment"># LPF</span><br>        <span class="hljs-comment"># print(low_specx.permute(0,2,1))</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.individual:<br>            low_specxy_ = torch.zeros([low_specx.size(<span class="hljs-number">0</span>),<span class="hljs-built_in">int</span>(<span class="hljs-variable language_">self</span>.dominance_freq*<span class="hljs-variable language_">self</span>.length_ratio),low_specx.size(<span class="hljs-number">2</span>)],dtype=low_specx.dtype).to(low_specx.device)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.channels):<br>                low_specxy_[:,:,i]=<span class="hljs-variable language_">self</span>.freq_upsampler[i](low_specx[:,:,i].permute(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)).permute(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">else</span>:<br>            low_specxy_ = <span class="hljs-variable language_">self</span>.freq_upsampler(low_specx.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)).permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># print(low_specxy_)</span><br>        low_specxy = torch.zeros([low_specxy_.size(<span class="hljs-number">0</span>),<span class="hljs-built_in">int</span>((<span class="hljs-variable language_">self</span>.seq_len+<span class="hljs-variable language_">self</span>.pred_len)/<span class="hljs-number">2</span>+<span class="hljs-number">1</span>),low_specxy_.size(<span class="hljs-number">2</span>)],dtype=low_specxy_.dtype).to(low_specxy_.device)<br>        low_specxy[:,<span class="hljs-number">0</span>:low_specxy_.size(<span class="hljs-number">1</span>),:]=low_specxy_ <span class="hljs-comment"># zero padding</span><br>        low_xy=torch.fft.irfft(low_specxy, dim=<span class="hljs-number">1</span>)<br>        low_xy=low_xy * <span class="hljs-variable language_">self</span>.length_ratio <span class="hljs-comment"># energy compemsation for the length change</span><br>        <span class="hljs-comment"># dom_x=x-low_x</span><br>        <br>        <span class="hljs-comment"># dom_xy=self.Dlinear(dom_x)</span><br>        <span class="hljs-comment"># xy=(low_xy+dom_xy) * torch.sqrt(x_var) +x_mean # REVERSE RIN</span><br>        xy=(low_xy) * torch.sqrt(x_var) +x_mean<br>        <span class="hljs-keyword">return</span> xy, low_xy* torch.sqrt(x_var)<br></code></pre></td></tr></table></figure><p>值得注意的是，为了使用某些py库没有复数层功能，作者还提供了如何使用实数层来实现这一功能。</p><p>即</p><p>复数中：<br>$$<br>Y&#x3D;XW<br>$$<br>则<br>$$<br>\begin{align}<br>Y_{real}&#x3D;X_{real}W_{real}-X_{imag}W_{imaag}<br>\\<br>Y_{imag}&#x3D;X_{real}W_{imag}+X_{imag}W_{real}<br>\end{align}<br>$$</p><h2 id="SparseTSF"><a href="#SparseTSF" class="headerlink" title="SparseTSF"></a>SparseTSF</h2><p>华南理工出品，ICML 2024 Oral</p><p><img src="/2025/20250110/x1.png"></p><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p><img src="/2025/20250110/x2.png"></p><p>算法还是在时域里面做。</p><p>具体步骤分为：切块，卷积提取特征，上下采样。</p><p>我们直接来看代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, configs</span>):<br>        <span class="hljs-built_in">super</span>(Model, <span class="hljs-variable language_">self</span>).__init__()<br><br>        <span class="hljs-comment"># get parameters</span><br>        <span class="hljs-variable language_">self</span>.seq_len = configs.seq_len<br>        <span class="hljs-variable language_">self</span>.pred_len = configs.pred_len<br>        <span class="hljs-variable language_">self</span>.enc_in = configs.enc_in<br>        <span class="hljs-variable language_">self</span>.period_len = configs.period_len<br>        <span class="hljs-variable language_">self</span>.d_model = configs.d_model<br>        <span class="hljs-variable language_">self</span>.model_type = configs.model_type<br>        <span class="hljs-keyword">assert</span> <span class="hljs-variable language_">self</span>.model_type <span class="hljs-keyword">in</span> [<span class="hljs-string">&#x27;linear&#x27;</span>, <span class="hljs-string">&#x27;mlp&#x27;</span>]<br><br>        <span class="hljs-variable language_">self</span>.seg_num_x = <span class="hljs-variable language_">self</span>.seq_len // <span class="hljs-variable language_">self</span>.period_len<br>        <span class="hljs-variable language_">self</span>.seg_num_y = <span class="hljs-variable language_">self</span>.pred_len // <span class="hljs-variable language_">self</span>.period_len<br><br>        <span class="hljs-variable language_">self</span>.conv1d = nn.Conv1d(in_channels=<span class="hljs-number">1</span>, out_channels=<span class="hljs-number">1</span>, kernel_size=<span class="hljs-number">1</span> + <span class="hljs-number">2</span> * (<span class="hljs-variable language_">self</span>.period_len // <span class="hljs-number">2</span>),<br>                                stride=<span class="hljs-number">1</span>, padding=<span class="hljs-variable language_">self</span>.period_len // <span class="hljs-number">2</span>, padding_mode=<span class="hljs-string">&quot;zeros&quot;</span>, bias=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&#x27;linear&#x27;</span>:<br>            <span class="hljs-variable language_">self</span>.linear = nn.Linear(<span class="hljs-variable language_">self</span>.seg_num_x, <span class="hljs-variable language_">self</span>.seg_num_y, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&#x27;mlp&#x27;</span>:<br>            <span class="hljs-variable language_">self</span>.mlp = nn.Sequential(<br>                nn.Linear(<span class="hljs-variable language_">self</span>.seg_num_x, <span class="hljs-variable language_">self</span>.d_model),<br>                nn.ReLU(),<br>                nn.Linear(<span class="hljs-variable language_">self</span>.d_model, <span class="hljs-variable language_">self</span>.seg_num_y)<br>            )<br><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        batch_size = x.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment"># normalization and permute     b,s,c -&gt; b,c,s</span><br>        seq_mean = torch.mean(x, dim=<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">1</span>)<br>        x = (x - seq_mean).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># 1D convolution aggregation</span><br>        x = <span class="hljs-variable language_">self</span>.conv1d(x.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.seq_len)).reshape(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.enc_in, <span class="hljs-variable language_">self</span>.seq_len) + x<br><br>        <span class="hljs-comment"># downsampling: b,c,s -&gt; bc,n,w -&gt; bc,w,n</span><br>        x = x.reshape(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.seg_num_x, <span class="hljs-variable language_">self</span>.period_len).permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># sparse forecasting</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&#x27;linear&#x27;</span>:<br>            y = <span class="hljs-variable language_">self</span>.linear(x)  <span class="hljs-comment"># bc,w,m</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.model_type == <span class="hljs-string">&#x27;mlp&#x27;</span>:<br>            y = <span class="hljs-variable language_">self</span>.mlp(x)<br><br>        <span class="hljs-comment"># upsampling: bc,w,m -&gt; bc,m,w -&gt; b,c,s</span><br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>).reshape(batch_size, <span class="hljs-variable language_">self</span>.enc_in, <span class="hljs-variable language_">self</span>.pred_len)<br><br>        <span class="hljs-comment"># permute and denorm</span><br>        y = y.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>) + seq_mean<br><br>        <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>时间序列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>生存分析速览</title>
    <link href="/2024/20241226/"/>
    <url>/2024/20241226/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>生存分析(survivalanalysis)是将事件的结果和出现这一结果所经历的时间结合起来分析的一类统计分析方法。不仅考虑事件是否出现,而且也考虑事件出现的时间长短,因此该类方法也被称之为事件时间分析(time-lo-event analysis)。生存分析起源于医学与生物科学。</p><span id="more"></span><p><strong>为什么不用直接比较样本生存时间，如中值或中位数？</strong></p><p>数据会缺失，可操作性比较差。</p><p><strong>三种分析：</strong></p><p>（1）描述分析：根据样本生存资料估计总体生存率或者是其他有关指标常用的就是K-M生存曲线</p><p>K-M生成分析</p><p>（2）比较分析</p><p>对不同组生存率进行比较分析常采用log-rank检验和Breslow检验。</p><p>（3）影响因素分析</p><p>通过生存分析模型来探讨影响生存时间的因素，常见的COX比例风险模型就是这种影响因素分析。</p><h2 id="（一些）重要概念"><a href="#（一些）重要概念" class="headerlink" title="（一些）重要概念"></a>（一些）重要概念</h2><p><strong>生存函数（survivor function）：</strong></p><p>定义为$S_Y(y)&#x3D;P(Y&gt;y)&#x3D;1-F_Y(y)$，即表示某样本的生存时间大于时刻 y 的概率。</p><p><strong>风险函数（hazard function）：</strong></p><p>定义为$h_Y(y)&#x3D;\frac{f_Y(y)}{S_Y(y)}$，可理解为到时刻y 时存活下来的个体在此后一个无限小的时间区间$[y+\Delta y]$ 内结局事件（失效、死亡）发生的概率。<br>$$<br>h_{Y}\left(y\right)&#x3D;\frac{f_{Y}(y)}{S_{Y}(y)}&#x3D;\frac{f_{Y}(y)}{1-F_{Y}(y)}<br>\ &#x3D;-\frac{\partial}{\partial y}log\left[1-F_{Y}\left(y\right)\right]<br>\ &#x3D;-\frac{\partial}{\partial y}log\left[S_{Y}\left(y\right)\right].<br>$$<br>故$S_Y(y)&#x3D;exp[-\int_0^y h_Y(t)dt]$，其中令$H_Y(t)&#x3D;\int_0^y h_Y(t)dt$，称为累计风险函数，也算是一种度量指标。</p><h2 id="KM曲线"><a href="#KM曲线" class="headerlink" title="KM曲线"></a>KM曲线</h2><p><img src="/2024/20241226/km1.jpg"></p><h2 id="Cox模型"><a href="#Cox模型" class="headerlink" title="Cox模型"></a>Cox模型</h2><p>$$h(t)&#x3D;h_0(t)\times exp(b_1x_1+b_2x_2+\ldots+b_px_p)$$</p><p>x为具有预测效应的多个变量（协变量），h0(t) 是基准风险函数</p><p>Cox需满足的前提：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">风险比(hazard ratio)  =&gt;  exp(协变量系数)<br>比例风险假定（PH 假定，proportional-hazards assumption） =&gt;  与h0(t)无关，风险比为固定值 =&gt;  协变量回归系数是固定的<br></code></pre></td></tr></table></figure><p>我们也可以通过KM曲线来判断。<strong>若两条生存曲线最后交叉，这说明PH条件不成立。</strong> </p><p><img src="/2024/20241226/cox2.jpg"></p><h2 id="Nelson-Aalen-累计风险函数图"><a href="#Nelson-Aalen-累计风险函数图" class="headerlink" title="Nelson-Aalen 累计风险函数图"></a>Nelson-Aalen 累计风险函数图</h2><p>其与与以KM估计式为基础的估计式相比，具有更好的小样本性质，由Nelson提出然后Aalen加以改进。</p><p>使用以下估计：</p><p>$\tilde{H}(t)&#x3D;\sum_{t_i\leq t}\frac{d_i}{n_i}$</p><p>$\tilde{S}(t)&#x3D;exp[-\tilde{H}(t)]$</p><h2 id="WeibullAFTFitter模型"><a href="#WeibullAFTFitter模型" class="headerlink" title="WeibullAFTFitter模型"></a>WeibullAFTFitter模型</h2><p>有$\lambda(x)&#x3D;\exp\left(\beta_{0}+\beta_{1}x_{1}+\ldots+\beta_{n}x_{n}\right)$和$\rho(y)&#x3D;\exp{(\alpha_{0}+\alpha_{1}y_{1}+\ldots+\alpha_{m}y_{m})}$</p><p>则</p><p>$S(t;x,y)&#x3D;\exp\left(-\left(\frac{t}{\lambda(x)}\right)^{\rho(y)}\right)$</p><p>$H(t;x,y)&#x3D;\left(\frac t{\lambda(x)}\right)^{\rho(y)}$</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>生物信息</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>奈奎斯特定理与香农定理</title>
    <link href="/2024/20241216/"/>
    <url>/2024/20241216/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>奈奎斯特准则和香农定理是计算机网络中和信息论中的基础理论，分别给出了无噪声和有噪声信道的最大数据传输速率。但我们可能很好奇，这些式子是怎么推出来的？为什么会和对数结合信噪比这么一个奇怪的东西扯上关系？我们可以看出有$mlogn$，这是否暗藏玄机？</p><p>这些内容其实都可以从香农本人的1949年的《通信的数学理论&#x2F;A mathematical theory of communication》中找到。</p><span id="more"></span><hr><p><strong>奈奎斯特准则：</strong></p><p>奈式准则其实就是等同于“采样频率不能高于2W”（称为“奈奎斯特第一准则”，奈奎斯特本人好像也没有单独把它作为一条的信道容量定理）。</p><p>信道容量都是由熵H定义的。由于采样频率不能超过2W，所以信道以秒为单位时要在最前面乘个2W的系数。即最大信道容量C&#x3D;2W*H</p><p>熵是$H&#x3D;-\sum plogp$。理想情况下，$p&#x3D;\frac{1}{v}$，故H为logv，即C&#x3D;2Wlogv。</p><p><strong>香农定理：</strong></p><p>有（高斯分布的）噪声、给定功率情况下，信道容量为信号熵减噪声熵。<br>而有以下结论：<br>（1）当平均功率不变的情况下，信号符合高斯分布时熵最大<br>（2）符合高斯分布的熵为$\log{\sqrt{2\pi e N}}$。N为功率，$\pi$为圆周率，e为自然常数。</p><p>故，最大信道容量为：</p><p>$2w\log{\sqrt{2\pi e(s+n)}}-2w\log{\sqrt{2\pi e n}}&#x3D;2w\log{\sqrt{1+\frac{s}{n}}}$</p>]]></content>
    
    
    
    <tags>
      
      <tag>信息论</tag>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SYN和FIN都能携带数据吗？</title>
    <link href="/2024/20241101/"/>
    <url>/2024/20241101/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>SYN和FIN是TCP协议中三次握手和四次挥手的重要标志位。</p><p>第三次握手携带数据是常用，且在RFC 793中明确指出可行的。那SYN和FIN中我们能携带数据吗？</p><p>但在有些教材中，指出SYN不能携带，<strong>这是错误的。</strong></p><span id="more"></span><hr><p>早期的TCP 协议RFC 793并没有明确说SYN和FIN能不能带数据，它只限制了不能立刻交付数据给上层。（“this is perfectly legitimate, so long as the receiving TCP doesn’t deliver the data to the user until it is clear the data is  valid (i.e., the data must be buffered at the receiver until the  connection reaches the ESTABLISHED state)”——RFC 793 的第3.4节）。</p><p>所以它理论上是允许SYN携带数据的。为了SYN携带数据更方便，后续也有对应的TCP fast open。甚至说，在RFC 1379，在第一章举了一个合法但“畸形”的，把SYN、数据、FIN都放在一起的例子。</p><p>再后来，TCP协议 RFC 9293直接明确指出<strong>SYN携带数据是合法的，且不能带是一种误解。</strong>（“Note that it is legal to send and receive application data on SYN segments (this is the “text in the segment” mentioned above). There has been significant misinformation and misunderstanding of this topic historically.”——RFC 9293的第3.10.7.3节）</p><p>对于FIN能携带数据的肯定表述，并没有在网络上或相关协议上找到。只找到stackoverflow一个网友拿RFC 793中的“For sequence number purposes, the SYN is considered to occur before the first actual data octet of the segment in which it occurs, while the FIN is considered to occur after the last actual data octet in a segment in which it occurs.”来证明是能携带。用这句话来证明，这是不严谨的。</p><p>总之，个人觉得，SYN和FIN携带数据是“法无禁止即可为”。</p><blockquote><p>题外话：我一直以为FIN和SYN携带数据都是不常用的，但是和B站UP主湖科大教书匠的讨论后，他说FTP有时候会出现携带FIN的数据包。由于我并无FTP服务器，对此并无验证。</p><p>但经我挂在电脑跑wireshark，只要时间长就能抓到了携带FIN。主要有两种，第一种是服务器向主机发出的最后的数据+FIN+ACK。第二种是主机发送FIN后，但收到服务器的RST报文。此后，主机会发送TCP Retransmission且携带数据+FIN+ACK。可见携带FIN的数据包并无想象中的那么少见。</p></blockquote><p>参考资料：</p><p>[1] <a href="https://datatracker.ietf.org/doc/html/rfc793">RFC 793</a></p><p>[2] <a href="https://datatracker.ietf.org/doc/html/rfc9293">RFC 9293</a></p><p>[3] <a href="https://datatracker.ietf.org/doc/html/rfc1379">RFC 1379</a></p><p>[4] <a href="https://stackoverflow.com/questions/8702646/can-a-tcp-packet-with-the-fin-flag-also-have-data">Can a TCP packet with the FIN flag also have data?</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>计算机网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图卷积等价于mixup</title>
    <link href="/2024/20241006/"/>
    <url>/2024/20241006/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（TMLR 2024）《On the Equivalence of Graph Convolution and Mixup 》</p><span id="more"></span><h2 id="图卷积等价于mixup"><a href="#图卷积等价于mixup" class="headerlink" title="图卷积等价于mixup"></a>图卷积等价于mixup</h2><p>图卷积实际上是对邻居进行平均，即可以写成:<br>$$<br>(\hat{x},\hat{y})&#x3D;\left(\frac{1}{|N_i|}\sum_{k\in N_i}x_k,y_i\right)<br>$$<br>而mixup实际上也是一种平均，即：<br>$$<br>(\hat{x},\hat{y})&#x3D;\left(\sum_{i&#x3D;1}^N \lambda_ix_i,\sum_{i&#x3D;1}^N \lambda_iy_i\right)<br>$$<br><img src="/2024/20241006/model.jpg"></p><h2 id="图卷积等价于训练时-测试时使用mixup"><a href="#图卷积等价于训练时-测试时使用mixup" class="headerlink" title="图卷积等价于训练时+测试时使用mixup"></a>图卷积等价于训练时+测试时使用mixup</h2><p>在以上的基础，作者进一步提出，图卷积等价于训练时+测试时使用mixup，即Homophily Relabel and Test-time Mixup。</p><h3 id="Homophily-Relabel"><a href="#Homophily-Relabel" class="headerlink" title="Homophily Relabel"></a>Homophily Relabel</h3><p>1）使用混合标签 $\hat Y&#x3D;AY$重新标记图中的所有结点。</p><blockquote><p>这其实就是标签传播（Label Propagation）。</p></blockquote><p>2）在重标后的图上，仅利用结点特征，使用$\hat Y$对所有结点使用MLP训练。</p><p>作者把它命名为HMLP。</p><p><img src="/2024/20241006/1.jpg"></p><h3 id="Test-time-Mixup"><a href="#Test-time-Mixup" class="headerlink" title="Test-time Mixup"></a>Test-time Mixup</h3><p>1）仅使用训练集的结点特征来训练一个 MLP。</p><p>2）在测试期间，利用训练好的 MLP 进行推理，并结合邻居聚合。</p><p>作者把它命名为TMLP。</p><p><img src="/2024/20241006/2.jpg"></p><h3 id="统一二者"><a href="#统一二者" class="headerlink" title="统一二者"></a>统一二者</h3><p><img src="/2024/20241006/3.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>思想验证区域（The Community）人物介绍</title>
    <link href="/2024/20240830/"/>
    <url>/2024/20240830/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>第三届青龙最佳综艺作品**《思想验证区域：The Community》<strong>，是一档汇集了12名不同理念人们的</strong>政治生存类综艺**，参与者皆为来自各行业的素人，男女各占一半，分别从<strong>政治、性别、阶级、开放性</strong>这四个领域划分了属性。</p><p>测试链接：<a href="https://thecommunity.co.kr/">https://thecommunity.co.kr/</a></p><p>为了方便，记录人物介绍。</p><hr><span id="more"></span><h2 id="李承国"><a href="#李承国" class="headerlink" title="李承国"></a>李承国</h2><p><img src="/2024/20240830/1.png"></p><p>代名：TED</p><p> 油管博主 右派 女权 富有 开放</p><h2 id="YUNB"><a href="#YUNB" class="headerlink" title="YUNB"></a>YUNB</h2><p><img src="/2024/20240830/2.jpg"></p><p>代名：迈克尔</p><p>资深玩家 </p><p>右派 平权 富有 传统</p><h2 id="河马"><a href="#河马" class="headerlink" title="河马"></a>河马</h2><p><img src="/2024/20240830/3.jpg"></p><p>代名：河马</p><p>作家</p><p>左派 女权 平民  开放</p><h2 id="金娜静"><a href="#金娜静" class="headerlink" title="金娜静"></a>金娜静</h2><p><img src="/2024/20240830/4.jpg"></p><p>代名：SUGAR </p><p>男性杂志模特 、自由职业播音员。梨花女子大学出身。喜欢玛丽莲梦露。</p><p>右派 平权 富有 开放</p><h2 id="金宰变"><a href="#金宰变" class="headerlink" title="金宰变"></a>金宰变</h2><p><img src="/2024/20240830/5.jpg"></p><p>代名：超人</p><p>国民力量党。反对共产主义的家庭。有家人朝鲜战争去世。</p><p>右派 平权 富有 传统</p><h2 id="朴成珉"><a href="#朴成珉" class="headerlink" title="朴成珉"></a>朴成珉</h2><p><img src="/2024/20240830/6.jpg"></p><p>代名：北极熊</p><p>政治人。文在寅党青瓦台最年轻的秘书官。</p><p>左派 女权 平民  开放</p><h2 id="李昌俊"><a href="#李昌俊" class="headerlink" title="李昌俊"></a>李昌俊</h2><p><img src="/2024/20240830/7.jpg"></p><p>代名：黑暗骑士</p><p>特种部队13年</p><p>右派 平权 平民 传统</p><h2 id="李智娜"><a href="#李智娜" class="headerlink" title="李智娜"></a>李智娜</h2><p><img src="/2024/20240830/8.jpg"></p><p>代名：GENIE</p><p>精英职员。毕业于香港大学。唯一富有拿到3分的人。市场营销。</p><p>右派 女权 富有 传统</p><h2 id="林贤瑞"><a href="#林贤瑞" class="headerlink" title="林贤瑞"></a>林贤瑞</h2><p><img src="/2024/20240830/9.jpg"></p><p>代名：本杰明</p><p>律师。首尔大学法学院。</p><p>右派 平权 平民 开放</p><p>（不良分子）</p><h2 id="李秀莲"><a href="#李秀莲" class="headerlink" title="李秀莲"></a>李秀莲</h2><p><img src="/2024/20240830/10.jpg"></p><p>代名：浪子</p><p>警卫员出身的演员</p><p>右派 平权 富有 传统</p><h2 id="全民基"><a href="#全民基" class="headerlink" title="全民基"></a>全民基</h2><p><img src="/2024/20240830/11.jpg"></p><p>代名：GREY</p><p>播音员出身的大数据专家</p><p>左派 女权 富有 开放</p><h2 id="安跟英"><a href="#安跟英" class="headerlink" title="安跟英"></a>安跟英</h2><p><img src="/2024/20240830/12.jpg"></p><p>代名：高爱信</p><p>从小打冰球。现任冰球指导。</p><p>左派 女权 富有 传统</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>利用物理知识进行预测的机器学习综述</title>
    <link href="/2024/20240828/"/>
    <url>/2024/20240828/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《<a href="https://arxiv.org/html/2408.09840">Machine Learning with Physics Knowledge for Prediction: A Survey</a>》</p><p>36页的综述，还是比较详细的。</p><hr><p>这项调查研究了将机器学习与物理知识相结合进行预测的广泛方法和模型，重点关注偏微分方程。这些方法引起了人们的极大兴趣，因为它们通过使用小型或大型数据集改进预测模型以及具有有用归纳偏差的表达预测模型，对推进科学研究和工业实践产生潜在影响。该调查分为两个部分。第一个考虑通过目标函数、结构化预测模型和数据增强将物理知识融入架构级别。第二个观点将<em>数据</em>视为物理知识，这促使人们将多任务、元和情境学习视为以数据驱动的方式整合物理知识的替代方法。最后，我们还提供了这些方法应用的工业视角，以及对物理信息机器学习的开源生态系统的调查。</p><span id="more"></span><hr><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p> 结合数据和机器学习方法的各种方式</p><p><img src="/2024/20240828/figure1.png"></p><p>贡献：</p><p>这项调查的贡献是从机器学习的角度关注物理信息学习的挑战。这包括研究基于物理的架构和损失函数，以及适用于基于物理的环境的模型和方法，例如潜变量模型、数据增强、多任务学习和元学习。该调查旨在实现几个关键目标。首先，我们的目标是向机器学习研究人员识别和传达物理信息学习中的突出挑战，为未来的研究方向提供途径。其次，该调查提供了重要的指导，将物理知识学习概念定位在更广泛的机器学习领域以及各个工程领域之间的联系中。第三，我们建立了一个精细的方法分类法，为分类和区分不同方法提供了一个结构化框架。此外，该调查还评估了用于评估这些方法和用例的工业相关性的框架。 最后，我们确定了可能影响工业应用的未来趋势。通过这些贡献，该调查旨在弥合理论研究和实际实施之间的差距，使基于物理的机器学习更加强大和有用，以解决现实世界中的棘手问题。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><strong>常微分方程</strong></p><p><strong>偏微分方程</strong></p><p><strong>系统识别（System Identification）：</strong></p><p>系统辨识是利用输入和输出的测量来构建动力系统数学模型的实践。这种实践是整个科学和工程的基础，以便根据观察到的现实来建立和校准数学模型。</p><p>系统识别涉及通过一系列输入来定义系统的动态$w_ t$,测量值$y_ t$,以及初始状态 $x_ {1}$ 在有限的地平线上$T$ 作为轨迹，即 $Y&#x3D;[ \mathbf{y}_ 1,…,\mathbf{y}_ T]$ 。</p><p>通常，假设马尔可夫动力系统，即状态$x_ t+1&#x3D;f_ \theta (x_ t,w_t)$和观察$y_ t&#x3D;g_ \theta(x_ t)$两者参数化为$ \theta$,以及状态和观测值之间的映射$g$通常被设计为简化学习问题并确保可观察性。此外，可以添加附加噪声项来捕获动态和传感器中的不确定性，从而产生可能性$p(x_ {t+1}\mid x_ t,w_ t,\theta)$ 和$p(\mathbf{y}_ t\mid\mathbf{x}_ t,\boldsymbol{\theta})$。</p><p>由于轨迹数据不是独立同分布(IID),因此数据集上的典型回归方法可能无法产生令人满意的结果，因为随着时间的推移，预测误差可能会因累积的预测误差而发生漂移。一种可能的解决方案是通过时间反向传播，</p><p>$$<br>L_{BPTT}(\theta) &#x3D; \sum_{k&#x3D;1}^{K} \sum_{t&#x3D;1}^{T-h} \sum_{j&#x3D;1}^{h} L(y_{t+j+1}^{(k)}, g_{\theta}(f_{\theta} \circ \cdots \circ f_{\theta}(x_{t}^{(k)}, w_{t}^{(k)})),<br>$$<br>其中考虑了预测误差$\kappa$时间范围内的轨迹$h$ 。这捕获了预测问题的顺序性质，但需要更昂贵且可能更难以优化的巨标。概率方法考虑联合分布$p(Y,X|W,\theta)$继续马尔可夫假设，分解为$p(x_1)\Pi_{t&#x3D;1}^Tp(y_t|x_t,\theta)p(x_{t+1}|x_t,w_t,\theta)$。潜在状态$x$使用贝叶斯过滤和平滑算法推断,以及模型参数$\theta$进行优化以最大化测量的对数边际可能性$\mathcal{L}_\mathrm{LML}(\boldsymbol{\theta})&#x3D;\log\int p(\boldsymbol{Y},\boldsymbol{X}|\boldsymbol{W},\boldsymbol{\theta})$d$\boldsymbol{X}$使用期望最大化 (EM)等算法。如果后验概率为1，则需要近似推理技术$p(X\mid Y,W,\theta)$不能以封闭形式计算。</p><p><strong>神经网络函数逼近</strong></p><h2 id="知识驱动的物理学先验"><a href="#知识驱动的物理学先验" class="headerlink" title="知识驱动的物理学先验"></a>知识驱动的物理学先验</h2><h3 id="从数据中学习差分模型"><a href="#从数据中学习差分模型" class="headerlink" title="从数据中学习差分模型"></a>从数据中学习差分模型</h3><p>学习物理过程模型的一个挑战是我们的现实存在于连续时间中，而数值算法需要离散化。在这种情况下，一个有用的归纳偏差是通过将积分纳入预测来定义连续时间内的（黑盒）模型。结果是一个更灵活的预测模型，可以在不规则的时间间隔上学习和预测，而不是严格的离散化。这种方法称为学习微分方程。</p><h4 id="常微分方程"><a href="#常微分方程" class="headerlink" title="常微分方程"></a>常微分方程</h4><p>ODE 和深度学习之间的联系——神经 ODE，是一个强大的工具。也许学习微分方程的最早尝试是He 等人提出的 ResNet 架构（即残差流块）。尽管最初的动机是减轻早期层参数的梯度衰减。 ResNet 块定义离散时间残差变换:$x_ {t+1} &#x3D; x_ t +f_ {\theta_ t}(x_ t)$</p><p>受 ResNet 架构的启发， Weinan ( <a href="https://arxiv.org/html/2408.09840v1#bib.bib33">2017</a> )和Haber 等人。 ( <a href="https://arxiv.org/html/2408.09840v1#bib.bib34">2018</a> )都提出参数化（无穷小）连续时间变换:</p><p>$\frac{\mathrm{d}x(t)}{\mathrm{d}t}&#x3D;f_ {\theta}(x(t),t),$</p><p>这相当于离散时间变换 $x_ {t+1}-x_  t&#x3D;\epsilon f_ \theta(x_ t)$，其中$\epsilon\to 0$。从神经网络的角度来看， Chen 等人表明方程上述相当于具有起始输入的“连续深度”神经网络 $x_0$ 和连续权重 $\theta$ 。这种网络的可逆性自然来自于 ODE 解的存在唯一性定理。</p><p>使用神经 ODE 进行预测和训练需要选择 ODE 求解技术，这又需要时间变量的离散化。神经 ODE 的强大之处在于其训练时的内存效率，无需通过求解器进行反向传播，从而使其能够有效地学习长范围数据序列。</p><h3 id="具有物理信息损失的学习解决方案领域"><a href="#具有物理信息损失的学习解决方案领域" class="headerlink" title="具有物理信息损失的学习解决方案领域"></a>具有物理信息损失的学习解决方案领域</h3><h3 id="代数结构的学习模型"><a href="#代数结构的学习模型" class="headerlink" title="代数结构的学习模型"></a>代数结构的学习模型</h3><h3 id="算子学习"><a href="#算子学习" class="headerlink" title="算子学习"></a>算子学习</h3><h3 id="动力系统的潜变量模型"><a href="#动力系统的潜变量模型" class="headerlink" title="动力系统的潜变量模型"></a>动力系统的潜变量模型</h3><h3 id="桥接系统辨识与功能逼近"><a href="#桥接系统辨识与功能逼近" class="headerlink" title="桥接系统辨识与功能逼近"></a>桥接系统辨识与功能逼近</h3><h3 id="不变性驱动的先验"><a href="#不变性驱动的先验" class="headerlink" title="不变性驱动的先验"></a>不变性驱动的先验</h3><h2 id="数据驱动的实验先验"><a href="#数据驱动的实验先验" class="headerlink" title="数据驱动的实验先验"></a>数据驱动的实验先验</h2><p>pass</p><h2 id="工业应用探讨"><a href="#工业应用探讨" class="headerlink" title="工业应用探讨"></a>工业应用探讨</h2><p>文献中描述的利用物理知识进行机器学习的工业应用案例</p><p><img src="/2024/20240828/table3.png"></p><p>尽管应用前景广阔，但仍不确定这些方法是否会取代传统的数值方法。在天气预报等数据丰富的环境中，机器学习已经取得了重大进展（Lam 等人， 2023 年；Kurth 等人， 2023 年） 。然而，在工业环境（例如过程工业或制造业）中应用机器学习的真正挑战在于，与消费市场的要求相比，它们的要求不同（Hoffmann 等人， 2021a ） 。工业环境通常处理稀疏数据，操作员可能会因其不透明性而对黑匣子系统持怀疑态度。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>物理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Classifying Nodes in Graphs without GNNs</title>
    <link href="/2024/20240824/"/>
    <url>/2024/20240824/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>图神经网络（GNN）是对图中节点进行分类的主要范例，但它们具有一些源于其消息传递架构的不良属性。最近，蒸馏方法成功地消除了测试时 GNN 的使用，但在训练期间仍然需要它们。作者提出了一种完全无 GNN 的节点分类方法，在训练或测试时不需要它们。该方法由三个关键部分组成：平滑约束、伪标签迭代和邻域标签直方图。</p><span id="more"></span><blockquote><p>PS：我觉得“无GNN”的说法并不准确，它只是没有把图结构和NN结合起来，而不是不用上任何图结构来进行学习。</p></blockquote><hr><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>蒸馏方法最近挑战了节点分类中的现有范式。 GNN 的标准做法是在图中所有标记节点上训练模型，并在测试时使用相同的模型进行节点分类。蒸馏方法消除了在测试时使用 GNN 的需要，尽管它们仍然需要训练 GNN。他们使用 GNN 来伪标记无监督节点。随后，训练 MLP 仅根据其节点特征来预测这些节点的标签，而不考虑其邻居的特征。值得注意的是，在多个数据集上，蒸馏方法与 GNN 具有竞争力。这个结果令人困惑，因为图结构在训练期间似乎有益，但在测试期间却没有。这就引出了一个问题：为什么蒸馏方法如此成功？</p><p>作者指出：应对数据集不在于提高模型表达能力，而在于降低模型样本复杂性。 GNN 通过有用的归纳偏差克服了这一挑战，而蒸馏则通过使用 GNN 伪标签增加训练集的大小来克服这一挑战。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>CoHOp - Consistency and Histogram Optimization</p><p><img src="/2024/20240824/psu_code.png"></p><p>该模型的损失分为两部分。</p><p>一部分为分类的交叉熵损失：<br>$$<br>L_{GT}(\Psi)&#x3D;\sum_{(x_i,y_i)\in V_{train}}CE(\Psi(x_i),y_i)<br>$$<br>一部分为：<br>$$<br>L_{consist}(\Psi)&#x3D;\sum_{v_i\in V}\left(\frac{1}{|N(v_i)|}\sum_{v_j\in N(v_i)}CE(\Psi(x_i),\Psi(x_j))\right)<br>$$</p><p>最终：<br>$$<br>L(\Psi) &#x3D; L_{GT}(\Psi) + \gamma \cdot L_{consist}(\Psi)<br>$$</p><h3 id="伪标签迭代"><a href="#伪标签迭代" class="headerlink" title="伪标签迭代"></a>伪标签迭代</h3><p>在每次迭代之前，我们将模型以高置信度进行预测的节点以及现有的地面实况训练节点添加到训练集中。我们将模型预测作为这些非真实节点的标签，并将它们称为伪标签。<br>$$<br>V_{train}^{I+1} &#x3D; V_{train} \cup {v_i | \max_{j&#x3D;1,…,C}(\Psi^I(x_i))_j &gt; \tau }<br>$$<br>另外还加了一个伪标签平滑：<br>$$<br>Y^{*}&#x3D;\lambda\cdot\hat{Y}+(1-\lambda)\cdot\hat{A}\hat{Y}<br>$$<br>其实就是标签传播</p><h3 id="直方图"><a href="#直方图" class="headerlink" title="直方图"></a>直方图</h3><p>$$<br>h_i’ &#x3D; \sum_{v_j \in N^\ell(v_i) \cap V_{train}} (\alpha^{d(v_i, v_j)} \cdot y_j)<br>$$</p><p>$N^\ell$指为$\ell$距离以内的。</p><p>$y_j\in {0,1}^C$为onthot的向量</p><p>最后会做一个归一化：<br>$$<br>h_i &#x3D; \frac{h_i’}{\sum_{j&#x3D;1}^C h_{ij}’}<br>$$<br>对于较大的数据集，例如 ogbn-products，这种计算变得很麻烦。为了解决这个问题，提出了 h 的有效近似。通过使用卷积运算扩展标签来联合计算图中所有节点的直方图。具体来说，矩阵 H 的行表示每个节点的非标准化直方图，通过以下方式获得：<br>$$<br>H’&#x3D;\sum_{k&#x3D;1}^\ell\left(\alpha\hat{A}\right)^k\tilde{Y}<br>$$<br>$\tilde{Y}$的第i行 为：</p><p><img src="/2024/20240824/for.png"></p><h4 id="精确算和近似算的比较"><a href="#精确算和近似算的比较" class="headerlink" title="精确算和近似算的比较"></a>精确算和近似算的比较</h4><p><img src="/2024/20240824/exa.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RW-NSGCN：通过负采样进行结构性攻击的稳健方法</title>
    <link href="/2024/20240814/"/>
    <url>/2024/20240814/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://arxiv.org/abs/2408.06665">论文地址</a></p><p>图结构网络通常包含拓扑扰动和权重扰动形式的潜在噪声和攻击，这可能导致 GNN 的分类性能下降。为了提高模型的鲁棒性，该论文提出了一种新方法：随机游走负采样图卷积网络（RW-NSGCN）。具体来说，RW-NSGCN 集成了用于负采样的随机游走（RWR）和 PageRank（PGR）算法，并采用基于行列式点过程（DPP）的 GCN 进行卷积运算。 RWR 利用全局和局部信息来管理噪声和局部变化，而 PGR 评估节点重要性以稳定拓扑结构。基于 DPP 的 GCN 确保负样本之间的多样性，并聚合其特征以产生鲁棒的节点嵌入，从而提高分类性能。</p><hr><span id="more"></span><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p>目前的方法主要集中于聚合来自邻居的数据风险评估和模式识别的节点，其忽略来自非相邻节点的信息，这会间接影响网络的整体流量和权重分布。这种局部视角限制了它们响应拓扑变化和重量分布波动的能力。因此，理解非邻居节点之间复杂的关系对于开发更准确的分类模型至关重要。</p><p>一些研究，如SDGCN [ <a href="https://arxiv.org/html/2408.06665v1#bib.bib25">25</a> ] ，引入了负采样机制，但其随机负采样方法未能充分利用节点和全局结构信息的重要性。因此，这些方法捕获非相邻节点之间复杂关系的能力有限，阻碍了对网络内节点全局影响力的准确评估。这反过来又影响节点表示和模型的整体稳定性。</p><p>**拓扑扰动：**这种攻击有选择地删除关键边缘，加剧图网络内自然发生的拓扑变化。它模拟极端情况下可能发生的碎片化和连接中断，旨在评估模型在面临重大结构扰动时的表现，并评估其处理现实网络复杂变化的能力。</p><p>**权重扰动：**这种攻击会修改边权重或引入随机噪声，以放大图中边权重的现有波动。该方法通过模拟极端权重扰动，评估模型处理信息流路径变化和节点影响力变化的鲁棒性，保证模型在动态环境下的可靠性。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="负样本的选取"><a href="#负样本的选取" class="headerlink" title="负样本的选取"></a>负样本的选取</h3><p>首先，通过计算最短路径长度$d(v_i,v_j)$在节点对之间，我们可以确定两个节点之间的关系链长度。然后，我们为每个节点生成一组可达节点$v_i$在不同的路径长度$l:$<br>$$N_l(v_{i})&#x3D;{v_{j}|d(v_{i},v_{j})&#x3D;l}$$<br>此步骤有助于识别与节点具有特定路径关系的节点集。</p><p><img src="/2024/20240814/method1.png"></p><p>$$s_{ij}&#x3D;\beta\cdot(1-\alpha)\left[(\mathbf{I}-\alpha\mathbf{P})^{-1}\mathbf{e}_i\right]_j+(1-\beta)\left[\alpha\mathbf{P}(\alpha\mathbf{P}\mathbf{p}+(1-\alpha)\mathbf{e})+(1-\alpha)\mathbf{e}\right]_j\quad\forall v_j\in\mathcal{N}_l(v_i),$$</p><p>其中$\mathbf{P}&#x3D;\mathbf{D}^{-1}\mathbf{A}$ 是转移矩阵，$\alpha$是重启概率，$\beta$是RWR和PGR的权重参数。</p><p>基于这些综合评分向量，我们从不同路径长度的节点集中选择得分最高的节点作为候选节点：</p><p>$$\text{candidates}(v_i)&#x3D;\bigcup\limits_{l &#x3D; 1}^{L}\operatorname{top}({s_{ij}|v_j\in\mathcal{N}_l(v_i)})$$</p><p>通过整合不同距离的非邻居节点的关键节点信息，节点可以捕获多尺度特征，获取从局部到全局视角的多样化信息，从而更全面地表征其特征。此外，合并来自不同距离的节点的信息增加了信息多样性，避免了仅仅依赖局部邻域信息的局限性，从而提高了模型的鲁棒性和泛化能力。当包含来自更远距离节点的信息时，模型可以更好地理解全局结构，捕获全局模式并处理远程依赖性，特别是在本地信息不足时提供有价值的上下文信息。综上所述，该方法有效解决了图结构网络中的拓扑漏洞和权重不稳定问题，增强了整体稳定性和鲁棒性。</p><h2 id="基于带有-DPP-采样的标签传播的-GCN"><a href="#基于带有-DPP-采样的标签传播的-GCN" class="headerlink" title="基于带有 DPP 采样的标签传播的 GCN"></a>基于带有 DPP 采样的标签传播的 GCN</h2><p>通过将行列式点过程（DPP）与图卷积网络（GCN）相结合，我们在保证节点多样性的同时，有效捕获节点和社区之间的特征关联，从而提高防御能力。</p><p>首先，模型使用DPP核矩阵来衡量节点对之间的关联强度，定义如下：<br>$$<br>L&#x3D;Q\cdot(S_{com}\cdot S_{com}^{T})\cdot exp(S_{node}-1)\cdot Q^{T}<br>$$</p><p>在此背景下，L表示 DPP核矩阵，其中每个元素 $L_{ij}$表示节点之间的关联强度 $v_ i、v_j$和均选自 candidates(v)。</p><p>这 $s$ 矩阵由三种类型的相似度矩阵组成：</p><p>$$(S_{node})_{ij} &#x3D; cos(x_i, x_j) &#x3D; \frac{x_i \cdot x_j}{|x_i||x_j|}$$</p><p>$$(S_{com})_{ij} &#x3D; cos(f_i, f_j) &#x3D; \frac{f_i \cdot f_j}{|f_i||f_j|}$$</p><p>$$Q_{ij} &#x3D; cos(c_i, f_j) &#x3D; \frac{c_i \cdot f_j}{|c_i||f_j|}$$</p><p>第一条公式量化了特征空间内节点的相似度。</p><p>第二条公式衡量了社区特征之间的相似度。</p><p>第三条公式表示中心节点和社区特征之间的相似度。</p><p>上述步骤旨在有效捕获节点和社区之间的特征关系，获取节点和社区之间丰富的信息。</p><p>构建DPP核矩阵L后 ，进行采样，保证所选节点样本具有足够的代表性和多样性，避免节点选择出现偏差和过度集中。</p><p>DPP得到的负样本主要用在GCN消息传递阶段：</p><p>$$x_{\mathrm{pos}}^{(l+1)}&#x3D;\mathrm{ReLU}\Big(\mathbf{W}^{(l)}\cdot\mathrm{normalize}(x^{(l)},\mathbf{D},\mathbf{A})\Big)$$</p><p>$$x_{\mathrm{neg}}^{(l+1)}&#x3D;\mathrm{ReLU}(W_{\mathrm{DPP}}^{(l)}\cdot\mathrm{normalize}(x^{(l)},D_{\mathrm{DPP}},A_{\mathrm{DPP}}))$$</p><p>$$x^{(l+1)}&#x3D;x_{\mathrm{pos}}^{(l+1)}-\lambda\cdot x_{\mathrm{neg}}^{(l+1)}$$</p><p>首先，我们使用标准的GCN卷积来传播正样本的特征，捕获正常行为的特征。</p><p>接下来，我们介绍 DPP 采样机制来选择负样本节点并传播其特征。</p><p>最后，通过减去负样本的特征（乘以平衡系数）来更新节点表示 $\lambda$）来自阳性样本。这种方法保证了所选节点子集在特征空间中是离散的，允许选择彼此不太相似的节点，从而避免冗余信息选择，有效防止过拟合，增强对图结构中异常行为的敏感性。</p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CAAFE-将大型语言模型与半自动化数据科学的表格预测器相结合</title>
    <link href="/2024/20240811/"/>
    <url>/2024/20240811/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><a href="https://arxiv.org/abs/2305.03403">CAAFE</a>这是一种针对表格数据集的特征工程方法，该方法利用LLM基于数据集的描述，迭代地为表格数据集生成附加的语义上有意义的特征。该方法生成用于创建新功能的 Python 代码以及对所生成功能的实用程序的解释。尽管方法简单，但 CAAFE 提高了 14 个数据集中的 11 个的性能 - 将所有数据集的平均 ROC AUC 性能从 0.798 提高到 0.822 - 类似于在我们的数据集上使用随机森林而不是逻辑回归所实现的改进。此外，CAAFE 可以通过为每个生成的特征提供文本解释来解释。 CAAFE 为数据科学任务中更广泛的半自动化铺平了道路，并强调了上下文感知解决方案的重要性。</p><p>Team AGA等人使用该方法同autogluon结合，获得了<a href="https://www.kaggle.com/competitions/playground-series-s4e8/discussion/524752">TPS S4E8 automl比赛</a>的第二名</p><span id="more"></span><hr><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><p><img src="/2024/20240811/motivation.png"></p><p>上下文信息可以极大地简化任务。在左侧，图中没有添加任何上下文信息，并且很难预测绿色查询点的标签。在右侧添加了上下文信息，并派生出有用的附加特征（周末或工作日），从中可以找到从特征到目标的映射。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="/2024/20240811/fig4.png"></p><p>上图显示了CAAFE在Tic-Tac-Toe Endgame数据集上的示例运行。用户生成的输入以蓝色显示，ml分类器生成的数据以红色显示，LLM生成的代码以语法高亮显示。生成的代码包含每个生成的特性的注释，该注释遵循我们提示中提供的模板(特性名称、有用性描述、生成代码中使用的特性以及这些特性的样例值)。在这次运行中，CAAFE在两次特征工程迭代中将验证数据集上的ROC AUC从0.888提高到1.0。</p><p><img src="/2024/20240811/full.png"></p><p>上图显示了CMC dataset的完整prompt。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>表格学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>最大率失真与图上的应用</title>
    <link href="/2024/20240708/"/>
    <url>/2024/20240708/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最大率失真（Maximal Coding Rate Reduction）是马毅组的作品。用于构建可解释性的神经网络。</p><p>马毅教授自称”弄明白了深度学习“，并把这一套理论命名为Deep (Convolution) Networks from First Principles。可以看出是比较大的名头。</p><span id="more"></span><h2 id="最大率失真"><a href="#最大率失真" class="headerlink" title="最大率失真"></a>最大率失真</h2><p>《Learning Diverse and Discriminative Representations via the Principle of Maximal Coding Rate Reduction》</p><p>2007年马毅等人提出了编码整个数据集的feature所需要的比特数。</p><p><img src="/2024/20240708/1.jpg"></p><p>一个很自然的想法即所有特征的空间最大，但类内的特征空间尽量小。<strong>智能即压缩！</strong> </p><p><img src="/2024/20240708/15.jpg"></p><p>其会使得特征空间正交化。</p><p><img src="/2024/20240708/2.jpg"></p><p><img src="/2024/20240708/3.jpg"></p><p>进一步我们可以使用projected gradient ascent。</p><p><img src="/2024/20240708/4.jpg"></p><p>这长得像线性回归，所以我们可以执行那一套。</p><p>我们还可以使用softmax来进行分类。</p><p><img src="/2024/20240708/5.jpg"></p><p>最终进行整理，我们可以得到以下结构，这正是马毅等提出的Redunet。</p><p><img src="/2024/20240708/redunet.jpg"></p><p>可以看到有resnet、MOE等的影子。</p><h2 id="图上的应用"><a href="#图上的应用" class="headerlink" title="图上的应用"></a>图上的应用</h2><p>我们主要讲WWW2022《Geometric Graph Representation Learning via Maximizing Rate Reduction》。ICASSP2024也有一篇《Maximal Coding Rate Reduction for Graph Embeddings》。</p><p>原始的MCR2的公式为<br>$$<br>\frac{1}{2}logdet(I+\frac{d}{m\epsilon^2}ZZ^T)-\sum \frac{\Pi_j}{2m}logdet(I+\frac{d}{tr(\Pi_j)\epsilon^2}Z\Pi_j Z^T)<br>$$<br>作者改写为：<br>$$<br>\frac{1}{2}logdet(I+\frac{\gamma_1p}{m\epsilon}ZZ^T)-\sum \frac{sum(\Pi_j)}{2m}logdet(I+\frac{p}{tr(\Pi_j)\epsilon}Z\Pi_j Z^T)<br>$$<br>p是特征维度，$\Pi$是邻接矩阵。</p><p>代码：<a href="https://github.com/ahxt/G2R">https://github.com/ahxt/G2R</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MaximalCodingRateReduction</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, gam1=<span class="hljs-number">1.0</span>, gam2=<span class="hljs-number">1.0</span>, eps=<span class="hljs-number">0.01</span></span>):<br>        <span class="hljs-built_in">super</span>(MaximalCodingRateReduction, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.gam1 = gam1<br>        <span class="hljs-variable language_">self</span>.gam2 = gam2<br>        <span class="hljs-variable language_">self</span>.eps = eps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">projection</span>(<span class="hljs-params">self, z: torch.Tensor</span>) -&gt; torch.Tensor:<br>        z = F.normalize(z, dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> z<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_discrimn_loss_empirical</span>(<span class="hljs-params">self, W</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Empirical Discriminative Loss.&quot;&quot;&quot;</span><br>        p, m = W.shape<br>        I = torch.eye(p).cuda()<br>        scalar = p / (m * <span class="hljs-variable language_">self</span>.eps)<br>        logdet = torch.logdet(I + <span class="hljs-variable language_">self</span>.gam1 * scalar * W.matmul(W.T))<br>        <span class="hljs-keyword">return</span> logdet / <span class="hljs-number">2.</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_compress_loss_empirical_all</span>(<span class="hljs-params">self, W, Pi</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;Empirical Compressive Loss.&quot;&quot;&quot;</span><br>        p, m = W.shape<br>        k, _ = Pi.shape<br>        sum_trPi = torch.<span class="hljs-built_in">sum</span>(Pi)<br><br>        I = torch.eye(p).cuda()<br>        compress_loss = <span class="hljs-number">0.</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>            trPi = torch.<span class="hljs-built_in">sum</span>(Pi[j]) + <span class="hljs-number">1e-8</span><br>            scalar = p / (trPi * <span class="hljs-variable language_">self</span>.eps)<br>            a = W.T * Pi[j].view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>            a = a.T<br>            log_det = torch.logdet(I + scalar * a.matmul(W.T))<br>            compress_loss += log_det * trPi / m<br>        num = data.x.shape[<span class="hljs-number">0</span>]<br>        compress_loss = compress_loss / <span class="hljs-number">2</span> * (num / sum_trPi)<br>        <span class="hljs-keyword">return</span> compress_loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X, A</span>):<br><br>        i = np.random.randint(A.shape[<span class="hljs-number">0</span>], size=num_node_batch)<br>        A = A[i,::]<br><br>        A = A.cpu().numpy()<br>        W = X.T<br>        Pi = A<br>        Pi = torch.tensor(Pi, dtype=torch.float32).cuda()<br><br>        discrimn_loss_empi = <span class="hljs-variable language_">self</span>.compute_discrimn_loss_empirical(W)<br>        compress_loss_empi = <span class="hljs-variable language_">self</span>.compute_compress_loss_empirical_all(W, Pi)<br>        total_loss_empi = - <span class="hljs-variable language_">self</span>.gam2 * discrimn_loss_empi + compress_loss_empi<br>        <span class="hljs-keyword">return</span> total_loss_empi<br></code></pre></td></tr></table></figure><p>可以看到效果是相当不错的。</p><p><img src="/2024/20240708/77.jpg"></p><p><img src="/2024/20240708/88.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图鲁棒性论文速览</title>
    <link href="/2024/20240629/"/>
    <url>/2024/20240629/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="《Learning-Graph-Neural-Networks-with-Noisy-Labels-（arxiv-2019-》"><a href="#《Learning-Graph-Neural-Networks-with-Noisy-Labels-（arxiv-2019-》" class="headerlink" title="《Learning Graph Neural Networks with Noisy Labels （arxiv 2019)》"></a>《Learning Graph Neural Networks with Noisy Labels （arxiv 2019)》</h2><p><strong>Motivation</strong>：标签噪声使泛化差距（测试准确率下降，训练准确率不变）增加。</p><span id="more"></span><p><strong>Method</strong>:</p><p><img src="/2024/20240629/1.png"></p><p><strong>Strength</strong>: 方法简单，计算量小</p><p><strong>Weakness</strong>: 结果疑似不好，根据NoisyGL</p><h2 id="《Symmetric-Cross-Entropy-for-Robust-Learning-with-Noisy-Labels-（ICCV-2019）》"><a href="#《Symmetric-Cross-Entropy-for-Robust-Learning-with-Noisy-Labels-（ICCV-2019）》" class="headerlink" title="《Symmetric Cross Entropy for Robust Learning with Noisy Labels （ICCV 2019）》"></a>《Symmetric Cross Entropy for Robust Learning with Noisy Labels （ICCV 2019）》</h2><p><strong>Motivation</strong>：交叉熵：（1）hard class处理不好；（2）遇到noisy label表现下降很多。作者受反向KL散度启发提出了另一种交叉熵</p><p><strong>Method</strong>: </p><p>交叉熵(CE)： -q(x)logp(x)</p><p>RCE:  -p(x)logq(x)</p><p>最终loss:  αCE+βRCE</p><p><strong>Strength</strong>:</p><p><strong>Weakness</strong>: 根据NoisyGL，效果比baseline高不了多少，可能是因为图数据的标签稀缺性</p><h2 id="《iterative-deep-graph-learning-for-graph-neural-networks-better-and-robust-node-embeddings-（Nips-2020-》"><a href="#《iterative-deep-graph-learning-for-graph-neural-networks-better-and-robust-node-embeddings-（Nips-2020-》" class="headerlink" title="《iterative deep graph learning for graph neural networks better and robust node embeddings （Nips 2020)》"></a>《iterative deep graph learning for graph neural networks better and robust node embeddings （Nips 2020)》</h2><p><strong>Motivation</strong>：</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/idgl.png"></p><p><strong>Strength</strong>:相似性使用了多头设置，学习能力更强。使用了历史邻接矩阵信息。<strong>Weakness</strong>:</p><h2 id="《Adversarial-Label-Flipping-Attack-and-Defense-for-Graph-Neural-Networks-（ICDM-2020）》"><a href="#《Adversarial-Label-Flipping-Attack-and-Defense-for-Graph-Neural-Networks-（ICDM-2020）》" class="headerlink" title="《Adversarial Label-Flipping Attack and Defense for Graph Neural Networks （ICDM 2020）》"></a>《Adversarial Label-Flipping Attack and Defense for Graph Neural Networks （ICDM 2020）》</h2><p><strong>Motivation</strong>：社区标签和GCN输出具有一致性</p><p><img src="/2024/20240629/af_moti.png"></p><p><strong>Method</strong>: 使用community信息来帮助分类。先Embedding再聚类（k-means+NodeVec），得到community标签。有了community label后，将训练GCN用来预测community label作为辅助自监督任务，将L-1层的隐藏特征输入，输出K-路softmax 分布。</p><p><img src="/2024/20240629/af1.png"></p><p><strong>Strength</strong>: 形式简单</p><p><strong>Weakness</strong>: 耗时严重（根据NoisyGL，运行时间最慢的第二个）。网络部分共享结构，此处不耗时。耗时应该在社区标签计算上。此处可能有改进空间。</p><h2 id="《Data-Augmentation-for-Graph-Neural-Networks-AAAI-2021）》"><a href="#《Data-Augmentation-for-Graph-Neural-Networks-AAAI-2021）》" class="headerlink" title="《Data Augmentation for Graph Neural Networks  (AAAI 2021）》"></a>《Data Augmentation for Graph Neural Networks  (AAAI 2021）》</h2><p><strong>Motivation</strong>：最明显的方法涉及添加或删除节点或边。但问题仍然存在，哪些边缘需要改变？</p><p><strong>Method</strong>:</p><p>（1）使用边缘预测器函数（文中使用的是GAE）来获取𝒢中所有可能和现有边缘的边缘概率。边缘预测器的作用是灵活的，一般可以用任何合适的方法代替。</p><p> (2) 使用预测的边概率，我们确定性地添加（删除）新（现有）边以创建修改后的图𝒢 ，该图用作 GNN 节点分类器的输入。</p><h2 id="《NRGNN-learning-a-lable-noise-resistant-graph-neural-network-on-sparsely-and-noisily-labeled-graphs-2021-KDD》"><a href="#《NRGNN-learning-a-lable-noise-resistant-graph-neural-network-on-sparsely-and-noisily-labeled-graphs-2021-KDD》" class="headerlink" title="《NRGNN learning a lable noise-resistant graph neural network on sparsely and noisily labeled graphs 2021 KDD》"></a>《NRGNN learning a lable noise-resistant graph neural network on sparsely and noisily labeled graphs 2021 KDD》</h2><p><strong>Motivation</strong>：由于图上的标签稀疏，错误标记的节点可能会影响其邻域内未标记的节点，从而难以接受正确标记节点的监督。为了解决这些问题，NRGNN 首先连接具有相似特征的节点以创建精细图。基于这个精细化的图，生成精确的伪标签，使未标记的样本能够接受来自正确标记的样本的更多监督，从而减少噪声标签的影响。</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/nrgnn.png"></p><p><strong>Strength</strong>: 使用了单独的边预测器，更好的利用特征信息</p><p><strong>Weakness</strong>: 边预测器单使用该loss而不使用分类器，疑似无增长。在理论上假设节点的特征相似性与其标签有较高的相关性</p><h2 id="《Unified-Robust-Training-for-Graph-NeuralNetworks-against-Label-Noise-（PAKDD-2021-》"><a href="#《Unified-Robust-Training-for-Graph-NeuralNetworks-against-Label-Noise-（PAKDD-2021-》" class="headerlink" title="《Unified Robust Training for Graph NeuralNetworks against Label Noise （PAKDD 2021)》"></a>《Unified Robust Training for Graph NeuralNetworks against Label Noise （PAKDD 2021)》</h2><p><strong>Motivation</strong>：结构邻近性（直接或间接连接）高的节点往往具有相似的标签-&gt;随机游走</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/PAKDD2021.png"></p><p><strong>Strength</strong>: 样本有重新加权，可以让可靠标签在梯度更新时做出更多共享。标签校正在高噪声下，不能保证性能增益，而通过先验分布损失的正则化能发挥作用。</p><p><strong>Weakness</strong>: 需要更多的时间复杂度和空间占用，因为要随机游走来收集</p><h2 id="《EvenNet-Ignoring-Odd-Hop-Neighbors-Improves-Robustness-of-Graph-Neural-Networks-（NIPS-2022-》"><a href="#《EvenNet-Ignoring-Odd-Hop-Neighbors-Improves-Robustness-of-Graph-Neural-Networks-（NIPS-2022-》" class="headerlink" title="《EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks （NIPS 2022)》"></a>《EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks （NIPS 2022)》</h2><p><strong>Motivation</strong>：源于符号网络的平衡理论提供了一个很好的视角：“敌人的敌人就是我的朋友，我朋友的朋友也是我的朋友。”</p><p><strong>Method</strong>: </p><p><img src="/2024/20240629/evennet.png"></p><h2 id="《Towards-Unsupervised-Deep-Graph-Structure-Learning-WWW-2022-》"><a href="#《Towards-Unsupervised-Deep-Graph-Structure-Learning-WWW-2022-》" class="headerlink" title="《Towards Unsupervised Deep Graph Structure Learning (WWW 2022)》"></a>《Towards Unsupervised Deep Graph Structure Learning (WWW 2022)》</h2><p><strong>Method</strong>:</p><p><img src="/2024/20240629/www.png"></p><h2 id="《Reliable-Representations-Make-A-Stronger-Defender-Unsupervised-Structure-Refinement-for-Robust-GNN（KDD-2022-》"><a href="#《Reliable-Representations-Make-A-Stronger-Defender-Unsupervised-Structure-Refinement-for-Robust-GNN（KDD-2022-》" class="headerlink" title="《Reliable Representations Make A Stronger Defender: Unsupervised Structure Refinement for Robust GNN（KDD 2022)》"></a>《Reliable Representations Make A Stronger Defender: Unsupervised Structure Refinement for Robust GNN（KDD 2022)》</h2><p><strong>Motivation</strong>：原始特征不能表示节点的节点的各种属性（如结构信息等）。</p><p>故方法要：1）携带特征信息，同时携带尽可能多的正确结构信息；2）对结构扰动不敏感。只有少数邻居的节点比高度节点更容易受到攻击。对抗性边缘倾向于连接两个低度节点。</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/KDD2022.png"></p><p><strong>Strength</strong>: 图增强提高学习能力。Advanced GCN中针对低度高度做了不同的聚合权重</p><p><strong>Weakness</strong>: 相似度策略比较单调。特征打乱如果按类内打乱效果会更好（（ICML 2024）Feature Distribution on Graph Topology Mediates the Effect of Graph Convolution: Homophily Perspective）。</p><h2 id="《toward-robust-graph-neural-networks-for-noisy-graphs-with-sparse-labels-2022WSDM-》"><a href="#《toward-robust-graph-neural-networks-for-noisy-graphs-with-sparse-labels-2022WSDM-》" class="headerlink" title="《toward robust graph neural networks for noisy graphs with sparse labels (2022WSDM)》"></a>《toward robust graph neural networks for noisy graphs with sparse labels (2022WSDM)》</h2><p><strong>Strength</strong>:使用了单独的边预测器，更好的利用特征信息</p><p><strong>Method</strong>: </p><p><img src="/2024/20240629/rsgnn.png"></p><h2 id="《Learning-on-Graphs-under-Label-Noise-（ICASSP-2023-》"><a href="#《Learning-on-Graphs-under-Label-Noise-（ICASSP-2023-》" class="headerlink" title="《Learning on Graphs under Label Noise （ICASSP 2023)》"></a>《Learning on Graphs under Label Noise （ICASSP 2023)》</h2><p><strong>Motivation</strong>：需要一种没有标签信息的正则化项，以获得针对标签噪声的卓越模型泛化能力</p><p><strong>Method</strong>: 首先对图进行增强，第一种是随机丢边，第二种是随机mask节点属性。再使用对比学习和标签矫正。</p><p><img src="/2024/20240629/iscapp2023.png"></p><p><strong>Strength</strong>:两次图增强都公用一套GCN，耗时小。使用了多种图增强技术。图增强过程中不涉及标签。</p><p><strong>Weakness</strong>:方法简单。相当于只进行了图增强+标签矫正。</p><h2 id="《Multi-teacher-Self-training-for-Semi-supervised-Node-Classification-with-Noisy-Labels-（ACM-MM-2023-》"><a href="#《Multi-teacher-Self-training-for-Semi-supervised-Node-Classification-with-Noisy-Labels-（ACM-MM-2023-》" class="headerlink" title="《Multi-teacher Self-training for Semi-supervised Node Classification with Noisy Labels （ACM MM 2023)》"></a>《Multi-teacher Self-training for Semi-supervised Node Classification with Noisy Labels （ACM MM 2023)》</h2><p><strong>Motivation</strong>：根据文献《Selc: Self-ensemble label correction improves learning with noisy labels.》，在带有噪声标签的数据集上训练神经网络模型时，模型倾向于首先学习简单(干净)的样本。</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/multiteacher.png"></p><p>使用先前的作为教师模型，并积累教师模型的参数。来进行标签处理</p><h2 id="《Robust-Training-of-Graph-Neural-Networks-via-Noise-Governance（WSDM-2023）》"><a href="#《Robust-Training-of-Graph-Neural-Networks-via-Noise-Governance（WSDM-2023）》" class="headerlink" title="《Robust Training of Graph Neural Networks via Noise Governance（WSDM 2023）》"></a>《Robust Training of Graph Neural Networks via Noise Governance（WSDM 2023）》</h2><p><strong>Motivation</strong>：作者指出，虽然 NRGNN 强调通过链接预测为未标记节点提供额外的监督，但它并没有区分错误标记和正确标记的节点。相反，它只是不加区别地连接具有相似特征的节点，这可能会导致错误标记节点的影响扩散。为了解决这个问题，作者提出了RTGNN，它在NRGNN的基础上构建，利用Co-teaching中的小损失准则来进一步区分可信和不可信节点，并纠正一些不可信节点的标签，减轻一些不可信节点的标签。</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/rtgnn.png"></p><p>边预测器部分类似NRGNN第二部分类似于coteaching，并对peer GCN进行KL散度正则化</p><p><strong>Strength</strong>: 划分了多种节点的类型，cl、ns、sr。具有NRGNN的优点。</p><p><strong>Weakness</strong>:NoisyGL中显示RTGNN疑似效果在大部分数据集上都不如GCN。多个神经网络，时间复杂度略高。</p><h2 id="《Robust-Node-Classification-on-Graph-Data-with-Graph-and-Label-Noise-（AAAI-2023-》"><a href="#《Robust-Node-Classification-on-Graph-Data-with-Graph-and-Label-Noise-（AAAI-2023-》" class="headerlink" title="《Robust Node Classification on Graph Data with Graph and Label Noise （AAAI 2023)》"></a>《Robust Node Classification on Graph Data with Graph and Label Noise （AAAI 2023)》</h2><p><strong>Method</strong>:</p><p><img src="/2024/20240629/aaai2023.png"></p><p><strong>Strength</strong>:</p><p><strong>Weakness</strong>: 耗时严重。NoisyGL显示是最慢的，几千倍于GCN</p><h2 id="《Clnode-Curriculum-learning-for-node-classification》（WSDM-2023）"><a href="#《Clnode-Curriculum-learning-for-node-classification》（WSDM-2023）" class="headerlink" title="《Clnode:Curriculum learning for node classification》（WSDM 2023）"></a>《Clnode:Curriculum learning for node classification》（WSDM 2023）</h2><p><strong>Motivation</strong>：这些工作通常假设所有训练节点的贡献相等。事实上，训练节点的质量差异很大</p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/clnode.png"></p><p><strong>Strength</strong>:将节点区分开来处理</p><p><strong>Weakness</strong>:假设同质性</p><h2 id="《Noise-robust-Graph-Learning-by-Estimating-and-Leveraging-Pairwise-Interactions-（TMLR-2023-》"><a href="#《Noise-robust-Graph-Learning-by-Estimating-and-Leveraging-Pairwise-Interactions-（TMLR-2023-》" class="headerlink" title="《Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions （TMLR 2023)》"></a>《Noise-robust Graph Learning by Estimating and Leveraging Pairwise Interactions （TMLR 2023)》</h2><p><strong>Motivation</strong>：</p><p><img src="/2024/20240629/pignn_mot.png"></p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/pignn.png"></p><p><strong>Strength</strong>: 有效地利用了节点相互的关系，找到了一种比标签噪声要小的噪声特性<strong>Weakness</strong>: 只在同亲图（即，如果两个节点相连，那么它们很有可能具有相同的节点标签）上表现优异</p><h2 id="《Universally-Robust-Graph-Neural-Networks-by-Preserving-Neighbor-Similarity-（arxiv-2024-》"><a href="#《Universally-Robust-Graph-Neural-Networks-by-Preserving-Neighbor-Similarity-（arxiv-2024-》" class="headerlink" title="《Universally Robust Graph Neural Networks by Preserving Neighbor Similarity （arxiv 2024)》"></a>《Universally Robust Graph Neural Networks by Preserving Neighbor Similarity （arxiv 2024)》</h2><p><strong>Motivation</strong>：普通鲁棒模型依靠自我特征的相似性来检测良性链接中的潜在恶意链接。然而，这种策略可能不适合在异质性场景下增强gnn的对抗鲁棒性。攻击与节点相似度呈负相关。图攻击者倾向于连接相似度得分值较低的节点对。</p><p>不同相似度矩阵下的攻击表现：相似度矩阵定义：</p><p><img src="/2024/20240629/arxiv2024_mot.png"></p><p><strong>Method</strong>:</p><p><img src="/2024/20240629/a2024.png"></p><p>其分为正knn和负knn。对于正knn，我们为每个节点挑选出最接近的top-𝑘邻居来构建相应的kNN图。对于负knn，和正knn相反，这种方式可以捕获极端不同的信息，并作为正图的对立物。在双knn图构建阶段之后，确定聚合机制的最佳选择以有效地传播节点特征，同时保持邻居相似度是很重要的。为此，我们引入了一种自适应邻居相似度引导的传播机制。节点表示学习的信息流的形式为:</p><p><img src="/2024/20240629/a20242.png"></p><p><strong>Strength</strong>:从正反两方面考虑问题，以往一般只考虑一边，可解释性强</p><p><strong>Weakness</strong>:是否忽略了中通滤波，如arxiv2023的Robust Mid-Pass Filtering Graph Convolutional Networks</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Softmax Linear Units Softmax</title>
    <link href="/2024/20240605/"/>
    <url>/2024/20240605/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>出自Anthropic’s transformer circuits thread</p><p><a href="https://transformer-circuits.pub/2022/solu/index.html">Softmax Linear Units Softmax</a></p><p>在本文中，我们报告了一种架构变化，该变化似乎大大增加了看起来“可解释”的 MLP 神经元的比例（即响应输入的可表达属性），而对 ML 性能的影响很小甚至没有。具体来说，我们用 softmax 线性单元（我们称之为 SoLU）替换激活函数，并表明这显着增加了 MLP 层中神经元的比例，这些神经元在快速调查中似乎对应于人类容易理解的概念、短语或类别，通过随机和盲法实验测量。然后，我们研究 SoLU 模型，并使用它们来获得有关 Transformer 中信息如何处理的一些新见解。然而，我们也发现了一些证据，证明叠加假设是正确的，并且天下没有免费的午餐：SoLU 可能通过“隐藏”其他特征来使某些特征更容易解释，从而使它们更加难以解释。尽管如此，SoLU 似乎仍然是一个净胜利，因为实际上它大大增加了我们能够理解的神经元的比例。</p><span id="more"></span><h2 id="SOLU的提出"><a href="#SOLU的提出" class="headerlink" title="SOLU的提出"></a>SOLU的提出</h2><p>GeLU是一种现代transformer常用的激活函数</p><p>$\mathrm{GELU}(x)&#x3D;xP(X\leq x)&#x3D;x\Phi(x)&#x3D;x\cdot\frac{1}{2}\left[1+\mathrm{erf}(x&#x2F;\sqrt{2})\right]$</p><p>一种近似版本：</p><p>$0.5x\Big(1+\tanh\Big[\sqrt{2&#x2F;\pi}\Big(x+0.044715x^{3}\Big)\Big]\Big)$ 或$x\sigma(1.702x)$</p><p>于是Anthropic研究者们考虑了将sigmoid换为softmax。</p><p>将此激活函数称为“softmax 线性单元”或 SoLU：</p><p>SoLU(x)&#x3D;x*softmax(x)</p><p>当 SoLU 应用于大值和小值的向量时，大值将抑制较小值：</p><p>SoLU(4,1,4,1)&#x3D;（1.9051, 0.0237, 1.9051, 0.0237）≈(2, 0, 2, 0)</p><p>也许更重要的是，保留了大的基础对齐向量，而跨多个维度分布的特征将被抑制到较小的量级：</p><p>SoLU(4,0,0,0) ≈ (4,0,0,0)</p><p>SoLU(1,1,1,1) ≈ ($\frac{1}{4}$, $\frac{1}{4}$, $\frac{1}{4}$, $\frac{1}{4}$)</p><p>初步实验发现，简单地使用 SoLU 激活函数似乎可以使神经元更容易解释，但会付出很大的性能代价。一般来说，没有任何其他改变的 SoLU 模型的性能相当于比实际尺寸小 30-50% 的模型，较大的模型受到的影响更大。  如果叠加假设成立，这正是我们所期望看到的——我们可以减少多语义性，但这样做会损害网络的机器学习性能。</p><p>然而，我们凭经验发现，通过在 SoLU 之后应用额外的 LayerNorm（类似于 </p><p>[25]</p><p> ），可以修复这种性能损失，同时保留可解释性增益。这极大地提高了 ML 性能，因此对于我们的大多数实验，我们实际应用的函数是 2 但请注意，我们尝试解释的激活是在额外层规范之前，而不是之后：<br>$$<br>f(x)&#x3D;LN(SoLU(x))&#x3D;LN(x*softmax(x))<br>$$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>“请注意，以任意性能代价来构建提高可解释性的架构既微不足道又无趣。”</strong></p><p><img src="/2024/20240605/train_curve.png"></p><p> 如图所示，SoLU 大致相当于所有模型大小的基线，始终落在模型大小的 1.05 倍和 0.95 倍乘数之间（与大多数情况下相比，大致相当于 ±0.01 纳特的损失变化）总共损失 1.6-3 纳特）。  尽管所有差异都很小并且更有可能是随机噪声（在 50B 模型上，SoLU 相当于将模型大小增加 1.01 倍），但在大模型尺寸下，SoLU 的性能可能会比基线稍好一些。 ）。</p><p><img src="/2024/20240605/downstream.png"></p><p>上图为下游任务的性能。由于SoLU 使用基线的一系列超参数，并且 SoLU 的最佳超参数可能与基线模型的最佳超参数不同。但表明 SoLU 至少与基线一样好。</p><p>总的来说，我们得出的结论是，与标准 Transformer 相比，带有 LayerNorm 的 SoLU 似乎实现了有竞争力的 ML 和训练性能。</p><h2 id="可解释性"><a href="#可解释性" class="headerlink" title="可解释性"></a>可解释性</h2><h3 id="定量结果"><a href="#定量结果" class="headerlink" title="定量结果"></a>定量结果</h3><p>我们关于哪些部分的神经元可以初步解释的实验结果如下图 4所示。对于从 1 层到 40 层的模型，SoLU 模型的神经元比基线的神经元更具可解释性，增加了大约 25 个绝对百分点，从约 35% 可解释到约 60% 可解释。  这将可解释神经元的比例增加了 1.7 倍。  尽管效果大小适中，但样本量、一致的间隙和一致的可解释神经元绝对率表明 SoLU 模型具有真实且持久的效果。</p><p><img src="/2024/20240605/fraction.png"></p><p> 蓝线显示了标记为初步表明基线变换器中模型大小范围从 1 到 64 层的解释的神经元分数。  红线显示了标记为初步暗示 SoLU 转换器中的解释的神经元部分。  绿点显示了标记为初步表明 16 层模型中带有额外层范数但不带有 SoLU 的解释的神经元的分数。  总体而言，在 1 层到 40 层的模型中，SoLU 将可解释神经元的比例增加了约 25%，而在 64 层模型中，增益要小得多。</p><p>我们不知道为什么 64L 模型从 SoLU 中受益较少，但一种可能的理论是，随着模型变得更大，它们的神经元代表更复杂的概念并且变得更难以理解。这表明神经元仍然是可解释的，但不再“容易解释”。</p><h3 id="定性"><a href="#定性" class="headerlink" title="定性"></a>定性</h3><h4 id="一层模型神经元"><a href="#一层模型神经元" class="headerlink" title="一层模型神经元"></a>一层模型神经元</h4><p><img src="/2024/20240605/1layer.png"></p><p>1 层 SoLU 模型中的神经元似乎对 Base64 编码的文本进行触发（左）。神经元对 logits 的扩展权重（右）增加了混合情况下很少出现在单词中的一堆标记的概率，同时降低了代表英语单词的多个标记的概率，这一事实证实了这一点。可以将其理解为一个可解释的规则，即在 Base64 文本上，下一个标记更有可能也是 Base64。</p><h4 id="较大模型中的前面的神经元（”DE-TOKENIZATION”）"><a href="#较大模型中的前面的神经元（”DE-TOKENIZATION”）" class="headerlink" title="较大模型中的前面的神经元（”DE-TOKENIZATION”）"></a>较大模型中的前面的神经元（”DE-TOKENIZATION”）</h4><p>接下来我们将探索转向更大的型号——我们剩下的示例将来自 16L、24L、40L 和 64L 型号的混合。  我们最有趣的发现之一是，大型网络的早期、中期和晚期层的神经元往往扮演着非常不同类型的角色，就像已知的卷积网络视觉模型不同深度的特征是不同的一样。  我们将在各自的部分中讨论每个神经元，从前面层的神经元开始。</p><p>前面层神经元似乎经常参与将标记的“人工”结构映射到更自然、语义上有意义的表示。</p><ul><li><strong>神经元对被分成多个标记的特定单词做出反应：</strong>“银行”、“措辞”、“胆固醇”、“自由主义者”、“平民”、“上海”、“尽管如此”……</li><li><strong>神经元对名人的名字做出反应：</strong>“马丁·路德·金”、“唐纳德·特朗普”、“林登·约翰逊”、“乔治·奥威尔”、“欧内斯特·海明威”、“穆罕默德·阿里”、“奥普拉·温弗瑞”……（参见 [17] ）</li><li><strong>神经元对其他名词做出反应：</strong>“人权观察”、“国际货币基金组织”、“马修飓风”、“皇家马德里”……</li><li><strong>神经元对复合词做出反应：</strong>“读书俱乐部”、“社会保障”、“计算机视觉”、“有组织犯罪”、“生日聚会”、“心脏病发作”……</li><li><strong>神经元响应 LaTeX“\”命令：</strong>“\left”、“\frac{”、“\begin”…</li></ul><h4 id="较大模型中的后层神经元（“RE-TOKENIZATION”）"><a href="#较大模型中的后层神经元（“RE-TOKENIZATION”）" class="headerlink" title="较大模型中的后层神经元（“RE-TOKENIZATION”）"></a>较大模型中的后层神经元（“RE-TOKENIZATION”）</h4><p>后期层神经元（靠近网络输出的神经元）通常会做与早期层神经元相反的事情：它们介导单词或上下文标记返回文字标记的转换。例如，最后一层中的一个神经元会触发标记“st”，同时增加后续标记是“rag”的可能性；本质上，这是一种将单词“stragglers”的表示形式逐个转换或指示为其组成标记以进行输出的方法。类似地，“nappies”输出神经元在标记“n”上触发，并增加标记“app”帮助写入“nappies”的概率。这些神经元本质上模拟了一个额外的输出词汇项，该词汇项仅在前面的标记满足某些条件时才可用。</p><p><img src="/2024/20240605/retoken.png"></p><h4 id="较大模型中的中间层神经元"><a href="#较大模型中的中间层神经元" class="headerlink" title="较大模型中的中间层神经元"></a>较大模型中的中间层神经元</h4><p>中间层的神经元通常代表更复杂、抽象的想法。例如，当且仅当数字涉及多个人时，有一个神经元似乎代表数字：</p><p><img src="/2024/20240605/number.png"></p><p>在这些层中可以发现各种各样有趣的神经元。我们观察到的一些常见类别包括：</p><ul><li>对特定类型的描述性从句激发的神经元：对描述声音的从句激发的神经元、对描述服装的从句激发的神经元、对音乐描述性从句激发的神经元（例如“C 大调”）、对从句激发的神经元描述写在物体上的文字，…</li><li>对话语标记做出反应的神经元：对强调某件事的重要性的标记做出反应的神经元（例如“令人惊奇的是”），对对冲做出反应的神经元（例如“在我看来……”），……</li><li>消除对标记的特殊解释的歧义的神经元：当用作成绩时对 A&#x2F;B&#x2F;C&#x2F;D 做出响应的神经元，对日期的“日”部分做出响应的神经元，对数字进行响应的神经元是菜谱中的一个数量，一个响应字符串中 C 风格格式说明符（例如“%s”或“%d”）的神经元，……</li></ul><p>但是有很多神经元很难归入这些类别，例如似乎有助于解析 ASCII 表列的神经元。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>可解释性</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图彩票论文速览</title>
    <link href="/2024/20240521/"/>
    <url>/2024/20240521/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文将介绍一系列的图彩票论文。</p><hr><h2 id="《a-unified-lottery-ticket-hypothesis-for-graph-neural-networks-2021ICML-pdf》"><a href="#《a-unified-lottery-ticket-hypothesis-for-graph-neural-networks-2021ICML-pdf》" class="headerlink" title="《a unified lottery ticket hypothesis for graph neural networks(2021ICML).pdf》"></a>《a unified lottery ticket hypothesis for graph neural networks(2021ICML).pdf》</h2><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="Lottery-Ticket-Hypothesis"><a href="#Lottery-Ticket-Hypothesis" class="headerlink" title="Lottery Ticket Hypothesis"></a>Lottery Ticket Hypothesis</h4><p>该论文首先提到了ICLR 2019最佳论文:The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks。该论文提出了彩票假说：密集的、随机初始化的、前馈网络包含子网络（中奖票），这些子网络在孤立地训练时，在类似数量的迭代中达到与原始网络相当的测试精度。</p><p><a href="https://arxiv.org/abs/1905.01067">《Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask》</a>也有类似的内容。展示了为什么将权重设置为零很重要，如何使用符号来进行重新初始化的网络训练，以及为什么掩蔽的行为类似于训练。最后，我们发现了超级掩码的存在，这些掩码可以应用于未经训练的随机初始化网络，以生成性能远远优于偶然的模型（MNIST 上为 86%，CIFAR-10 上为 41%）。</p><p>ICLR2020 的《<a href="http://proceedings.mlr.press/v119/malach20a/malach20a.pdf">Proving the Lottery Ticket Hypothesis: Pruning is All You Need</a>宣称证明了The Lottery Ticket Hypothesis。一句话概括：只要对随机初始化的神经网络做个好剪枝，不怎么训练也能有个好效果。</p><p>该文证明了：</p><p>Fix some target fully-connected ReLU-network F of width k, depth d and input dimension n.Fix$\delta&gt;0$.Then,arandomly-initialized network $G$ of width $poly(d,n,k,1&#x2F;\epsilon,\log(1&#x2F;\delta))$ and depth 2d, has w.p. $\geq1-\delta$ a subnetwork $\tilde{G}$ that approximates F up to $\epsilon.$</p><p>简单的说，给定一个深度为d的Relu目标网络。那么一个深度为2d，且足够宽的随机网络里，必然可以找到一个可以逼近目标网络的子网络。</p><span id="more"></span><h3 id="本文"><a href="#本文" class="headerlink" title="本文"></a>本文</h3><p>本文这项工作不仅是第一个将 LTH 推广到 GNN 的工作，而且也是第一个将 LTH 从简化模型扩展到新的数据模型联合简化前景的工作。</p><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><p><img src="/2024/20240521/image-20240518120157877.png"></p><h5 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h5><p>GLT 的推理时间复杂度为,其中 L 是层数。</p><p>$||\boldsymbol{m} _g\odot\boldsymbol{A}|| _0$是稀疏图中剩余边的数量。</p><p>F是节点特征的维度，$|\mathcal{V}|$是节点的数量。内存复杂度为$o(\mathcal{L} \times \left| \mathcal{V} \right| \times \mathcal{F}+ \mathcal{L} \times \left| m_ \theta \right| _ 0 \times \mathcal{F}^2)$。在我们的实现中，剪枝的边将从$\varepsilon$ （边集合）中删除，并且不会参与下一轮的计算。</p><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a><strong>代码</strong></h4><p>我们来直接看<a href="https://github.com/VITA-Group/Unified-LTH-GNN/blob/main/NodeClassification/main_pruning_imp.py">代码</a>：</p><h5 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h5><p>主函数中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&quot;__main__&quot;</span>:<br><span class="hljs-comment">####...........</span><br>    rewind_weight = <span class="hljs-literal">None</span><br>    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>        final_mask_dict, rewind_weight = run_get_mask(args, seed, p, rewind_weight)<br>        <span class="hljs-comment">###</span><br>        <span class="hljs-comment">###从final_mask_dict中保存mask到rewind_weight，剪枝但保持其他权重和初始化一样</span><br>        <span class="hljs-comment">###</span><br>        best_acc_val, final_acc_test, final_epoch_list, adj_spar, wei_spar = run_fix_mask(args, seed, rewind_weight)<br>        <span class="hljs-comment">###省略所有的print</span><br></code></pre></td></tr></table></figure><p>每一个epochs中包括了run_get_mask和run_fix_mask，前者是获得mask，后者是保持mask，对模型继续训练。</p><h5 id="run-get-mask函数"><a href="#run-get-mask函数" class="headerlink" title="run_get_mask函数"></a><strong>run_get_mask函数</strong></h5><p>模型代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">net_gcn</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embedding_dim, adj</span>):<br>        <span class="hljs-variable language_">self</span>.adj_mask1_train = nn.Parameter(<span class="hljs-variable language_">self</span>.generate_adj_mask(adj))<br><span class="hljs-comment">###省略</span><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, adj, val_test=<span class="hljs-literal">False</span></span>):<br>        adj = torch.mul(adj, <span class="hljs-variable language_">self</span>.adj_mask1_train)<span class="hljs-comment">#点乘mask</span><br>        adj = torch.mul(adj, <span class="hljs-variable language_">self</span>.adj_mask2_fixed)<span class="hljs-comment">#点乘mask</span><br>        adj = <span class="hljs-variable language_">self</span>.normalize(adj)<br>        <span class="hljs-keyword">for</span> ln <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.layer_num):<br>            x = torch.mm(adj, x)<br>            x = <span class="hljs-variable language_">self</span>.net_layer[ln](x)<br>            <span class="hljs-keyword">if</span> ln == <span class="hljs-variable language_">self</span>.layer_num - <span class="hljs-number">1</span>:<br>                <span class="hljs-keyword">break</span><br>            x = <span class="hljs-variable language_">self</span>.relu(x)<br>            <span class="hljs-keyword">if</span> val_test:<br>                <span class="hljs-keyword">continue</span><br>            x = <span class="hljs-variable language_">self</span>.dropout(x)<br>        <span class="hljs-keyword">return</span> x<br>    <span class="hljs-comment">###省略</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_adj_mask</span>(<span class="hljs-params">self, input_adj</span>):<br>        <br>        sparse_adj = input_adj<br>        zeros = torch.zeros_like(sparse_adj)<br>        ones = torch.ones_like(sparse_adj)<br>        mask = torch.where(sparse_adj != <span class="hljs-number">0</span>, ones, zeros)<br>        <span class="hljs-keyword">return</span> mask<br>    <span class="hljs-comment">###省略</span><br></code></pre></td></tr></table></figure><p>训练部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">####....</span><br>loss_func = nn.CrossEntropyLoss()<br>net_gcn = net.net_gcn(embedding_dim=args[<span class="hljs-string">&#x27;embedding_dim&#x27;</span>], adj=adj)<br>pruning.add_mask(net_gcn)<span class="hljs-comment">#给边加mask</span><br><span class="hljs-comment">####....</span><br><span class="hljs-keyword">if</span> rewind_weight_mask:<br>        net_gcn.load_state_dict(rewind_weight_mask) <span class="hljs-comment">#恢复权重</span><br><span class="hljs-comment">####....</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args[<span class="hljs-string">&#x27;total_epoch&#x27;</span>]):<br>        optimizer.zero_grad()<br>        output = net_gcn(features, adj)<br>        loss = loss_func(output[idx_train], labels[idx_train])<br>        loss.backward()<br>        pruning.subgradient_update_mask(net_gcn, args) <span class="hljs-comment"># l1 norm</span><br>        optimizer.step()<br>        <span class="hljs-comment">####以下为验证部分，忽略</span><br></code></pre></td></tr></table></figure><p>我们仔细分析这部分：</p><p>对于add_mask函数，对边加mask。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">add_mask</span>(<span class="hljs-params">model, init_mask_dict=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> init_mask_dict <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>: <br>        mask1_train = nn.Parameter(torch.ones_like(model.net_layer[<span class="hljs-number">0</span>].weight))<br>        mask1_fixed = nn.Parameter(torch.ones_like(model.net_layer[<span class="hljs-number">0</span>].weight), requires_grad=<span class="hljs-literal">False</span>)<br>        mask2_train = nn.Parameter(torch.ones_like(model.net_layer[<span class="hljs-number">1</span>].weight))<br>        mask2_fixed = nn.Parameter(torch.ones_like(model.net_layer[<span class="hljs-number">1</span>].weight), requires_grad=<span class="hljs-literal">False</span>)<br>    <span class="hljs-keyword">else</span>:<br>        mask1_train = nn.Parameter(init_mask_dict[<span class="hljs-string">&#x27;mask1_train&#x27;</span>])<br>        mask1_fixed = nn.Parameter(init_mask_dict[<span class="hljs-string">&#x27;mask1_fixed&#x27;</span>], requires_grad=<span class="hljs-literal">False</span>)<br>        mask2_train = nn.Parameter(init_mask_dict[<span class="hljs-string">&#x27;mask2_train&#x27;</span>])<br>        mask2_fixed = nn.Parameter(init_mask_dict[<span class="hljs-string">&#x27;mask2_fixed&#x27;</span>], requires_grad=<span class="hljs-literal">False</span>)<br>    AddTrainableMask.apply(model.net_layer[<span class="hljs-number">0</span>], <span class="hljs-string">&#x27;weight&#x27;</span>, mask1_train, mask1_fixed)<br>    AddTrainableMask.apply(model.net_layer[<span class="hljs-number">1</span>], <span class="hljs-string">&#x27;weight&#x27;</span>, mask2_train, mask2_fixed)<br>....   <br><span class="hljs-comment">#AddTrainableMask.apply部分</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">apply</span>(<span class="hljs-params">cls, module, name, mask_train, mask_fixed, *args, **kwargs</span>):<br>    method = cls(*args, **kwargs)  <br>    method._tensor_name = name<br>    orig = <span class="hljs-built_in">getattr</span>(module, name)<br>    module.register_parameter(name + <span class="hljs-string">&quot;_mask_train&quot;</span>, mask_train.to(dtype=orig.dtype))<br>    module.register_parameter(name + <span class="hljs-string">&quot;_mask_fixed&quot;</span>, mask_fixed.to(dtype=orig.dtype))<br>    module.register_parameter(name + <span class="hljs-string">&quot;_orig_weight&quot;</span>, orig)<br>    <span class="hljs-keyword">del</span> module._parameters[name]<br>    <span class="hljs-built_in">setattr</span>(module, name, method.apply_mask(module))<br>    module.register_forward_pre_hook(method)<br>    <span class="hljs-keyword">return</span> method<br></code></pre></td></tr></table></figure><p>对于subgradient_update_mask函数，他是一个 l1 norm</p><p>具体而言</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">subgradient_update_mask</span>(<span class="hljs-params">model, args</span>):<br>    model.adj_mask1_train.grad.data.add_(args[<span class="hljs-string">&#x27;s1&#x27;</span>] * torch.sign(model.adj_mask1_train.data))<br>    model.net_layer[<span class="hljs-number">0</span>].weight_mask_train.grad.data.add_(args[<span class="hljs-string">&#x27;s2&#x27;</span>] * torch.sign(model.net_layer[<span class="hljs-number">0</span>].weight_mask_train.data))<br>    model.net_layer[<span class="hljs-number">1</span>].weight_mask_train.grad.data.add_(args[<span class="hljs-string">&#x27;s2&#x27;</span>] * torch.sign(model.net_layer[<span class="hljs-number">1</span>].weight_mask_train.data))<br></code></pre></td></tr></table></figure><p>简单来说，我们知道，$\frac{d}{dx}|x|&#x3D;sgn(x)$，这里相当于做了一个梯度下降。</p><p>其余的就是传统的三件套</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer.zero_grad()<br>loss.backward()<br>optimizer.step()<br></code></pre></td></tr></table></figure><h5 id="run-fix-mask函数"><a href="#run-fix-mask函数" class="headerlink" title="run_fix_mask函数"></a><strong>run_fix_mask函数</strong></h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_func = nn.CrossEntropyLoss()<br>net_gcn = net.net_gcn(embedding_dim=args[<span class="hljs-string">&#x27;embedding_dim&#x27;</span>], adj=adj)<br>pruning.add_mask(net_gcn)<br>net_gcn = net_gcn.cuda()<br>net_gcn.load_state_dict(rewind_weight_mask)<br>adj_spar, wei_spar = pruning.print_sparsity(net_gcn)<br><span class="hljs-comment">#多了这部分，将所有的mask都移出训练</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> net_gcn.named_parameters():<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;mask&#x27;</span> <span class="hljs-keyword">in</span> name:<br>        param.requires_grad = <span class="hljs-literal">False</span><br>optimizer = torch.optim.Adam(net_gcn.parameters(), lr=args[<span class="hljs-string">&#x27;lr&#x27;</span>], weight_decay=args[<span class="hljs-string">&#x27;weight_decay&#x27;</span>])<br>acc_test = <span class="hljs-number">0.0</span><br>best_val_acc = &#123;<span class="hljs-string">&#x27;val_acc&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;epoch&#x27;</span> : <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;test_acc&#x27;</span>: <span class="hljs-number">0</span>&#125;<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">200</span>):<span class="hljs-comment">#不能指定epochs</span><br>    optimizer.zero_grad()<br>    output = net_gcn(features, adj)<br>    loss = loss_func(output[idx_train], labels[idx_train])<br>    loss.backward()<br>    <span class="hljs-comment">#此处少了pruning.subgradient_update_mask(net_gcn, args) # l1 normsubgradient_update_mask</span><br>    optimizer.step()<br>    <span class="hljs-comment">####以下为验证部分，忽略</span><br></code></pre></td></tr></table></figure><p>基本和run_get_mask一样，不同在于，将mask移出训练，也少了l1，和限定epochs。</p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>pass</p><h2 id="《searching-lottery-tickets-in-graph-neural-networks-a-dual-perspective-2023ICLR-pdf"><a href="#《searching-lottery-tickets-in-graph-neural-networks-a-dual-perspective-2023ICLR-pdf" class="headerlink" title="《searching lottery tickets in graph neural networks a dual perspective(2023ICLR).pdf&gt;"></a>《searching lottery tickets in graph neural networks a dual perspective(2023ICLR).pdf&gt;</h2><p>代码：<a href="https://github.com/Lyccl/RGLT">https://github.com/Lyccl/RGLT</a></p><h3 id="相关研究"><a href="#相关研究" class="headerlink" title="相关研究"></a>相关研究</h3><p>探索了其对偶问题并提出对偶彩票假说 DLTH：给定随机初始化的网络，其随机挑选的子网络可以被转换成彩票子网络，并得到与 LTH 找到的彩票子网络相当甚至更好的准确率。</p><p>算法</p><p>DiffPool+mask+GIR（Gradually Increased Regularization）</p><p>它的mask矩阵只作用在领接矩阵上。</p><p><img src="/2024/20240521/image-20240518184812842.png" alt="image-20240518184812842"></p><p>整体算法逻辑为：</p><p>1.DiffPool模型训练+GIR</p><p>2.one_shot_prune</p><p>3.run_fine_tune</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python">model = DiffPool(input_dim,<br>                     hidden_dim,<br>                     embedding_dim,<br>                     label_dim,<br>                     activation,<br>                     prog_args.gc_per_block,<br>                     prog_args.dropout,<br>                     prog_args.num_pool,<br>                     prog_args.linkpred,<br>                     prog_args.batch_size,<br>                     <span class="hljs-string">&#x27;meanpool&#x27;</span>,<br>                     assign_dim,<br>                     prog_args.pool_ratio)<br><span class="hljs-comment">###省略</span><br>weight_decays = get_weight_decays(count)<br>masks, unmasks = getMasks(model, w_ratio)<span class="hljs-comment">#随机获取mask?</span><br>logger = train(<br>    mask,<br>    train_dataloader,<br>    model,<br>    optimizer,<br>    prog_args,<br>    weight_decays,<br>    count,<br>    masks,<br>    val_dataset=val_dataloader)<br><span class="hljs-comment">#省略评估</span><br>one_shot_prune(model, unmasks)<br>new_logger = run_fine_tune(mask, model, optimizer, count, prog_args, train_dataloader, weight_decays, masks, unmasks,<br>                           logger)<br></code></pre></td></tr></table></figure><h3 id="DiffPool"><a href="#DiffPool" class="headerlink" title="DiffPool"></a>DiffPool</h3><p>这部分该论文的代码和dgl库的Diffpool完全一样，该论文加了个mask。</p><p>例如下图所示，多了红框处的代码。</p><p><img src="/2024/20240521/image-20240518185703168.png" alt="image-20240518185703168"></p><p>该论文出自NeurIPS 2018，它是一种可微图池化模块，可以生成图的层次表示，并可以以端到端的方式与各种图神经网络架构相结合。</p><p>模型框架：</p><p><img src="/2024/20240521/image-20240518185452420.png" alt="image-20240518185452420"></p><p>DIFFPOOL 可以表达为 :</p><p>$\text{}\left(A^{(l+1)},X^{(l+1)}\right)&#x3D;\mathrm{DiFF~POOL}\left(A^{(l)},Z^{(l)}\right)$</p><p>即<br>$$<br>\begin{aligned}&amp;X^{(l+1)}&#x3D;S^{(l)^{T}}Z^{(l)}\in\mathbb{R}^{n_ {l+1}\times d}\quad(3)\&amp;A^{(l+1)}&#x3D;S^{(l)^{T}}A^{(l)}S^{(l)}\in\mathbb{R}^{n_ {l+1}\times n_ {l+1}}\quad(4)\end{aligned}<br>$$<br>Z称为嵌入矩阵，S称为分配矩阵。</p><p>并设计了两套GNN，来获得嵌入矩阵和分配矩阵。<br>$$Z^{(l)}&#x3D;\mathrm{GNN}_ {l,\mathrm{~embed}}\left(A^{(l)},X^{(l)}\right)$$</p><p>$$S^{(l)}&#x3D;\mathrm{softmax}\big(\mathrm{GNN}_ {l,\mathrm{pool}}\big(A^{(l)},X^{(l)}\big)\big)$$<br>Note: 最后一层设置聚类分配矩阵设置输出大小为 1。</p><p>作者说，4很难通过梯度进行训练，所以本文采用 最小化Frobenius norm<br>$$<br>L_ {\mathrm{LP}}&#x3D;\left|A^{(l)},S^{(l)}S^{(l)^{T}}\right|_ {F}<br>$$</p><blockquote><p>这里写的不明白，应该是$$L_ {\mathrm{LP}}&#x3D;\left|A^{(l)}-S^{(l)}S^{(l)^{T}}\right|_ {F}$$</p></blockquote><p>每个聚类分配矩阵 被希望接近于一个 one-hot 向量，以便明确每个簇的隶属关系，所以本文通过最小化簇分配的熵：<br>$$<br>\bar{L}_ {E}&#x3D;\frac{1}{n}\sum_ {i&#x3D;1}^{n}H(S_ {i})<br>$$<br>其中：H为熵函数$H(X)&#x3D;-\sum_x\in\tau p(x)\log(x);$<br>$S_i$为 $S$的第$i$行；</p><blockquote><p>然而，据作者<a href="https://github.com/RexYing/diffpool/issues/24">所说</a>，原因是由于分配预测包含 [0,1] 之间的值，因此交叉熵比 Frobenius 范数中的 l2 更有效。<a href="https://github.com/RexYing/diffpool">官方的代码</a>是使用了交叉熵来代替 Frobenius 范数。</p><p>具体而言：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.link_loss = -adj * torch.log(pred_adj+eps) - (<span class="hljs-number">1</span>-adj) * torch.log(<span class="hljs-number">1</span>-pred_adj+eps)<br></code></pre></td></tr></table></figure></blockquote><p><strong>模型训练部分：</strong></p><p>也是常规的backward三件套，直接来看loss。</p><blockquote><p>loss &#x3D; model.loss(ypred, graph_labels)<br>reg_loss &#x3D; Regularization(model, weight_decays[int(count &#x2F; 10)], masks, p&#x3D;2)<br>pool_loss &#x3D; cau_loss(mask, model, weight_decays[int(count &#x2F; 10)])<br>my_reg &#x3D; reg_loss(model)<br>loss &#x3D; loss + my_reg + pool_loss</p></blockquote><p>即对应论文的</p><p><img src="/2024/20240521/image-20240518191841521.png" alt="image-20240518191841521"></p><p>由于正则项的系数会发生递增变化，也就是Gradually Increased Regularization。</p><p>我们仔细看下去</p><p>对于cau_loss，他是对mask进行正则项的计算，对应上式的第2部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cau_loss</span>(<span class="hljs-params">mask, model, weight_decay</span>):<br>    reg_loss = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> name, w <span class="hljs-keyword">in</span> model.named_parameters():<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;mask&#x27;</span> <span class="hljs-keyword">in</span> name:<span class="hljs-comment">##对mask进行正则项的计算</span><br>            temp = np.array(Tensor.cpu(w.data) * Tensor.cpu(mask))<br>            new_data = torch.from_numpy(temp).cuda()<br>            l2_reg = torch.norm(new_data, p=<span class="hljs-number">2</span>)<br>            reg_loss = reg_loss + l2_reg<br>    reg_loss = weight_decay * reg_loss<br>    <span class="hljs-keyword">return</span> reg_loss<br></code></pre></td></tr></table></figure><p>对于reg_loss，他是对模型参数进行正则项的计算，对应上式的第3部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">····<br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;weight&#x27;</span> <span class="hljs-keyword">in</span> name:<span class="hljs-comment">#对模型参数进行正则项的计算</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;norms&#x27;</span> <span class="hljs-keyword">in</span> name:<br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;bn&#x27;</span> <span class="hljs-keyword">in</span> name:<br>            <span class="hljs-keyword">continue</span><br>        weight = (name, param)<br>        weight_list.append(weight)<br>····<br></code></pre></td></tr></table></figure><p>注意：mask和masks</p><h3 id="one-shot-prune"><a href="#one-shot-prune" class="headerlink" title="one_shot_prune"></a>one_shot_prune</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs py"><span class="hljs-keyword">def</span> <span class="hljs-title function_">getMasks</span>(<span class="hljs-params">model, w_ratio</span>):<br>    <span class="hljs-comment"># w_ratio代表剩余网络参数的比例</span><br>    unmasks = []<br>    masks = []<br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;weight&#x27;</span> <span class="hljs-keyword">in</span> name:<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;norms&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;bn&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-built_in">print</span>(name)<br>            mask = torch.zeros_like(param)<br>            unmask = torch.ones_like(param)<br>            shape0 = mask.shape[<span class="hljs-number">0</span>]<br>            shape1 = mask.shape[<span class="hljs-number">1</span>]<br>            mask = mask.reshape(-<span class="hljs-number">1</span>)<br>            indices = np.random.choice(np.arange(torch.tensor(mask.shape).item()), replace=<span class="hljs-literal">False</span>,<br>                                       size=<span class="hljs-built_in">int</span>(torch.tensor(mask.shape).item() * (<span class="hljs-number">1</span> - w_ratio)))<span class="hljs-comment">#随机抽取</span><br>            mask[indices] = <span class="hljs-number">1</span><br>            mask = mask.reshape(shape0, shape1)<br>            unmask = unmask - mask<br>            masks.append(mask)<br>            unmasks.append(unmask)<br>    <span class="hljs-keyword">return</span> masks, unmasks<br><span class="hljs-comment">###省略</span><br>masks, unmasks = getMasks(model, w_ratio)<br><span class="hljs-comment">###省略</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">one_shot_prune</span>(<span class="hljs-params">model, unmasks</span>):<br>    my_count = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;weight&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;norms&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                    <span class="hljs-keyword">continue</span><br>                <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;bn&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                    <span class="hljs-keyword">continue</span><br>                param[:] = param * unmasks[my_count]<br>                my_count += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>进行<strong>随机</strong>剪枝操作,和UGS等不同。</p><h3 id="run-fine-tune"><a href="#run-fine-tune" class="headerlink" title="run_fine_tune"></a>run_fine_tune</h3><p>和train函数是一样的，多了每一epoch后执行类似于one shot_prune的操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):<br>    <span class="hljs-comment">###省略省略</span><br>    训练<br>    <span class="hljs-comment">###省略省略</span><br>    count = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;weight&#x27;</span> <span class="hljs-keyword">in</span> name:<br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;norms&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;bn&#x27;</span> <span class="hljs-keyword">in</span> name:<br>                <span class="hljs-keyword">continue</span><br>            param[:] = param * unmasks[count]<br>            count += <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h3 id="理论部分"><a href="#理论部分" class="headerlink" title="理论部分"></a>理论部分</h3><h4 id="时间复杂度计算"><a href="#时间复杂度计算" class="headerlink" title="时间复杂度计算"></a>时间复杂度计算</h4><p>GLT是$o(\mathcal{L}\times\left|\boldsymbol{m}_ g\odot\boldsymbol{A}\right|_ 0\times\mathcal{F}+\mathcal{L}\times\left|\boldsymbol{m}_ \theta\right|_0\times\left|\mathcal{V}\right|\times\mathcal{F}^2)$</p><p>而DGLT是$\mathcal{O}\left(\left|\left|m_ {A}\odot A_ {all}\right|\right|_ {0}\times F+\left|\left|m^{*}\right|\right|_ {0}\times\left|\mathcal{V}\right|\times F^{2}\right)+\mathcal{O}\left(\mathcal{K}\right)$,其中 $m_ {A}&#x3D; { m_ {A}^{0},:\hat{m}_ {A}^{1}\ldots m_ {A}^{L} }$ 所有领接矩阵的mask。$\mathcal{O}(K)$ 为学习节点嵌入和分配矩阵的推理时间复杂度。它们由多个矩阵相乘得到，推理时间复杂度为$\mathcal{O}\left(\mathcal{K}\right)&#x3D;\mathcal{O}\left(L\times|\mathcal{V}|^{3}+L\times|\mathcal{V}|\times F\right).$</p><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p>pass</p><h3 id="另外"><a href="#另外" class="headerlink" title="另外"></a>另外</h3><p><a href="https://openreview.net/forum?id=Dvs-a3aymPe">https://openreview.net/forum?id=Dvs-a3aymPe</a></p><p>DGLT 声称可以将随机预定义的图转换为具有高信息量形式的适当条件。如果这个猜想是正确的，那么它具有相当有希望的实际意义——它表明训练 GNN 模型的消息传递功能（即信息聚合）实际上是不必要的，因为只需要选择邻接矩阵的目标大小或目标GNN的子结构，然后使用层次图稀疏（HGS）算法或逐渐增加正则化进行信息挤出。</p><h2 id="《Brave-the-Wind-and-the-Waves-Discovering-Robust-and-Generalizable-Graph-Lottery-Tickets-2023PAMI-pdf》"><a href="#《Brave-the-Wind-and-the-Waves-Discovering-Robust-and-Generalizable-Graph-Lottery-Tickets-2023PAMI-pdf》" class="headerlink" title="《Brave_the_Wind_and_the_Waves_Discovering_Robust_and_Generalizable_Graph_Lottery_Tickets(2023PAMI).pdf》"></a>《Brave_the_Wind_and_the_Waves_Discovering_Robust_and_Generalizable_Graph_Lottery_Tickets(2023PAMI).pdf》</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>在现实场景中，未见过的测试数据的分布通常是多种多样的。我们将分布外（OOD）数据的失败归因于无法辨别因果模式，而因果模式在分布变化中仍然保持稳定。在传统的空间图学习中，当图&#x2F;网络稀疏度超过一定的高水平时，模型性能会急剧恶化。更糟糕的是，由于手头的训练集有限，修剪后的 GNN 很难推广到看不见的图数据。为了解决这些问题，我们提出了弹性图彩票（RGLT），以在 GNN 中找到更强大和更通用的 GLT。具体来说，我们通过每个剪枝点的瞬时梯度信息重新激活一部分权重&#x2F;边缘。经过充分的修剪后，我们进行环境干预以推断潜在的测试分布。最后，我们执行最后几轮模型平均值以进一步提高泛化能力。</p><p>处理大型图有两个主要研究方向，要么简化图，要么压缩 GNN 模型。第一种，各种图形采样策略或稀疏化方法。在第二个流上所做的努力要少得多，即修剪 GNN  ，因为 GNN 通常比其他学科中的 DNN 参数化程度较低。</p><p>GLT仍然有改进空间：</p><p>**鲁棒性降低：**在 GLT 中，当图（或网络）稀疏度达到一定程度 时，GNN 的性能将急剧下降，例如超过70%。从概念上讲，GLT 通过基于幅度的剪枝来识别“幸运”图彩票，这可以看作是极化剪枝，在后续训练中不为中等幅度的权重或边缘留下一些余地。在高稀疏度下，模型很难探索完整的权重空间，并且由于稀疏度约束 ，模型更新路线被切断。</p><p><strong>泛化能力降低：</strong></p><p>然而，图上的剪枝可能会降低模型的泛化性，因为 GNN 与深度学习网络（例如卷积神经网络）一样需要大量数据。</p><p>此外，《<a href="https://arxiv.org/abs/2206.08684">Sparse Double Descent: Where Network Pruning Aggravates Overfitting</a>》（ICML2022）揭示了一个相反的现象——网络剪枝有时甚至会在超稀疏和某些中度稀疏现象下恶化泛化性。该文是第一个报告稀疏双下降现象的工作。更具体地说，证明高模型稀疏度可以显着减轻过度拟合，而中等模型稀疏度可能导致更严重的过度拟合。极端的模型稀疏性 ( →100% ) 往往会丢失所有学到的信息。另外，还得到了和 <em>lottery ticket hypothesis</em> 的相反的结论，从原始初始化重新训练稀疏模型可能不会始终获胜。例如，在某些情况下，随机重新初始化的修剪模型可以在很大程度上超越在某些稀疏度下具有原始初始化的模型。</p><p>这个意外问题使 GLT 在具有不同样本和实例的实际应用程序中的使用变得复杂。</p><h3 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h3><p><img src="/2024/20240521/image-20240518123750349.png" alt="image-20240518123750349"></p><p>首先，我们执行鲁棒彩票搜索（LoRS）来生成稀疏网络和图的组合。在每次迭代中，我们根据边和权重的大小来修剪边和权重，然后重新激活具有前 k 个梯度的边和权重。然后，我们在核心子图上利用 Lottery Graph Intervention (LoGI) 来推断测试分布，并将增强图传递到剪枝模型以进行下一轮训练。在最后几轮中，我们进行模型平均以进一步提高模型的泛化性。值得注意的是，LoRS 可以独立运行来发现鲁棒图彩票和我们的 LoGI，而 LoGI 算法依赖于 LoRS 识别的核心子图。我们提出的两种算法协同工作，有助于大规模 GNN 应用的落地。</p><h4 id="Formulation"><a href="#Formulation" class="headerlink" title="Formulation"></a>Formulation</h4><p>本文意图解决一个更有挑战性的问题，</p><p>提高模型的泛化能力。假设 $S\text{ 是环境 }^{1}$的支持（support of the environments，？），$f(·)$ 是预测函数，我们的目标是最小化不同数据分布下的经验风险：<br>$$\min\limits_ {f}\max\limits_ {e\in\mathcal{S}}\mathbb{E}_ {(\mathcal{G},Y)\sim p(\mathcal{G},Y|e)}:[\mathcal{L}\left(f\left(\mathcal{G}\right),Y\right)|e]$$</p><blockquote><p>我的理解是类似于最小化$L^\infty$距离</p></blockquote><h4 id="Robust-Lottery-Searching-LoRS"><a href="#Robust-Lottery-Searching-LoRS" class="headerlink" title="Robust Lottery Searching (LoRS)"></a>Robust Lottery Searching (LoRS)</h4><p><img src="/2024/20240521/image-20240518125402298.png" alt="image-20240518125402298"></p><p>前面的步骤和UGS类似，多了一步，将丢弃的边中梯度最大的若干个恢复，代码如右图红框所示。</p><p><img src="/2024/20240521/image-20240518195611405.png" alt="image-20240518195611405"></p><h4 id="lottery-Graph-Intervention-LoGI"><a href="#lottery-Graph-Intervention-LoGI" class="headerlink" title="lottery Graph Intervention (LoGI)"></a>lottery Graph Intervention (LoGI)</h4><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><p>基于UGS的代码，有大量相同的地方。</p><p>和UGS一样，主函数也是包括</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> .....<br>run_get_mask<br>    XXXX<span class="hljs-comment">###从final_mask_dict中保存mask到rewind_weight，剪枝但保持其他权重和初始化一样</span><br>    run_fix_mask<br></code></pre></td></tr></table></figure><h4 id="run-fix-mask"><a href="#run-fix-mask" class="headerlink" title="run_fix_mask"></a>run_fix_mask</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.Adam(net_gcn.parameters(), lr=args[<span class="hljs-string">&#x27;lr&#x27;</span>], weight_decay=args[<span class="hljs-string">&#x27;weight_decay&#x27;</span>])<br>optimizer_aug = torch.optim.AdamW(gl.parameters(), lr=args[<span class="hljs-string">&#x27;lr_a&#x27;</span>])<br>l2_loss = torch.nn.MSELoss()<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args[<span class="hljs-string">&#x27;epochs&#x27;</span>] ):<br>    beta = <span class="hljs-number">1</span> * args[<span class="hljs-string">&quot;beta&quot;</span>] * epoch / args[<span class="hljs-string">&#x27;epochs&#x27;</span>] + args[<span class="hljs-string">&quot;beta&quot;</span>] * (<span class="hljs-number">1</span> - epoch / args[<span class="hljs-string">&#x27;epochs&#x27;</span>] )<br>    <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args[<span class="hljs-string">&#x27;T&#x27;</span>]):<br>        ori_tensor = net_gcn(features, adj, gragh_editor=<span class="hljs-literal">True</span>)<br>        graph_loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(args[<span class="hljs-string">&#x27;K&#x27;</span>]):<br>            edge_index_k = gl(adj, dataset_tr.graph[<span class="hljs-string">&#x27;num_nodes&#x27;</span>], args[<span class="hljs-string">&#x27;num_sample&#x27;</span>], k, args[<span class="hljs-string">&#x27;rate&#x27;</span>])<br>            output = net_gcn(features, edge_index_k)<br>            labels = labels.squeeze(dim=-<span class="hljs-number">1</span>)<br>            loss = loss_func(output, labels)<br>            optimizer.zero_grad()<br>            loss.backward()<br>            optimizer.step()<br>            <br>            graph_tensor = net_gcn(features, edge_index_k, gragh_editor=<span class="hljs-literal">True</span>)<br>            graph_loss = graph_loss + l2_loss(ori_tensor, graph_tensor)<br>        inner_loss = -<span class="hljs-number">1.5</span>*graph_loss <span class="hljs-comment">#????</span><br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;inner_loss&#x27;</span>,inner_loss)<br>        optimizer_aug.zero_grad()<br>        inner_loss.backward()<br>        optimizer_aug.step()<br></code></pre></td></tr></table></figure><p>不断优化gcn，劣化gl</p><h2 id="《Analyzing-Adversarial-Vulnerabilities-of-Graph-Lottery-Tickets-ICASSP2024-pdf》"><a href="#《Analyzing-Adversarial-Vulnerabilities-of-Graph-Lottery-Tickets-ICASSP2024-pdf》" class="headerlink" title="《Analyzing_Adversarial_Vulnerabilities_of_Graph_Lottery_Tickets(ICASSP2024).pdf》"></a>《Analyzing_Adversarial_Vulnerabilities_of_Graph_Lottery_Tickets(ICASSP2024).pdf》</h2><p>和finding_adversarially_robust_graph lottery tickets原作者，内容基本一样。</p><p>除了少了平滑项。</p><h3 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h3><p>pass</p><h2 id="《finding-adversarially-robust-graph-lottery-tickets-under-review-pdf》"><a href="#《finding-adversarially-robust-graph-lottery-tickets-under-review-pdf》" class="headerlink" title="《finding_adversarially_robust_graph lottery tickets(under review).pdf》"></a>《finding_adversarially_robust_graph lottery tickets(under review).pdf》</h2><p><img src="/2024/20240521/image-20240520164219825.png" alt="image-20240520164219825">被拒了。</p><p>AC拒稿理由：</p><p>本文提出了一种减少图彩票对图结构的对抗性扰动的脆弱性的技术。结果似乎对这个问题相当有效。审稿人提出了一些担忧，包括设置本身（结构扰动真的是正确的威胁模型吗？关注这一点是否依赖于其他方面不受攻击？）、方法本身的复杂性（超参数太多）以及大小正在研究的图表的数量（它们足够大吗？）。我同意第一个担忧：这真的是一个重要问题吗？如果对图彩票的对抗性攻击是一个重要问题，那么这些类型的攻击在实践中是否重要？我对接受持矛盾态度，并且基于所研究问题的重要性，我倾向于拒绝。对于这个特定问题来说，这似乎是一个合理的贡献，但问题本身却非常小众。</p><h3 id="相关工作-1"><a href="#相关工作-1" class="headerlink" title="相关工作"></a>相关工作</h3><p>pass</p><h3 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h3><p>总所周知，两层的GCN可以表示为<br>$$<br>Z&#x3D;f({ A, X },\Theta)&#x3D; \mathcal{S}(\hat{A} \sigma ( \hat A X W_ {(0)}) W_ {(1)})<br>$$<br>设计了一个transductive semi-supervised node classification (SSNC) loss：<br>$$<br>\mathcal{L} _ 0 (f({A, X}, \Theta))&#x3D;-\sum _ {l \in \mathcal{Y} _ {TL}} \sum _ {j&#x3D;1} ^C  Y _ {l_j} log( Z _ {l_j})<br>$$<br>其中$\mathcal{Y}_ {TL}$是训练节点的索引，C是类总数，$Y_l$是$v_l$one hot 标签。</p><p>posion 攻击者的目标是找到一个最优的扰动A ‘，欺骗GNN做出错误的预测。这可以表述为一个双层优化问题(Zugner et al.， 2018;zugner &amp; gunnemann, 2019):<br>$$<br>\begin{align}<br>arg \max\mathcal{L}_ {atk}(f({A’,X},\Theta ^\ast))\\<br>A’\in\Phi(A)\\<br>\mathrm{s.t.}\quad\Theta^{\ast}&#x3D;\arg\min_ {\Theta}\mathcal{L}_ {0}(f({A’,X},\Theta))<br>\end{align}<br>$$<br>其中$\Phi(A)$是满足$\frac{|A’-A|{0}}{|A|{0}}\leq\Delta$的领接矩阵。$\mathcal{L}_ {atk}$ 是攻击loss函数，$\Delta$ 是 perturbation rate，$\Theta ^\ast$是摄动图上GNN的最优参数。</p><p>为了帮助消除对抗边和鼓励特征平滑，对于homophilic graphs：<br>$$<br>\mathcal{L}_ {fs} (A’,X)&#x3D;\frac{1}{2} \sum_ {i,j&#x3D;1}A_ {ij}’ (x_i-x_j)^2<br>$$<br>对于heterophilic graphs：<br>$$<br>\mathcal{L}_ {fs}(A’)&#x3D;\frac{1}{2}\sum_ {i,j&#x3D;1}A_ {ij}’(y_ {i}-y_ {j})^{2}<br>$$</p><blockquote><p>以上有点像<em>dirichlet</em> energy。</p><p><em>dirichlet</em> energy：<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{2}\sum _ {i,j}W[i,j] (x[j]-x[i])^2<br>$$<br>进一步归一化：<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{2}\sum _ {i,j}W[i,j] (\frac{x[j]}{\sqrt{1+d_j}}-\frac{x[i]}{\sqrt{1+d_i}})^2<br>$$<br>（上式来自《<a href="https://proceedings.neurips.cc/paper/2021/file/b6417f112bd27848533e54885b66c288-Paper.pdf">Dirichlet Energy Constrained Learning for Deep Graph Neural Networks</a>》）</p><p>或<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{4}\sum _ {i,j}W[i,j]|\frac{x[j]}{\sqrt{d_j}}-\frac{x[i]}{\sqrt{d_i}}|_2^2<br>$$<br>（上式来自《<a href="https://openreview.net/pdf?id=kS7ED7eE74">A Fractional Graph Laplacian Approach to Oversmoothing</a>》）</p><p>其中d为节点的度。</p></blockquote><p>其中yi∈R P为输入图G上运行DeepWalk算法得到的节点i, j的位置特征，P为节点位置特征个数。</p><blockquote><p> 查看上面部分的代码，我们可以发现：</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">feature_smoothing</span>(<span class="hljs-params">self, adj, X</span>):<br> adj = (adj.t() + adj)/<span class="hljs-number">2</span><br> rowsum = adj.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)<br> r_inv = rowsum.flatten()<br> D = torch.diag(r_inv)<br> L = D - adj<br><br> r_inv = r_inv  + <span class="hljs-number">1e-3</span><br> r_inv = r_inv.<span class="hljs-built_in">pow</span>(-<span class="hljs-number">1</span>/<span class="hljs-number">2</span>).flatten()<br> r_inv[torch.isinf(r_inv)] = <span class="hljs-number">0.</span><br> r_mat_inv = torch.diag(r_inv)<br> L = r_mat_inv @ L @ r_mat_inv<br><br> XLXT = torch.matmul(torch.matmul(X.t(), L), X)<br> loss_smooth_feat = torch.trace(XLXT)<br> <span class="hljs-keyword">return</span> loss_smooth_feat<br></code></pre></td></tr></table></figure><p> 迹的计算又出现了。</p></blockquote><p>另外，作者还训练了一个简单的两层MLP。mlp使用训练集做训练，然后对使用训练好的MLP来预测测试节点的标签。称这些标签为伪标签。最后，利用MLP预测置信度较高的测试节点计算测试节点CE损失项。</p><p>设$Y_ {P L}$为MLP预测置信度较高的测试节点集，$Y_ {mlp}$为MLP的预测值。CE损失为：<br>$$<br>\mathcal{L} _ 1(f({A’,X},\Theta))&#x3D;-\sum_ {l\in\mathcal{Y}_ {TL}}\sum_ {j&#x3D;1}^C Y_ {mlp_ {l_j}}\log(Z_ {l_j})<br>$$<br>最终loss为：<br>$$<br>\begin{align}<br>\mathcal{L}_ {ARGS}&#x3D;\alpha\mathcal{L}_ {0}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\beta\mathcal{L}_ {fs}(m_ {g}\odot A’,X)\\+\gamma\mathcal{L}_ {1}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\lambda_ {1}||m_ {g}||_ {1}+\lambda_ {2}||m_ {\theta}||_ {1}<br>\end{align}<br>$$<br>其中，$\alpha$和$\gamma$设置为1。$m_g$用于领接矩阵，$m_ \theta$用于模型权重。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/SubhajitDuttaChowdhury/ARGS">github</a></p><p>完全基于UGS的代码，有大量相同的地方。</p><p>和UGS一样，主函数也是包括</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> .....<br>run_get_mask<br>    XXXX<span class="hljs-comment">###从final_mask_dict中保存mask到rewind_weight，剪枝但保持其他权重和初始化一样</span><br>    run_fix_mask<br></code></pre></td></tr></table></figure><p>run_get_mask函数不同点：run_get_mask中加入了平滑项和伪标签的分类误差。</p><p>即loss为<br>$$<br>\begin{align}<br>\mathcal{L}_ {run_get_mask}&#x3D;\alpha\mathcal{L}_ {0}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\beta\mathcal{L}_ {fs}(m_ {g}\odot A’,X)\\<br>+\gamma\mathcal{L}_ {1}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\lambda_ {1}||m_ {g}||_ {1}+\lambda_ {2}||m_ {\theta}||_ {1}<br>\end{align}<br>$$</p><p>run_fix_mask函数不同点：run_fix_mask中加入了伪标签的分类误差。</p><p>即loss为<br>$$<br>\mathcal{L}_ {run_fix_mask}&#x3D;\alpha\mathcal{L}_ {0}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))+\gamma\mathcal{L}_ {1}(f({m_ {g}\odot A’,X},m_ {\theta}\odot\Theta))<br>$$</p><h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><p>pass</p><h2 id="《inductive-lottery-ticket-learning-for-graph-neural-networks-under-review-pdf》"><a href="#《inductive-lottery-ticket-learning-for-graph-neural-networks-under-review-pdf》" class="headerlink" title="《inductive lottery ticket learning for graph neural networks(under review).pdf》"></a>《inductive lottery ticket learning for graph neural networks(under review).pdf》</h2><p>Accepted by JCST 2023</p><p>Rejected by ICLR 2022</p><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>过往的有以下缺点</p><p>1）也就是说，边缘遮罩被限制在给定的图中，使得UGS在归纳设置中不可行，因为边缘遮罩很难推广到看不见的边或全新的图。</p><p>2)对每条边单独应用掩码只能提供对边缘的局部理解，而不是整个图的全局视图(例如，在节点分类中)或多个图(例如，在图分类中)</p><p>此外，创建可训练边缘掩模的方式会使gnn的参数加倍，这在某种程度上违背了修剪的目的。</p><p>因此，这些边缘掩模可能是次优的，以指导修剪。(3)不理想的图剪枝会对模型权值的剪枝产生负面影响。更糟糕的是，低质量的权值剪枝会反过来放大边缘掩模的误导信号。它们相互影响，形成恶性循环。我们将所有这些UGS的局限性归因于它的转导性质。因此，在归纳设置中进行组合修剪对于高质量中奖彩票至关重要。</p><h3 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h3><p><img src="/2024/20240521/image-20240518132924951.png" alt="image-20240518132924951"></p><p>本文提出了一个AutoMasker，具体而言，他设计了一套网络用来生成mask的选择。</p><p>它使用一个GNN $g(·)$来获取每个节点的 representations。</p><p>$H&#x3D;g(A,X)$</p><p>每一行代表着节点的representation。故可由计算节点的重要性，<br>$$<br>s_ {ij}&#x3D;\sigma{(\alpha_ {ij})},a_ {ij} &#x3D; MLP([h_i,h_j])<br>$$</p><p>对于图，我们采用AutoMasker来预测每个图的所有边的重要性。然后根据掩码值对某图的边进行排序，对最小值为5%的边进行剪接，得到二值图掩码mG。</p><p>对于GNN，我们根据权重量级对参数进行排序，并对最低量级的参数进行20%的修剪，得到二值模型掩码mΘ。在当前的稀疏度水平下，我们现在成功地得到了模型的稀疏化图g0 &#x3D; (mG A, X)和稀疏化掩码mΘ。</p><p>最后，我们需要检查稀疏性是否满足我们的条件。如果满足稀疏性，则算法完成;如果没有，我们需要重用找到的GLT来更新原始图和GNN模型，并迭代使用步骤1和步骤2(图1中虚线箭头)，直到满足条件。</p><p><img src="/2024/20240521/image-20240518133345382.png" alt="image-20240518133345382"></p><h3 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h3><h4 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h4><p>GAT:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">GATNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train_dataset</span>):<br>        <span class="hljs-built_in">super</span>(GATNet, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = GATConv(train_dataset.num_features, <span class="hljs-number">256</span>, heads=<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.lin1 = torch.nn.Linear(train_dataset.num_features, <span class="hljs-number">4</span> * <span class="hljs-number">256</span>)<br>        <span class="hljs-variable language_">self</span>.conv2 = GATConv(<span class="hljs-number">4</span> * <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, heads=<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.lin2 = torch.nn.Linear(<span class="hljs-number">4</span> * <span class="hljs-number">256</span>, <span class="hljs-number">4</span> * <span class="hljs-number">256</span>)<br>        <span class="hljs-variable language_">self</span>.conv3 = GATConv(<span class="hljs-number">4</span> * <span class="hljs-number">256</span>, train_dataset.num_classes, heads=<span class="hljs-number">6</span>, concat=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.lin3 = torch.nn.Linear(<span class="hljs-number">4</span> * <span class="hljs-number">256</span>, train_dataset.num_classes)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, edge_index, data_mask=<span class="hljs-literal">None</span></span>):<br>        x = F.elu(<span class="hljs-variable language_">self</span>.conv1(x, edge_index, edge_weight=data_mask) + <span class="hljs-variable language_">self</span>.lin1(x))<br>        x = F.elu(<span class="hljs-variable language_">self</span>.conv2(x, edge_index, edge_weight=data_mask) + <span class="hljs-variable language_">self</span>.lin2(x))<br>        x = <span class="hljs-variable language_">self</span>.conv3(x, edge_index, edge_weight=data_mask) + <span class="hljs-variable language_">self</span>.lin3(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><p>Masker</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Masker</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train_dataset, hidden=<span class="hljs-number">128</span></span>):<br>        <span class="hljs-built_in">super</span>(Masker, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.conv1 = GATConv(train_dataset.num_features, hidden, heads=<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.lin1 = torch.nn.Linear(train_dataset.num_features, <span class="hljs-number">4</span> * hidden)<br>        <span class="hljs-variable language_">self</span>.conv2 = GATConv(<span class="hljs-number">4</span> * hidden, hidden, heads=<span class="hljs-number">4</span>)<br>        <span class="hljs-variable language_">self</span>.lin2 = torch.nn.Linear(<span class="hljs-number">4</span> * hidden, <span class="hljs-number">4</span> * hidden)<br>        <span class="hljs-variable language_">self</span>.conv3 = GATConv(<span class="hljs-number">4</span> * hidden, hidden, heads=<span class="hljs-number">6</span>, concat=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.lin3 = torch.nn.Linear(<span class="hljs-number">4</span> * hidden, hidden)<br>        <span class="hljs-variable language_">self</span>.mlp = torch.nn.Linear(hidden * <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-variable language_">self</span>.sigmoid = torch.nn.Sigmoid()<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, edge_index</span>):<br>        x = F.elu(<span class="hljs-variable language_">self</span>.conv1(x, edge_index) + <span class="hljs-variable language_">self</span>.lin1(x))<br>        x = F.elu(<span class="hljs-variable language_">self</span>.conv2(x, edge_index) + <span class="hljs-variable language_">self</span>.lin2(x))<br>        x = <span class="hljs-variable language_">self</span>.conv3(x, edge_index) + <span class="hljs-variable language_">self</span>.lin3(x)<br>        link_score = <span class="hljs-variable language_">self</span>.concat_mlp_score(x, edge_index)<br>        <span class="hljs-keyword">return</span> link_score<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">concat_mlp_score</span>(<span class="hljs-params">self, x, edge_index</span>):<br>        row, col = edge_index<br>        link_score = torch.cat((x[row], x[col]), dim=<span class="hljs-number">1</span>)<br>        link_score = <span class="hljs-variable language_">self</span>.mlp(link_score)<br>        link_score = <span class="hljs-variable language_">self</span>.sigmoid(link_score).view(-<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> link_score<br></code></pre></td></tr></table></figure><p>GAT和Masker相比，masker的隐藏层更小，多了inner_product_score（上文省略了）和concat_mlp_score的函数。GAT最后一层是分类器，Masker最后一层输出边的分数。</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">optimizer = torch.optim.Adam([&#123;<span class="hljs-string">&#x27;params&#x27;</span>: model.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: <span class="hljs-number">0.005</span>&#125;,<br>                                  &#123;<span class="hljs-string">&#x27;params&#x27;</span>: masker.parameters(), <span class="hljs-string">&#x27;lr&#x27;</span>: masker_lr&#125;])<span class="hljs-comment">#不同学习率</span><br><span class="hljs-comment">####省略</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, total_epoch + <span class="hljs-number">1</span>):<br>        loss, mask_distribution = train_model_and_masker(model, masker, optimizer, train_loader)<br>        <span class="hljs-comment">##评估省略</span><br>pruning.pruning_model(model, <span class="hljs-number">0.2</span>, random=<span class="hljs-literal">False</span>)<br>_ = pruning.see_zero_rate(model)<br>model_mask_dict = pruning.extract_mask(model)<br><br>masker.load_state_dict(best_masker_state_dict)<br>pruning.grad_model(masker, <span class="hljs-literal">False</span>)<br><br>train_dataset_pru = pruning.masker_pruning_dataset(train_dataset_pru, masker, <span class="hljs-number">1</span>, <span class="hljs-number">0.05</span>)<br>val_dataset_pru = pruning.masker_pruning_dataset(val_dataset_pru, masker, <span class="hljs-number">2</span>, <span class="hljs-number">0.05</span>)<br>test_dataset_pru = pruning.masker_pruning_dataset(test_dataset_pru, masker, <span class="hljs-number">2</span>, <span class="hljs-number">0.05</span>)<br><span class="hljs-comment">##省略print</span><br>things_dict[<span class="hljs-string">&#x27;train_dataset_pru&#x27;</span>] = train_dataset_pru <br>things_dict[<span class="hljs-string">&#x27;val_dataset_pru&#x27;</span>] = val_dataset_pru <br>things_dict[<span class="hljs-string">&#x27;test_dataset_pru&#x27;</span>] = test_dataset_pru <br>things_dict[<span class="hljs-string">&#x27;rewind_weight&#x27;</span>] = rewind_weight<br>things_dict[<span class="hljs-string">&#x27;rewind_weight2&#x27;</span>] = rewind_weight2<br>things_dict[<span class="hljs-string">&#x27;model_mask_dict&#x27;</span>] = model_mask_dict<br></code></pre></td></tr></table></figure><h5 id="train-model-and-masker函数"><a href="#train-model-and-masker函数" class="headerlink" title="train_model_and_masker函数"></a>train_model_and_masker函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_op = torch.nn.BCEWithLogitsLoss()<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_model_and_masker</span>(<span class="hljs-params">model, masker, optimizer, train_loader</span>):<br><span class="hljs-comment">###省略</span><br>    total_loss = <span class="hljs-number">0</span><br>    mask_distribution = []<br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_loader:<br>        data = data.to(device)<br>        optimizer.zero_grad()<br>        data_mask = masker(data.x, data.edge_index)<br>        mask_distribution.append(pruning.plot_mask(data_mask))<br>        out = model(data.x, data.edge_index, data_mask)<br>        loss = loss_op(out, data.y)<br>        total_loss += loss.item() * data.num_graphs<br>        loss.backward()<br>        optimizer.step()<br>    mask_distribution = torch.tensor(mask_distribution).mean(dim=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> total_loss / <span class="hljs-built_in">len</span>(train_loader.dataset), mask_distribution<br></code></pre></td></tr></table></figure><p>和UGS的过程其实差不多，权重&#x3D;mask*权重，使用CEloss进行训练。UGS的mask为网络中的参数，而该算法的mask则由另一套神经网络生成。</p><h5 id="pruning-model"><a href="#pruning-model" class="headerlink" title="pruning_model"></a>pruning_model</h5><p>本部分使用了pytorch 的torch.nn.utils.prune</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pruning_model</span>(<span class="hljs-params">model, px, random=<span class="hljs-literal">False</span></span>):<br>    <span class="hljs-keyword">if</span> px == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">pass</span><br>    <span class="hljs-keyword">else</span>:<br>        parameters_to_prune =[]<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> model.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                parameters_to_prune.append((m,<span class="hljs-string">&#x27;weight&#x27;</span>))<br>                <span class="hljs-comment"># print(m)</span><br>        <br>        parameters_to_prune = <span class="hljs-built_in">tuple</span>(parameters_to_prune)<br>        <span class="hljs-keyword">if</span> random:<br>            prune.global_unstructured(<br>                parameters_to_prune,<br>                pruning_method=prune.RandomUnstructured,<br>                amount=px,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            prune.global_unstructured(<br>                parameters_to_prune,<br>                pruning_method=prune.L1Unstructured,<br>                amount=px,<br>            )<br></code></pre></td></tr></table></figure><blockquote><p>L1：基于权重绝对值</p><p>random：完全随机</p></blockquote><h5 id="grad-model"><a href="#grad-model" class="headerlink" title="grad_model"></a>grad_model</h5><p>源代码为<code>pruning.grad_model(masker, False)</code>，冻结梯度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_model</span>(<span class="hljs-params">model, fix=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>        param.requires_grad = fix<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>信息瓶颈与在图中的应用</title>
    <link href="/2024/20240520/"/>
    <url>/2024/20240520/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Information Bottleneck又称信息瓶颈，是一个基于信息论的算法。</p><span id="more"></span><h2 id="Information-Bottleneck"><a href="#Information-Bottleneck" class="headerlink" title="Information Bottleneck"></a>Information Bottleneck</h2><p>最早来自2000年Naftali Tishby等人的的<a href="https://arxiv.org/abs/physics/0004057">《The information bottleneck method》</a></p><p>具体来说，它试图优化以下两个目标：</p><ol><li><strong>压缩性 (Compression)</strong>：压缩表示T应该包含尽可能少的关于原始变量X的信息。这通过最小化互信息 I(X;T) 来实现。</li><li><strong>相关性 (Relevance)</strong>：压缩表示T应该尽可能多地保留关于相关变量Y的信息。这通过最大化互信息 I(T;Y) 来实现。</li></ol><p>故信息瓶颈可以表示为一个带权衡参数 β 的优化问题，最大化：<br>$$<br>L[p(t∣x)]&#x3D;I(T;Y)-βI(X;T)<br>$$</p><h2 id="Variational-Information-Bottleneck"><a href="#Variational-Information-Bottleneck" class="headerlink" title="Variational Information Bottleneck"></a>Variational Information Bottleneck</h2><p>难点在于互信息很难计算。所以，我们需要使用变分推理来近似它们。</p><p>在VIB中，通常会构建一个编码器 (Encoder) $p_\theta(z|x)$和一个解码器&#x2F;预测器 (Decoder&#x2F;Predictor)$q_\theta(y|z)$，其中：</p><ul><li><strong>编码器$p_\theta(z|x)$</strong>：由神经网络参数化（参数为 θ），将输入X映射到一个潜在的随机表示Z（即瓶颈T）。这个潜在表示通常被设计为遵循一个先验分布，比如标准正态分布 r(z)&#x3D;N(0,I)。</li><li><strong>解码器&#x2F;预测器$q_\theta(y|z)$</strong>：也由神经网络参数化（参数为 $\phi$），根据潜在表示Z来预测目标变量Y。</li></ul><p>即最大化:</p><p>$$<br>\mathcal{L} _ {IB}[p(z|x)] &#x3D; I(Z;Y) - \beta I(X;Z)<br>$$</p><p>其中：</p><ul><li>$I(Z;Y)$ 是Z和Y之间的互信息，越大越好。</li><li>$I(X;Z)$ 是X和Z之间的互信息，越小越好（通过$\beta$来权衡）。</li><li>$\beta &gt; 0$ 是拉格朗日乘子，控制压缩程度和保留相关信息之间的平衡。</li></ul><p>但是直接计算和优化这两个互信息项非常困难，主要原因如下：</p><ol><li><strong>$I(Z;Y)$ 的计算</strong>：<br>$$<br>I(Z;Y) &#x3D; \mathbb{E} _ {p(z,y)} \left[ \log \frac{p(z,y)}{p(z)p(y)} \right] &#x3D; H(Y) - H(Y|Z)<br>$$<br>其中 $H(Y)$ 是Y的熵（通常是常数，不影响优化），而 $H(Y|Z) &#x3D; - \mathbb{E} _ {p(z)} \left[ \mathbb{E} _ {p(y|z)} [\log p(y|z)] \right]$。</li></ol><p>要计算 $p(y|z)$，我们需要知道从Z到Y的真实条件概率，这在复杂模型中是未知的。而且，即使知道了 $p(y|z)$，计算期望也可能涉及难以处理的积分。边缘分布 $p(z) &#x3D; \int p(z|x)p(x)dx$ 的计算也可能非常复杂。</p><ol start="2"><li><strong>$I(X;Z)$ 的计算</strong>：<br>$$<br>I(X;Z) &#x3D; \mathbb{E} _ {p(x,z)} \left[ \log \frac{p(x,z)}{p(x)p(z)} \right] &#x3D; H(Z) - H(Z|X)<br>$$</li></ol><p>这里同样涉及到难以计算的边缘分布 $p(z)$ 和条件熵 $H(Z|X) &#x3D; - \mathbb{E} _ {p(x)} \left[ \mathbb{E} _ {p(z|x)} [\log p(z|x)] \right]$。对于连续高维的Z，或者当 $p(z|x)$ 是一个复杂的神经网络时，$p(z)$ 的计算和期望的计算都非常棘手。</p><p><strong>变分推断的引入</strong></p><p>变分推断的核心思想是，当直接处理一个复杂的概率分布或量（如互信息）很困难时，我们引入一个更简单、更易于处理的<strong>变分分布</strong>（通常带有可学习的参数），并优化这个变分分布来逼近我们感兴趣的真实分布或量。</p><p>在VIB中，我们主要对 $I(Z;Y)$ 和 $I(X;Z)$ 进行变分近似。</p><p><strong>1. 近似 $I(Z;Y)$ (保留相关信息项)</strong></p><p>我们希望最大化 $I(Z;Y) &#x3D; H(Y) - H(Y|Z)$。由于 $H(Y)$ 相对于编码器 $p(z|x)$ 是常数，所以最大化 $I(Z;Y)$ 等价于最小化条件熵 $H(Y|Z)$。</p><p>$$<br>H(Y|Z) &#x3D; - \mathbb{E} _ {p(z,y)}[\log p(y|z)] &#x3D; - \mathbb{E} _ {p(x)p(y|x)p(z|x)}[\log p(y|z)]<br>$$</p><p>这里的 $p(y|z)$ 是真实的从Z到Y的后验概率，我们通常不知道它。因此，我们引入一个<strong>变分近似分布 $q _ {\phi}(y|z)$</strong>，这是一个由参数 $\phi$（例如神经网络的权重）控制的、试图从Z预测Y的模型。</p><p>我们可以推导出 $I(Z;Y)$ 的一个<strong>变分下界 (Variational Lower Bound)</strong>：</p><p>$$<br>I(Z;Y) &#x3D; H(Y) - H(Y|Z)<br>$$</p><p>$$<br>H(Y|Z) &#x3D; - \mathbb{E} _ {p(z)} [ \sum_y p(y|z) \log p(y|z) ]<br>$$<br>我们知道 $KL(p(y|z) || q _ {\phi}(y|z)) \ge 0$，即 $\mathbb{E} _ {p(y|z)}[\log \frac{p(y|z)}{q _ {\phi}(y|z)}] \ge 0$<br>所以<br>$$<br>\mathbb{E} _ {p(y|z)}[\log p(y|z)] \ge \mathbb{E} _ {p(y|z)}[\log q _ {\phi}(y|z)]<br>$$</p><p>因此，<br>$$<br>-H(Y|Z) \ge \mathbb{E} _ {p(z)} [ \mathbb{E} _ {p(y|z)}[\log q _ {\phi}(y|z)] ]<br>$$</p><p>或者说，<br>$$<br>H(Y|Z) \leq - \mathbb{E} _ {p(z,y)}[\log q _ {\phi}(y|z)]<br>$$</p><p>所以，<br>$$<br>I(Z;Y) \ge H(Y) + \mathbb{E} _ {p(z,y)}[\log q _ {\phi}(y|z)]<br>$$<br>由于 $H(Y)$ 是常数，最大化 $I(Z;Y)$ 的下界就等价于最大化 $\mathbb{E} _ {p(z,y)}[\log q _ {\phi}(y|z)]$，或者说最小化负对数似然 $-\mathbb{E} _ {p(z,y)}[\log q _ {\phi}(y|z)]$。这个期望可以写作：<br>$$<br>\mathbb{E} _ {p(x)p(y|x)} [ \mathbb{E} _ {p _ {\theta}(z|x)} [\log q _ {\phi}(y|z)] ]<br>$$</p><p>这一项实际上就是训练一个从Z预测Y的模型（解码器&#x2F;预测器 $q _ {\phi}(y|z)$）时的标准监督学习损失（例如，对于分类问题是交叉熵，对于回归问题是均方误差的负数）。我们希望通过学习编码器 $p _ {\theta}(z|x)$（参数为 $\theta$）和解码器 $q _ {\phi}(y|z)$ 来最大化这个下界。</p><p><strong>2. 近似 $I(X;Z)$ (压缩信息项)</strong></p><p>我们希望最小化 $I(X;Z)$。<br>$$<br>\begin{align}<br>I(X;Z) &#x3D; H(Z) - H(Z|X)<br>\\<br>H(Z|X) &#x3D; - \mathbb{E} _ {p(x)} \left[ \int p _ {\theta}(z|x) \log p _ {\theta}(z|x) dz \right]<br>\\<br>H(Z) &#x3D; - \int p(z) \log p(z) dz<br>\end{align}<br>$$<br>其中 $p(z) &#x3D; \int p _ {\theta}(z|x) p(x) dx$。</p><p>直接计算 $p(z)$ 非常困难。VIB在这里引入另一个<strong>固定的、简单的先验分布 $r(z)$</strong> (例如，标准正态分布 $\mathcal{N}(0,I)$)。然后，目标是将 $p(z)$ 推向 $r(z)$。<br>我们可以利用KL散度的性质：</p><p>$$<br>\begin{align}<br>I(X;Z)&amp;&#x3D;\mathbb{E} _ {p_(x,z)} [ \log \frac{p _ {\theta}(z|x)p(x)}{p(x)p(z)}]<br>\\&amp;&#x3D; \mathbb{E} _ {p(x)} [ \mathbb{E} _ {p _ {\theta}(z|x)} [ \log \frac{p _ {\theta}(z|x)}{p(z)} ] ]<br>\\&amp;&#x3D; \mathbb{E} _ {p(x)} [KL(p _ {\theta}(z|x) || p(z))]<br>\\&amp;&#x3D;\mathbb{E} _ {p(x)}[\int p _ {\theta}(z|x) \log \frac{p _ {\theta}(z|x)}{p(z)} dz]<br>\\&amp;&#x3D;\mathbb{E} _ {p(x)}[ \int p _ {\theta}(z|x) \log \left( \frac{p _ {\theta}(z|x)}{r(z)} \cdot \frac{r(z)}{p(z)} \right) dz]<br>\\&amp;&#x3D;\mathbb{E} _ {p(x)}[ \int p _ {\theta}(z|x) \left( \log \frac{p _ {\theta}(z|x)}{r(z)} + \log \frac{r(z)}{p(z)} \right) dz]<br>\\&amp;&#x3D;\mathbb{E} _ {p(x)}[\int p _ {\theta}(z|x) \log \frac{p _ {\theta}(z|x)}{r(z)} dz+\int p _ {\theta}(z|x) \log \frac{r(z)}{p(z)} dz]<br>\\&amp;&#x3D;\mathbb{E} _ {p(x)}[KL(p{\theta}(z|x) || r(z))+\mathbb{E}{p _ {\theta}(z|x)} \left[\log \frac{r(z)}{p(z)}\right]]<br>\end{align}<br>$$</p><p>而</p><p>$$<br>\begin{align}<br>\mathbb{E} _ {p(x)}\left[\mathbb{E}{p _ {\theta}(z|x)} \left[\log \frac{r(z)}{p(z)}\right]\right]&amp;&#x3D;\mathbb{E}{p(x,z)} \left[\log \frac{r(z)}{p(z)}\right]<br>\\&amp;&#x3D;\int_z \int_x p(x)p_\theta(z|x)log\frac{r(z)}{p(z)}dxdz<br>\\&amp;&#x3D;\int_z \left(\int_x p(x)p_\theta(z|x)dx\right)log\frac{r(z)}{p(z)}dz<br>\\&amp;&#x3D;\int_z p(z)log\frac{r(z)}{p(z)}dz<br>\\&amp;&#x3D;-KL(p(z)||r(z))<br>\end{align}<br>$$</p><p>故利用非负性即有：</p><p>$$<br>I(X;Z) \le \mathbb{E} _ {p(x)}[KL(p _ {\theta}(z|x) || r(z))]<br>$$</p><p>一个更直接且在VIB论文中被采用的思路是，我们希望 $Z$ 尽可能独立于 $X$，同时又保留预测 $Y$ 所需的信息。如果 $Z$ 对 $X$ 的依赖性很小，那么 $p _ {\theta}(z|x)$ 对于不同的 $x$ 应该看起来都差不多，并且都接近某个简单的先验 $r(z)$。因此，我们直接将 $I(X;Z)$ 替换为一个可以计算的上界或一个相关的量，即 $\mathbb{E} _ {p(x)}[KL(p _ {\theta}(z|x) || r(z))]$。</p><p>当 $r(z)$ 是一个固定的先验分布时，最小化 $\mathbb{E} _ {p(x)}[KL(p _ {\theta}(z|x) || r(z))]$ 会迫使编码器 $p _ {\theta}(z|x)$ 的输出（对于所有x）都接近这个先验 $r(z)$。这就限制了Z中能包含的关于X的特定信息量，从而达到了压缩的目的。</p><p><strong>组合得到最终的VIB目标函数</strong></p><p>将上述两部分的近似代入原始IB目标 $\max [I(Z;Y) - \beta I(X;Z)]$，我们得到VIB的目标（通常是最小化其负值）：</p><p>最小化<br>$$<br>\mathcal{L} _ {VIB}(\theta, \phi) &#x3D; \mathbb{E} _ {p(x)p(y|x)} \left[ - \mathbb{E} _ {p _ {\theta}(z|x)}[\log q _ {\phi}(y|z)] + \beta KL(p _ {\theta}(z|x) || r(z)) \right]<br>$$</p><p>这里：</p><ul><li>第一项 $-\mathbb{E} _ {p _ {\theta}(z|x)}[\log q _ {\phi}(y|z)]$ 是<strong>预测损失</strong>（或重构损失，取决于任务）。我们希望它小，对应于最大化 $I(Z;Y)$ 的下界。</li><li>第二项 $KL(p _ {\theta}(z|x) || r(z))$ 是<strong>压缩损失</strong>或正则化项。我们希望它小，对应于最小化（或约束）$I(X;Z)$。</li><li>$p _ {\theta}(z|x)$ 是编码器网络，将输入X映射到潜在表示Z的分布。</li><li>$q _ {\phi}(y|z)$ 是解码器&#x2F;预测器网络，从Z预测Y。</li><li>$r(z)$ 是一个预先选择的简单先验分布（例如，$\mathcal{N}(0,I)$）。</li><li>$\beta$ 是权衡这两个损失项的超参数。</li></ul><h2 id="Graph-Information-Bottleneck"><a href="#Graph-Information-Bottleneck" class="headerlink" title="Graph Information Bottleneck"></a>Graph Information Bottleneck</h2><p>我们希望一个好的图表示是鲁棒的、稳定的，并且拥有迁移的能力。具体来说，我们希望即使数据出现了分布漂移或者受到人为的扰动后，好的图表示仍然可拥有之前所述的性质。我们给出的核心想法是：一个好的图表示需要捕捉最少充分信息量minimal sufficient information。</p><p>于是我们使用信息瓶颈，类似地：<br>$$<br>\min  _ {\mathbb{P}\left(Z _ {X}^{(L)} \mid \mathcal{D}\right) \in \Omega} \operatorname{GIB} _ {\beta}\left(\mathcal{D}, Y ; Z _ {X}^{(L)}\right) \triangleq\left[-I\left(Y ; Z _ {X}^{(L)}\right)+\beta I\left(\mathcal{D} ; Z _ {X}^{(L)}\right)\right]<br>$$<br>同样地，我们要推出上下界：<br>$$<br>I\left(Y ; Z _ {X}^{(L)}\right) \geq 1+\mathbb{E}\left[\log \frac{\prod _ {v \in V} \mathbb{Q} _ {1}\left(Y _ {v} \mid Z _ {X, v}^{(L)}\right)}{\mathbb{Q} _ {2}(Y)}\right]+\mathbb{E} _ {\mathbb{P}(Y) \mathbb{P}\left(Z _ {X}^{(L)}\right)}\left[\frac{\prod _ {v \in V} \mathbb{Q} _ {1}\left(Y _ {v} \mid Z _ {X, v}^{(L)}\right)}{\mathbb{Q} _ {2}(Y)}\right]<br>$$</p><p>$$<br>I\left(\mathcal{D} ; Z _ {X}^{(L)}\right) \leq I\left(\mathcal{D} ;\left\{Z _ {X}^{(l)}\right\} _ {l \in S _ {X}} \cup\left\{Z _ {A}^{(l)}\right\} _ {l \in S _ {A}}\right) \leq \sum _ {l \in S _ {A}} \mathrm{AIB}^{(l)}+\sum _ {l \in S _ {X}} \mathrm{XIB}^{(l)},<br>$$</p><p>其中<br>$$<br>\mathrm{AIB}^{(l)}&#x3D;\mathbb{E}\left[\log \frac{\mathbb{P}\left(Z _ {A}^{(l)} \mid A, Z _ {X}^{(l-1)}\right)}{\mathbb{Q}\left(Z _ {A}^{(l)}\right)}\right], \mathrm{XIB}^{(l)}&#x3D;\mathbb{E}\left[\log \frac{\mathbb{P}\left(Z _ {X}^{(l)} \mid Z _ {X}^{(l-1)}, Z _ {A}^{(l)}\right)}{\mathbb{Q}\left(Z _ {X}^{(l)}\right)}\right]<br>$$</p><p>作者提出了GIB-cat和GIB-Bern分别对应不同的边分布。</p><p>GIB-cat假设：<br>$$<br>Z _ {A, v}&#x3D;\cup _ {t&#x3D;1}^{\mathcal{T}}\left\{u \in V _ {v t} \mid u \stackrel{\text { iid }}{\sim} \operatorname{Cat}\left(\frac{1}{\left|V _ {v t}\right|}\right)\right\}<br>$$<br>GIB-Bern假设：<br>$$<br>Z _ {A, v}&#x3D;\cup _ {t&#x3D;1}^{\mathcal{T}}\left\{u \in V _ {v t} \mid u \stackrel{\text { iid }}{\sim}Bernoulli(\alpha)\right\}<br>$$<br>AIB的经验估计为：<br>$$<br>\widehat{\mathrm{AIB}}^{(l)}&#x3D;\mathbb{E} _ {\mathbb{P}\left(Z _ {A}^{(l)} \mid A, Z _ {X}^{(l-1)}\right)}\left[\log \frac{\mathbb{P}\left(Z _ {A}^{(l)} \mid A, Z _ {X}^{(l-1)}\right)}{\mathbb{Q}\left(Z _ {A}^{(l)}\right)}\right]<br>$$<br>对于XIB，假设$Z _ {X, v} \sim \sum _ {i&#x3D;1}^{m} w _ {i} \operatorname{Gaussian}\left(\mu _ {0, i}, \sigma _ {0, i}^{2}\right)$，则<br>$$<br>\widehat{\mathrm{XIB}}^{(l)}&#x3D;\log \frac{\mathbb{P}\left(Z _ {X}^{(l)} \mid Z _ {X}^{(l-1)}, Z _ {A}^{(l)}\right)}{\mathbb{Q}\left(Z _ {X}^{(l)}\right)}&#x3D;\sum _ {v \in V}\left[\log \Phi\left(Z _ {X, v}^{(l)} ; \mu _ {v}, \sigma _ {v}^{2}\right)-\log \left(\sum _ {i&#x3D;1}^{m} w _ {i} \Phi\left(Z _ {X, v}^{(l)} ; \mu _ {0, i}, \sigma _ {0, i}^{2}\right)\right)\right]<br>$$</p><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><img src="/2024/20240520/GIB1.png"></p><p><img src="/2024/20240520/GIB2.png"></p><p><img src="/2024/20240520/GIB3.png"></p><h2 id="Robust-Graph-Information-Bottleneck"><a href="#Robust-Graph-Information-Bottleneck" class="headerlink" title="Robust Graph Information Bottleneck"></a>Robust Graph Information Bottleneck</h2><p>NIPS2023</p><p>用来应对双边图结构噪声</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
      <tag>信息论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MambaOut-Do We Really Need Mamba for Vision?</title>
    <link href="/2024/20240516/"/>
    <url>/2024/20240516/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>mamba是继transformer之后大火的结构之一。也涌现了各种mamba，各种领域的mamba。本博客<a href="../20240131/">之前</a>也介绍了这一算法。</p><p>出自NUS的Weihao Yu, Xinchao Wang等人提出了纯卷积的mamaout意图打败mamba。</p><p>MambaOut 模型在 ImageNet 图像分类上超越了所有视觉 Mamba 模型，表明 Mamba 对于该任务确实是不必要的。至于检测和分割，MambaOut 无法与最先进的视觉 Mamba 模型的性能相媲美，这展示了 Mamba 在长序列视觉任务中的潜力。</p><p>论文：<a href="https://arxiv.org/abs/2405.07992">arxiv</a></p><p>代码：<a href="https://github.com/yuweihao/MambaOut">github</a></p><span id="more"></span><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="/2024/20240516/mambaout.png"></p><h2 id="概念讨论"><a href="#概念讨论" class="headerlink" title="概念讨论"></a>概念讨论</h2><p><img src="/2024/20240516/x3.png"></p><p>从记忆角度来看因果注意力和类 RNN 模型的机制说明，其中$x_i$ 表示第 i 步骤的输入标记。 (a) 因果注意力将所有先前标记的键 和值  存储为内存。通过不断添加当前 token 的 key 和 value 来更新内存，因此内存是无损的，但缺点是随着序列的延长，整合旧内存和当前 token 的计算复杂度会增加。因此，注意力可以有效地管理短序列，但可能会遇到较长序列的困难。</p><p>相反，类似 RNN 的模型将先前的标记压缩为固定大小的隐藏状态 ℎ ，用作内存。这种固定大小意味着 RNN 内存本质上是有损的，无法与注意力模型的无损内存容量直接竞争。尽管如此，类似 RNN 的模型在处理长序列时可以表现出明显的优势，因为无论序列长度如何，将旧内存与当前输入合并的复杂性保持不变。</p><p>总之，Mamba 非常适合具有以下特征的任务：<br>• 特征1：任务涉及处理长序列。<br>• 特征2：任务需要因果标记混合模式。</p><h2 id="是否适合"><a href="#是否适合" class="headerlink" title="是否适合"></a>是否适合</h2><p>接下来，论文将讨论视觉识别任务是否表现出这两个特征。</p><h3 id="视觉识别任务的序列是否很长"><a href="#视觉识别任务的序列是否很长" class="headerlink" title="视觉识别任务的序列是否很长"></a>视觉识别任务的序列是否很长</h3><p>对于ImageNet上的图像分类，典型的输入图像大小为 224$^{2}$,从而产生块大小为 16$^{2}$的 14$^{2}&#x3D;196$标记。显然，196 远小于$\tau_\mathrm{small}$和$\tau_\mathrm{base}$ ,表明 ImageNet 上的图像分类不符合长序列任务的条件。</p><p>对于 COCO 上的对象检测和实例分割，推理图像大小为$800\times1280$,对于 ADE2oK 上的语义分割，推理图像大小为$512\times2048$ ,令牌数量约为4K，给定补丁大小$16^2$。从$4K&gt;\tau_\mathrm{small}$和$4K\approx\tau_\mathrm{base}$开始，COCO上的检测和ADE2oK上的分割都可以被认为是长序列任务。</p><h3 id="视觉识别任务需要因果标记混合模式吗"><a href="#视觉识别任务需要因果标记混合模式吗" class="headerlink" title="视觉识别任务需要因果标记混合模式吗"></a>视觉识别任务需要因果标记混合模式吗</h3><p>作者指出，完全可见的令牌混合模式允许不受限制的混合范围，而因果模式则限制当前令牌只能访问先前令牌的信息。视觉识别被归类为理解任务，其中模型可以立即看到整个图像，从而消除了对令牌混合的限制。对令牌混合施加额外的限制可能会降低模型性能。</p><h2 id="实验验证"><a href="#实验验证" class="headerlink" title="实验验证"></a>实验验证</h2><p><img src="/2024/20240516/x6.png"></p><p>Gated CNN 块的算法的PyTorch 代码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GatedCNNBlock</span>(nn.Module):<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, expension_ratio=<span class="hljs-number">8</span>/<span class="hljs-number">3</span>, kernel_size=<span class="hljs-number">7</span>, conv_ratio=<span class="hljs-number">1.0</span>,</span><br><span class="hljs-params">norm_layer=partial(<span class="hljs-params">nn.LayerNorm,eps=<span class="hljs-number">1e-6</span></span>),</span><br><span class="hljs-params">act_layer=nn.GELU,</span><br><span class="hljs-params">drop_path=<span class="hljs-number">0.</span></span>):<br><span class="hljs-built_in">super</span>().__init__()<br><span class="hljs-variable language_">self</span>.norm = norm_layer(dim)<br>hidden = <span class="hljs-built_in">int</span>(expension_ratio * dim)<br><span class="hljs-variable language_">self</span>.fc1 = nn.Linear(dim, hidden * <span class="hljs-number">2</span>)<br><span class="hljs-variable language_">self</span>.act = act_layer()<br>conv_channels = <span class="hljs-built_in">int</span>(conv_ratio * dim)<br><span class="hljs-variable language_">self</span>.split_indices = (hidden, hidden - conv_channels, conv_channels)<br><span class="hljs-variable language_">self</span>.conv = nn.Conv2d(conv_channels, conv_channels, kernel_size=kernel_size, padding=kernel_size//<span class="hljs-number">2</span>, groups=conv_channels)<br><span class="hljs-variable language_">self</span>.fc2 = nn.Linear(hidden, dim)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>shortcut = x <span class="hljs-comment"># [B, H, W, C] = x.shape</span><br>x = <span class="hljs-variable language_">self</span>.norm(x)<br>g, i, c = torch.split(<span class="hljs-variable language_">self</span>.fc1(x), <span class="hljs-variable language_">self</span>.split_indices, dim=-<span class="hljs-number">1</span>)<br>c = c.permute(<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>) <span class="hljs-comment"># [B, H, W, C] -&gt; [B, C, H, W]</span><br>c = <span class="hljs-variable language_">self</span>.conv(c)<br>c = c.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>) <span class="hljs-comment"># [B, C, H, W] -&gt; [B, H, W, C]</span><br>x = <span class="hljs-variable language_">self</span>.fc2(<span class="hljs-variable language_">self</span>.act(g) * torch.cat((i, c), dim=-<span class="hljs-number">1</span>))<br><span class="hljs-keyword">return</span> x + shortcut<br></code></pre></td></tr></table></figure><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><p>下表显示了224*224分辨率下模型在ImageNet上的性能。我们的MambaOut模型采用gate CNN块[60]。Mamba区块源自Gated CNN区块，包含了一个额外的SSM(状态空间模型)。很明显，视觉mamba模型不如MambaOut的性能，更不用说超越最先进的卷积或卷积注意力混合模型了。注意，vmamba9将mamba块的元架构修改为MetaFormer，不同于其他可视化mamba模型和MambaOut</p><p><img src="/2024/20240516/imagenet.jpg"></p><h3 id="分割"><a href="#分割" class="headerlink" title="分割"></a>分割</h3><p><img src="/2024/20240516/seg1.jpg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Graph-MLP--Node Classification without Message Passing in Graph</title>
    <link href="/2024/20240513/"/>
    <url>/2024/20240513/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《<a href="https://arxiv.org/abs/2106.04051">Graph-MLP: Node Classification without Message Passing in Graph</a>》【arxiv 2021】</p><p>最近的图神经网络（GNN）都依赖于邻接矩阵来指导特征聚合过程中邻居之间的消息传递，研究工作主要集中在强大的消息传递模块上。然而本文表明没有任何消息传递模块也是可行的。相反地，本文通过设计了一个邻近对比（NContrast）损失，通过隐式利用邻接信息来弥合 GNN 和 MLP 之间的差距。</p><p>在模型层面，Graph-MLP仅包括多层感知器、激活函数和层归一化。</p><span id="more"></span><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="/2024/20240513/diagram.png"></p><p><img src="/2024/20240513/Graph-MLP.png"></p><p>constrastive loss可以写成:<br>$$<br>\ell_i&#x3D;-\log\frac{\sum_{j&#x3D;1}^B 1_{[j\neq i]}\gamma_{ij}\exp\left(sim\left(z_i,z_j\right)&#x2F;\tau\right)}{\sum_{k&#x3D;1}^B 1_{[k\neq i]}\exp\left(sim\left(z_i,z_k\right)&#x2F;\tau\right)}<br>$$<br>其中</p><p>sim是余弦相似度，<br>$$<br>\begin{align}<br>\gamma_{ij}\begin{cases}&#x3D;0,&amp;\text{node }j \ \text{is the }r\text{-hop neighbor of node }i\\<br>\neq0,&amp;\text{node }j \ \text{is not the }r\text{-hop neighbor of node }i\end{cases}<br>\end{align}<br>$$</p><p>和</p><p>$$<br>\begin{align}<br>loss_{NC}&#x3D;\alpha \frac{1}{B}\sum_{i&#x3D;1} ^ B\ell_i<br>\\<br>loss_{final}&#x3D;loss_{CE}+loss_{NC}<br>\end{align}<br>$$</p><h2 id="算法解释"><a href="#算法解释" class="headerlink" title="算法解释"></a>算法解释</h2><p>我们可以简化sample的过程，sim改为取向量内积。</p><p>这时候对比loss可以变为<br>$$<br>\ell&#x3D;-\log\left(\frac{\sum_jA_{ij}\exp(\langle z_i,z_j\rangle)}{\sum_j(A_{ij}^c)\exp(\langle z_i,z_j\rangle)}\right)<br>$$<br>其中$A$是k-hop邻接矩阵，$A^\mathrm{c}$是A的补图。上面的loss可以分成positive和negative两个部分.。</p><p><strong>positive的部分:</strong><br>$$\ell_p&#x3D;\log\left(\sum_jA_{ij}\exp(\langle z_i,z_j)\rangle\right)$$<br>对$\ell_p$ 求导，我们有:<br>$$\frac{\partial\ell_p}{\partial z_i}&#x3D;\frac{1}{\sum_jA_{ij}\exp(\langle z_i,z_j\rangle)}\biggl(\sum_jA_{ij}\exp(\langle z_i,z_j\rangle)z_j\biggr)$$<br>对$z_i$做梯度下降，假设步长为$\eta$ ,在第t个iteration当中，我们有<br>$$z_{i}^{t-1&#x2F;2}&#x3D;z_{i}^{t}+\eta\frac{\partial\ell_{p}}{\partial z_{i}}&#x3D;z_{i}^{t}+\frac{\eta}{\sum_{j}A_{ij}\exp(\langle z_{i}^{t},z_{j}^{t}\rangle)}\left(\sum_{j}A_{ij}\exp(\langle z_{i}^{t},z_{j}^{t}\rangle)z_{j}^{t}\right)$$</p><p>上式类似于一个，加了残差连接的graph attention networks。</p><p><strong>negative部分：</strong><br>$$<br>\begin{aligned}<br>&amp;z_{i}^{t+1}&#x3D;z_{i}^{t+1&#x2F;2}-\eta\frac{\partial\ell_{n}}{\partial z_{i}}&#x3D;z_{i}^{t+1&#x2F;2} \<br>&amp;-\frac{\eta}{\sum_{j}A_{j}^{c}\exp(\langle z_{i}^{t},z_{j}^{t}\rangle)}\Bigg(\sum_{j}A_{j}^{c}\exp(\langle z_{i}^{t},z_{j}^{t}\rangle)z_{j}^{t}\Bigg)<br>\end{aligned}<br>$$<br>是为了让k-hop不相邻的节点的representation不接近，从而避免trival solution的产生。</p><p>【1】中提到，从GNN角度上来看是对A的补图做了类似于《<a href="https://arxiv.org/abs/2101.00797">Beyond Low-frequency Information in Graph Convolutional Networks</a>》当中的high-pass filter 。</p><blockquote><p>存疑</p></blockquote><p><strong>参考：</strong></p><p>【1】《<a href="https://zhuanlan.zhihu.com/p/385848111">Graph-MLP: 用MLP与优化优雅地超越GNN</a>》</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN与注意力</title>
    <link href="/2024/20240512/"/>
    <url>/2024/20240512/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>2014年，Volodymyr的《Recurrent Models of Visual Attention》一文中首先将注意力其应用在视觉领域，后来伴随着2017年Ashish Vaswani的《Attention is all you need》中Transformer结构的提出，注意力机制大成。</p><p>在注意力机制中，除去耳熟能详的SENet和CBAM，也有一些主动去学习注意区域的网络算法。</p><p>比如以下两篇，都出自deepmind。</p><p><a href="https://arxiv.org/pdf/1412.7755">《MULTIPLE OBJECT RECOGNITION WITH VISUAL ATTENTION》</a>ICLR 2015</p><p>《<a href="https://arxiv.org/pdf/1502.04623">DRAW: A Recurrent Neural Network For Image Generation</a>》</p><span id="more"></span><center><figure>    <img src="house_read.gif" alt="RNN 学习读取门牌号" width="25%" style="max-width: 300px;">    <img src="house_generate.gif" alt="RNN 学习绘制门牌号" width="25%" style="max-width: 300px;"></figure></center><h2 id="DRAM"><a href="#DRAM" class="headerlink" title="DRAM"></a>DRAM</h2><p>模型架构：</p><p><img src="/2024/20240512/model.png"></p><h3 id="Glimpse-Network"><a href="#Glimpse-Network" class="headerlink" title="Glimpse Network**"></a>Glimpse Network**</h3><p>该网络是一种非线性函数，接收当前输入的图像patch或者说是glimpse,$x_n$,以及其位置序列$l_n$,其中$l_n&#x3D;(x_n,y_n)$作为输入，输出一个向量$g _ {n}$。</p><p>Glimpse Network 的工作就是从原始图像中从位置$l _ {n}$附近提取一组有用的特征。利用$G _ {image}(x_n|W _ {image})$来表示函数$G _ {image}(\dot)$的输出向量$G _ {image}(\dot)$的输入是图像patch $x_n$,然后以权重$W_image${参 数 化 }，$G _ {image}( \dot )$ 通常由三个卷积隐层构成，没有pooling layer,然后紧跟着一个全连接层。</p><p>另外，$G _ {loc}(l _ {n}|W _ {loc})$利用全连接的隐层来映射出位置元组，$G _ {loc}(l _ {n}|W _ {loc})$ 和$G _ {image}(x_n|W _ {image})$拥有相同的维度。</p><p>我们将这两个向量进行点乘操作得到最终的 glimpse feature vector $g_n:$<br>$$g _ {n}&#x3D;G _ {image}(x _ {n}|W _ {image})G _ {loc}(l_n|W _ {loc}).$$</p><h3 id="Recurrent-Network"><a href="#Recurrent-Network" class="headerlink" title="Recurrent Network"></a><strong>Recurrent Network</strong></h3><p>RN汇聚从每一个glimpse上提取的信息，并且以一种 coherent manner组合起来以保存空间信息。</p><p>从 glimpse Network 得到的glimpse feature vector $g_n$作为每一个时刻 RN的输入。RN 由两个循环层和非线性函数$R _ {recur}$构成，定义两个 recurrent layer的输出为$r^{(1)}$ and $r^{(2)}:$</p><p>$$<br>\begin{align}<br>r^{(1)}&#x3D;R _ {recur}(g_n,r _ {n-1}^{(1)}|W _ {r1})\\<br>r^{(2)}&#x3D;R _ {recur}(g_n,r _ {n-1}^{(2)}|W _ {r2})<br>\end{align}<br>$$</p><p>我们用LSTN单元来处理非线性$R _ {recur}$,因为其具有学习长期依赖(long-range dependencies)和稳定的学习动态(stable learning dynamics)的能力。</p><h3 id="Emission-Network"><a href="#Emission-Network" class="headerlink" title="Emission Network"></a><strong>Emission Network</strong></h3><p>该网络将RN当前的状态作为输入，然后预测出 glimpse network 应该向何处提取图像patch。这个就像一个指挥中心，从 RN 中基于当前的内部状态，控制着attention。由一个全连接隐层构成，将feature vector $r_n^{(2)}$从recurrent layer的顶部映射成坐标元组$l _ {n+1}:$</p><p>$$<br>\hat{l} _ {n+1}&#x3D;E(r_n^{(2)}|W_e)<br>$$</p><h3 id="Context-Network"><a href="#Context-Network" class="headerlink" title="Context Network"></a><strong>Context Network</strong></h3><p>Context Network 提供了RN的输出状态，其输出用于 emission Network 来预测第一个glimpse的位置。Context Network $C(*)$ 将下采样的图像 $I _ {coarse}$作为输入，并且出书一个固定长度的向量$c_I$。该结构信息提供了一个可见的提示，即：所给图像潜在的感兴趣区域的位置。采用三个卷积层将粗糙的图像$I _ {coarse}$映射成特征向量作为 RN 的top recurrrent layer $r^2$ 的初始状态。底层 $r^{1}$初始化为零向量，原因后面会具体介绍。</p><h3 id="Classification-Network"><a href="#Classification-Network" class="headerlink" title="Classification Network"></a><strong>Classification Network</strong></h3><p>分类网络基于底层RN的最终的特征向量 $r _ {N}^{(1)}$预测出一个类别标签y。由一个全连接隐层和softmax 输出层构成：<br>$$P(y|I)&#x3D;O(r_n^1|W_o)$$</p><p>理想情况下，深度循环attention model 应该学习去寻找对分类物体相关的位置。Contextual information的存在，提供了一个”short cut”的解决方案，使得模型通过组合不同glimpse的信息从而更加容易学习到Contextudl information。</p><p>我们阻止如此不想要的行为，通过链接 context network and classification network 到不同的 recurrent layer。所以，最终使得contextual information 不能被直接用于 classification network,然后只影响模型产生的glimpse locations序列。</p><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>给定一张图像 I 的类别标签 y，我们可以利用交叉熵目标函数，将学习过程规划为 有监督分类问题。attention model 基于每一个 glimpse 得到潜在的位置变量 l，然后提取出对应的patches。所以我们可以通过忽略 glimpse locations 来最大化 类别标签的似然性 ：</p><p>$$<br>logp(y|I,W)&#x3D;log\sum _ {l}p(l|I,W)p(y|l,I,W)<br>$$</p><p>忽略的目标函数可以通过其 variational free energy lower bound F 来学到：</p><p>$$<br>\begin{align}\log\sum _ {l}p(l|I,W)p(y|l,I,W)&amp;\geq\sum _ {l}p(l|I,W)\log p(y,l|I,W)+H[l]\\&amp;&#x3D;\sum _ {l}p(l|I,W)\log p(y|l,I,W)<br>\end{align}<br>$$</p><p>将上述函数求关于模型参数W的导数，可以得到学习的规则：<br>$$<br>\begin{align}\frac{\partial\mathcal{F}}{\partial W}<br>&amp;&#x3D;\sum _ {l}p(l|I,W)\frac{\partial\log p(y|l,I,W)}{\partial W}+\sum _ {l}\log p(y|l,I,W)\frac{\partial p(l|I,W)}{\partial W}<br>\\&amp;&#x3D;\sum _ {l}p(l|I,W)\left[\frac{\partial\log p(y|l,I,W)}{\partial W}+\log p(y|l,I,W)\frac{\partial\log p(l|I,W)}{\partial W}\right]\end{align}<br>$$</p><p>在glimpse 序列中的每个glimpse，很难在训练中估计许多成指数式glimpse<br>locations。上面公式的和可以利用蒙特卡罗采样(Monte Carlo Samples)的方法来进行<br>估计：<br>$$<br>\tilde{l}^m\sim p(l_n|I,W)&#x3D;\mathcal{N}(l_n;\hat{l}_n,\Sigma)<br>$$</p><p>$$<br>\frac{\partial\mathcal{F}}{\partial W}\approx\frac{1}{M}\sum _ {m&#x3D;1}^{M}\left[\frac{\partial\log p(y|\tilde{l}^m,I,W)}{\partial W}+\log p(y|\tilde{l}^m,I,W)\frac{\partial\log p(\tilde{l}^m|I,W)}{\partial W}\right]<br>$$</p><p>上面第二个公式提供了一种实际的算法来训练深度attention model。即，在每次glimpse 之后，我们从模型中预测出的glimpse location中进行采样。这些样本然后用于标准的后向传播来得到当前模型参数的估计。似然性$logp(y|l^m,I,W)$有一个unbounded range,可以在梯度估计中引入大量的 high variance。特别是，当图像中采样的位置偏离物体时，log 似然性会引入一个不想要的较大的梯度更新，并且通过剩下的模型进行后向传播。</p><p>我们可以减小公式中的 variance,通过将$logp(y|l^{m},I,W)$替换为 0&#x2F;1离<br>散指示函数 R,然后利用一个baseline technique 来解决该问题：</p><p>$$<br>\begin{align}<br>R&#x3D;\begin{cases}<br>1\quad y&#x3D;\arg\max_y\log p(y|\tilde{l}^m,I,W) \\<br>0\quad \text{otherwise}<br>\end{cases}<br>\\<br>b_n&#x3D;E _ {baseline}(r_n^{(2)}|W _ {baseline})<br>\end{align}<br>$$</p><p>像所展示的那样，recurrent network state vector $r_n^{(2)}$用来估计每一个glimpse 基于状态的baseline $b$,此举明显的改善了学习的效率。该baseline 有效地中心化了随机变量 R,并且可以通过向期望的R值讲行回归而学习到。给定指示函数和baseline,我们有如下的梯度更新：<br>$$<br>\dfrac{\partial\mathcal{F}}{\partial W}\approx\dfrac{1}{M}\sum\limits _ {m&#x3D;1}^{M}\left[\dfrac{\partial\log p(y|\tilde{l}^m,I,W)}{\partial W}+\lambda(R-b)\dfrac{\partial\log p(\tilde{l}^m|I,W)}{\partial W}\right]<br>$$</p><p>其中，超参数$\lambda$平衡了两个梯度成分的尺度。实际上，通过利用 0&#x2F;1指示函数，上式的学习规则就等价于利用强化学习的方法来训练attention model。当看作是强化学习更新时，公式13的第二项就是基于glimpse policy的期望奖励R的对应W的梯度无偏估计。(原文：the second term in equation 13 is an unbiased estimate of the gradient with respect to W of the expected reward R under the model glimpse policy。)<br>在推理的过程中，前馈位置估计可以用作位置坐标的决策性估计，以此提取模型下一个输入图像patch。该模型表现的像一个常规的前馈网络。我们的marginallized objective function equation 5 通过利用位置序列的采样，提供了一个预测期望的类别估计，并且对这些估计进行平均：<br>$$<br>\mathbb{E}_l[p(y|I)]\approx\frac{1}{M}\sum _ {m&#x3D;1}^Mp(y|I,\tilde{l}^m)<br>$$</p><p>通过平均分类估计，这允许attention model 可以多次评价。作者的实验表明平均 log 概率可以得到最优的性能。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2024/20240512/dramresult.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>注意力</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FINDING ADVERSARIALLY ROBUST GRAPH LOTTERY TICKETS</title>
    <link href="/2024/20240511/"/>
    <url>/2024/20240511/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《<a href="https://openreview.net/pdf?id=FcBmz8nLnq">FINDING ADVERSARIALLY ROBUST GRAPH LOTTERY TICKETS </a>》NeurIPS 2023&amp;ICLR 2024</p><span id="more"></span><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="Graph-Lottery-Ticket-Hypothesis"><a href="#Graph-Lottery-Ticket-Hypothesis" class="headerlink" title="Graph Lottery Ticket Hypothesis"></a>Graph Lottery Ticket Hypothesis</h3><h4 id="The-Lottery-Ticket-Hypothesis"><a href="#The-Lottery-Ticket-Hypothesis" class="headerlink" title="The Lottery Ticket Hypothesis"></a>The Lottery Ticket Hypothesis</h4><p>该论文首先提到了ICLR 2019最佳论文:The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks。该论文提出了彩票假说：密集的、随机初始化的、前馈网络包含子网络（中奖票），这些子网络在孤立地训练时，在类似数量的迭代中达到与原始网络相当的测试精度。</p><p>具体而言，他的训练策略如下：</p><p><img src="/2024/20240511/LTH.png"></p><p><a href="https://arxiv.org/abs/1905.01067">《Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask》</a>也有类似的内容。展示了为什么将权重设置为零很重要，如何使用符号来进行重新初始化的网络训练，以及为什么掩蔽的行为类似于训练。最后，我们发现了超级掩码的存在，这些掩码可以应用于未经训练的随机初始化网络，以生成性能远远优于偶然的模型（MNIST 上为 86%，CIFAR-10 上为 41%）。</p><p>ICLR2020 的《<a href="http://proceedings.mlr.press/v119/malach20a/malach20a.pdf">Proving the Lottery Ticket Hypothesis: Pruning is All You Need</a>宣称证明了The Lottery Ticket Hypothesis。一句话概括：只要对随机初始化的神经网络做个好剪枝，不怎么训练也能有个好效果。</p><p>Pruning is All You Need！</p><p>该文证明的结论是：</p><p>Fix some target fully-connected ReLU-network F of width k, depth d and input dimension n.Fix$\delta&gt;0$.Then,arandomly-initialized network $G$ of width $poly(d,n,k,1&#x2F;\epsilon,\log(1&#x2F;\delta))$ and depth 2d, has w.p. $\geq1-\delta$ a subnetwork $\tilde{G}$ that approximates F up to $\epsilon.$</p><p>简单的说，给定一个深度为d的Relu目标网络。那么一个深度为2d，且足够宽的随机网络里，必然可以找到一个可以逼近目标网络的子网络。</p><p>具体证明可见刘斯坦的<a href="https://www.zhihu.com/question/473365094/answer/2044032805">解答</a></p><h4 id="In-Graph"><a href="#In-Graph" class="headerlink" title="In Graph"></a>In Graph</h4><p>《<a href="https://arxiv.org/abs/2102.06790">A Unified Lottery Ticket Hypothesis for Graph Neural Networks</a>》一文将LTH应用在了GNN上。</p><p>具体而言，在端到端训练过程中，UGS分别对邻接矩阵和GNN模型权值应用两个可微二元掩模张量。训练完成后，移除最小大小的元素，将相应的掩码位置更新为0，分别从邻接矩阵和GNN中去除低评分的边和权重。然后将稀疏GNN权重参数倒绕到其原始初始化。（类似于原始的LTH）</p><p>实验结果表明，UGS可以在不影响预测精度的前提下显著降低推理计算成本。在这项工作中，我们的目标是为受到对抗性扰动的数据集找到glt。当我们将UGS算法直接应用于摄动图时，glt的性能精度与干净的glt相比明显较低，这需要开发新的优化方法来寻找对抗鲁棒的glt。</p><h2 id="分析对抗性攻击对图属性的影响"><a href="#分析对抗性攻击对图属性的影响" class="headerlink" title="分析对抗性攻击对图属性的影响"></a>分析对抗性攻击对图属性的影响</h2><p>像MetaAttack、PGD和PR-BCD这样的对抗性攻击通过引入新边或删除现有边来毒害图结构，导致原始图属性发生变化。<br>我们分析了干净边和对抗边连接的节点在属性特征上的差异。</p><p><img src="/2024/20240511/impact.png"></p><p>图a和图b描述了受PGD攻击的homophilic和 heterophilic图数据集中连接节点属性特征差的密度分布。在同态图中，攻击倾向于连接属性特征差异较大的节点。防御技术可以潜在地利用这些信息来区分图中的良性和敌对边缘。然而，这不是 heterophilic图的情况。相反，我们利用节点的位置特征，使用像DeepWalk这样的位置编码技术(Perozzi等人，2014)。有趣的是，从图c中我们可以看到，在heterophilic图中，攻击倾向于连接位置特征差较大的节点。ARGS使用这些图的属性来迭代地从homophilic和 heterophilic图中修剪对抗性边。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>$\mathcal{L}$<br>总所周知，两层的GCN可以表示为<br>$$<br>Z&#x3D;f({ A, X },\Theta)&#x3D; \mathcal{S}(\hat{A} \sigma ( \hat A X W_{(0)}) W_{(1)})<br>$$<br>设计了一个transductive semi-supervised node classification (SSNC) loss：<br>$$<br>\mathcal{L} _0 (f(\left{A, X\right}, \Theta))&#x3D;-\sum _ {l \in \mathcal{Y} _{TL}} \sum _{j&#x3D;1} ^C  Y _{l_j} log( Z <em>{l_j})<br>$$<br>其中$\mathcal{Y}</em>{TL}$是训练节点的索引，C是类总数，$Y_l$是$v_l$one hot 标签。</p><p>posion 攻击者的目标是找到一个最优的扰动A ‘，欺骗GNN做出错误的预测。这可以表述为一个双层优化问题(Zugner et al.， 2018;zugner &amp; gunnemann, 2019):<br>$$<br>arg \max\mathcal{L}<em>{atk}(f(\left{A’,X\right},\Theta ^\ast))\<br>A’\in\Phi(A)\<br>\mathrm{s.t.}\quad\Theta^{\ast}&#x3D;\arg\min</em>{\Theta}\mathcal{L}<em>{0}(f(\left{A’,X\right},\Theta))<br>$$<br>其中$\Phi(A)$是满足$\frac{|A’-A|{0}}{|A|{0}}\leq\Delta$的领接矩阵。$\mathcal{L}</em>{atk}$ 是攻击loss函数，$\Delta$ 是 perturbation rate，$\Theta ^\ast$是摄动图上GNN的最优参数。</p><p>为了帮助消除对抗边和鼓励特征平滑，对于homophilic graphs：<br>$$<br>\mathcal{L}<em>{fs} (A’,X)&#x3D;\frac{1}{2} \sum</em>{i,j&#x3D;1}A_{ij}’ (x_i-x_j)^2<br>$$<br>对于heterophilic graphs：<br>$$<br>\mathcal{L}<em>{fs}(A’)&#x3D;\frac{1}{2}\sum</em>{i,j&#x3D;1}A_{ij}’(y_{i}-y_{j})^{2}<br>$$</p><blockquote><p>以上有点像<em>dirichlet</em> energy。</p><p><em>dirichlet</em> energy：<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{2}\sum _{i,j}W[i,j] (x[j]-x[i])^2<br>$$<br>进一步归一化：<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{2}\sum _{i,j}W[i,j] (\frac{x[j]}{\sqrt{1+d_j}}-\frac{x[i]}{\sqrt{1+d_i}})^2<br>$$<br>（上式来自《<a href="https://proceedings.neurips.cc/paper/2021/file/b6417f112bd27848533e54885b66c288-Paper.pdf">Dirichlet Energy Constrained Learning for Deep Graph Neural Networks</a>》）</p><p>或<br>$$<br>tr(x^\top Lx)&#x3D;|\nabla_Gx|_2^2&#x3D;\frac{1}{4}\sum _{i,j}W[i,j]|\frac{x[j]}{\sqrt{d_j}}-\frac{x[i]}{\sqrt{d_i}}|_2^2<br>$$<br>（上式来自《<a href="https://openreview.net/pdf?id=kS7ED7eE74">A Fractional Graph Laplacian Approach to Oversmoothing</a>》）</p><p>其中d为节点的度。</p></blockquote><p>其中yi∈R P为输入图G上运行DeepWalk算法(Perozzi et al.， 2014)得到的节点i, j的位置特征，P为节点位置特征个数。</p><p>另外，作者还训练了一个简单的两层MLP。mlp只使用节点特征进行训练。然后我们使用训练好的MLP来预测测试节点的标签。称这些标签为伪标签。最后，利用MLP预测置信度较高的测试节点计算测试节点CE损失项。</p><p>设$Y_{P L}$为MLP预测置信度较高的测试节点集，$Y_{mlp}$为MLP的预测值。CE损失为：<br>$$<br>\mathcal{L}<em>1(f({A’,X},\Theta))&#x3D;-\sum</em>{l\in\mathcal{Y}<em>{TL}}\sum</em>{j&#x3D;1}^CY_{mlp_{l_j}}\log(Z_{l_j})<br>$$<br>最终loss为：<br>$$<br>\mathcal{L}<em>{ARGS}&#x3D;\alpha\mathcal{L}</em>{0}(f({m_{g}\odot A’,X},m_{\theta}\odot\Theta))+\beta\mathcal{L}<em>{fs}(m</em>{g}\odot A’,X)\+\gamma\mathcal{L}<em>{1}(f({m</em>{g}\odot A’,X},m_{\theta}\odot\Theta))+\lambda_{1}||m_{g}||<em>{1}+\lambda</em>{2}||m_{\theta}||<em>{1}<br>$$<br>其中，$\alpha$和$\gamma$设置为1。$m_g$用于领接矩阵，$m</em>\theta$用于模型权重。</p><p><img src="/2024/20240511/algo.png"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/2024/20240511/UGS.png"></p><p><img src="/2024/20240511/result.png"></p><p>Inference MACs（Multiply-Accumulate Operations）是指推断过程中的乘累加操作数量。</p><p><img src="/2024/20240511/vsother.png"></p><p>在大规模图数据集中：</p><p><img src="/2024/20240511/biggraph.png"></p><p>消融实验：</p><p><img src="/2024/20240511/aba.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>prompt压缩</title>
    <link href="/2024/20240429/"/>
    <url>/2024/20240429/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>prompt压缩技术可以减少输入prompt的长度，同时保持各种任务的性能。</p><span id="more"></span><h2 id="Selective-Context"><a href="#Selective-Context" class="headerlink" title="Selective Context"></a>Selective Context</h2><p>出自《<a href="https://arxiv.org/abs/2310.06201">Compressing Context to Enhance Inference Efficiency of Large Language Models</a>》-EMNLP 2023。使用了一个很简单但有效的方法。</p><h3 id="算法介绍"><a href="#算法介绍" class="headerlink" title="算法介绍"></a>算法介绍</h3><p>作者提出了self-information的概念，用来衡量了一段文本相对与一个language model所蕴含的信息。</p><p>self-information计算也极其简单。<br>$$<br>I(x) &#x3D; -log(P(x))<br>$$<br>P指语言模型，但不需要是大模型本体，可以是小号的LLMs，例如1.3b的OPT和OpenAI-Curie(6.3b)。</p><p>prompt压缩的整体过程十分简单。</p><ul><li>首先计算文本中每个短语的self-informaiton。</li><li>再根据想要的压缩比设置百分位数，得到阈值。</li><li>最后删去低于阈值的短语。</li></ul><p>我们也可以直接上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_get_self_info_via_gpt2</span>(<span class="hljs-params">self, text: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>], <span class="hljs-type">List</span>[<span class="hljs-built_in">float</span>]]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.lang == <span class="hljs-string">&#x27;en&#x27;</span>:<br>            text = <span class="hljs-string">f&quot;&lt;|endoftext|&gt;<span class="hljs-subst">&#123;text&#125;</span>&quot;</span><br>        <span class="hljs-keyword">elif</span> <span class="hljs-variable language_">self</span>.lang == <span class="hljs-string">&#x27;zh&#x27;</span>:<br>            text = <span class="hljs-string">f&quot;[CLS]<span class="hljs-subst">&#123;text&#125;</span>&quot;</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            encoding = <span class="hljs-variable language_">self</span>.tokenizer(text, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>)<br>            encoding = encoding.to(<span class="hljs-variable language_">self</span>.device)<br>            outputs = <span class="hljs-variable language_">self</span>.model(**encoding)<br>            logits = outputs.logits<br>            probs = torch.softmax(logits, dim=-<span class="hljs-number">1</span>)<br>            self_info = -torch.log(probs)<br>        <br><br>        input_ids = encoding[<span class="hljs-string">&#x27;input_ids&#x27;</span>]<br>        input_ids_expaned = input_ids[:, <span class="hljs-number">1</span>:].unsqueeze(-<span class="hljs-number">1</span>)<br>    <br>        tokens = [<span class="hljs-variable language_">self</span>.tokenizer.decode(token_) <span class="hljs-keyword">for</span> token_ <span class="hljs-keyword">in</span> input_ids.squeeze().tolist()[<span class="hljs-number">1</span>:]]<br>        <span class="hljs-keyword">return</span> tokens, self_info[:, :-<span class="hljs-number">1</span>].gather(-<span class="hljs-number">1</span>, input_ids_expaned).squeeze(-<span class="hljs-number">1</span>).squeeze(<span class="hljs-number">0</span>).tolist()<br></code></pre></td></tr></table></figure><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>使用了<strong>以下三个数据源</strong>作为测试数据：</p><ul><li><a href="https://link.zhihu.com/?target=http://ShareGPT.com">http://ShareGPT.com</a>：收集用户与ChatGPT对话记录的网站</li><li>Arxiv</li><li>BBC News</li></ul><p><img src="/2024/20240429/exp1.png"></p><h2 id="LLMLingua"><a href="#LLMLingua" class="headerlink" title="LLMLingua"></a>LLMLingua</h2><p>LLMLingua是Huiqiang Jiang, Qianhui Wu, Xufang Luo等出自微软的研究者提出的一系列prompt压缩算法。</p><h3 id="LLMLingua-1"><a href="#LLMLingua-1" class="headerlink" title="LLMLingua"></a>LLMLingua</h3><p>占位</p><h3 id="LongLLMLingua"><a href="#LongLLMLingua" class="headerlink" title="LongLLMLingua"></a>LongLLMLingua</h3><p>zhanw</p><h3 id="LLMLinguav2"><a href="#LLMLinguav2" class="headerlink" title="LLMLinguav2"></a>LLMLinguav2</h3><p>暂无</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>光电与概念漂移</title>
    <link href="/2024/20240410/"/>
    <url>/2024/20240410/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这部分的论文比较少，且难找。</p><p>主要介绍以下三篇论文：</p><p><strong>论文一：</strong> 《Online prediction of photovoltaic power considering concept drift》<span id="more"></span></p><p>——2023 IEEE Power &amp; Energy Society General Meeting (PESGM)</p><p><strong>论文二：</strong> 《An adaptive deep learning framework for day-ahead forecasting of photovoltaic power generation 》</p><p>——2022 Sustainable Energy Technologies and Assessments</p><p><strong>论文三：</strong> 《Deep Learning for Load Forecasting with Smart Meter Data: Online Adaptive Recurrent Neural Network》——2021 Applied Energy</p><h2 id="论文一"><a href="#论文一" class="headerlink" title="论文一"></a>论文一</h2><p><strong>历史研究：</strong></p><p>《Frequency-Domain Decomposition and Deep Learning Based Solar PV Power Ultra-Short-Term Forecasting Model》使用了CNN实现对光伏发电低频和高频分量的超短期预测。</p><p>《A Graph Neural Network Based Deep Learning Predictor for Spatio-Temporal Group Solar Irradiance Forecasting》通过卷积图神经网络（CGNN）和LSTM捕获光伏预测的关键特征和时间相关性，提高了预测精度。         </p><p>然而，上述模型的输入输出映射是静态的，将其应用于在线预测时存在挑战。这是因为在线光伏发电功率预测的输入是流数据，其分布会随着环境的动态而变化，导致从历史数据获得的映射可能不再适用。数据分布随时间以不可预见的方式发生变化的现象被称为概念漂移 (CD) ，而 CD 已被确定为概念漂移的来源随着时间的推移，数据驱动模型的准确性不断下降，光伏发电功率预测尚未得到足够的重视。</p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-4.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-5.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-6.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-7.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-8.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-9.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-10.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-11.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-12.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-13.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-14.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-15.jpg"></p><h2 id="论文二"><a href="#论文二" class="headerlink" title="论文二"></a>论文二</h2><p><strong>历史研究：</strong></p><p>过往的论文使用了LSTM和autoencoder等取得了明显进步。       </p><p>（1）当前的DL模型通常是离线或静止的，这意味着通过周期性地浏览所有历史数据来训练模型一次，然后用于推断未来的光伏输出。</p><p>这种方法无法利用新到达的数据可能提供的基本信息，比如一些可以对模型训练产生积极影响的隐藏在新数据中的信息当然，可以通过利用先前的数据和新到达的数据来重新训练模型，但随着数据量的增加，这将产生巨大的计算资源。理想情况下，模型应该能够及时从新到达的数据中学习，而无需重新训练先前的数据。</p><p>（2）由于某些不可预见的变化，流数据的基本分布会随着时间的推移而变化，导致所谓的概念漂移。然而，目前的DL模型并没有考虑到这种影响。例如，安装额外的光伏机组和光伏系统的不可预见的光伏机组故障将不可避免地改变数据分布，这导致输入特征变量和输出目标变量之间现有的映射关系崩溃。在存在概念漂移的情况下，传统的深度学习模型的预测性能将显著下降。</p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-18.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-19.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-20.jpg"></p><h2 id="论文三"><a href="#论文三" class="headerlink" title="论文三"></a>论文三</h2><p><strong>历史研究：</strong></p><p>智能电表和其他传感器的普及为建筑物乃至个人家庭层面基于传感器的负载预测创造了新的机会。</p><p>循环神经网络（RNN）等机器学习方法在负载预测方面取得了巨大成功，但这些方法采用离线学习：它们只接受一次训练，并错过了从新到达的数据中学习的机会。</p><p>此外，它们不太适合处理概念漂移；例如，如果由于安装新设备而导致负载发生变化，它们的预测性能将会下降。</p><p>因此，本文提出了在线自适应 RNN，这是一种能够不断从新到达的数据中学习并适应新模式的负载预测方法。在新数据到达时更新模型，无需重新训练，也无需保留历史数据。</p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-24.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-25.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-26.jpg"></p><p><img src="/2024/20240410/ddac987d11740b7dd9d7ba9223eef8e1-27.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>光电</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>每个人都应该掌握的快速调研法</title>
    <link href="/2024/20240403/"/>
    <url>/2024/20240403/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来着Up主老蒋巨靠谱的<a href="https://www.bilibili.com/video/BV1pF411f7yD/">《面试必备！每个人都应该掌握的快速调研法》</a></p><hr><span id="more"></span><p>**问题1：**如何获取目标信息。（如何筛选靠谱的高质量的信息源）</p><p>**问题2：**如何问对问题。（从信息海中抽取什么样的信息）</p><p>**问题3：**如何进行归纳分析</p><p>————————————————————</p><p>针对互联网搜素展开的调研步骤</p><p><strong>步骤一：提出问题</strong></p><p>调研之前先尝试提出问题，明确我们通过调研想解答哪些问题。（从调研动机中去寻找问题）</p><p><strong>步骤二：初步搜索</strong></p><p>搜索的渠道（搜索引擎、微信搜索里搜索关键词、知乎、行研报告、播客、chatgpt）</p><p><strong>步骤三：深度搜索，以搜养搜。用搜索结果扩充自己的搜索关键词。</strong></p><p>笔记本左侧记录信息散点，右侧记录看到这些信息散点后脑子中出现的衍生关键词以及衍生的新问题。</p><p>举例子：</p><p><strong>信息散点：</strong></p><p>香调类型公认科学的分类法是香调轮盘</p><p>调香师是香水行业的关键岗位，主要集中在香精公司。</p><p>中国香水行业市场规模300-500亿人民币。</p><p><strong>衍生问题：</strong></p><p>香调轮盘</p><p>知名调香师</p><p>调香师工资</p><p>为什么香水公司不养调香师</p><p>增长率、</p><p>中国品牌</p><p>过去几年资本动作</p><p><strong>步骤四：整理归纳</strong></p><p>左侧的信息散点复盘一遍，分门别类按照一种逻辑去捋下来，分成几个模块，把散落在各处的信息分门别类装进这几个模块里，从而得到一个框架。</p><p>商业领域的框架一般来说分成：市场情况、上下游从业者情况、技术和工业工艺相关信息、中国市场的特点、行业发展史。</p><p>社会问题的调研一般来说分成：事件或者热点的timeline梳理、各参与方的信息归因逻辑、解决方法。</p><p><strong>步骤五：通过调研回答一些最基本直达本源的综合的高级问题。</strong></p><p>可以试着回到第一步，回顾初心，能否给出答案，比如香水行业的问题：香水是不是智商税、香水品牌之间竞争的核心壁垒是什么、香水到底能满足人们什么样的需求也就是这个行业为什么存在。</p><p>————————————————————</p><p><strong>非搜索调研方法</strong></p><p><strong>专家调研：</strong></p><p>与该领域的资深从业者进行直接一对一的聊天，需要具备逻辑能力，能够不断的提出正确的问题一步一步深入某个领域。</p><p><strong>学术性调研：</strong></p><p>适用于某个学科领域中的启蒙学习，比如学习经济学以及人口学，需要看论文看书，通过论文的参考书目，通过自媒体的文章、通过京东书籍页面搜索、咨询熟人，找到一本合格的启蒙书，然后点开这个书的豆瓣页面或者微信读书页面，找到下方该书籍归属于的书单，能发现想要深入了解领域的书，如果据此没有找到靠谱的书单，很可能是这本启蒙书质量一般。可以重复这个过程，最终找到很好的阅读list，在这个阅读list里面摘要出工人比较好的3-5本启蒙书，都读一遍，就可以超越多数人在这个领域内的认知了。</p><p>在较大的领域，可以找中国最好的研究生院校这个系，找到该系导师推荐的考研参考书目，里面有靠谱的教材。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>残差结构的讨论</title>
    <link href="/2024/20240308/"/>
    <url>/2024/20240308/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>本文主要围绕《<a href="https://arxiv.org/pdf/2304.14802.pdf">ResiDual: Transformer with Dual Residual Connections</a>》和《<a href="https://arxiv.org/abs/1603.05027">Identity Mappings in Deep Residual Networks</a>》展开。</p><p>在深度学习中，乃至当今很火的transformer，残差是一个很重要的部分。</p><p>残差结构的讨论也是老生常谈的话题了，比如transformer中应该选择Pre Norm与Post Norm呢。</p><span id="more"></span><p>关于Pre Norm与Post Norm，苏神的博客“<a href="https://kexue.fm/archives/9009">为什么Pre Norm的效果不如Post Norm？</a>”有更多的讨论。</p><h2 id="双残差"><a href="#双残差" class="headerlink" title="双残差"></a>双残差</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>《ResiDual: Transformer with Dual Residual Connections》结合Pre Norm与Post Norm，提出了ResiDual。</p><p><img src="/2024/20240308/ResiDual.png"></p><p>ResiDual吸收两者之长，避两者之短，没有了Post-LN的Gradient Vanish，也没有了Pre-LN的Representation Collapse。</p><p><img src="/2024/20240308/Analysis.png"></p><h3 id="实验结果："><a href="#实验结果：" class="headerlink" title="实验结果："></a>实验结果：</h3><p><img src="/2024/20240308/experiment.png"></p><h2 id="残差块的有效性和改进"><a href="#残差块的有效性和改进" class="headerlink" title="残差块的有效性和改进"></a>残差块的有效性和改进</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>何恺明提出的《Identity Mappings in Deep Residual Networks》针对残差块为什么有效，以及如何改进进行了一系列实验，最终得出一个结论：identity mapping支路应该要保证“clean”，也就是不应该在该支路后做ReLU激活函数，这才能保证前向传播和反向传播过程中的信息畅通无阻. 基于此，文章提出了新的残差块，其结构如下图：</p><p><img src="/2024/20240308/resnetv2.png"></p><p>对于ResNet V2来说，identity mapping支路没有ReLU激活函数，在前向传播和后向传播时都能够畅通无阻，真正实现了恒等映射. ResNet V2的“最终呈现结果”也比较简单，就是把原先residual block中的卷积、BN和ReLU调换了顺序。</p><h3 id="实验结果：-1"><a href="#实验结果：-1" class="headerlink" title="实验结果："></a>实验结果：</h3><p>文章更大的篇幅都在实验证明各种结构的效果。</p><p>下图是不同顺序的残差块结构和实验结果，指标是分类错误率:</p><p><img src="/2024/20240308/shortcut.png"></p><p><img src="/2024/20240308/shortcutexp.png"></p><p>不同的激活函数放置：</p><p><img src="/2024/20240308/diffact.png"></p><p><strong>图(b) vs 图(a)：</strong>(b)中将BN层也放到了identity mapping支路中来，进一步改变了该支路的信息流，导致信息传播更为阻塞. 这也是为什么，这个结构的结果比baseline更差.</p><p><strong>图(c) vs 图(a)：</strong>(c)将identity mapping分支的ReLU激活函数前移在了residual分支中，因此residual分支的输出永远是大于等于0的，这意味着模型在前向传播的过程中，数值只会单调递增，这无疑会影响模型的表征能力. 这也是为什么，这个结构的结果比baseline更差. 【residual分支其输出应该要保证在(-∞, +∞)】</p><p><strong>图(d) vs 图(a)：</strong>(d)将identity mapping分支的ReLU激活函数放在了residual分支的第一个卷积前.</p><p>**图(e) vs 图(a)：**在上图(d)的基础上，(e)又将residual分支的BN进行了前移. ResNet V2将BN和ReLU统称为activation，图(e)相当于是在图(a)baseline的基础上做了一个pre-activation. 具体转换也可以见下图. 前移之后的结构既保证了identity mapping分支“clean”，也获得更优的模型效果.</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNNS ARE NOT TRANSFORMERS (YET)</title>
    <link href="/2024/20240303/"/>
    <url>/2024/20240303/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文：RNNS ARE NOT TRANSFORMERS (YET) -THE KEY BOTTLENECK ON IN-CONTEXT RETRIEVAL</p><p>本文研究了递归神经网络(rnn)和transformer在解决算法问题方面的表示能力差距。理论分析表明，CoT改善了rnn，但不足以缩小与transformer的差距。</p><p>我们证明，采用技术来增强 RNN 的上下文检索能力，包括检索增强生成（RAG）和添加单个 Transformer 层，可以使 RNN 能够通过 CoT 解决所有多项式时间可解决的问题，从而缩小了与transformer的代表性差距。</p><hr><span id="more"></span><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>现代的RNN得到了一系列的发展，包括RWKV、RetNet和Mamba。</p><p>过去有许多研究证明了RNN不如Transformrt。比如2024年的《Repeat after me: Transformers are better than state space models at copying》证明常量记忆 RNN 没有足够的表示能力来解决对给定的输入向量子集进行平均的任务（q稀疏平均）和重复输入序列（复制），而存在可以解决这些任务的浅层 Transformer。</p><p>然而，上述结果并不排除通过额外的提示技术或微小的架构改变来增强 RNN 可能缩小与 Transformer 差距的可能性。事实上，Transformer 本身也不完美，可能需要在推理时使用额外的技术才能在某些任务上表现良好。作为一个著名的例子， 思想链 （CoT）  是一种提示技术，要求模型在给出最终答案之前生成一系列中间标记，众所周知，它对于 Transformers 在需要数学或算法推理的任务上表现良好至关重要。部分研究者从表示能力的角度解释了这一点：变压器本身没有足够的表示能力来解决超出特定电路复杂性类别的问题（$TC^0$)，但通过 CoT，他们甚至可以模拟任何多项式时间图灵机。</p><p>CoT 在 Transformers 上的有效性自然会引发以下问题：</p><p><strong>类似的增强（例如采用 CoT）能否改进 RNN，使其与 Transformer 相媲美？</strong></p><h3 id="本文贡献"><a href="#本文贡献" class="headerlink" title="本文贡献"></a>本文贡献</h3><p>本文通过研究各种方法来缩小 RNN 和 Transformer 在算法问题上的表示能力上的差距，从理论上回答了上述问题。通过一系列下限和上限结果，我们表明 CoT 提高了 RNN 的表示能力，但为了缩小与 Transformer 的差距，仅 CoT 不足以克服 RNN 的一个关键瓶颈：它们无法从上下文中检索信息，我们简称为上下文检索。</p><p>我们进一步说明，解决上下文检索瓶颈足以弥补这一差距：如果采用增强上下文检索能力的技术，包括涉及检索增强生成（RAG）和附加， RNN 可以解决所有多项式时间可解决的问题单个 Transformer 层。我们的主要贡献如下：</p><p>1.<strong>CoT 改进了 RNN，但无法缩小与 Transformer 的表示差距。</strong></p><ul><li><p>从积极的一面来看，CoT 使 RNN 严格具有更强的表达能力。</p></li><li><p>从消极的一面来看，我们表明采用 CoT 不足以缩小 RNN 和Transformer 之间的表示差距：RNN 的内存效率从根本上限制了它们执行上下文检索的能力，即使使用 CoT 也是如此。通过证明带有 CoT 的 RNN 无法解决一组直接要求上下文检索（包括联想召回）的基本算法问题，这一点得到了具体体现。通过证明 RNN 无法解决确定图是否是树的经典问题，我们进一步举例说明了在看似不相关的任务中隐含地需要上下文检索。</p></li><li><p>另一方面，我们证明 Transformers 具有轻松解决上述许多任务的表征能力，包括判断是否是树。此外，具有 CoT 的 Transformers 甚至可以有效地模拟具有 CoT 的 RNN，而参数数量只需要很小的乘法因子。</p></li></ul><ol start="2"><li><strong>增强 RNN 的上下文检索能力可以缩小表示差距</strong></li></ol><ul><li><p>我们证明，允许 RNN 调用函数调用来执行特定的上下文检索原语，足以提高其表示能力，以解决 CoT 的所有多项式时间可解问题，从而缩小RNN 和 Transformer 之间的表示差距。</p></li><li><p>另外，由于一层 Transformer 足以执行许多上下文检索操作，因此我们证明，通过在架构末尾添加一个 Transformer 层来隐式增强 RNN 的上下文检索能力也足以缩小差距</p></li></ul><h2 id="CoT-能否提高-RNN-的表示能力？"><a href="#CoT-能否提高-RNN-的表示能力？" class="headerlink" title="CoT 能否提高 RNN 的表示能力？"></a>CoT 能否提高 RNN 的表示能力？</h2><p>在本节中，我们的目标是了解带有 CoT 的 RNN 的表示能力。</p><p>我们首先展示了积极的结果，即带有 CoT 的 RNN 可以解决在没有 CoT 固定状态大小的情况下 RNN 无法完成的任务。然后我们继续了解 CoT 是否可以使 RNN 具有像 Transformer 一样的表现力。</p><p>我们表明，即使使用 CoT，RNN 仍然难以解决明确需要上下文检索的问题，并且这种表示差距会传播到看似与检索无关的推理任务，例如 IsTree。最后，我们表明这种差距确实是单方面的：只存在 Transformer 需要的参数比 RNN 指数少的任务，但反之则不然。</p><h3 id="CoT-严格改进-RNN"><a href="#CoT-严格改进-RNN" class="headerlink" title="CoT 严格改进 RNN"></a>CoT 严格改进 RNN</h3><p><img src="/2024/20240303/Theorem41.png"></p><h3 id="CoT-无法缩小与-Transformer-的代表性差距"><a href="#CoT-无法缩小与-Transformer-的代表性差距" class="headerlink" title="CoT 无法缩小与 Transformer 的代表性差距"></a>CoT 无法缩小与 Transformer 的代表性差距</h3><h4 id="上下文检索的简单问题"><a href="#上下文检索的简单问题" class="headerlink" title="上下文检索的简单问题"></a>上下文检索的简单问题</h4><p>首先，我们证明 RNN 在解决几个直接测试上下文检索能力的简单算法问题时与 Transformer 存在显着的表示差距。</p><p><img src="/2024/20240303/Theorem46.png"></p><p>对于上限，我们证明 Transformer 可以通过利用称为“注意力机制”的注意力机制来解决问题匹配，它接受查询令牌并关注与某些预定义坐标上的查询令牌相匹配的先前键。这种机制允许Transformer像键值字典一样读取其上下文窗口，从而可以完美解决问题。对于计数问题，我们另外使用数数注意力机制，通过均匀地关注查询标记的每次出现来计算查询标记的出现次数。</p><h4 id="了解-RNN-超越简单上下文检索问题的表示能力"><a href="#了解-RNN-超越简单上下文检索问题的表示能力" class="headerlink" title="了解 RNN 超越简单上下文检索问题的表示能力"></a>了解 RNN 超越简单上下文检索问题的表示能力</h4><p>一个自然的问题是，如果算法问题不直接测试上下文检索能力，我们是否可以希望 RNN 具有解决它的表示能力？</p><p>在这种情况下，RNN 和 Transformer 是否具有相同的表示能力？</p><p>我们表明，RNN 中有限的内存大小仍然可能成为解决算法问题的瓶颈。即使检索能力没有在算法问题中显式测试，在推理答案时仍然可能隐式地需要它。</p><p>我们通过一个最小的算法问题示例（称为IsTree)：</p><p>给定一个无向图G的n节点，判断是否是一棵树，即每一对节点是否都由一条简单路径连接。 IsTree 的经典解决方案是运行深度优先搜索 (DFS)，它需要O(n)时间。</p><p><img src="/2024/20240303/Theorem47.png"></p><h4 id="Transformer-明显比-RNN-更具表现力"><a href="#Transformer-明显比-RNN-更具表现力" class="headerlink" title="Transformer 明显比 RNN 更具表现力"></a>Transformer 明显比 RNN 更具表现力</h4><p>上述定理表明，存在一些任务，其中 Transformer 所需的内存比 RNN 少得多。</p><p>然而，他们并没有排除是否存在相应任务，这种任务其中 Transformer 会比 RNN 更加冗余并且需要指数级更多的参数。</p><p>然而，以下定理证实了常规 RNN 不存在这样的任务。</p><p><img src="/2024/20240303/Theorem48.png"></p><h2 id="增强上下文检索能力缩小表征差距"><a href="#增强上下文检索能力缩小表征差距" class="headerlink" title="增强上下文检索能力缩小表征差距"></a>增强上下文检索能力缩小表征差距</h2><p>在 前面 中，我们表明 RNN 在上下文检索方面存在缺陷，因此导致与Transformer 的表示存在显着差距。</p><p>在本节中，我们的目标是了解：如果我们增强RNN 的上下文检索能力，RNN 与 Transformer 是否仍然存在表示差距？</p><p>我们通过考虑显式和隐式方法来增强上下文检索能力来回答这个问题，并表明这两种方法都可以缩小 RNN 和 Transformer 在解决算法问题时的表示差距。</p><p><img src="/2024/20240303/Incontextrag.png"></p><h3 id="Explicit-Retrieval-Through-Regular-Expression"><a href="#Explicit-Retrieval-Through-Regular-Expression" class="headerlink" title="Explicit Retrieval Through Regular Expression"></a>Explicit Retrieval Through Regular Expression</h3><p>首先，我们探索 RNN 与检索增强生成 (RAG) 的强大功能，它使 LM 能够检索相关信息以协助生成。在我们的上下文中，我们特别感兴趣的是允许 LM 调用函数从其上下文中检索信息，我们将其称为In-context Retrieval Augmented Generation (In-context RAG).</p><p><img src="/2024/20240303/Theorem51.png"></p><h3 id="通过仅附加一层-Transformer-层进行隐式检索"><a href="#通过仅附加一层-Transformer-层进行隐式检索" class="headerlink" title="通过仅附加一层 Transformer 层进行隐式检索"></a>通过仅附加一层 Transformer 层进行隐式检索</h3><p>我们在本节中正式表明，这种形式的隐式检索可以缩小 RNN 和 Transformer 在解决算法问题时的表示差距。我们考虑以下混合架构，它通过将单个 Transformer 层附加到 RNN 输出来组合 RNN 和 Transformer。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><img src="/2024/20240303/fig5.png"></p><p>训练了三种不同的架构：</p><p>（1）LLaMa</p><p>（2）Mamba</p><p>（3） Mamba with one additional layer of LLaMA block representing hybrid architectures.</p><p>图中可观察到：</p><p>1.CoT 提高了 Transformer 和 RNN 的性能。然而，随着图尺寸的增加，RNN 的性能急剧下降，并且 Transformer 的性能始终优于 RNN。这与我们的理论一致，即 CoT 可以提高 RNN 模型的表达能力，但表达能力仍然不足以解决 IsTree 任务</p><p>2.通过正则表达式的检索增强使所有模型达到近乎完美的准确性。这与我们的理论一致，即通过正则表达式进行检索增强可以提高 RNN 模型解决算法任务的表达能力</p><p>3.混合模型显示了所有模型中最好的性能，即使没有 CoT 或显式检索增强也能达到近乎完美的精度，这也反映在理论结果中。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BitNet b1.58</title>
    <link href="/2024/20240229/"/>
    <url>/2024/20240229/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>微软研究院、国科大同一团队（作者部分变化）的研究者推出了 BitNet 的重要 1-bit 变体，即 BitNet b1.58，其中每个参数都是三元并取值为 {-1, 0, 1}。他们在原来的 1-bit 上添加了一个附加值 0，得到二进制系统中的 1.58 bits。</p><blockquote><p> log_2(3)&#x3D;1.58</p></blockquote><p>论文：<a href="https://arxiv.org/pdf/2402.17764.pdf">The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</a></p><span id="more"></span><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="BitNet"><a href="#BitNet" class="headerlink" title="BitNet"></a>BitNet</h3><p>BitNet是之前的工作，《<a href="https://arxiv.org/abs/2310.11453">Scaling 1-bit Transformers for Large Language Models</a>》，每个参数都是二元并取值为 {-1,  1}.</p><p><img src="/2024/20240229/BitLinear.png"></p><h4 id="BitLinear"><a href="#BitLinear" class="headerlink" title="BitLinear"></a>BitLinear</h4><p>我们首先用sgn函数将权值二值化为+1或- 1。我们在二值化之前将权重集中为零均值，以在有限的数值范围内增加容量。在二值化后使用一个比例因子β来减小实值和二值化后的权重之间的l2误差。权W∈Rn×m的二值化可以表示为:</p><p><img src="/2024/20240229/BitLinear1.png"></p><h3 id="BitNet-b1-58"><a href="#BitNet-b1-58" class="headerlink" title="BitNet b1.58"></a>BitNet b1.58</h3><p>BitNet b1.58 基于 BitNet 架构，并且用 BitLinear 替代 nn.Linear 的 Transformer。BitNet b1.58 是从头开始训练的，具有 1.58 bit 权重和 8 bit 激活。与原始 BitNet 架构相比，它引入了一些修改，总结为如下：</p><p><img src="/2024/20240229/bitnet.png"></p><p>用于激活的量化函数与 BitNet 中的实现相同，只是该研究没有将非线性函数之前的激活缩放到 [0, Q_b] 范围。相反，每个 token 的激活范围为 [−Q_b, Q_b]，从而消除零点量化。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="与LLaMA的对比"><a href="#与LLaMA的对比" class="headerlink" title="与LLaMA的对比"></a>与LLaMA的对比</h3><p>表 1 总结了 BitNet b1.58 和 LLaMA LLM 的困惑度和成本：在困惑度方面，当模型大小为 3B 时，BitNet b1.58 开始与全精度 LLaMA LLM 匹配，同时速度提高了 2.71 倍，使用的 GPU 内存减少了 3.55 倍。特别是，当模型大小为 3.9B 时，BitNet b1.58 的速度是 LLaMA LLM 3B 的 2.4 倍，消耗的内存减少了 3.32 倍，但性能显著优于 LLaMA LLM 3B。</p><p><img src="/2024/20240229/tab1.png"></p><p>表 2 结果表明，随着模型尺寸的增加，BitNet b1.58 和 LLaMA LLM 之间的性能差距缩小。更重要的是，BitNet b1.58 可以匹配从 3B 大小开始的全精度基线的性能。与困惑度观察类似，最终任务（ end-task）结果表明 BitNet b1.58 3.9B 优于 LLaMA LLM 3B，具有更低的内存和延迟成本。</p><p><img src="/2024/20240229/tab2.png"></p><h3 id="内存和延迟："><a href="#内存和延迟：" class="headerlink" title="内存和延迟："></a>内存和延迟：</h3><p>该研究进一步将模型大小扩展到 7B、13B 和 70B 并评估成本。图 2 显示了延迟和内存的趋势，随着模型大小的增加，增长速度（speed-up）也在增加。特别是，BitNet b1.58 70B 比 LLaMA LLM 基线快 4.1 倍。这是因为 nn.Linear 的时间成本随着模型大小的增加而增加，内存消耗同样遵循类似的趋势。延迟和内存都是用 2 位核测量的，因此仍有优化空间以进一步降低成本。</p><p><img src="/2024/20240229/fig2.png"></p><h3 id="能耗"><a href="#能耗" class="headerlink" title="能耗"></a>能耗</h3><p>该研究还对 BitNet b1.58 和 LLaMA LLM 的算术运算能耗进行了评估，主要关注矩阵乘法。图 3 说明了能耗成本的构成。BitNet b1.58 的大部分是 INT8 加法计算，而 LLaMA LLM 则由 FP16 加法和 FP16 乘法组成。根据 [Hor14，ZZL22] 中的能量模型，BitNet b1.58 在 7nm 芯片上的矩阵乘法运算能耗节省了 71.4 倍。</p><p>该研究进一步报告了能够处理 512 个 token 模型的端到端能耗成本。结果表明，随着模型规模的扩大，与 FP16 LLaMA LLM 基线相比，BitNet b1.58 在能耗方面变得越来越高效。这是因为 nn.Linear 的百分比随着模型大小的增加而增长，而对于较大的模型，其他组件的成本较小。</p><p><img src="/2024/20240229/fig3.png"></p><h3 id="吞吐量"><a href="#吞吐量" class="headerlink" title="吞吐量"></a>吞吐量</h3><p>该研究比较了 BitNet b1.58 和 LLaMA LLM 在 70B 参数体量上在两个 80GB A100 卡上的吞吐量，使用 pipeline 并行性 [HCB+19]，以便 LLaMA LLM 70B 可以在设备上运行。实验增加了 batch size，直到达到 GPU 内存限制，序列长度为 512。表 中 显示 BitNet b1.58 70B 最多可以支持 LLaMA LLM batch size 的 11 倍，从而将吞吐量提高 8.9 倍</p><p><img src="/2024/20240229/tab4.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pure Noise to the Rescue of Insufficient Data</title>
    <link href="/2024/20240227/"/>
    <url>/2024/20240227/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>ICML 2022 &#x2F; MLRC 2022 OutstandingPaper</p><hr><p><strong>Q</strong>: 这篇论文试图解决什么问题？</p><p><strong>A</strong>: 这篇论文试图解决的问题是深度神经网络在处理不平衡数据集时的性能问题。在现实世界的数据集中，某些类别的样本可能比其他类别稀少，这导致模型在训练过程中容易偏向于多数类，从而在少数类上表现不佳，这种现象称为类别不平衡（class imbalance）。为了解决这个问题，论文提出了一种名为Oversampling with Pure Noise Images (OPeN)的新方法，旨在通过在训练过程中引入纯噪声图像来改善模型对少数类（minority class）的泛化能力。此外，论文还提出了一种新的批量归一化层（Distribution Aware Routing Batch Normalization, DAR-BN），用于分别归一化噪声图像和自然图像，以进一步提高模型性能。</p><span id="more"></span><p><img src="/2024/20240227/Model.png"></p><p><strong>Q</strong>: 有哪些相关研究？</p><p><strong>A</strong>: 相关研究主要集中在解决类别不平衡问题，提高模型在少数类上的分类性能。以下是一些与本论文相关的研究：</p><ol><li><strong>类别不平衡问题的研究</strong>：<ul><li><strong>重采样技术</strong>：通过增加少数类的样本数量或减少多数类的样本数量来平衡数据集。</li><li><strong>损失函数调整</strong>：例如，使用加权损失函数，为少数类分配更高的权重，以减少模型对多数类的偏好。</li></ul></li><li><strong>数据增强方法</strong>：<ul><li><strong>Cutout</strong>：通过随机裁剪图像的一部分并将其置为零，增加模型的鲁棒性。</li><li><strong>SimCLR</strong>：一种对比学习框架，通过随机变换图像来学习图像的表示。</li></ul></li><li><strong>批量归一化（Batch Normalization, BN）的改进</strong>：<ul><li><strong>Instance Normalization</strong>：对每个样本进行归一化，而不是整个批次。</li><li><strong>Group Normalization</strong>：将批次分成多个组，并对每个组进行归一化。</li></ul></li><li><strong>特定于不平衡数据集的网络架构</strong>：<ul><li><strong>WideResNet</strong>：一种深度残差网络，通过增加宽度来提高模型性能。</li><li><strong>ResNet</strong>：一种深度残差网络，通过引入残差连接来解决深度网络训练中的退化问题。</li></ul></li><li><strong>对抗性训练</strong>：<ul><li><strong>Adversarial Examples</strong>：通过生成对抗性样本来提高模型的鲁棒性。</li></ul></li><li><strong>自适应数据增强策略</strong>：<ul><li><strong>AutoAugment</strong>：一种自动学习数据增强策略的方法，通过搜索算法找到最优的数据增强策略。</li></ul></li></ol><p>这些研究为解决类别不平衡问题提供了多种方法和思路，本论文提出的OPeN方法和DAR-BN层则是在这些研究基础上的进一步探索。</p><p><strong>Q</strong>: 论文如何解决这个问题？</p><p><strong>A</strong>: 论文提出了一种名为Oversampling with Pure Noise Images (OPeN)的方法来解决类别不平衡问题，具体步骤如下：</p><ol><li><strong>纯噪声图像生成</strong>：在训练过程中，OPeN会在每个mini-batch中随机替换一部分图像为纯噪声图像。这些噪声图像是完全随机生成的，与数据集中的任何真实图像都无关。替换的概率与类别的频率成反比，即少数类图像被替换的概率更高。</li></ol><p><img src="/2024/20240227/OPeN.png"></p><p>2.<strong>分布感知路由批量归一化（DAR-BN）</strong>：由于传统的批量归一化（BN）假设输入数据来自单一分布，而OPeN方法中同时存在自然图像和噪声图像，这违反了BN的假设。为了解决这个问题，论文提出了DAR-BN，它通过学习自然图像的激活映射的仿射参数来归一化噪声图像，从而允许模型分别处理这两种类型的输入。</p><p><img src="/2024/20240227/DAR-BN.png"></p><p><img src="/2024/20240227/diff_shift.png"></p><p>自然图像和噪声图像之间的分布偏移如下。自然图像和纯随机噪声图像的激活图。激活图是预训练的VGG16模型的第一个卷积块的输出(我们从64个通道中采样16个)。<br>我们看到自然图像和纯噪声图像的特征是从不同的分布中采样的。</p><p><img src="/2024/20240227/shift.png"></p><ol start="3"><li><strong>训练策略</strong>：OPeN方法在训练的后期阶段开始引入噪声图像，这种策略称为延迟过采样（Deferred Resampling）。这样做的目的是让模型在训练初期专注于学习数据的真实分布，而在后期通过引入噪声图像来鼓励模型泛化。</li><li><strong>数据增强的鲁棒性</strong>：论文还展示了OPeN方法在不同数据增强策略下的性能，包括随机水平翻转、随机裁剪、Cutout和SimCLR等，证明了OPeN方法在各种数据增强条件下都能提高模型性能。</li><li><strong>在平衡数据集上的应用</strong>：除了在不平衡数据集上的应用，论文还探讨了在平衡数据集上添加纯噪声图像作为数据增强的可能性，并发现这种方法也能提高模型的分类性能。</li></ol><p>通过这些方法，论文旨在提高模型对少数类图像的识别能力，同时保持对多数类图像的识别准确性，从而在类别不平衡的数据集上实现更好的泛化性能。</p><p><strong>Q</strong>: 论文做了哪些实验？</p><p><strong>A</strong>: 论文中进行了一系列的实验来验证OPeN方法的有效性，具体包括：</p><ol><li><strong>主实验</strong>：<ul><li>使用CIFAR-10-LT和CIFAR-100-LT数据集，这些数据集是CIFAR-10和CIFAR-100的长尾变体，具有类别不平衡特性。</li><li>对比了四种不同的过采样策略：Empirical Risk Minimization (ERM)、Resampling (RS)、Deferred Resampling (DRS)和OPeN。</li><li>测量了不同策略在验证集上的准确率，特别是针对少数类的性能。</li></ul></li><li><strong>批量归一化层的比较</strong>：<ul><li>在CIFAR-10-LT和CIFAR-100-LT数据集上，比较了标准批量归一化（Standard BN）、辅助批量归一化（Auxiliary BN）和分布感知路由批量归一化（DAR-BN）的性能。</li><li>验证了DAR-BN在OPeN方法中的重要性。</li></ul></li><li><strong>数据增强方法的鲁棒性测试</strong>：<ul><li>在CIFAR-10-LT数据集上，使用不同强度的数据增强方法（如随机水平翻转、裁剪、Cutout和SimCLR），并比较了ERM、DRS和OPeN的性能。</li></ul></li><li><strong>在平衡数据集上的应用</strong>：<ul><li>在完整的CIFAR-10数据集上，添加纯噪声图像作为数据增强，并测量了模型性能的提升。</li></ul></li><li><strong>额外实验</strong>：<ul><li>在CIFAR-10-LT（IR&#x3D;50）和CIFAR-100-LT（IR&#x3D;100, 50）数据集上评估OPeN的性能。</li><li>对输入归一化值进行了超参数搜索，以确定最佳的均值和标准差。</li><li>对批量大小和噪声比例进行了超参数搜索，以找到最优的模型性能。</li></ul></li><li><strong>模型架构的比较</strong>：<ul><li>使用ResNet-32网络架构重复OPeN实验，并与WideResNet-28-10的结果进行比较。</li></ul></li><li><strong>随机种子对数据集生成的影响</strong>：<ul><li>分析了不同随机种子对CIFAR-10-LT数据集生成的影响，并比较了在不同数据集上训练的模型性能。</li></ul></li><li><strong>模型先验的分析</strong>：<ul><li>使用t-SNE可视化模型嵌入，分析OPeN是否改变了模型对噪声和分布外（OOD）图像的分类倾向。</li></ul></li></ol><p>这些实验旨在全面评估OPeN方法在不同条件下的性能，以及其对模型泛化能力的影响。</p><p><strong>Q</strong>: 有什么可以进一步探索的点？</p><p><strong>A</strong>: 尽管论文已经对OPeN方法进行了深入的研究，但仍有一些潜在的探索点可以进一步研究：</p><ol><li><strong>不同数据集的泛化性</strong>：在其他类型的不平衡数据集上测试OPeN方法，例如医学图像、自然语言处理或音频数据，以验证其泛化能力。</li><li><strong>模型架构的敏感性</strong>：研究不同深度学习架构（如ResNet、DenseNet、EfficientNet等）对OPeN方法的响应，以及如何调整这些架构以最大化OPeN的性能。</li><li><strong>噪声图像的生成策略</strong>：探索不同的噪声图像生成策略，例如使用条件生成对抗网络（cGAN）生成与少数类图像风格相似的噪声图像。</li><li><strong>噪声比例的优化</strong>：系统地研究不同噪声比例对模型性能的影响，以及是否存在最优的噪声比例。</li><li><strong>训练策略的改进</strong>：研究不同的训练策略，如动态调整噪声比例、引入噪声的时机（例如在训练的早期阶段）等。</li><li><strong>模型解释性</strong>：研究OPeN方法如何影响模型的可解释性，以及是否可以通过可视化技术来理解模型是如何利用噪声图像进行学习的。</li><li><strong>对抗性攻击的鲁棒性</strong>：评估OPeN训练的模型在面对对抗性攻击时的鲁棒性。</li><li><strong>长期记忆和遗忘</strong>：研究OPeN方法是否会影响模型的长期记忆能力，以及如何平衡新信息的学习和旧信息的保留。</li><li><strong>跨领域应用</strong>：探索OPeN方法在跨领域任务中的应用，例如迁移学习或多任务学习。</li><li><strong>理论分析</strong>：从理论上分析OPeN方法的工作原理，以及它如何影响模型的学习过程。</li></ol><p>这些探索点可以帮助我们更深入地理解OPeN方法的工作原理，以及如何将其应用于更广泛的领域和任务。</p><p><strong>Q</strong>: 总结一下论文的主要内容</p><p><strong>A</strong>: 这篇论文的主要内容可以总结如下：</p><ol><li><strong>问题背景</strong>：现实世界的数据集往往存在类别不平衡问题，即某些类别的样本数量远少于其他类别。这导致深度学习模型在训练时容易偏向于多数类，从而在少数类上表现不佳，影响模型的泛化能力。</li><li><strong>提出方法</strong>：为了解决这一问题，论文提出了一种名为Oversampling with Pure Noise Images (OPeN)的新方法。OPeN通过在训练过程中引入纯噪声图像来鼓励模型对少数类进行更好的泛化。同时，论文还提出了一种新的批量归一化层（DAR-BN），用于分别处理噪声图像和自然图像，以提高模型性能。</li><li><strong>实验设计</strong>：论文在CIFAR-10-LT和CIFAR-100-LT这两个长尾数据集上进行了实验，对比了不同的过采样策略，包括ERM、RS、DRS和OPeN，并评估了这些策略在验证集上的准确率。</li><li><strong>实验结果</strong>：<ul><li>OPeN方法在提高少数类准确率方面表现出色，从而提高了整体的分类准确率。</li><li>DAR-BN在与OPeN结合使用时，相较于其他批量归一化方法，显著提升了模型性能。</li><li>OPeN方法在不同数据增强策略下均表现出鲁棒性，且在平衡数据集上也能通过添加纯噪声图像来提高分类性能。</li></ul></li><li><strong>额外探索</strong>：论文还进行了额外的实验，包括在不同数据集和不平衡程度上评估OPeN，以及在ResNet架构上的应用。此外，还探讨了随机种子对数据集生成的影响，以及模型先验对OPeN性能的影响。</li><li><strong>讨论与挑战</strong>：论文讨论了OPeN方法的有效性，并指出其性能提升可能与更大的网络架构或更复杂的数据增强策略有关。同时，论文也指出了在复现过程中遇到的挑战，如代码的可用性、数据集的一致性以及超参数的选择。</li><li><strong>结论</strong>：论文得出结论，OPeN方法能够有效地改善不平衡数据集上的模型性能，特别是在提高少数类准确率方面。DAR-BN层对于OPeN方法的成功至关重要。此外，论文还提出了未来研究的方向，如在更广泛的数据集和任务上应用OPeN方法，以及进一步探索其背后的理论机制。</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Fuyu</title>
    <link href="/2024/20240226/"/>
    <url>/2024/20240226/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Transformer一作Ashish Vaswani所在的AI公司Adept，发布了Fuyu-8B，是一个多模态模型的小版本，目前唯一一个以像素patch作为图像输入的多模态模型，利用这种方式实现了任意分辨率的无损input。</p><p><a href="https://huggingface.co/spaces/adept/fuyu-8b-demo">体验地址</a></p><span id="more"></span><hr><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在最新一代基础模型中，多模态模型通常共享一个共同的结构。它们通常包括一个独立的图像编码器，其输出通过交叉注意力机制或适配器集成到大型语言模型（LLM）中。这一模式已经得到广泛应用，如PALM-e、PALI-X、QWEN-VL、LLaVA 1.5和Flamingo等模型都采用了这种方式。这些模型通常以固定的图像分辨率进行操作。在推断过程中，超出此分辨率的图像必须被缩小，而具有不同宽高比的图像则需要进行填充或扭曲。</p><p><strong>LLaVA-1.5:</strong></p><p><img src="/2024/20240226/LLaVA-1.5.png"></p><p>在训练方面，许多其他多模态模型都经历了多步训练过程。图像编码器通常与LLM分开训练，通常使用对比训练目标，这可能很复杂。必须决定何时冻结各个组件的权重。有些模型甚至包括额外的高分辨率图像训练阶段，以确保能够处理高分辨率图像。</p><p>当决定如何按比例扩展这些模型的各个组件时，会面临一些挑战。例如，需要决定在编码器和解码器之间分配额外的参数，以及在训练过程中如何分配计算资源。然而，Adept提出的模型避开了这些复杂性。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>从架构上来看，Fuyu是一个简单的、仅包含解码器的Transformer，其规格与Persimmon-8B相同，没有专用的图像编码器。图像块被直接投影到Transformer的第一层，绕过了嵌入查找。这种方法将传统的Transformer解码器视为图像Transformer，尽管没有池化操作。</p><p><img src="/2024/20240226/architecture.png"></p><p>其能够支持任意图像分辨率。为了实现这一点，只需将图像标记序列视为文本标记序列即可。模型删除了特定于图像的位置嵌入，并按光栅扫描顺序输入所需数量的图像标记。为了告诉模型何时截断，我们只需使用一个特殊的图像换行符。该模型可以使用其现有的位置嵌入来推理不同的图像大小，并且我们可以在训练时使用任意大小的图像，从而无需单独的高分辨率和低分辨率训练阶段。</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>VQAv2和OKVQA是自然图像问答数据集，COCO是字幕数据集，AI2D是涉及科学图表的多项选择数据集。</p><table><thead><tr><th>Eval Task</th><th>Fuyu-8B</th><th>Fuyu-Medium</th><th>LLaVA 1.5 (13.5B)</th><th>QWEN-VL (10B)</th><th>PALI-X (55B)</th><th>PALM-e-12B</th><th>PALM-e-562B</th></tr></thead><tbody><tr><td>VQAv2</td><td>74.2</td><td>77.4</td><td>80</td><td>79.5</td><td>86.1</td><td>76.2</td><td>80.0</td></tr><tr><td>OKVQA</td><td>60.6</td><td>63.1</td><td>n&#x2F;a</td><td>58.6</td><td>66.1</td><td>55.5</td><td>66.1</td></tr><tr><td>COCO Captions</td><td>141</td><td>138</td><td>n&#x2F;a</td><td>n&#x2F;a</td><td>149</td><td>135</td><td>138</td></tr><tr><td>AI2D</td><td>64.5</td><td>73.7</td><td>n&#x2F;a</td><td>62.3</td><td>81.2</td><td>n&#x2F;a</td><td>n&#x2F;a</td></tr></tbody></table><h3 id="问答基准缺陷"><a href="#问答基准缺陷" class="headerlink" title="问答基准缺陷"></a><strong>问答基准缺陷</strong></h3><p>问答数据集存在很大缺陷——它们使用复杂的评分机制，要求您以特定格式进行回答，并且通常注释不正确。</p><p>考虑以下两个图像：</p><p><img src="/2024/20240226/defeat.png"></p><p>对于 OKVQA 数据集左侧的图像，当被问到“玩具熊正在演奏什么乐器？”时，模型回答“军鼓”——这显然是正确的！然而，它的得分为 0，因为所有参考答案都只是“鼓”。同样，对于右侧的 VQAv2 图像，当询问“图像中的食物类型是什么？”时，模型正确地回答“鱼，胡萝卜”，但它也得到 0 分，因为参考解决方案列表没有包含这些词。</p><h3 id="文档理解"><a href="#文档理解" class="headerlink" title="文档理解"></a>文档理解</h3><p>Fuyu 还可以理解文档——包括复杂的信息图表和旧的 PDF：</p><p><img src="/2024/20240226/jobs.png"></p><blockquote><p>Question:  “Which is the metro in California that has a good job Outlook?”</p><p>Fuyu’s answer:  “Los Angeles”</p></blockquote><p><img src="/2024/20240226/pack_spinner.png"></p><blockquote><p>Question:   “What was the pack spinner capacity?”</p><p>Fuyu’s answer:  “118 packs.”</p></blockquote><h3 id="图表理解"><a href="#图表理解" class="headerlink" title="图表理解"></a>图表理解</h3><p>最后，该模型可以理解有关科学图表的复杂关系查询：</p><p><img src="/2024/20240226/red_tree_vole.png"></p><blockquote><p>Question:  “If in the food web shown in the diagram, Douglas fir tree needles are absent, which organism would starve?”</p><p>Fuyu’s answer:   “Red tree vole”</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>多模态</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Sora</title>
    <link href="/2024/20240219/"/>
    <url>/2024/20240219/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>openai 发布的人工智能文生视频大模型 (但openai并未单纯将其视为视频模型，而是作为”世界模拟器”</p><span id="more"></span> <p><img src="/2024/20240219/mind.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DLinear-Are Transformers Effective for Time Forecasting</title>
    <link href="/2024/20240214/"/>
    <url>/2024/20240214/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>这篇论文使用一个简单的线性层模型超过了众多Transformer系列复杂模型。不仅让人疑问：时序预测中Transformer的发展是否真的有效？</p><p><a href="https://github.com/cure-lab/LTSF-Linear">源代码</a>。出自AAAI 2023</p><span id="more"></span><h2 id="核心代码"><a href="#核心代码" class="headerlink" title="核心代码"></a>核心代码</h2><p>核心代码比较简单，本质上是先使用卷积学习滑动平均，再减去滑动平均（时间序列去趋势化的惯用操作）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">moving_avg</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Moving average block to highlight the trend of time series</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size, stride</span>):<br>        <span class="hljs-built_in">super</span>(moving_avg, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.kernel_size = kernel_size<br>        <span class="hljs-variable language_">self</span>.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># padding on the both ends of time series</span><br>        front = x[:, <span class="hljs-number">0</span>:<span class="hljs-number">1</span>, :].repeat(<span class="hljs-number">1</span>, (<span class="hljs-variable language_">self</span>.kernel_size - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        end = x[:, -<span class="hljs-number">1</span>:, :].repeat(<span class="hljs-number">1</span>, (<span class="hljs-variable language_">self</span>.kernel_size - <span class="hljs-number">1</span>) // <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        x = torch.cat([front, x, end], dim=<span class="hljs-number">1</span>)<br>        x = <span class="hljs-variable language_">self</span>.avg(x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>))<br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">series_decomp</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Series decomposition block</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size</span>):<br>        <span class="hljs-built_in">super</span>(series_decomp, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.moving_avg = moving_avg(kernel_size, stride=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        moving_mean = <span class="hljs-variable language_">self</span>.moving_avg(x)<br>        res = x - moving_mean<br>        <span class="hljs-keyword">return</span> res, moving_mean<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DLinear</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Decomposition-Linear</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, configs</span>):<br>        <span class="hljs-built_in">super</span>(DLinear, <span class="hljs-variable language_">self</span>).__init__()<br>        <span class="hljs-variable language_">self</span>.seq_len = configs[<span class="hljs-string">&#x27;seq_len&#x27;</span>]<br>        <span class="hljs-variable language_">self</span>.pred_len = configs[<span class="hljs-string">&#x27;pred_len&#x27;</span>]<br><br>        <span class="hljs-comment"># Decompsition Kernel Size</span><br>        kernel_size = <span class="hljs-number">25</span><br>        <span class="hljs-variable language_">self</span>.decompsition = series_decomp(kernel_size)<br>        <span class="hljs-variable language_">self</span>.individual = configs[<span class="hljs-string">&#x27;individual&#x27;</span>]<br>        <span class="hljs-variable language_">self</span>.channels = configs[<span class="hljs-string">&#x27;enc_in&#x27;</span>]<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.individual:<br>            <span class="hljs-variable language_">self</span>.Linear_Seasonal = nn.ModuleList()<br>            <span class="hljs-variable language_">self</span>.Linear_Trend = nn.ModuleList()<br>            <br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.channels):<br>                <span class="hljs-variable language_">self</span>.Linear_Seasonal.append(nn.Linear(<span class="hljs-variable language_">self</span>.seq_len,<span class="hljs-variable language_">self</span>.pred_len))<br>                <span class="hljs-variable language_">self</span>.Linear_Trend.append(nn.Linear(<span class="hljs-variable language_">self</span>.seq_len,<span class="hljs-variable language_">self</span>.pred_len))<br><br>                <span class="hljs-comment"># Use this two lines if you want to visualize the weights</span><br>                <span class="hljs-comment"># self.Linear_Seasonal[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))</span><br>                <span class="hljs-comment"># self.Linear_Trend[i].weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-variable language_">self</span>.Linear_Seasonal = nn.Linear(<span class="hljs-variable language_">self</span>.seq_len,<span class="hljs-variable language_">self</span>.pred_len)<br>            <span class="hljs-variable language_">self</span>.Linear_Trend = nn.Linear(<span class="hljs-variable language_">self</span>.seq_len,<span class="hljs-variable language_">self</span>.pred_len)<br>            <br>            <span class="hljs-comment"># Use this two lines if you want to visualize the weights</span><br>            <span class="hljs-comment"># self.Linear_Seasonal.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))</span><br>            <span class="hljs-comment"># self.Linear_Trend.weight = nn.Parameter((1/self.seq_len)*torch.ones([self.pred_len,self.seq_len]))</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># x: [Batch, Input length, Channel]</span><br>        seasonal_init, trend_init = <span class="hljs-variable language_">self</span>.decompsition(x)<br>        seasonal_init, trend_init = seasonal_init.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>), trend_init.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.individual:<br>            seasonal_output = torch.zeros([seasonal_init.size(<span class="hljs-number">0</span>),seasonal_init.size(<span class="hljs-number">1</span>),<span class="hljs-variable language_">self</span>.pred_len],dtype=seasonal_init.dtype).to(seasonal_init.device)<br>            trend_output = torch.zeros([trend_init.size(<span class="hljs-number">0</span>),trend_init.size(<span class="hljs-number">1</span>),<span class="hljs-variable language_">self</span>.pred_len],dtype=trend_init.dtype).to(trend_init.device)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.channels):<br>                seasonal_output[:,i,:] = <span class="hljs-variable language_">self</span>.Linear_Seasonal[i](seasonal_init[:,i,:])<br>                trend_output[:,i,:] = <span class="hljs-variable language_">self</span>.Linear_Trend[i](trend_init[:,i,:])<br>        <span class="hljs-keyword">else</span>:<br>            seasonal_output = <span class="hljs-variable language_">self</span>.Linear_Seasonal(seasonal_init)<br>            trend_output = <span class="hljs-variable language_">self</span>.Linear_Trend(trend_init)<br><br>        x = seasonal_output + trend_output<br>        <span class="hljs-keyword">return</span> x.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)[:,:,<span class="hljs-number">0</span>] <span class="hljs-comment"># to [Batch, Output length, Channel]</span><br></code></pre></td></tr></table></figure><h2 id="时间序列问题"><a href="#时间序列问题" class="headerlink" title="时间序列问题"></a>时间序列问题</h2><p>对于历史数据L的序列，我们可以往后预测T个未来时间序列长度。</p><p>当T&gt;1时，可以通过迭代单步预测来获得多步预测，也可以直接优化多步预测目标，前者称为iterated multi-step (IMS) forecasting，后者称为direct multi-step (DMS) forecasting。IMS会受到误差累积的影响。</p><h2 id="Transformer的-有效性"><a href="#Transformer的-有效性" class="headerlink" title="Transformer的 有效性"></a>Transformer的 有效性</h2><p>Transformer的主要工作动力来自于它的多头自注意机制，它具有显著的提取长序列元素之间语义相关性的能力(例如，文本中的单词或图像中的二维补丁)。然而，自我注意在一定程度上具有排列不变性和“反序性”。虽然使用各种类型的位置编码技术可以保留一些有序信息，但在其上加上自注意后，仍然不可避免地存在时间信息丢失。对于语义丰富的应用程序，如自然语言中，这通常不是一个严重的问题。例如，即使我们重新排序其中的一些单词，句子的语义也会在很大程度上保留下来。然而，在分析时间序列数据时，通常数值数据本身缺乏语义，我们主要感兴趣的是对连续点之间的时间变化进行建模。也就是说，顺序本身起着最关键的作用。因此，我们提出了以下有趣的问题:变形金刚对长期时间序列预测真的有效吗?</p><h2 id="基于Transformer的长时间序列预测方法"><a href="#基于Transformer的长时间序列预测方法" class="headerlink" title="基于Transformer的长时间序列预测方法"></a>基于Transformer的长时间序列预测方法</h2><p><img src="/2024/20240214/transformer.png"></p><h2 id="DLinear模型"><a href="#DLinear模型" class="headerlink" title="DLinear模型"></a>DLinear模型</h2><p>DLinear模型首先将历史时间序列数据分解为趋势(Trend)数据$X_t\in \mathbb{R}^{L\times C}$ 和剩余(Remainder)数据$X_s&#x3D;X-X_t$两部分，然后对分解得到的两个序列分别应用单层线性网络：</p><p>$$<br>H_s&#x3D;W_sX_s\in\mathbb{R}^{T\times C},W_s\in \mathbb{R}^{T\times L}\<br>H_t&#x3D;W_tX_t\in\mathbb{R}^{T\times C},W_t\in \mathbb{R}^{T\times L}<br>$$</p><p>最终的输出为，两个单层线性网络的输出结果相加：$\hat{X}&#x3D;H_s+H_t$</p><p>另外，如果数据集的变量具有不同的特征，即不同的季节性和趋势，那么在不同变量之间共享权重可能表现不好。所以，文章设计了两种DLinear：</p><p>DLinear-S：每个变量共享相同的线性层；<br>DLinear-I：每个变量拥有独立的线性层。</p><h2 id="模型代码"><a href="#模型代码" class="headerlink" title="模型代码"></a>模型代码</h2><p>占位</p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="数据集设置"><a href="#数据集设置" class="headerlink" title="数据集设置"></a>数据集设置</h3><p>使用的数据集涉及交通、电力、天气、汇率等多个领域，均为多元时间序列。</p><p><img src="/2024/20240214/dataset.png"></p><p>Mean Squared Error (MSE) 和 Mean Absolute Error (MAE)</p><h3 id="对比模型"><a href="#对比模型" class="headerlink" title="对比模型"></a><strong>对比模型</strong></h3><p>六种Transformer-based方法： </p><p>（1）FEDformer（阿里达摩院出品，ICML 2022）</p><p><img src="/2024/20240214/FEDformer1.png"></p><p><img src="/2024/20240214/FEDformer2.png"></p><p>（2）Autoformer（NIPS 2021）</p><p><img src="/2024/20240214/Autoformer1.png"></p><p><img src="/2024/20240214/Autoformer2.png"></p><p>（3）Informer（AAAI 2021 Best Paper）</p><p><img src="/2024/20240214/Informer1.png"></p><p><img src="/2024/20240214/Informer2.png"></p><p>（4）Pyraformer（ICLR 2022）</p><p>（5）LogTrans（NIPS 2019）</p><p>（6）Reformer（ ICLR 2020）</p><p>朴素的DMS方法：Closest Repeat (Repeat-C)。即 repeats the last value in the look-back window</p><p><img src="/2024/20240214/Efficiency.png"></p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/2024/20240214/result.png"><br>最好的结果用粗体突出显示，transformer的最好结果用下划线突出显示。因此，与基于transformers的模型的结果相比，线性模型的结果是最好的。</p><p>现有的基于Transformer的TSF方法对于时间关系提取并不是很有效，DLinear是长期预测任务的强大基线。 FEDformer实现了相对较高的预测精度，可能是因为FEDformer不太依赖Transformer中的自注意力机制，相反，它采用了经典的时间序列分析技术，如傅里叶变换，这在时间特征提取中起着重要作用。</p><h2 id="一些问题"><a href="#一些问题" class="headerlink" title="一些问题"></a>一些问题</h2><p><strong>问题一</strong>：现有的ltsf-transformer能否很好地从较长的输入序列中提取时间关系?</p><p>一般来说，一个强大的TSF模型，具有较强的时间关系提取能力，应该能够在更大的look back窗口尺寸下获得更好的结果。</p><p>现有的基于transformer的模型的性能随着回看窗口大小的增加而恶化或保持稳定。相比之下，所有LTSF-Linear的性能都随着回看窗口大小的增加而显著提高。因此，如果给定较长的序列，现有的解决方案倾向于过拟合时间噪声而不是提取时间信息，并且输入大小为96正好适用于大多数transformer。</p><p><img src="/2024/20240214/bigwindow.png"></p><p><strong>问题二</strong>：我们可以从长期预测中学到什么</p><p>虽然回顾窗口中的时间动态对短期时间序列预测的准确性有显著影响，但我们假设长期预测仅取决于模型是否能够很好地捕捉趋势和周期性。也就是说，预测范围越远，回顾窗口本身的影响就越小。</p><p>为了验证上述假设，在表中，我们比较了来自两个不同回顾窗口的数据对相同未来720时间步的预测精度:(i)原始输入L&#x3D;96设置(称为Close)和(ii)原始96时间步之前的远输入L&#x3D;96设置(称为far)。</p><p><img src="/2024/20240214/whatlong.png"></p><p>从实验结果来看，SOTAtransformer的性能略有下降，表明这些模型仅从相邻的时间序列序列中捕获相似的时间信息。由于捕获数据集的内在特征通常不需要大量的参数，例如。一个参数可以表示周期性。使用太多的参数甚至会导致过拟合，这部分解释了为什么LSTF-linear比基于transformer的方法表现得更好</p><p><strong>问题三</strong>：self-attention 对LTSF是否一直有效?</p><p>我们验证现有Transformer(例如，Informer)中的这些复杂设计是否必要。在表4中，我们逐步将Informer转换为Linear。首先，我们将每个自注意层替换为一个线性层，称为Att .- linear，因为自注意层可以被视为一个权值动态变化的全连接层。<br>此外，我们在Informer中抛弃了其他辅助设计(例如FFN)，留下嵌入层和线性层，称为Embed + linear。最后，我们将模型简化为一个线性层。令人惊讶的是，Informer的性能随着逐渐简化而增长，这表明至少对于现有的LTSF基准测试来说，自关注方案和其他复杂模块是不必要的。</p><p><img src="/2024/20240214/p3.png"></p><p>**问题四：**现有的Ltsf-Trasformer能很好地保持时间顺序吗?</p><p>总所周知自注意力是全局性的，不在乎顺序的。</p><p>然而，在时间序列预测中，序列顺序往往起着至关重要的作用。我们认为，即使使用位置和时间嵌入，现有的基于transformer的方法仍然遭受时间信息丢失。</p><p>在表中，我们在嵌入策略之前对原始输入进行洗牌。提出了两种洗牌策略:洗牌;随机洗牌整个输入序列和Half-Ex（将输入序列的前半部分与后半部分交换。）有趣的是，与Exchange Rate的原始设置(Ori.)相比，即使在随机打乱输入序列时，所有基于transformer的方法的性能也不会波动。相反，LTSF-Linear的性能会受到严重损害。这表明具有不同位置和时间嵌入的ltsf - transformer保留了相当有限的时间关系，并且容易对有噪声的金融数据进行过拟合，而LTSF-Linear可以自然地对顺序进行建模，并避免使用较少的参数进行过拟合。</p><p><img src="/2024/20240214/ex.png"></p><p>**问题五：**不同的embedding策略效果如何?</p><p><img src="/2024/20240214/embedding.png"></p><p>在表中，没有位置嵌入(wo&#x2F;Pos.)， Informer的预测误差大大增加。没有时间戳嵌入(wo&#x2F;Temp.)会随着预测长度的增加而逐渐损害Informer的性能。由于Informer对每个令牌使用单个时间步长，因此有必要在令牌中引入时态信息。</p><p>FEDformer和Autoformer不是在每个令牌中使用单个时间步长，而是输入一系列时间戳来嵌入时间信息。因此，它们可以在没有固定位置嵌入的情况下获得相当甚至更好的性能。然而，如果没有时间戳嵌入，由于全局时间信息的丢失，自耦器的性能会迅速下降。相反，由于FEDformer中提出的频率增强模块引入了时间感应偏置，因此移除任何位置&#x2F;时间戳嵌入的影响较小。</p><p>**问题六：**训练数据大小是现有ltsftransformer的限制因素吗?</p><p>完整数据集 (17,544*0.7 hours), 命名为Ori., </p><p>短一些的数据集(8,760 hours, i.e., 1 year)</p><p><img src="/2024/20240214/p6.png"></p><p>我们可以看出数据集变小甚至还使一部分性能变好了。</p><h2 id="相关评价"><a href="#相关评价" class="headerlink" title="相关评价"></a>相关评价</h2><p><strong>知乎用户brightnova</strong>：我在自己的数据集上亲测，这篇论文的DLinear方法效果远远不如informer算法效果。</p><p><strong>知乎用户蛋壳儿</strong>回复以上：算法和数据 是息息相关的，他这个算法 在周期性强的数据集上有很好的效果，和Autoformer一样.<br>Informer关注突变的关键数据，仔细看看注意力机制就知道了。</p><p><strong>知乎用户stupidboys</strong>：我们的论文发现了在时间序列上正确使用transformer的方式，效果超越DLinear和其他formers，欢迎指正：A Time Series is Worth 64 Words: Long-term Forecasting with Transformers</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>时间序列</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Depth Anything-Unleashing the Power of Large-Scale Unlabeled Data</title>
    <link href="/2024/20240211/"/>
    <url>/2024/20240211/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Depth Anything是一种鲁棒的单目深度估计解决方案，其充分利用各种未标记的图像和预训练模型中丰富的语义先验，具有优异的零样本深度估计能力。</p><p>由香港大学、浙江大学等人提出。</p><p><a href="https://huggingface.co/spaces/Xenova/depth-anything-web?continueFlag=f04486fe34faf2ce849faa271c27a9d2">网页版Demo</a></p><hr><span id="more"></span><p>Depth Anything是在1.5M标记图像和62M未标记图像上联合训练的。</p><p><img src="/2024/20240211/pipeline.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>计算机视觉</tag>
      
      <tag>深度估计</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>周耀辉解析《春秋》</title>
    <link href="/2024/20240206/"/>
    <url>/2024/20240206/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><strong>（以下内容被证实是伪作）</strong></p><span id="more"></span><p>全文转载自林日曦对周耀辉的记载。</p><hr><p>一首歌之所以能扣人心弦，有三个不可或缺的因素：动人的旋律、能引起共鸣的歌词、演绎者恰到好处的感情投入。这样一个“想爱却无法爱”的常见题材，被词、曲、唱三者通力演绎，创造出了全新的境界。 张敬轩对这首歌的演绎无可挑剔，感情到位、唱功完美，大气、磅礴、悲壮、浓烈。</p><p>那夜，酒吧，我和你。我们是良朋，是知己，可我与你的关系却始终只能停留在友情的节点之上。面对这段感情，我的常态是压抑和克制，进亦难，退亦难，时常沉沦在美好的假设和亲近的幻觉中。 最传神的是一个“谁”字。常人落笔，惯用“我将酒喝掉”，又或是“你将酒喝掉”，于是便沦为一个常见的借酒表白的故事。而“谁将酒喝掉”，究竟是谁，不得而知，可能是我，可能是你，也可能是大家都醉了。</p><p>“谁”不用“我”，是淡化自己的主动性和重要性，创造一个与自己没什麽干系的、吐露心声的条件。一个“谁”字，看似无意著笔，忐忑中那种小心翼翼的潜流暗涌却纤毫毕现。借著昏黄光晕下温润的醉意，我才敢掩耳盗铃般说出自己心意，潜意识中以为大家都醉了，说的都是醉话，听者无心，说者或许也可当作无意，仿佛就能在现实生活中不留一丝痕迹。 </p><p>一个“多”字也用得意味深长，歌词中最常见的表达是“那天我喝多了酒，不小心讲出我爱你”，“我讲得多了”，一个“多”字，可能是我倾诉了所有肺腑之言，也可能是我的言语超越了朋友的界限。 你凝视我的双眼，柔情婉转地对我说——我们还是做朋友吧。随后摇著我的手，安慰著我，这个细节更能形象地表现出那种委婉、遗憾和难堪的场面。我没有等到想要的答案，却得到了一段仿佛更深刻、更坚定的友谊，多麽讽刺。 如果只想写一首备胎歌曲，有这四行就够了，可能把“得不到”三个字说出三千种样式的夕爷，开篇亮出这四句叙事词只是为后面情感高潮的推进作出铺垫。</p><p>与我一起，或许是为了排遣寂寞，寻求有裙下之臣拥戴的安全感和满足感。回到朋友的位置，仿佛那夜从未发生过，你如往常一样和我谈天说地，偶尔给我接近暧昧的暗示，它们如同甜蜜的毒药，也许这就是我的命运，我没办法推开，一厢情愿妄想能得到你的慰藉。 仍忍不住继续关心你，心中再颓丧也要装作开怀的模样和你开著一个个令人心碎的玩笑。我仿佛已成为你手中的扯线木偶，木偶怎可能反过来操控人呢？即便无限接近，也只能是接近，哪怕我们无限接近谈情的暧昧，也只能是终生知己而已。</p><p>与我一起，或许是为了排遣寂寞，寻求有裙下之臣拥戴的安全感和满足感。回到朋友的位置，仿佛那夜从未发生过，你如往常一样和我谈天说地，偶尔给我接近暧昧的暗示，它们如同甜蜜的毒药，也许这就是我的命运，我没办法推开，一厢情愿妄想能得到你的慰藉。 仍忍不住继续关心你，心中再颓丧也要装作开怀的模样和你开著一个个令人心碎的玩笑。我仿佛已成为你手中的扯线木偶，木偶怎可能反过来操控人呢？即便无限接近，也只能是接近，哪怕我们无限接近谈情的暧昧，也只能是终生知己而已。 快乐是相似的理由，难过却有一千种借口。神如果真的善良，就不会只给我们一个继续做朋友的借口。可这又能怨谁呢，是怨你怨己、怨天怨地，还是指责上帝只安排了相遇与相知，却没有安排相爱的结局？难道真要去责怪虚无缥缈的神，连假装善心的祝福都不愿施舍吗？ 不怪上天没有给我命中的你，只怪自己没有运气得到你。</p><p>“我没有为你伤春悲秋不配有憾事，你没有共我踏过万里不够剧情延续故事”，曾听过许多超越生死的爱情故事，经曆了生离死别，感受了痛彻心扉，才化成一段又一段能够记载在书籍里的文字。或许你早就拥有了最绚烂的故事，而我却无缘参与其中。情未深，缘未至，没有为你伤春悲秋，怎会洞悉你的所有心事；没有与你踏遍千里，又怎能在彼此的世界留下足迹；没有为你等到白头，怎会有厚重的回忆可以延续我们的故事……伤春悲秋、踏过万里，从时间与空间上来说，我们都缺乏擦出爱火的心灵碰撞，和灵犀相通的生命契机。没为你真正付出过，也没与你真正经历过，我不够资格拥有憾事，也不够经历来写进故事。我的“伤春悲秋”，更多是自己内心的纠缠和幻想，一切只是我自以为是的感天动地、撕心裂肺。我开始不断否定自己曾经的付出，如果什么都没有发生过，又怎有资格要求结局？该用什么方式来表达对感情的执著呢，或许应该像艺术作品中那样，思念成疾，因情愁一夜白头，可是我并没有。我太年轻，还不能成熟地去面对痛苦，多么希望如天使般无忧，拥有超脱世俗的快乐，这属于弱者的怅然失意，不过是独角戏中的自作自受、无病呻吟。我没有资格伤心，更没有立场怨恨，我们之间不曾有过荡气回肠、刻骨铭心的故事，一直以来都是我自己一厢情愿，要怪只能怪自己太过天真。</p><p>已经过了因为失意而寻求同情与怜悯的年纪，也再无顔面继续摆出弱者的姿态，博取你的同情、内疚和安慰、关怀，更没办法真的就此伤心落发远离凡世。每个默默淌泪的深夜过后，仍然扮作一切如常，日日与你微笑面对。心灰意冷都自觉可耻，反複悲伤显得毫无新意，这份悲恸快要无力承受。男儿有泪不轻弹，我的眼泪有无奈、有悲哀，有自弃，却没有一丝骨气，我的情殇不过是不成熟和欠大志罢了。</p><p>从前的人生仿佛一张白纸，一心想和她共同书写未来，可从未谈情，何来爱恨？渴望被她改写一生，却只写下无措的开始。 如果轰轰烈烈爱过，便可以宣之于口、公诸于世；如果凄凄惨惨被伤，也可以控诉你的绝情，光明正大地流泪，理直气壮地痛心。这种痛快淋漓的快乐或者悲痛，便是“出师有名”那个“名”，有名有份、有理有据，最悲哀的是无名无分的暗恋，所有的千回百转、心伤神透，都是自己的事。</p><p>所拥有的，只是一个人的伤春悲秋，一个人的惊涛骇浪。 从未拥有过你，我与你的情感牵系，连一件实证都找不到。我们的故事这么短，这么浅，怎能去媲美《春秋》的宏大。这样单薄、飘渺的记忆，于青书史册也好，于你的人生也罢，都是那样轻，轻到不值一提。 人生的最后如果要写一部回忆录的话，有多少值得珍藏的文字，初恋的憧憬，新婚的期待，暗涌的情愫，你能记下的，一定是轰轰烈烈，刻骨铭心的情事，而不是像这样催眠式的自我感动。这轻飘飘两页虚无缥缈的经历，对你而言不过是随时可撕毁的废纸。</p><p>按照正常朋友见面的频率，每天见一面，上万面需要多久？大约是30年。或许只有这种“不似凡人”的事迹，才够写入《春秋》之中，真正的痛苦，还需要数万次的积累，经曆无数的聚散离合才能够成就。 为一个人花光一生运气，直至两鬓斑白的执著铭记，永远躺在尘埃里扮弱者玩失意……</p><p>悲伤并不值得歌颂，一生那么长，就算为她悲伤了几个春秋，依有余生的时光要好好走下去。重庆森林里的那个深情的男人，终也换掉了毛巾，换掉了拖鞋，换掉了鱼罐头的口味，失恋很痛，却不会是永久的伤。认可了这个结局，便懂得了彻悟后道理，也是“春秋”最终的立意。</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Mamba---Linear-Time Sequence Modeling with Selective State Spaces</title>
    <link href="/2024/20240131/"/>
    <url>/2024/20240131/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文两位作者Albert Gu和Tri Dao，博士都毕业于斯坦福大学，导师为Christopher Ré。</p><p>Albert Gu现在是CMU助理教授，多年来一直推动SSM架构发展。他曾在DeepMind 工作，目前是Cartesia AI的联合创始人及首席科学家。</p><p>Tri Dao，以FlashAttention、FlashDecoding系列工作闻名，现在是普林斯顿助理教授，和Together AI首席科学家，也在Cartesia AI担任顾问。</p><p>Code：<a href="https://link.zhihu.com/?target=https://github.com/state-spaces/mamba">github</a></p><hr><span id="more"></span><p>SSM指的是结构化状态空间序列模型（Structured state space sequence models，S4）</p><p>S4模型可由四个参数 $(\Delta,A,B,C)$ 定义，他们分两个阶段定义序列到序列的转换。</p><p><img src="/2024/20240131/S4.png"></p><p>离散化有</p><p><img src="/2024/20240131/ZOH.png"></p><hr><p>离散化证明：<br>$$<br>\dot{x}(t)&#x3D;Ax(t)+Bu(t) \<br>y(t)&#x3D;Cx(t)+Du(t)<br>$$</p><p>为了方便，我们对$e^{-At}x(t)$进行积分，我们得到：<br>$$<br>x(t)&#x3D;e^{At}x(0)+\int_0^te^{A(t-\tau)}Bu(\tau)d\tau<br>$$<br>我们需要对上述进行离散化。</p><p><img src="/2024/20240131/discret.png"></p><hr><p>在参数从 (∆,A,B,C) ↦ (A,B,C) 转化之后，模型可以通过两种方式计算，一种是线性递归 (式2) （使用RNN），另一种是全局卷积 (3) （使用CNN）。</p><p> 通常在训练时使用卷积模式 (式3) （提前看到整个输入序列），推理时使用递归模式 (式2) （每次看到一个时间步的输入）。</p><h3 id="线性时不变性（Linear-Time-Invariance，LTI）"><a href="#线性时不变性（Linear-Time-Invariance，LTI）" class="headerlink" title="线性时不变性（Linear Time Invariance，LTI）"></a>线性时不变性（Linear Time Invariance，LTI）</h3><p>  S4 的状态模型参数 (Δ,A,B,C)  在<strong>所有时间步</strong>中都是<strong>固定不变</strong>的，这一特性被称为线性时不变性。LTI 是递归和卷积的基石，为构建序列模型提供了一个简化但功能强大的框架。</p><p> 迄今为止，所有结构化 SSM 都是 LTI 模型，因为存在基本的效率限制。然而，本文工作的一个核心观点是，<strong>LTI 模型在对某些类型的数据进行建模时具有根本性的局限性</strong>，本文的技术贡献在于<strong>消除 LTI 限制，同时克服效率瓶颈</strong>。</p><h3 id="一些可以被认为是SSM的结构"><a href="#一些可以被认为是SSM的结构" class="headerlink" title="一些可以被认为是SSM的结构"></a>一些可以被认为是SSM的结构</h3><ul><li><p><strong>Linear attention</strong>: 线性注意力，它是自注意力的近似，涉及一个递归，可以看作是一个退化的线性 SSM。</p></li><li><p><strong>H3</strong>: 它在S4的基础上进行了扩展；可以被看作是一种由两个门控连接夹着一个 SSM 的架构（如下图）。H3还在主SSM层之前插入了一个标准的局部卷积，将这部分定义为一个shift-SSM。</p></li></ul><p><img src="/2024/20240131/H3.jpg"></p><ul><li><p><strong>Hyena</strong>: 使用与 H3 相同的架构，但用 MLP 参数化全局卷积取代了 S4 层。</p></li><li><p><strong>RetNet</strong>: 在架构中增加了一个额外的门，并使用更简单的 SSM，允许另一种可并行计算的路径，使用多头注意力（MHA）的变体来代替卷积。</p></li><li><p><strong>RWKV</strong>: 是最近基于另一种线性注意近似（attention-free Transformer）设计的用于语言建模的 RNN。它的主要 “WKV “机制涉及 LTI 递归，可视为两个 SSM 的比值。</p></li></ul><p>其他的方法还有S5、QRNN、SRU等。</p><h3 id="S6的提出"><a href="#S6的提出" class="headerlink" title="S6的提出"></a>S6的提出</h3><p>S4是线性时间不变（LTI）模型，具有局限性。从递归模型的角度来看，它们恒定的动态（例如 $\bar{A},\bar{B}$ 不能使它们从上下文中选择正确的信息，或以输入依赖的方式影响沿序列传递的隐藏状态。从卷积模型的角度来看，虽然全局卷积可以解决标准复制任务，但在处理需要内容意识的选择性复制任务时则存在困难，因为输入到输出之间的间距是变化的，无法用静态卷积核建模。</p><p> 序列模型的效率与有效性之间的权衡由它们压缩状态的能力决定：高效的模型必须有一个小的状态，而有效的模型必须包含所有必要的上下文信息。作者提出，构建序列模型的一个基本原则是<strong>选择性</strong>，或者说具有上下文意识的能力，<strong>专注于或过滤掉输入到序列状态的信息</strong>。特别是，选择机制控制信息如何在序列维度上传播或相互作用。（Mamba就像是每次参考前面所有内容的一个概括，越往后写对前面内容概括得越狠，丢掉细节、保留大意）</p><p>于是有了以下改动：</p><p><img src="/2024/20240131/S6.png"></p><p>其中</p><p><img src="/2024/20240131/LTI.png"></p><p>模型从时不变变成了<strong>时可变</strong>。</p><p>由于失去了LTI的性质，不能像之前的S4一样通过FFT来训练了。本文提出了<strong>IO-aware</strong>的parallel scan（一种memory bounded算子）算法来进行高效训练，<strong>降低整体的读写量</strong>从而提高wall-time efficiency。上面提到的outer product的参数化方式也对降低整体读写量很有帮助（大致思路是 (A¯,B¯)(\bar{A}, \bar{B})(\bar{A}, \bar{B}) 在SRAM里面on-the-fly算出来，避免<a href="https://www.zhihu.com/search?q=materialization&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22article%22,%22sourceId%22:%22661237120%22%7D">materialization</a>带来的读写开销）</p><h3 id="简化的-SSM-架构"><a href="#简化的-SSM-架构" class="headerlink" title="简化的 SSM 架构"></a>简化的 SSM 架构</h3><p><img src="/2024/20240131/model.png"></p><p>本文的简化区块设计结合了 H3 区块（大多数 SSM 架构的基础）和现代神经网络中无处不在的 MLP 区块。这只是简单地重复 Mamba 模块，而不是交错使用这两个模块。与 H3 模块相比，Mamba 用激活函数取代了第一个乘法门。与 MLP 模块相比，Mamba 在主分支上增加了一个 SSM。对于 $\sigma$ ，使用 SiLU &#x2F; Swish 激活。</p><p>Mamba架构通过一个可控的扩展因子 E 来扩大模型维度 D。在每个块中，大部分参数（3ED^2）用于线性投影（2ED^2用于输入投影，ED^2用于输出投影）。与线性投影相比，SSM的参数（Δ, B, C 的投影和矩阵 A）数量要少得多。通过重复这个块，并与标准的归一化和残差连接交错，构成了Mamba架构。</p><p> 在实验中，扩展因子 E 总是固定为2，使用两层堆叠的块来匹配Transformer的交错多头注意力（Multi-Head Attention, MHA）和MLP块的 12D^2 参数。采用SiLU&#x2F;Swish激活函数，使得Gated MLP成为流行的“SwiGLU”变体。最后，受到RetNet在类似位置使用归一化层的启发，还使用了一个可选的归一化层（选择了LayerNorm）。</p><h3 id="参数的影响"><a href="#参数的影响" class="headerlink" title="参数的影响"></a>参数的影响</h3><ol><li><strong>参数Δ的作用</strong>： 它<strong>控制着模型对当前输入 x_t 的关注程度，以及应该保留多少历史状态信息</strong>。调节 Δ 的大小，可以模拟不同的系统行为，从完全关注当前输入到完全保留历史状态，实现对输入的选择性关注。Δ 在SSMs中起着类似于RNN门控信号的作用，如在定理1中提到的 g_t，但在SSMs的框架下提供了一种更一般化的形式。 当 Δ 很大时，模型会重置状态 h，这相当于让模型更多地关注当前的输入 x，而非之前的状态。当 Δ 很小时，模型保持现有状态 h 的持久性，对当前的输入 x 给予较少的关注，从而忽略它。 SSMs 可以被看作是一个连续系统通过时间步长 Δ 离散化后的结果。在这个离散化的连续系统中，大的 Δ（趋向于无穷）意味着系统在较长时间内专注于当前输入，相当于“选择”了当前输入并忘记了当前状态。相反，小的 Δ（趋向于零）意味着当前输入是短暂的，可以被忽略。</li><li><strong>参数A的作用</strong>：  虽然理论上 A 也可以具备选择性，但 A 对模型的主要影响是通过它与 Δ 的相互作用来实现的（通过离散化公式 A&#x3D;exp(ΔA) ）。作者认为，只要 Δ 具有选择性，就可以保证整个模型的选择性，而且 Δ 是提高模型性能的关键。为了保持模型的简洁性，作者选择不让 A 参数具备选择性。</li><li><strong>参数 B 和 C 的作用</strong>：  选择性最重要的特性是<strong>过滤掉无关信息</strong>，从而将序列模型的上下文<strong>压缩</strong>成有效的状态。在 SSM 中，修改 B 和 C 使其具有选择性，可以使模型能够更精细地控制输入和状态的流动。</li></ol><ul><li>B 控制着输入 x_t 是否被引入到状态 h_t  中，即选择性地决定哪些输入对状态的更新至关重要。</li><li>C 影响着状态 h_t 如何转化为输出 y_t，即如何基于当前的状态信息生成最终的输出。</li></ul><p>这意味着，SSM可以基于当前处理的数据内容以及已经编码在隐藏状态中的上下文信息，来动态地调整其内部状态的更新方式和输出。这增加了模型处理序列数据时的灵活性和有效性，因为它可以根据数据的具体特征和任务需求来优化信息流。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/2024/20240131/SL2048.png"></p><p><img src="/2024/20240131/SL8192.png"></p><p>其中Transformer++指的是带有Rope和SwiGLU的版本（i.e., LLaMa用的）。</p><p>结果中也有国人研发的号称取代transformer的RWKV。</p><p>而这些模型只能匹敌普通的transformer，只有mamba才能与transformer++匹敌。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>On Embeddings for Numerical Features in Tabular Deep Learning</title>
    <link href="/2024/20240116/"/>
    <url>/2024/20240116/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该论文使用表格深度学习的一些新embedding。<a href="https://arxiv.org/pdf/2203.05556.pdf">论文地址</a></p><p>出自NIPS 2022</p><p>Hardy Xu利用该方法获得了<a href="https://www.kaggle.com/competitions/playground-series-s3e26/discussion/464887">kaggle Playground Series - Season 3, Episode 26</a>: Multi-Class Prediction of Cirrhosis Outcomes的第二名。</p><hr><p>本文提出了两种不同的构建块，适用于构建数字特征的嵌入。第一种是分段线性编码，它为原始标量值产生替代的初始表示，并基于特征装箱。第二种依赖于周期激活函数。</p><span id="more"></span><h2 id="Piecewise-linear-encoding"><a href="#Piecewise-linear-encoding" class="headerlink" title="Piecewise linear encoding"></a>Piecewise linear encoding</h2><p><img src="/2024/20240116/model.png"></p><p>bin的选择：</p><p>（1）根据百分数</p><p>（2）类似于的决策树C4.5中的处理。</p><h2 id="Periodic-activation-functions"><a href="#Periodic-activation-functions" class="headerlink" title="Periodic activation functions"></a>Periodic activation functions</h2><p><img src="/2024/20240116/pcf.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>表格学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Self-Supervision is All You Need for Solving Rubik’s Cube</title>
    <link href="/2024/20240113/"/>
    <url>/2024/20240113/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该论文使用NN来解魔方，耳目一新的方法。<a href="https://arxiv.org/pdf/2106.03157.pdf">论文地址</a></p><p>TMLR 2023.</p><hr><p>简单来说，该方法利用了组合搜索的一个基本特性：等概率的情况下路径越短，随机发生的可能性就越大。这意味着随机训练争夺的累积概率随着移动次数的减少而增加：$1&#x2F;\mathbb{M}^N$，其中$\mathbb{M}$表示移动集，N表示路径长度。</p><span id="more"></span><p><img src="/2024/20240113/model.png"></p><p>训练一个DNN来获得达到某一目标状态的概率图，也就是获得最后一步，也就是倒着来。</p><p>训练DNN时，我们的方法用目标状态初始化目标问题，并应用一系列随机移动对其进行加扰。在每一步，DNN都会根据当前状态的模式来预测最后一次应用的移动。作为训练损失，我们计算最后一步的实际概率分布和预测概率分布之间的分类交叉熵。算法1概述了训练过程，上图展示了魔方上的一个示例数据点。</p><p><img src="/2024/20240113/1.png"></p><p>我们通过顺序反转DNN预测的移动来搜索追溯到目标状态的解决方案路径。我们采用最佳优先搜索算法，并对最有希望的候选路径进行优先排序，我们根据其所有组成移动的概率的累积乘积对其进行评估。累积乘积可以表示为$\prod_{i&#x3D;1} \hat p_i$，其中$\prod$表示第$i$次朝向目标的反向移动的预测概率。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LLaMA2</title>
    <link href="/2023/20230719/"/>
    <url>/2023/20230719/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><span id="more"></span><table><thead><tr><th></th><th>是否改动</th><th>LLaMA</th><th>LLaMA2</th></tr></thead><tbody><tr><td>模型整体构架</td><td>无</td><td>transformer</td><td>transformer</td></tr><tr><td>规范化函数</td><td>无</td><td>RMSNorm</td><td>RMSNorm</td></tr><tr><td>位置编码</td><td>无</td><td>RoPE</td><td>RoPE</td></tr><tr><td>激活函数</td><td>无</td><td>SiLU</td><td>SiLU</td></tr><tr><td>注意力机制</td><td>有</td><td>多头注意力机制</td><td>分组查询多头注意力机制</td></tr><tr><td>前馈函数</td><td>无</td><td>逐元素前馈函数</td><td>逐元素前馈函数</td></tr><tr><td>连接</td><td>无</td><td>残差连接</td><td>残差连接</td></tr><tr><td>掩码</td><td>无</td><td>因果掩码</td><td>因果掩码</td></tr><tr><td>推理</td><td>有</td><td>自回归推理</td><td>自回归推理</td></tr></tbody></table><p>RoPE由苏神提出，通过绝对位置编码的方式实现相对位置编码，是一种可用于线性Attention的相对位置编码。</p><p>SiLU：$y&#x3D;x*sigmoid(x)$</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>世另我（三）</title>
    <link href="/2023/me3/"/>
    <url>/2023/me3/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="密码错误.提示：无." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容">  <script id="hbeData" type="hbeData" data-hmacdigest="017af70fc8865fcd22518bf1cc32a93f04ac0892ba01099552f929ad8f977bb0">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">请输入密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习科研的十年——陈天奇</title>
    <link href="/2023/20230711/"/>
    <url>/2023/20230711/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>陈天奇是机器学习领域著名的青年华人学者之一，本科毕业于上海交通大学ACM班，博士毕业于华盛顿大学计算机系，研究方向为大规模机器学习。TVM、MXNET、XGBoost 作者，CMU 助理教授，OctoML CTO。在gpt的潮流下，领导的团队提出了MLC LLM ，为在各类硬件上原生部署任意大型语言模型提供了解决方案，可将大模型应用于移动端（例如 iPhone）、消费级电脑端（例如 Mac）和 Web 浏览器。</p><p>该文写于2019年。</p><hr><p>十年前，MSRA的夏天，刚开始尝试机器学习研究的我面对科研巨大的不确定性，感到最多的是困惑和迷茫。十年之后，即将跨出下一步的时候，未来依然是如此不确定，但是期待又更多了一些。这其中的变化也带着这十年经历的影子。</p><span id="more"></span><h2 id="起始：-科研是什么"><a href="#起始：-科研是什么" class="headerlink" title="起始： 科研是什么"></a>起始： 科研是什么</h2><p>我从大三开始进入交大APEX实验室，有幸随着戴文渊学长做机器学习，当时的我觉得“机器学习”这个名字十分高大上然后选择了这个方向，但是做了一年之后依然摸不着头脑，心中十分向往可以做科研，独立写论文的生活，却总是不知道如何下手。文渊在我进实验室的一年后去了百度。当时还没有得到学长真传的我，开始了我科研的第一阶段，从大四到硕士的第二年，期间一直自己摸索，不断地问自己 “科研是什么”。</p><p>和课程作业不同，学术研究没有具体的问题，具体的方法，具体的答案。文渊的离开让我一下子不知道该怎么做，当时的我的想法很简单，快点寻找一个具体的方向，完成一篇论文。因为ACM班的机会暑假在MSRA的短暂实习，虽然学会了很多东西，但并没有给我答案。MSRA回来之后，在实验室薛老师的建议下，我选择了一个现在看来正确而又错误的方向 – 深度学习。那是AlexNet出现之前两年，深度学习的主流热点是非监督学习和限制玻尔兹曼机。没有导师的指导，没有工具，当时我靠着实验室的两块显卡和自己写的CUDA代码开始了死磕深度学习的两年半。实验室的学长问我，你准备要干啥，我说：“我要用卷积RBM去提升ImageNet的分类效率。” 这一个回答开启了图书馆和实验室的无数个日日夜夜，为了给实验室的老机器多带一块高功率的显卡，我们打开了一台机器的机箱，在外面多塞了一个外接电源。我的生活就持续在调参的循环中：可视化权重的图片, 看上去那么有点像人脸，但是精度却总是提不上来，再来一遍。从一开始hack显卡代码的兴奋，到一年之后的焦虑，再到时不时在树下踱步想如何加旋转不变的模型的尝试，在这个方向上，我花费了本科四年级到硕士一年半的所有时间，直到最后还是一无所获。现在看来，当时的我犯了一个非常明显的错误 – **常见的科学研究要么是问题驱动，比如“如何解决ImageNet分类问题”；要么是方法驱动，如 “RBM可以用来干什么”。**当时的我同时锁死了要解决的问题和用来解决问题的方案，成功的可能性自然不高。如果我在多看一看当时整个领域的各种思路，比如Lecun在很早的时候就已经做end to end，或许结局会不那么一样吧。</p><p>当然没有如果，赌上了两年半的时间的我留下的只是何时能够发表论文的紧张心情。焦虑的我开始打算换一个方向，因为RBM当时有一个比较经典的文章应用在了推荐系统上，我开始接触推荐系统和kddcup。比较幸运的是，这一次我并没有把RBM作为唯一的一个方法，而是更加广泛地去看了推荐系统中的矩阵分解类的算法，并且在实验室搭建了一个比较泛用的矩阵分解系统。推荐系统方向的耕耘逐渐有了收获，我们在两年KDDCup11中获得了不错的成绩。KDD12在北京，放弃了一个过年的时间，我完成了第一篇关于基于特征的分布式矩阵分解论文，并且非常兴奋地投到了KDD。四月底的时候，我们收到了KDD的提前拒搞通知 – 论文连第一轮评审都没有过。收到拒搞通知时候的我的心情无比沮丧，因为这是第一篇自己大部分独立推动完成的文章。转折在五月，KDDCup12 封榜，我们拿到了第一个track的冠军，我依然还记得拿到KDDCup12冠军的那一个瞬间，我在状态里面中二地打了excalibur，仿佛硕士期间的所有阴霾一扫而尽。那时候的我依然还不完全知道科研是什么，但是隐隐之中觉得似乎可以继续试试。</p><h2 id="第零年：-可以做什么"><a href="#第零年：-可以做什么" class="headerlink" title="第零年： 可以做什么"></a>第零年： 可以做什么</h2><p>我对于科研看法的第一个转折，在于我硕士临近毕业的时候。李航老师来到我们实验室给了关于机器学习和信息检索的报告，并且和我们座谈。在报告的过程中，我异常兴奋，甚至时不时地想要跳起来，因为发现我似乎已经知道如何可以解决这么多有趣问题的方法，但是之前却从来没有想过自己可以做这些问题。联系了李航老师之后，在同一年的夏天，我有幸到香港跟随李航和杨强老师实习。实验室的不少学长们曾经去香港和杨强老师工作，他们回来之后都仿佛开了光似地在科研上面突飞猛进。去香港之后，我开始明白其中的原因 – 研究视野。经过几年的磨练，那时候的我或许已经知道如何去解决一个已有的问题，但是却缺乏其他一些必要的技能 – <strong>如何选择一个新颖的研究问题，如何在结果不尽人意的时候转变方向寻找新的突破点，如何知道整个领域的问题之间的关系等等。</strong>“你香港回来以后升级了嘛。” – 来自某大侠的评论。这也许是对于我三个月香港实习的最好概括的吧。香港实习结束的时候我收获了第一篇正式的一作会议论文(在当年的ICML)。</p><blockquote><p>第一篇就是ICML….</p></blockquote><p>因为KDDCup的缘故，我认识了我现在博士导师Carlos的postdoc Danny，Danny把我推荐给了Carlos(UW)和Alex(CMU)。我在申请的时候幸运地拿到了UW和CMU的offer。在CMU visit的时候我见到了传说中的大神学长李沐，他和我感叹，现在正是大数据大火的时候，但是等到我们毕业的时候，不知道时代会是如何，不过又反过来说总可以去做更重要的东西。现在想起这段对话依然依然唏嘘不已。我最后选择了UW开始了我六年的博士生活。</p><p>感谢博士之前在APEX实验室和香港的经历，在博士开始的时候我似乎已经不再担心自己可以做什么了。</p><h2 id="第一年：-意外可以收获什么"><a href="#第一年：-意外可以收获什么" class="headerlink" title="第一年： 意外可以收获什么"></a>第一年： 意外可以收获什么</h2><p>如果给我在UW的第一年一个主题的话，或许是“意外”。在交大时候因为兴趣的关系一直去蹭系统生物研究员敖平老师的组会探讨随机过程和马尔可夫链。到UW的第一个学期，我无意看到一篇探讨如何用Lagevin过程做采样的文章，我想这不就是之前组会上探讨过的东西么，原来这些方法也可以用到机器学习上。我直接借用了原来的交大学会的知识完成了第一篇高效采样HMC的文章。我后来并没有继续在这个方向上面耕耘下去，不过另外一位同在组会的学弟继续基于这个方向完成了他的博士论文。</p><p>同样的在这一年，我和导师开始“质疑深度学习” – 如果别的的机器学习模型，有足够大的模型容量和数据，是否可以获得和深度学习一样的效果呢？当时Carlos看好kernel methods，而我因为过去的一些经历决定尝试Tree Boosting。虽然最后在vision领域依然被卷积网络打败而尝试挑战失败，但是为了挑战这一假说而实现高效Tree boosting的系统经过小伙伴建议开源成为了后来的XGBoost。</p><p>在第一年暑假结束的时候，因为偶然的原因，我开始对quantile sketch算法感兴趣。这里主要的问题是如何设计一个近似的可以合并的数据结构用来查找quantile。这个方向有一个经典的方案GK-sketch的论文，但是只能够解决数据点没有权重的情况。经过一两天的推导，我在一次去爬山的路上终于把结论推广到了有权重的情况。有趣的是新的证明比起原来的证明看起来简单很多。这个结论没有单独发表，但是后来意想不到地被用到了分布式XGBoost算法中，证明也收录在了XGboost文章的附录中。</p><p>研究并不是一朝一夕，做想做的事情把它做好，开始的时候兴趣使然，而在几年之后意想不到的地方获得的收获，这样的感觉走非常不错。</p><h2 id="第二年和第三年：-选择做什么"><a href="#第二年和第三年：-选择做什么" class="headerlink" title="第二年和第三年： 选择做什么"></a>第二年和第三年： 选择做什么</h2><p>在新生聚会上，Carlos对我说，你已经有论文的发表经历了，接下来要静下心来做发大的，“只做best paper水平的研究”。和很多nice的导师不同，Carlos对于学生的要求非常严格，说话也是非常直白甚至于“尖刻“。很多的老师不论我们提出什么样的想法，总会先肯定一番，而Carlos则会非常直接地提出质疑。一开始的时候会非常不习惯，感觉到信心受到了打击，但是慢慢习惯之后开始习惯这样风格。到<strong>现在看来，诚实的反馈的确是我收益最大的东西。我进入博士的一年之后，主要在想的问题是做什么样的问题，可以值得自己深入付出，做扎实有影响力的工作。</strong></p><p>在博士的第三年，Carlos在建议我把XGBoost写成论文，用他的话说：“写一篇让读者可以学到东西的文章”。和传统的写法不同，我们在文章的每一个章节插入了实验结果验证当章节提出的观点。而他对于做图的处理也成为了我现在的习惯，直接在图里面插入箭头注释，减少读者的阅读负担。经过几次打磨论文终于成为了我们想要的模样。</p><p>博士前对于深度学习遗憾让我又逐渐把目光转回到深度学习。这个时候，我选择了不再一个人作战，在博士的第二年和第三年，我和兴趣使然的小伙伴们合作，一起开始了MXNet的项目。</p><blockquote><p>哈哈MXNet还有沐神</p></blockquote><p>项目从零开始，在短短的一年时间里面做出完整的架构。我第一次看到集合了大家的力量齐心协力可以创造出什么样的东西。研究的乐趣不光是发表论文，更多还是可以给别人带来什么，或者更加大胆地说 – 如何一起改变世界。</p><p>博士第二年暑假，我在小伙伴的介绍下进入Google Brain跟随Ian Goodfellow实习。当时GAN的论文刚刚发表，我也有幸在成为Ian的第一个实习生。</p><blockquote><p>Ian Goodfellow….这就是大神之路吗…..</p></blockquote><p>实习的开始，我们讨论需要做的问题，Ian和我把可能要做的项目画在一个风险和回报的曲线上，让我选择。到最后我选择了自己提出的一个课题，在这个曲线里面风险最高，回报也最高。我一直有一个理想，希望可以构建一个终身学习的机器学习系统，并且解决其中可能出现的问题。这个理想过于模糊，但是我们想办法拿出其中的一个可能小的目标 – 知识迁移。如果一个机器学习系统要终生学习，那么在不断收集数据之后必然需要扩充模型的规模来学习更广或者更深，按照现在的做法我们在模型改变之后只能抛弃原来的模型重新训练，这显然是不够高效的。是否有一个方法可以从已经训练好的网络上面进行知识迁移也就成为了一个重要的问题。我先花了一个半月的时间尝试了比较显然的Knowledge distillation的方法一直没有得到正面的结果。在最后的一个月，我改变了思路。实习结束的前一个星期，我打开Tensorborard上最近一组实验的结果：实验表明新的思路正面的效果。这最后几步的幸运也让我的这一个冒险之旅有了一个相对圆满的结果。这篇论文最后被发表在了ICLR上，也是我最喜欢的结果之一。</p><p>博士的第三年，我和小伙伴们开发了一种可以用低于线性复杂度就可以训练更深模型的内存优化算法。当时我非常兴奋地把这一结果写下来然后把稿子后给导师看。他和我说：Hmm,这个结果如果投到NeurIPS的话或许可以中一篇poster，但是这并不是特别有意思。在我沉默之后他又补充了一句：论文并非越多越好，相反你可能要尝试优化你的论文里面最低质量的那一篇。最后我们只是把这篇论文挂在了Arxiv上。Carlos的说法或许比较极端（这篇论文依然影响了不少后面的工作），但也的确是对的，用李沐之前说过的一句话概括，<strong>保证每一篇论文的质量接近单调提升，已经是一件难以做到但是又值得最求的事情。</strong></p><p>**选择做什么眼光和做出好结果的能力一样重要，眼界决定了工作影响力的上界，能力决定了到底是否到达那个上界。**交大时敖平老师曾经和我说过，一个人做一件简单的事情和困难的事情其实是要花费一样多的时间。因为即使再简单的问题也有很多琐碎的地方。要想拿到一些东西，就必然意味着要放弃一些其他东西，既然如此，<strong>为什么不一直选择跳出舒适区，选一个最让自己兴奋的问题呢。</strong></p><blockquote><p>我其实很喜欢跳出自己的舒适区，选一个能让自己兴奋的问题，但总是失败….</p></blockquote><h2 id="第四年之后：-坚持做什么"><a href="#第四年之后：-坚持做什么" class="headerlink" title="第四年之后： 坚持做什么"></a>第四年之后： 坚持做什么</h2><p>博士第三年，我和小伙伴们参加GTC，结束后老黄party的角落里，我一个人在发呆。深度学习的框架发展已经铺开，可接下来应该做什么，我一下子感到迷茫。第三年的暑假我没有去实习，而是决定一个人在学校尝试开发脑海中显现的抽象概念 – 深度学习中间表示。暑假结束之后，我完成了第一个版本，可以比较灵活地支持深度学习系统里面的计算图内存优化。但是总是觉得还缺少着什么 – 系统的瓶颈依然在更接近底层的算子实现上。暑假之后在去加州的飞机上，我尝试在纸上画出为了优化矩阵乘法可能的循环变换，回来之后，我们决定推动一个更加大胆的项目 – 尝试用自动编译生成的方式优化机器学习的底层代码。</p><p>这个项目早在之前我也有一些想法，但是一直没有敢去吃这个螃蟹。原因是它的两个特点：从零开始，横跨多领域。因为要做底层代码生成和想要支持新的硬件，我们需要重新重新搞清楚很多在之前被现有的操作系统和驱动隐藏掉的问题，这就好象是在一个荒岛上一无所有重新搭建起一个城堡一样。而这里面也涉及了系统，程序语言，体系结构和机器学习等领域。这让我想起之前在ACM班时候重头搭建编译器和MIPS处理器并且连接起来的经历。也是那段经历让我觉得为了解决问题去吃多个领域的螃蟹是个让人兴奋的事情。那段经历给我留下的第二个印记是理解了合作和传承的重要性。这门课程设计有一个传统，每一门课程的老师都由上一届学长担任。每一届的同学都会在之前的基础上有所改进。我也曾经为这门课做过一些微小的贡献。演化到现在，这门课程已经从只做简单的答辩，到现在已经有在线评测的OJ。大家一起的合作塑造了这个课程。推动新的机器学习系统和塑造这门课程一行，需要各个团队的同学合作，足够时间的耐心关注和不断地改进。</p><p>我的合作者们也被“卷入”到了这个项目中。我的体系结构合作者一直想要设计新的AI硬件，我在雏形完成之后花了大量的时间讨论如何协同设计新的硬件的问题。我们开始讨论怎么管理片上内存，怎么可以比较容易地生成指令集，甚至怎么调度内存读写和计算并行的问题都暴露出来。有一天，我和合作者说我们需要引入虚拟线程的概念来隐藏内存读写开销，然后他很快和我说，这是体系结构里面经典的超线程技术，发明人正是我们的系主任Hank。我们也在不断地重新发现经典的问题的解决方法在新场景的应用，让我觉得上了一堂最好的体系结构课程。</p><p>两年间的不少关键技术问题的突破都是在有趣的时候发生的。我在排队参观西雅图艺术博物馆的infinity mirror展览的途中把加速器内存拷贝支持的第一个方案写在了一张星巴克的餐巾纸上。到后来是程序语言方向的同学们也继续参与进来。我们争论最多的是如何如何平衡函数式语言和经典计算图做让大家都可以搞懂的中间表达，这一讨论还在不断继续。经过大家的努力，TVM的第一篇论文在项目开始的两年之后终于发表。两年间参与项目的同学也从两个人，到一个团队，再到一个新的lab和一个社区，这两年也是我博士期间最充实的两年。</p><p>因为做了不少“跨界”的工作，我常被问起你到底属于哪个领域。过去半年一直在各地给报告，报告这样开头：算法突破，数据的爆发，计算硬件的提升三者支撑了机器学习的变革，而整合这三者的，则是机器学习系统。这也是为什么我要做机器学习系统的原因。曾经一个教授问我这样的问题，如果明天有一样新的化学反应过程可能带来机器学习的变革，你会怎么做。我答道：“我投入会去学习研究这个化学过程”。虽然我不知道遥远的未来会需要什么，到底是系统，算法，还是化学，从问题出发，用尽所有可能的方法去最好地解决机器学习问题，应该这就是我想要坚持的研究风格吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在写这篇总结的时候，心中有不少感叹。我常想，如果我在焦虑死磕深度学习的时候我多开窍一些会发生什么，如果我并没有在实习结束的时候完成当时的实验，又会是什么。但现在看来，<strong>很多困难和无助都是随机的涨落的一部分，付出足够多的时间和耐心，随机过程总会收敛到和付出相对的稳态。</strong></p><p>每个人的研究道路都各不相同，我的经历应该也是千万条道路中其中一条罢了。<strong>博士的经历就好像是用五年多时间作为筹码投资给自己，去突破自己做自己原来想不到的事情。中不管坎坷曲折都是无可替代的一部分。</strong></p><p>科研从来不是一个人的事情，对于我来说特别是如此。我在交大的时候和一群年轻的同学一起摸索推荐系统的算法，而在博士期间搭建的每一个系统都包含了很多合作者一起的努力。也正是大家一起的努力才带来了现在的成果。我个人在这十年间受到了不少老师，同学，家人的鼓励和帮助，感谢他们他们给予了我这无比珍贵的十年时光。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>华为预测天气大模型</title>
    <link href="/2023/20230708/"/>
    <url>/2023/20230708/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://www.nature.com/articles/s41586-023-06185-3">华为发表于nature的《Accurate medium-range global weather forecasting with 3D neural networks》</a>。</p><hr><p>现阶段，AI 气象预报模型精度不足主要有两个原因：</p><ul><li>第一，现有的 AI 气象预报模型都是基于 2D 神经网络，无法很好地处理不均匀的 3D 气象数据。</li><li>第二，AI 方法缺少数学物理机理约束，因此在迭代过程中会不断积累迭代误差。</li></ul><p>为了解决上述问题，来自华为云的研究人员提出了一种新的高分辨率全球 AI 气象预报系统：盘古气象（Pangu-Weather）大模型。</p><span id="more"></span><hr><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>天气预报对于科学和社会都很重要。目前，最准确的预报系统是数值天气预报（NWP）方法，该方法将大气状态表示为离散网格，并数值求解描述这些状态之间转变的偏微分方程<a href="https://www.nature.com/articles/s41586-023-06185-3#ref-CR1">1</a>。然而，这个过程的计算成本很高。最近，基于人工智能的方法<a href="https://www.nature.com/articles/s41586-023-06185-3#ref-CR2">2</a>已经显示出将天气预报加速几个数量级的潜力，但预报精度仍然明显低于 NWP 方法。</p><p>作者证明，配备地球特定先验的三维深度网络可以有效处理天气数据中的复杂模式，并且分层时间聚合策略可以减少中期预测中的累积误差。经过 39 年的全球数据训练，盘古天气程序与世界上最好的 NWP 系统（欧洲中期天气中心的业务综合预报系统）相比，对所有测试变量的再分析数据获得了更强的确定性预报结果预测 (ECMWF)<a href="https://www.nature.com/articles/s41586-023-06185-3#ref-CR3">3</a> . 方法也适用于极端天气预报和集合预报。当用再分析数据初始化时，跟踪热带气旋的精度也高于ECMWF-HRES。</p><h2 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h2><p>首先，将高度信息整合到一个新的维度中，以便深度神经网络的输入和输出可以在三个维度上概念化。</p><p>作者进一步设计了一个三维（3D）地球特定变压器（3DEST）架构，将地球特定先验注入深层网络。实验表明，3D 模型通过将高度表达为单独的维度，能够捕获不同压力水平下大气状态之间的关系，从而产生显着的精度增益 </p><p>其次，作者应用了分层时间聚合算法，该算法涉及训练一系列具有增加的预测提前期的模型。因此，在测试阶段，中期天气预报所需的迭代次数大大减少，累积预报误差也得到缓解。第五代ECMWF再分析（ERA5）数据的实验验证了盘古天气在确定性预报和极端天气预报方面的优势，同时速度比业务IFS快10000倍以上。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2023/20230708/model.png"></p><h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>（1）盘古天气是根据再分析数据进行训练和测试的，但现实世界的预报系统是在观测数据上工作的。这些数据来源之间存在差异</p><p>（2）其次，本文没有研究降水等一些天气变量。忽略这些因素可能会导致当前模型缺乏一些能力，例如利用降水数据来准确预测小规模极端天气事件，例如龙卷风爆发。</p><p>（3）基于人工智能的方法产生更平滑的预测结果，增加了低估极端天气事件严重程度的风险。我们研究了一个特殊情况，即气旋跟踪，但还有很多工作要做。</p><p>（4）使用不同提前期的模型可能会引入时间不一致。这是一个具有挑战性的话题，值得进一步研究。</p><h2 id="哈哈哈"><a href="#哈哈哈" class="headerlink" title="哈哈哈"></a>哈哈哈</h2><p>论文多次提到了”要是能拥有更强大的集群和更大的GPU内存，就能更进一步“的意思，大家都缺卡吗，哈哈哈。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一些博士生对超大型语言模型时代NLP研究的看法</title>
    <link href="/2023/20230703/"/>
    <url>/2023/20230703/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://arxiv.org/abs/2305.12544">《A PhD Student’s Perspective on Research in NLP in the Era of Very Large Language Models》</a></p><p>通过汇集来自不同背景的博士生的意见，探索NLP领域丰富的研究方向，避免将研究仅仅局限于大型语言模型。提出了十四个研究领域，每个领域包含了2-4个具体的研究方向，涉及多语言性、推理、知识库、语言基础、计算社会学等主题。</p><p>作者全部来自于密歇根大学LIT组，也拥有着不同的国家背景，譬如中国、美国、日本、墨西哥等。</p><p>LIT(Language and Information Technologies)是密歇根大学的一个研究小组，致力于自然语言处理、信息检索和应用机器学习方面的研究项目。<br>语言与信息技术研究小组于2002年在北德克萨斯大学成立，旨在促进自然语言处理、信息检索和应用机器学习方面的研究和教育。<br>该小组于2013年搬到了密歇根大学。</p><span id="more"></span><hr><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>大型语言模型的最新进展使许多生成式NLP应用程序得以部署。<br>与此同时，它也导致了一种误导性的公众话语，即“一切都解决了”。<br>毫不奇怪，这反过来又使许多NLP研究人员——尤其是那些刚开始从事NLP研究的人——想知道他们应该关注哪些NLP研究领域。<br>本文件是一个丰富的NLP研究方向的汇编，具有丰富的探索空间，反映了一个学术研究实验室中不同群体的博士生的观点。<br>虽然我们确定了许多研究领域，但还有许多其他领域存在;<br>我们不涵盖那些目前由LLM解决的领域，但LLM在性能上落后的领域，或者那些专注于LLM开发的领域。</p><h2 id="1、-背景"><a href="#1、-背景" class="headerlink" title="1、 背景"></a>1、 背景</h2><p>当前LLM的输出质量可与人类的表现相媲美，并具有集成来自大量数据源的信息的额外好处，远远超过个人一生所能积累的信息。<br>受益于LLM的应用程序数量正在不断增长，在许多情况下，llm被用来取代整个复杂的管道。<br>LLM变得“有利可图”，导致行业兴趣和资金激增，有关LLM的研究论文数量也大幅增加。<br>例如，在Google Scholar上搜索“语言模型”，可以找到过去5年里发表的5万篇论文，占过去25年里发表的大约15万篇论文的三分之一。</p><p>虽然LLM的这些进步是非常真实和令人兴奋的，并且给许多新部署的生成语言应用程序带来了希望，但llm也“吸走了房间里的空气”。DARPA最近的一次融资呼吁已经完全用LLM取代了NLP这个术语:在他们为该项目寻找的专家名单中，我们看到“计算机视觉”和“机器学习”领域与“大型语言模型”并列(但没有“自然语言处理”)。用llm代替NLP，主要有两个原因。</p><p>第一，语言的空间洞见、方法和广泛应用。第二，即使技术上不新颖，LLM仍然是一个排他性的领域，因为训练所需的数据量和计算量。</p><p>现实情况是，NLP不仅仅是LLM。本文档是博士生的想法汇编，基于他们最初的专业知识和现有的兴趣，并围绕以下问题进行头脑风暴:“在NLP领域有哪些丰富的探索领域可以导致博士论文，并涵盖法学硕士范围之外的空间。”</p><p>剧透警告:有很多这样的研究领域!</p><p>在编写本文件的想法时，我们遵循了三个主要指导原则。</p><p>首先，我们的目标是确定研究的领域丰富，适合探索的领域;例如，可以写博士论文的领域。</p><p>其次，我们希望突出那些不直接依赖付费资源的研究方向;虽然使用现有的付费api可以在某些任务中取得成果，例如构建合成数据集，但构建没有付费api就无法运行的系统并不符合学术核心研究目标。</p><p>最后，第三，我们针对的研究方向是可以找到合理计算成本的解决方案，这些解决方案可以在学术实验室中更典型地实现。</p><h2 id="2、Multilinguality-and-Low-Resource-Languages"><a href="#2、Multilinguality-and-Low-Resource-Languages" class="headerlink" title="2、Multilinguality and Low-Resource Languages"></a>2、Multilinguality and Low-Resource Languages</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>多语言模型被设计用于处理多种语言，无论是用于机器翻译（MT）任务还是其他任务。一个主要挑战是处理资源不足的语言，因为这些语言的培训数据有限，这可能导致翻译质量差，在这些语言上表现不佳。研究界提出了几种技术来克服这一挑战，例如数据扩充，包括通过反翻译生成合成数据（Sennrich等人，2015；Edunov等人，2018）、并行语料库挖掘（Artetxe和Schwenk，2018）或OCR（Rijhwani等人，2020；Ignat等人，2022）；以及多语言模型，这是一种预先训练的模型，可以处理多种语言，并可以对低资源语言进行微调，以提高翻译质量。最近为低资源语言开发多语言模型的努力包括NLLB-200（NLLB Team et al.，2022），这是一种经过训练的最先进的专家混合（MoE）模型。</p><h3 id="缺陷。"><a href="#缺陷。" class="headerlink" title="缺陷。"></a><strong>缺陷。</strong></h3><p>现有的MT模型，如NLLB-200（NLLB Team et al.，2022），在许多低资源语言（如非洲语言）上仍然表现不佳。例如，最近的工作测试了ChatGPT MT在低资源语言（例如马拉地语、巽他语和布吉尼语）上的性能，发现总体性能较差，尤其是在非拉丁语脚本中（Bang et al.，2023b）。他们还发现，ChatGPT在低资源语言到英语的翻译方面可以表现得相当好，但不能进行英语到低资源语言的翻译。此外，世界上大约7000种语言中的绝大多数都不存在机器翻译系统。</p><h3 id="研究方向"><a href="#研究方向" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.在当前低资源和极低资源的语言基准上提高MT性能。在FLORES-200等现有基准上，仍有很大的改进空间。这一基准最近激发了人们对为低资源语言（如非洲语言）创建其他基准的兴趣（V egi等人，2022；Reid等人，2021）。资源极低的语言没有显著的网络存在，因此没有足够的比特文本来训练MT系统。这些语言可能有《圣经》的译本（世界上翻译最多的文件），这可以作为开发MT系统的起点（McCarthy等人，2020；Mueller等人，2020）。最近也有人对手动创建平行语料库感兴趣，例如Amis语言（Zheng et al.，2022）和Minankabau语言（Koto和Koto，2020），但这一过程既昂贵又耗时。在缺乏双语甚至单语培训语料库的情况下</p><p>2.适用于所有语言的多语言模型。尽管最近的LLM声称是多语言的，但在预测、分类或生成等任务中，它们在所有语言中的表现并不一样好。一些模型部分基于网络文本进行训练，如Common Crawl（Smith et al.，2013），其中主要包含英语文本。开放的问题包括需要多少数据以及语言的组合才能在多种语言上实现类似的性能。此外，跨语言投影仍然是其他语言模型的潜在数据来源，通过利用主要语言中的可用数据以及现有的机器翻译系统，将模型架构转移到其他语言上</p><p>3.代码切换。语码转换（CS）是一种说话人在遵守至少一种语言的语法结构的同时在语言之间交替的现象。CS数据对NLP任务提出了一系列独特的挑战。CS的性质导致说话者创建“新”单词，这意味着为容纳CS数据而设计的模型必须对词汇表外的标记具有鲁棒性（Çetino˘glu et al.，2016）。训练数据很难获得，也使得学习CS特定模型变得困难。一个活跃的研究领域是确定LLM可以在多大程度上生成合成CS数据；以前的方法通常使用平行语料库来替换具有语法规则作为约束的标记（Xu和Yvon，2021；Lee和Li，2020）。其他研究领域包括探索模型在多大程度上可以在不同的语言组合中推广，以及可以有效区分高度相似语言的学习模型，例如同一母语的方言</p><h2 id="3-Reasoning"><a href="#3-Reasoning" class="headerlink" title="3.Reasoning"></a>3.Reasoning</h2><h3 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h3><p>推理是人类智力的一个基本方面，通过使用逻辑原理和认知过程从前提、事实和知识中进行推理，在解决问题或决策中发挥着关键作用。推理类型多种多样，包括演绎推理、归纳推理、溯因推理、定量推理、因果推理和道德推理。提高NLP中的推理技能对于问答、阅读理解和对话系统等任务至关重要，因为它可以增强模型在看不见的场景中的泛化能力。NLP研究已经发生了重大变化，从早期的基于规则和符号的方法发展到20世纪90年代的统计方法，其中利用了概率模型和机器学习算法。近年来，深度学习和神经网络已经彻底改变了该领域，在各种任务上实现了最先进的性能。然而，在获得类似人类的推理和概括能力、推动持续的研究方面仍然存在挑战</p><h3 id="缺陷"><a href="#缺陷" class="headerlink" title="缺陷"></a>缺陷</h3><p>尽管LLM在许多推理基准上表现出了令人印象深刻的性能（Brown等人，2020b；欧阳等人，2022；张等人，2022年；Touvron等人，2023a；OpenAI，2023），但仍有几个方向仍然具有挑战性。他们很难稳健地管理形式推理（Jin等人，2022b；Stolfo等人，2023；Jin et al.，2023a），因为我们经常看到LLM容易出现形式或符号系统不会犯的错误。此外，由于他们的大多数训练都与文本世界交互，NLP模型在推理时仍然缺乏现实世界经验的基础（Ignat等人，2021）。最后，更基本的问题还有待回答，比如区分经验知识和理性推理，以及揭示LLM是如何推理的。</p><h3 id="研究方向-1"><a href="#研究方向-1" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.稳健的形式推理（Robust formal reasoning）。长期以来，形式推理一直是神经网络的一项具有挑战性的任务。LLM远未完全掌握数字推理（Stolfo等人，2023；Miao等人，2020）、逻辑推理（Jin等人，2022b）和因果推断（Jin et al.，2023a，c）等形式任务，经常会犯明显的错误（Goel等人，2021；Jin et al，2020）。为此，一个健壮的模型应该知道如何泛化。为了稳健地管理形式推理，可以探索多种方向，例如结合神经网络和符号人工智能的优势。一项流行的工作是集成外部推理系统，如计算器、python解释器、数据库或搜索引擎中的知识检索（Schick et al.，2023；Mialon等人，2023）。</p><p>2.物理现实世界中的有根据的推理。虽然目前的模型产生了连贯的、与背景相关的反应，但它们往往缺乏对物理世界及其限制的理解。这可能导致语言上看似合理的反应，而这些反应在实践中是荒谬或不切实际的。为了解决这个问题，一个方向是探索如何结合外部知识源、多模式数据或模拟世界场景，以奠定模型的推理技能。</p><p>3.在社会背景下进行负责任的推理。随着越来越多的应用程序使用NLP模型，可以预见的是，模型将需要做出复杂的决策，其中包括道德推理作为中间步骤。例如，在创建网站时，可能需要考虑一些道德选择，例如迎合某些子群体，或者过度优化用户注意力或点击率。这些决策原则普遍存在于我们的日常生活中，涉及大小任务。我们认为，在不同的社会背景和文化背景下，在理解或提高人工智能系统对社会复杂和道德复杂的场景进行推理的能力方面，还有很多需要研究的地方（Jin et al.，2023b；Hendrycks et al.，2021）。我们预计，将需要与领域专家和政策制定者进行跨学科合作。</p><p>4.正式定义推理，设计合适的评价框架。人们越来越需要完善推理的定义，因为LLM开始使知识和推理之间的区别变得模糊——当一个模型记住一个推理模式时，它算是对推理或知识的掌握吗？模型已经开始通过模式匹配显示出对模板化解决方案的日益精通，这似乎是许多人想要的推理。从根本上说，这引出了一个问题，即人类擅长哪些智慧闪光点，以及这些闪光点与凭经验学习如何进行模板匹配有何不同。除了重新定义推理之外，另一个悬而未决的问题是如何测试模型的推理技能。我们面临着诸如数据污染、古德哈特定律（一个数据集一旦被利用就无法反映技能）以及缺乏评估多步骤推理的可靠指标等问题。</p><p>5.分析提示如何帮助推理。有两种类型的提示对LLM的影响值得检验：情境学习和思维链。最近的工作表明，对上下文中的例子进行调节与微调模型具有类似的效果（Akyürek et al.，2022），研究人员开始解码模型开始从给定上下文中提取的机制，例如感应头（Olsson et al.，2021）。除了上下文中的指令外，我们还可以使用思维链提示通过中间步骤提示LLM。这种方法将推理任务分解为更小的子问题，类似于人类解决问题。然而，语言模型是真的推理还是只是生成统计上相似的序列，以及人工智能系统可以在多大程度上从少数样本中学习推理，这是有争议的。</p><h2 id="4-Knowledge-Bases"><a href="#4-Knowledge-Bases" class="headerlink" title="4. Knowledge Bases"></a>4. Knowledge Bases</h2><h3 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h3><p>知识库是关于真实世界对象、抽象概念或事件的事实的集合。知识库中的知识通常表示为三元组，由头部实体、尾部实体及其关系组成。例如（巴拉克·奥巴马，出生地，檀香山）就是一个三胞胎表示出生地关系的例子。一些知识库更多地关注事实知识，如DBPedia（Auer et al.，2007）和Y AGO（Suchanek et al.，2017），而另一些知识库则更多地关注常识，如ConceptNet（Speer等人，2017）和ASER（Zhang et al.，2020）。知识库已被用于许多下游应用，包括关系提取（Weston et al.，2013）、机器阅读（Yang和Mitchell，2017）和咨询对话中的反思生成（Shen et al.，2022）。许多人发现，整合外部知识可以提高此类知识密集型任务的绩效（Yu et al.，2022）。此外，知识库通常是在关系和实体的明确本体中构建的，这使得人类能够更容易地解释基于知识库的推断。</p><h3 id="缺陷-1"><a href="#缺陷-1" class="headerlink" title="缺陷"></a>缺陷</h3><p>尽管LLM是在广泛的数据集上训练的，并证明了其处理各种任务的能力（Brown等人，2020a；Bubeck等人，2023a），但其内部知识在许多方面仍然有限，既有一般知识，也有特定领域的知识（Ofek等人，2016）或特定文化的知识（Yin等人，2022）。此外，LLM经常产生幻觉，产生基于虚假事实的索赔。尽管从人类反馈中强化学习（RLHF）可以缓解这种现象，但幻觉的问题是该模型固有的。将模型的输出建立在明确的知识库上可能会减少幻觉，并使用户能够更容易地验证断言的正确性。它也为用大量现有作品进行逻辑推理开辟了可能性。</p><h3 id="研究方向-2"><a href="#研究方向-2" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.以知识为导向的LLM。将知识整合到LLM中是解决幻觉问题的一个很有前途的研究方向，方法是将模型的响应建立在经过验证的知识资源上。ChatGPT试图通过插件来解决这一问题，这表明LLM本身不会解决这个问题，而是取决于单个用例。有人试图通过DialogGPT等系统检索或生成用于增强响应生成的知识（Zhang et al.，2019）。必应（Bing）等搜索引擎在撰写回复之前也会对事实问题进行网络查询。然而，LLM应该如何最有效地与定制的外部知识库进行交互仍然是一个悬而未决的问题。</p><blockquote><p>很好奇GPT联网功能是如何实现的，是不是也是一种knowledge bases的方法呢？</p></blockquote><p>2..知识库的自动构建。许多应用程序都可以受益于专门的知识库，无论是为了提高人类的可解释性，还是作为一个独立的资源。自动构建此类知识库是一个有趣的方向，需要解决许多挑战，如知识覆盖率、知识的真实性、知识链接等。当为医疗保健或化学等专业领域构建知识库时，这些挑战会被放大。然而，一旦这些问题得到解决，研究人员将能够利用LLM从最新的原始文本和本体中动态地策划知识库，用于复杂的应用，例如跟踪PubMed文章中的药物交互。</p><p>3.一般和文化常识。NLP模型中可用的文化知识通常仅限于少数西方文化，并不能解释世界文化观的巨大多样性（Arora等人，2023）。随着NLP应用程序的日益广泛，这种限制可能会对这些应用程序的用户产生直接的不利影响，因为不考虑他们的价值观、信仰和世界观。需要做更多的工作来理解包括LLM在内的NLP模型在了解不同文化群体方面的局限性。此外，一旦更好地理解了这些局限性，一个主要的开放研究方向就是如何获得和表达编码这些文化观点的知识，以及如何以及何时调用这些文化知识。</p><h2 id="5-语言基础（Language-Grounding）"><a href="#5-语言基础（Language-Grounding）" class="headerlink" title="5. 语言基础（Language Grounding）"></a>5. 语言基础（Language Grounding）</h2><h3 id="背景-3"><a href="#背景-3" class="headerlink" title="背景"></a>背景</h3><p>语言基础是指在非语言世界中将语言表达与其指称联系起来的能力（Patel和Pavlick，2022）。非语言世界可以是物理的也可以是非物理的，例如，TextWorld（Côtéet al.，2018）。重大的研究进展是由于利用感官数据来构建数据集和任务，以教授ML模型如何进行语言基础。流行的任务包括视觉问答（Agrawal et al.，2015；Singh et al.，2019）、图像和视频字幕（Mokady et al.，2021；周等人，2019），文本到图像检索（Wang et al.，2022；方等人，2021）和文本到图像&#x2F;视频生成（Ramesh et al.，2020；Villegas等人，2022）。像CLIP（Radford et al.，2021）这样的模型证明，大规模的图像文本预训练可以有利于基于转换器的视觉语言模型。顺应这一趋势，更多的多模态模型，如GPT-4，显著增加了它们的训练语料库（OpenAI，2023），并添加了新的模态，如音频（Zellers等人，2022）。</p><h3 id="缺陷-2"><a href="#缺陷-2" class="headerlink" title="缺陷"></a>缺陷</h3><p>尽管GPT-4等最新的多模态模型表现出令人印象深刻的零样本性能，因为它们优于大多数微调但较小的多模态模式，但它们也有成本。首先，他们缺乏对世界的真实了解（Hendricks和Nematzadeh，2021；Thrush等人，2022），他们缺乏领域知识，无法概括到现实生活中的环境中（例如，野外数据中的个性化情况）。其次，这些模型很难解释，甚至无法解释。他们在生成新数据时偶尔会表现出不可靠的行为，如幻觉（例如，图像&#x2F;视频生成、图像&#x2F;视频字幕）。最后，只有少数大学和机构能够负担得起适当使用这些资源的费用。GPU的成本不断上升，使用不同的模式，尤其是视觉模式，在计算机内存和计算方面都要昂贵得多。</p><h3 id="研究方向-3"><a href="#研究方向-3" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.如何最好地组合多种模式。高效和有效地组合不同的模式，即音频、视频、文本和其他模式，仍然是一个悬而未决的问题。不同的模式往往是相辅相成的（例如，手势可以用来表达对口头表达的内容的信心），从而减少了对数十亿个数据点的依赖。然而，在某些情况下，模态最终会相互竞争，因此许多单模态模型的性能优于多模态模型（Wang et al.，2019a；Huang等人，2021）。</p><p>2.以较少研究的模式为基础。大多数关于基础的工作都围绕着视觉、文本或音频模式展开。然而，在接地背景下研究较少的模式，如生理、感觉或行为，在测量驾驶员警觉性（Jie et al.，2018；Riani et al.，2020）、检测抑郁（Bilalpur et al.，2023）或检测欺骗行为（Abouelenien et al.，2016）等不同应用中都很有价值。这些模式在整个管道中提出了有趣的问题，从数据收集和表示开始，一直到评估和部署。</p><p>3.立足于“野外”和不同领域。大多数关于接地的研究都是在实验室环境中收集的数据上进行的，或者是在电影（Lei et al.，2019）或烹饪（Zhou et al.，2018）等室内活动的图像和视频上进行的。对更真实的环境和户外“野外”数据的研究要少得多（Castro等人，2022）。这些数据在可用性、质量、分布等方面提出了新的挑战，开辟了新的研究方向。此外，将这些模型应用于不同的领域（例如，机器人、医学、导航、教育、可访问性）需要适应使用更少的数据点或不同类型的数据，同时需要领域内的专业知识来更好地理解问题设置。</p><h2 id="6-Computational-Social-Science"><a href="#6-Computational-Social-Science" class="headerlink" title="6. Computational Social Science"></a>6. Computational Social Science</h2><h3 id="背景-4"><a href="#背景-4" class="headerlink" title="背景"></a>背景</h3><p>计算社会科学（CSS），即使用计算方法研究社会科学，至少在一定程度上没有受到LLM的影响。虽然它们可以自动化一些与CSS相关的语言任务，如情绪分析和立场检测（Liang et al.，2022），但诸如“人类如何在社交网络中分享新闻”或“灾难性社会事件中语言使用的文化差异”等问题被认为在很大程度上超出了生成模型的范围。在过去的十年里，随着人工智能在社会科学中的成功和影响，计算和数据驱动方法已经渗透到社会科学的主要领域（Lazer et al.，20092020），产生了新的跨学科领域，如计算通信研究、计算经济学和计算政治学。</p><h3 id="缺陷-3"><a href="#缺陷-3" class="headerlink" title="缺陷"></a>缺陷</h3><p>虽然NLP继续对CSS的成型研究产生巨大影响，但大型基础模型在假设和评估该领域的想法方面没有得到充分利用。生成模型旨在通过自然语言端到端地为用户提供服务，并且由于高昂的微调成本或专有技术，通常无法满足定制这些大型模型的需求。在缺乏专家或微调LLM的情况下，此类模型在CSS中的应用仍然局限于通用数据标记和处理，如立场检测或情绪分析。</p><h3 id="研究方向-4"><a href="#研究方向-4" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.总体级别的数据注释和标记。CSS研究人员已经在人类互动的大型数据集上应用了不太完美的模型，以帮助他们缩小社会概念的范围并对其进行研究。虽然一些注释可以由LLM处理（Gilardi等人，2023），但对人类众包工作者的需求不太可能消失。在CSS中尤其如此，因为研究人员最感兴趣的是人口层面的趋势，而不是个人层面的准确性。</p><p>2.开发有助于抽象、概念和方法的新CSS。近年来，单词和句子级别的嵌入对CSS产生了很大的影响。在引入嵌入之前，主题建模，如LDA（Blei et al.，2003）和关键字提取在CSS中已经很普遍。这些是在CSS中以高抽象级别封装通用功能的方法的示例，因为它们经常用于CSS的几个子领域的研究。随着CSS研究人员转向使用更强大的人工智能技术，为他们解锁新功能的概念和算法尚待开发。</p><p>3.多元文化和多语言CSS。大多数CSS研究集中在英语或少数其他主要语言上，主要涉及西方文化。然而，社会科学中有许多重要问题需要进行大规模、多语言和多文化的分析。例如，语言是如何演变的，或者不同文化的价值观是如何变化的？这是一个未来工作的领域，可能会对社会科学产生复合影响。</p><h2 id="7-NLP-on-online-environments"><a href="#7-NLP-on-online-environments" class="headerlink" title="7.NLP  on online environments"></a>7.NLP  on online environments</h2><h3 id="背景-5"><a href="#背景-5" class="headerlink" title="背景"></a>背景</h3><p>NLP对在线环境的影响可以通过两种对抗性现象来观察：内容生成和节制。内容的快速生成，如LLM生成的文章和社交媒体更新，可以得到各种利益相关者的支持。许多人很可能通过生成假新闻和虚假信息来实现网站的高点击率，这引发了需要及时监管的社会问题。相反，节制是一种把关的形式。通过使用NLP来监控和分析数字平台上用户生成的内容（Nakov等人，2021；Kazemi等人，2021a），以删除违反政策的材料，内容审核可以保持在线生态系统中的平衡（Thorne等人，2018；Nakov et al.，2021；Gillespie，2020；Kazemiet al.，2021a；Shaar等人，2020）。</p><h3 id="缺陷-4"><a href="#缺陷-4" class="headerlink" title="缺陷"></a>缺陷</h3><p>关于内容生成和审核有几个问题。对于生成，确定生成的基本目的并避免恶意操纵用户是当务之急。对于节制，一个令人担忧的问题是，目前的节制模型仍然不透明、不精确、不负责任，而且人们对其了解甚少（Gorwa等人，2020）。此外，在构建检测不期望内容的模型方面存在一些现有的挑战，包括为不期望内容设计分类法的困难、数据标记的耗时性，以及学术数据集在揭示真实世界数据分布方面的不足（Markov et al.，2023）。此外，NLP辅助的事实核查通常是用英语构建的，因此越来越需要低资源和跨语言的NLP来帮助解决世界上资源不足地区的错误信息。检测和揭穿错误信息还涉及多模式处理，因为错误信息以各种形式传播。网络信号，如谁喜欢或转发内容，也对丰富的信息进行编码，这些信息可以与其他方式一起附加，以帮助改进错误信息检测。此外，用于事实核查的NLP可以在很大程度上受益于专注于检索和知识增强方法，因为为了检查索赔的真实性，需要搜索并找到索赔的相关上下文。</p><h3 id="研究方向-5"><a href="#研究方向-5" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.检测和揭穿网上的错误信息。互联网上的误导性内容越来越多，未来几年，由于人工智能生成的内容越来越受欢迎，数量的增加可能是不可避免的。NLP可以在几个方面用来减缓误导性内容的传播。为了向事实核查人员和记者提供帮助，NLP系统仍然没有得到充分利用，这为构建事实核查技术留下了一个黄金机会，使事实核查人员能够加大工作力度（Kazemi等人，2022）。</p><p>2.确保代表的多样性。随着LLM生成内容的流行，大多数人的声音最终可能会在网络上被放大，因为LLM等数据驱动模型往往会记住其语料库中最具代表性的数据类型。因此，随着LLM生成的内容将越来越多地在网上使用，缺乏多样性，尤其是边缘化群体声音的代表性将是一个令人担忧的问题。</p><p>3.避免不当调节和检测过度调节。与内容生成中的异质性问题类似，内容调节技术也可能忽略代表性不足的群体或特定文化和社会环境中表达的细微差别。重要的是要使审核算法对所有群体都公平。</p><p>相反，由于各种政治利益（例如，伊朗希望限制对妇女自由的讨论），政府可能会限制网上讨论的话题。追踪哪些话题和观点在互联网上被过滤或降级，反思政治环境中的言论自由，确实成为一个重要的方向。</p><p>4.识别生成内容背后的利益相关者。随着机器生成内容的激增，判断信任哪些信息将变得越来越具有挑战性。一个有希望的方向是开发NLP模型，以确定生成内容背后的利益相关者，以及他们的兴趣类型，如商业利润（例如，来自广告或客户吸引力）或政治利益（例如，影响更多的人持有某些意见，这将在很大程度上有利于利益集团）。</p><h2 id="8-Child-Language-Acquisition"><a href="#8-Child-Language-Acquisition" class="headerlink" title="8. Child Language Acquisition"></a>8. Child Language Acquisition</h2><h3 id="背景-6"><a href="#背景-6" class="headerlink" title="背景"></a>背景</h3><p>虽然有人声称LLM“显示出AGI的火花”（Bubeck et al.，2023b），但它们并没有模仿人类在学习语言时所遵循的路径（Bowerman和Levinson，2001）。理想情况下，我们希望更小、更高效的语言模型与环境基础紧密结合（Lazaridou et al.，2017）。在实现高效AGI的道路上，我们有一个难以逾越的底线：儿童的语言习得。大多数儿童通过有限的互动和对语言的观察，最多可以习得三种语言。虽然我们还不完全了解孩子们是如何学习语言的，但我们知道他们不需要太字节的文本训练实例。</p><p>还有越来越多的研究探索LLM与儿童语言习得之间的联系，特别是在统计学习的背景下（Wilcox et al.，2022），最近的研究探索如何使用LLM来建模和模拟儿童用于习得语言的统计学习机制（Contreras Kallens et al.，2023）。这一领域的发展对低资源和濒危语言有着更广泛的影响，因为样本高效的语言建模算法可以为全新的语言和文化解锁LLM级别的功能。</p><h3 id="缺陷-5"><a href="#缺陷-5" class="headerlink" title="缺陷"></a>缺陷</h3><p>实现这样一个有效的基线——儿童——的数据效率是令人兴奋的，但没有灵丹妙药：心理学家、神经科学家和语言学家是几十年来一直在研究儿童语言习得的科学家之一，尽管他们对人类儿童的语言习得过程有了更深入的了解，我们还没有开发出一种以可比的数据效率在计算上再现相同过程的工作理论。</p><p>这种缺乏进展的情况可归因于研究儿童的困难，因为招募和IRB对此类研究的批准都对可以收集的数据类型施加了限制。除此之外，收集到的少量数据往往在可表达性方面受到限制，因为尚未学习过一门语言的儿童无法有效交流，这限制了实验设计。在广泛的儿童语言研究中，家长们都会在场，以确保孩子们能够专注于实验并遵循指导方针。此外，当你无法控制实验的受试者时，很难控制混杂变量。</p><h3 id="研究方向-6"><a href="#研究方向-6" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.示例高效的语言学习。这是一个成熟的领域，有机会提高我们对语言的理解，并开发更高效的数据NLP工具。对样本有效的语言学习进行基础和理论研究是非常必要的。对于对核心NLP感兴趣的研究人员来说，在较小的数据范围内实现最先进的计算理论和算法是一个令人兴奋的领域，追求最先进的性能可能很快就会转向数据效率分数。与这个方向相关的是为样本有效的Lamlanguage学习建立基线的目标。有一个下限目标（例如，X小时的互动达到Y分）可以使NLP社区能够更准确地了解数据效率方面的进展。</p><p>2.儿童语言习得的基准发展。随着大型语言和多模式系统的发展，有机会简化和扩展儿童语言基准构建。例如，在精心构建的监督基准上的受控实验可以通过儿童长时间学习语言的大型视频数据集来增强。此外，这些数据集可以用于训练专门针对儿童学习语言的方式定制的模型，这可以实现理解儿童语言使用的新方法，以及开发能够从更少的例子中学习的模型，类似于人类学习语言的方法。</p><p>3.语言模型是儿童语言习得的生物学模型。生物模型是指对特定生物系统的研究，该系统被认为与特定的人类系统具有关键的相似性，以获得对所讨论的人类系统的见解和理解。麦克洛斯基著名地主张利用神经模型作为生物模型来研究人类的认知行为，从而发展有关该行为的理论（麦克洛斯基，1991）。随着NLP模型开始显示出与人类语言使用的一些相似之处，我们现在有机会探索有关人类婴儿如何获得语言的理论。例如，（Chang和Bergen，2021）通过创建单个单词的学习曲线和习得年龄，研究了语言模型中单词习得的过程。利用现有的数据集，如WordBank（Frank et al.，2016）和CHILDES（MacWhinney，1992），以及新的基准，以及越来越强大的语言模型，我们现在有能力进行实验来分析语言习得（例如，音素水平的习得、内在奖励），并获得对儿童语言习得的新见解。</p><h2 id="9-Non-Verbal-Communication"><a href="#9-Non-Verbal-Communication" class="headerlink" title="9. Non-Verbal Communication"></a>9. Non-Verbal Communication</h2><h3 id="背景-7"><a href="#背景-7" class="headerlink" title="背景"></a>背景</h3><p>非语言交流包括手势、面部表情、肢体语言和姿势等。手语是一种特殊的非语言交流形式，是聋人使用的主要交流媒介。几项研究表明了非语言交流在日常互动中的重要性（McNeill，1992；Alibali等人，2000年）。最近在NLP中的工作强调了将非语言信息整合到现有语言表征中的重要性，作为获得更丰富表征的一种方式，包括例如语言模型（Wang et al.，2019b）或视觉模型（Fan et al.，2021）；先前的其他研究表明，面部表情或手势等非语言交流与语言渠道一致，不同的文化或语言背景可能与对这些非语言表达的不同解释有关（Abzaliev et al.，2022；Matsumoto和Assar，1992）。还有一整套研究侧重于手语的理解和生成（Joze，2019；Bragg et al.，2019），以及手语使用者不同社区之间的交流（Camgoz et al.，2020）。</p><p>理解非语言模态和语言之间的一致性仍然是一个悬而未决的问题，特别是考虑到其中一些模态使用不同的频谱（连续与离散）的挑战。相应地，这些信号的离散化和解释可能很困难，导致它们的联合使用或将这些非语言信息整合到现有的基于语言的大型模型中面临挑战。在手语研究中，在理解和生成手语方面仍然存在许多悬而未决的问题，包括汇编具有代表性的手语数据集和开发有效的计算模型。</p><h3 id="缺陷-6"><a href="#缺陷-6" class="headerlink" title="缺陷"></a>缺陷</h3><p>理解非语言模态和语言之间的一致性仍然是一个悬而未决的问题，特别是考虑到其中一些模态使用不同的频谱（连续与离散）的挑战。相应地，这些信号的离散化和解释可能很困难，导致它们的联合使用或将这些非语言信息整合到现有的基于语言的大型模型中面临挑战。在手语研究中，在理解和生成手语方面仍然存在许多悬而未决的问题，包括汇编具有代表性的手语数据集和开发有效的计算模型。</p><h3 id="研究方向-7"><a href="#研究方向-7" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.非语言口译。由于非语言交际的许多子领域都需要非语言信息，因此对这些信息的表示、离散化和解释是一个丰富的探索方向。例如，虽然之前的工作已经确定了一个潜在的面部表情“代码手册”（Song et al.，2013），但还需要更多的工作来找到可以在模式、上下文和文化中使用的理想表示集。对这些表情和手势的解释，以及它们在不同模式之间的一致性，仍然是一个悬而未决的问题。特别是，LLM的日益使用有可能为通过文本描述理解非语言交流开辟新的范式。例如，当LLM被提示“请回答我描述的手势：一个人张开双臂，微笑着向另一个人移动”时，它会回答“你描述的手势很可能是拥抱，表示友好或深情的问候或告别……”，这可以用作拥抱手势的文本表示。</p><p>2.手语的理解、生成和翻译。一个开放的研究问题是手语词典（Athitsos et al.，2008）和语料库（Li et al.，2020）的开发，它们可以用于训练和评估计算模型。这些资源对于开发和测试识别和解释模型至关重要，但创建这些资源往往既困难又昂贵。在手语理解中，最大的挑战之一是开发能够准确识别和解释手语手势的有效模型。这很困难，因为手语在手势方面表现出相对较高的可变性，包括手势形状、动作和方向的差异；此外，其他非手动特征，如面部表情、身体姿势和眼睛凝视，往往在手语中发挥作用，这会使识别过程更加复杂。最后，手语生成也是一个开放的研究领域。</p><p>3.有效的联合言语和非言语交流。最终，在沟通过程中应该同时考虑语言和非语言信号。我们希望人工智能系统同样能够理解“我不知道”，耸耸肩，或者**(打不出来，此处是一个颜表情）**。共同表示、融合和解释这些信号最终是人工智能辅助通信的长期目标。开放的研究问题不仅包括为这些模式中的每一种开发语言模型，还包括有效的融合方法，这将使同时进行言语和非言语交流的大型联合模型成为可能。</p><h2 id="10-Synthetic-Datasets"><a href="#10-Synthetic-Datasets" class="headerlink" title="10. Synthetic Datasets"></a>10. Synthetic Datasets</h2><h3 id="背景-8"><a href="#背景-8" class="headerlink" title="背景"></a>背景</h3><p>在NLP研究中，当更传统的人类数据收集不可行、昂贵或存在隐私问题时，通常需要合成数据（Mattern等人，2022）。随着生成模型的发展（Tang et al.，2023），合成数据生成在各个领域都具有适用性。例子包括低资源语言的反翻译（Sennrich et al.，2015；Edunov et al.，2018）、语义解析（Rosenbaum et al.，2022a）、意图分类（Rosenb鲍姆et al.，2020）、结构化数据生成（Borisov et al.，2022）或医学对话生成（Chintagunta et al.。2021a；Liednikova等人，2020）。如果需要领域自适应，该过程通常包括对模型进行预训练（Chintagunta等人，2021b），促使模型生成数据集，并自动或通过专家验证评估其质量。</p><h3 id="缺陷-7"><a href="#缺陷-7" class="headerlink" title="缺陷"></a>缺陷</h3><p>合成数据的使用面临着诸如数据质量控制困难（Kim et al.，2022）（由于缺乏文本生成的评估指标）、缺乏多样性、数据生成模型中的潜在偏见以及数据生成模型的固有局限性（如难以捕获长程依赖性）等挑战（Orbach和Goldberg，2020；Guan et al.，2020）。</p><h3 id="研究方向-8"><a href="#研究方向-8" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.知识提炼。知识提炼是将知识从教师模型转移到通常较小的学生模型的任务。例如，Kim等人（2022）将他们的合成对话框数据集框定为从InstructGPT中提取的。虽然早期的提炼方法涉及从教师模型的软输出逻辑中学习（Hinton et al.，2015），但这标志着直接利用LLM输出作为合成示例的趋势（West et al.，2022）。这允许从业者以不同的方式转换或控制生成的数据，例如使用微调模型来过滤质量。此外，合成数据可以用来直接模拟LLM的行为，使用更小、集中的模型，例如Alpaca的情况（Taori等人，2023）</p><p>2.对生成的数据属性的控制。目前，主要的方法是为自然文本规范提供说明和示例，但优化这些提示通常依赖于简单的试错方法。此外，通过指令或示例指定属性可能不精确或有噪声。开发稳健、可控和可复制的合成数据生成管道仍然是一个悬而未决的研究问题。</p><p>3.转换现有数据集。给定一个现有的数据集，我们可以应用各种更改来创建一个语义保持的新数据集，但要使用新的样式。常见的方法包括格式更改（例如，将新闻文章的数据集从HTML转换为纯文本格式）、模态转换（例如，生成图像或视频的文本描述或为视听内容生成字幕或字幕）或风格转换（Jin et al.，2022a）（例如，把文本的写作风格从冗长翻译为简洁）。</p><h2 id="11-可解释性"><a href="#11-可解释性" class="headerlink" title="11. 可解释性"></a>11. 可解释性</h2><h3 id="背景-9"><a href="#背景-9" class="headerlink" title="背景"></a>背景</h3><p>可解释性是理解和解释机器学习模型的决策过程，使其更加透明和合理的任务（Danilevsky et al.，2020）。可解释的NLP系统可以通过使最终用户、从业者和研究人员了解模型的预测机制来促进推力，并确保合乎道德的NLP实践。从历史上看，传统的NLP系统，如基于规则的方法（Woods，1973）、隐马尔可夫模型（Ghahramani，2001；Rabiner，1989）和逻辑回归（Cramer，2002），本质上是可解释的，称为白盒技术。然而，NLP的最新进展，其中大多数是黑盒方法，是以可解释性的损失为代价的。为了解决这个问题，可解释性已成为一个研究方向，专注于开发深入了解NLP模型内部工作的技术（Mathews，2019；Danilevsky等人，2020）。关键研究发现包括注意力机制、基于规则的系统和可视化方法，这些方法有助于弥合复杂语言模型和人类可解释性之间的差距，最终有助于负责任地部署NLP系统。</p><h3 id="缺陷-8"><a href="#缺陷-8" class="headerlink" title="缺陷"></a>缺陷</h3><p>NLP中可解释性研究的现状侧重于理解模型预测、特征重要性和决策过程。注意力机制（V aswani et al.，2017）、LIME（Ribeiro et al.，2016）和SHAP（Lundberg和Lee，2017）等技术已经出现，可以深入了解模型行为。然而，在稳健性、可推广性和伦理考虑等方面仍然存在差距。此外，可解释性方法往往缺乏标准化，难以解决transformer等复杂的大规模模型，限制了它们在现实世界场景中的适用性。</p><h3 id="研究方向-9"><a href="#研究方向-9" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.探测。一个有希望的方向是通过设计能够揭示语言的探究任务来研究NLP模型的内部表征，包括LLM（Hewitt和Manning，2019；休伊特和梁，2019）和模型所捕获的世界知识（Elhage et al.，2022；Geva et al.，2022022）。这有助于理解模型的推理能力，并识别潜在的偏差（Li et al.，2022；Meng等人，2022）。</p><p>2.机械解释能力。虽然探究主要着眼于模型学习的特征的属性，但机械可解释性旨在揭示模型中有助于其决策过程的潜在机制和算法（Nanda et al.，2023；Conmy et al.，2021）。它从神经网络中提取计算子图（Conmy et al.，2023；王等人，2023，Geiger et al.，2021），其高层目标是对整个深度神经网络进行逆向工程（Chughtai et al.，2022）。</p><blockquote><p>有点感兴趣</p></blockquote><p>3.提高human-in-the-loop的可解释性。NLP中的人在环可解释性研究侧重于结合人的反馈和专业知识来增强模型的可解释性。这种方法旨在提高模型的透明度，促进更好的决策，并促进人工智能系统和用户之间的信任。通过让人类参与进来，研究人员可以识别和解决偏见，确保伦理考虑，并开发更可靠和可理解的NLP模型。有各种有前景的方向，例如主动学习和交互式解释生成（Mosca等人，2023；Mosqueira-Rey等人，2023.）。</p><p>4.基于参考文献生成的文本。可解释性涉及理解为什么提供某种生成NLP模型输出，并评估其正确性，可能通过校准（Naeini等人，2015）。事实上的正确性并不是生成模型必须遵循的限制；相反，他们通常被训练通过预测下一个最有可能出现的文本来模仿人类的书面文本。反过来，这种预测的文本容易产生幻觉（Ji et al.，2022），导致用户缺乏信任。一个有前途的解决方案是通过附加参考文献和显示任何额外的推理步骤，为模型输出的事实提供可靠的来源。例如，引文可以与其参考书目一起包括，或者指向训练数据（或文档数据库）中文档的指针可以附加到输出中。这样一个系统应该评估这些来源在多大程度上支持模型提出的主张。</p><h2 id="12-高效NLP"><a href="#12-高效NLP" class="headerlink" title="12. 高效NLP"></a>12. 高效NLP</h2><h3 id="背景-10"><a href="#背景-10" class="headerlink" title="背景"></a>背景</h3><p>高效NLP是一个旨在优化NLP模型资源利用的研究方向。这一目标源于应对语言模型规模不断扩大所带来的挑战的需要，以及日益增长的资源消耗对NLP的进步提出了新的挑战（Touvron等人，2023b；张等人，2023）。事实上，人们普遍认为，扩大规模是在NLP任务上实现最先进表现的重要方法，尤其是那些随着规模定律而出现的技能（Wei et al.，2022；Bowman，2023）。然而，开发LLM需要大量的能源和财政资源来进行培训和推理，这引发了人们对人工智能碳足迹和NLP产品开发的经济负担的担忧（Strubell等人，2019）。鉴于这些问题，先前的研究强调了有效减少二氧化碳当量排放（CO2e）和兆瓦时（MWh）以及提高电力使用效率的迫切需要（Patterson等人。</p><blockquote><p>怎么都是提到碳排放？我还以为是为了减少训练时长之类的。</p></blockquote><h3 id="缺陷-9"><a href="#缺陷-9" class="headerlink" title="缺陷"></a>缺陷</h3><p>在包括数据管理、模型设计和训练范式在内的各个维度上，提高NLP的效率有很大的空间，这提供了许多研究机会。解决数据效率问题涉及到解决诸如增强重复数据消除技术、评估数据质量和管理大量数据等挑战。在细化模型设计时，关键挑战包括提高注意力机制的效率，开发用于参数约简的替代无参数模块，以及优化模型深度或效率。最后，在训练范式领域，在促进工程、微调和快速调整技术方面存在进步的潜力。</p><h3 id="研究方向-10"><a href="#研究方向-10" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.可以通过重复数据消除来提高数据效率，消除冗余或有噪声的数据，从而用更少的数据项提高性能。尽管现有的工作旨在通过删除有噪声的示例和消除无用数据的重复来提高模型性能，减少数据点（Lee等人，2022；Mishra和Sachdeva，2020；Hoffmann等人，2022），但缺乏针对大量语料库（&gt;700B令牌）或原始网络数据管理的有效数据消除方法。</p><p>2.模型设计。大量方法通过改进注意力机制来提高模型效率（Tay等人，202022；Dao等人，2022；Ma等人，2022）。然而，在转换器架构中处理超长上下文建模仍然存在挑战。稀疏模型可以扩大模型的宽度，以提高表达能力，同时减少理论FLOP。值得注意的实践包括在基于变压器的模型的前馈层中应用专家混合架构（Fedus等人，2022022；Du等人，2022）。设计这样的模型需要特定于体系结构的实现，并且需要进行多次试验才能获得最佳体系结构。它的性能也不稳定（Mustafa等人，2022）。</p><p>3.高效的下游任务自适应。有效的微调旨在通过更新一小部分参数，使预先训练的模型适应下游任务（Pfeiffer等人，2020；Moosavi等人，2022；Schick和Schütze，2021）。提示调谐/前缀调谐在不改变模型参数的情况下用额外学习的向量修改激活（V alipour等人，2022；Lester等人，2021）。然而，有必要找到一种有效的自动提示构建方法。</p><h2 id="13-NLP在教育"><a href="#13-NLP在教育" class="headerlink" title="13. NLP在教育"></a>13. NLP在教育</h2><h3 id="背景-11"><a href="#背景-11" class="headerlink" title="背景"></a>背景</h3><p>NLP在教育中的应用有着丰富的历史，包括专门的研讨会，如由建筑教育应用特别兴趣小组组织的年度ACL关于创新利用NLP构建教育应用的研讨会。这些应用程序包括帮助学习者的工具（例如，语言学习应用程序，如Duolingo*，或语法纠正工具，如Grammarly*），帮助教师和组织评分的工具（如，用于GRE论文评分的电子评分系统（Burstein等人，1997）），帮助课程和评估开发的工具（例如，开发多项选择题的系统（Kurdi et al.，2020））和教育研究人员的工具（如，构建课堂互动表示的系统（Alic et al.，2022））。自发布以来，研究人员一直在测试BERT（Devlin et al.，2019）和RoBERTa（Liu et al.，2017）等模型在这些领域的应用，现在开始纳入更大的模型。</p><h3 id="缺陷-10"><a href="#缺陷-10" class="headerlink" title="缺陷"></a>缺陷</h3><p>教育领域中部署的许多NLP应用程序都是在LLM广泛使用之前开发的，我们很可能很快就会看到基于LLM的特定任务模型的大规模部署。虽然之前的许多工作包括独立的应用程序，但开发可以很容易地融入现有教育管道的模型，例如通过整合学生迄今为止所学的知识，是一个有待进一步探索的领域。重要的是，教育的一个长期目标是根据学生个人的需求个性化材料和评估，而NLP有潜力为这一目标做出贡献。</p><h3 id="研究方向-11"><a href="#研究方向-11" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.可控制的文本生成。对话系统和更普遍的文本生成以前已经在教育应用中使用。在这个空间内，可控的文本生成可以用于更个性化的体验，例如，使用自动生成的与学生兴趣相关的故事向学生介绍新术语，或者修改故事，使不同阅读水平的小学生可以访问。同样，虽然我们已经在阅读理解方面看到了大量的工作，但我们现在可以开始想象应用程序，在这些应用程序中，将根据学生之前的经验以及他们之前接触过的测试来测试文本的理解，以获得更具适应性的学习体验。</p><p>2.教育解释生成。个性化的课堂材料还可以包括根据学生对材料的理解（或缺乏理解）为他们生成解释。例如，NLP系统可以用来帮助学生理解学术论文中的一个棘手句子，或者改写老师给出的答案，希望找到与学生知识体系相关的解释。自动评分也是NLP做出许多贡献的领域（Mohler和Mihalcea，2009），但它仍然包括开放的研究问题，例如为不太完美的评分提供解释。</p><p>3.智能辅导系统。智能辅导系统显示出个性化教育的巨大前景（Mousavinasab等人，2021）。可以开发NLP方法来生成有针对性的练习问题，并解释学生在广泛领域的错误，从英语或历史到物理或计算机科学。随着NLP向更可靠地模仿人类推理的方向发展，这些系统可能会得到改进；目前，在没有人参与的情况下，在教育中部署NLP时需要小心，因为即使给出简单的数学问题，NLP模型（包括最新的LLM（OpenAI，2023））也经常会自信地给出错误的答案和解释。</p><p>值得一提的是，由于学术不诚实的可能性增加，教育界对LLM的接受在很大程度上是令人担忧的。这导致课程和大学采取了一些政策来规范如何在课程中使用人工智能，例如耶鲁大学的政策。*整体课程是否会进行调整，以积极的方式纳入LLM尚待观察，但我们乐观地认为，如果在适当的情况下部署，这一最新进展可以对教育产生积极影响。</p><h2 id="14-NLP在医疗保健领域"><a href="#14-NLP在医疗保健领域" class="headerlink" title="14. NLP在医疗保健领域"></a>14. NLP在医疗保健领域</h2><h3 id="背景-12"><a href="#背景-12" class="headerlink" title="背景"></a>背景</h3><p>NLP在医疗保健中的应用可以根据其使用和对提供者、患者和公共卫生官员等关键利益相关者的影响进行分类（Zhou et al.，2022；Olaronke和Olaleke，2015）。当关注卫生服务提供者时，NLP通常用于支持临床决策，方法是（1）聚合和整合可用数据和研究，以及（2）从数据中提取相关信息。这些任务涉及重要的挑战，如医疗保健数据的标准化、健康概念的准确标记、提取和检索以及患者状况的分类（Dash等人，2019）。类似地，NLP用于处理患者对应用程序信息的请求，例如健康相关问题的问答，以及与医疗或疾病相关的信息的检索。最近在这一领域的工作集中在心理健康领域的语言分析，涵盖了专业治疗（Sharma et al.，2020；PérezRosas）</p><p>类似地，NLP用于处理患者对应用程序信息的请求，例如健康相关问题的问答，以及与医疗或疾病相关的信息的检索。最近在这一领域的工作集中在心理健康领域的语言分析，涵盖了专业治疗（Sharma等人，2020；PérezRosas等人，2017；Min等人，2022）和社交媒体对话（Tabak和Purver，2020；Lee等人，2021；Biester等人，2020）。关于协助公共卫生官员，NLP正在用于公共卫生监测，以确定疾病和风险因素或高危人群（Naseem等人，2022；Jimeno Y epes等人，2015；Y ates等人，2014），也用于缓和网上错误信息或公众情绪等方面（Hou等人，2019；Kazemi等人，2021b）</p><h3 id="缺陷-11"><a href="#缺陷-11" class="headerlink" title="缺陷"></a>缺陷</h3><p>NLP在医疗保健领域最明显的局限性之一是缺乏高质量的注释临床数据。尽管社交媒体数据在某些情况下可能有用，但临床数据在开发临床决策工具时至关重要，而且由于隐私和道德问题，临床数据往往无法公开。另一个缺点是缺乏语言多样性，因为迄今为止的工作主要集中在英语或其他高资源语言上（Mondal et al.，2022），但对少数民族语言的投入较少。此外，缺乏对基于NLP的卫生系统的人类评估，这使得在现实世界中衡量其有效性具有挑战性。当前的自动评估指标不一定与患者的结果有关。因此，在评估NLP动力工具在医疗保健中的疗效时，必须进行以人为中心的研究</p><h3 id="研究方向-12"><a href="#研究方向-12" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.医疗基准建设。尽管最近LLM的文档报告了各种医疗问答基准或医疗许可文本的非常高的性能，但医疗保健中还有许多其他任务缺乏实现类似良好性能所需的数据。由于隐私问题，对医疗数据集的访问往往受到限制，因此可能需要其他方法来编制此类基准。合成数据集就是这样一种选择（Chintagunta等人，2021a；Liednikova等人，2020）。其他选择，包括将现有数据集转述为数据扩充的一种形式；或者使用LLM作为引导数据集的起点。另一个开放的研究方向是评估基准的质量。此外，还需要进行研究，以找到以低资源语言或低资源领域生成新的健康数据集的有效方法</p><p>2.用于临床决策的NLP。NLP系统可以用作集思广益或决策工具，帮助专家进行评估和决策过程。它们可以用于综合新知识（例如，关于医学发现的最新研究论文），并将其提供给医生。此外，将一般医学知识和个人患者信息结合起来需要新的知识整合策略。由于临床诊断和治疗是高风险决策，因此NLP系统的可靠性和可解释性至关重要，以提供其预测背后的清晰推理。这些过程还需要与医学专家进行跨学科合作，以确保该系统与他们的领域知识和临床实践相一致。</p><p>3.药物发现。药物发现是生物医学和化学研究中经常考虑的一个关键研究领域，但最近引起了NLP研究人员的注意。NLP方法可以有效地从大量科学文献、专利、学术媒体、临床记录和其他生物医学来源中提取和分析信息。开放的研究方向包括药物-靶标相互作用的识别和优先顺序、新候选药物的发现、化合物性质的预测以及药物设计的优化。新的NLP方法也可以有助于识别新的药物靶点关联，并可以实现更有效的药物再利用工作。</p><blockquote><p>既然可以从科学文献中提取，那是不是可以和图像或化学式等结合，那未来的方向是不是多模态的方向呢？</p></blockquote><h2 id="15-NLP与伦理"><a href="#15-NLP与伦理" class="headerlink" title="15. NLP与伦理"></a>15. NLP与伦理</h2><h3 id="背景-13"><a href="#背景-13" class="headerlink" title="背景"></a>背景</h3><p>人们越来越认识到伦理在NLP中的作用，尤其是随着具有潜在深远社会影响的越来越强大的模型的发展。在开发NLP模型时，有重要的伦理考虑因素（Bender等人，2020），并且正在进行旨在解决双重使用、公平和隐私等关键伦理方面的研究工作。</p><h3 id="缺陷-12"><a href="#缺陷-12" class="headerlink" title="缺陷"></a>缺陷</h3><p>除了上述问题外，围绕最近LLM的使用和应用的其他伦理问题包括：缺乏归因、模型可解释性差、技能退化、劳动力市场混乱、模型滥用和模型废弃。除了对人们进行道德教育外，我们还需要进一步调查这些担忧的程度，并确定NLP技术如何减少其影响</p><h3 id="研究方向-13"><a href="#研究方向-13" class="headerlink" title="研究方向"></a>研究方向</h3><p>1.两用。许多具有积极影响的NLP应用程序可能同时以有害的方式使用。可以通过部署前的讨论和部署后的数据调查来识别NLP模型和应用程序可能造成的危害，以识别潜在的有害应用程序。此外，开发有助于检测、劝阻和防止有害使用的NLP系统（如事实核查器）至关重要。对抗性NLP还可以用于探索NLP系统的局限性和漏洞，并提高其鲁棒性。</p><p>2.公平。需要一种方法来评估NLP模型的公平性，并检测和减轻偏差。这包括调查数据集创建实践及其与模型偏差的相关性（Wang et al.，2020）。此类研究应检查对数据集创建的更严格要求是否可以减少偏见和不平等，这些偏见和不公平可能因基于偏见数据训练或评估的模型而加剧。</p><p>3.隐私。由于个性化NLP应用程序（包括教育或医疗保健等领域）需要了解用户，NLP系统中的隐私保护已成为一个重要的研究方向。需要新的技术来识别和匿名敏感的用户信息，同时保持数据用于分析和决策的效用。这包括差分隐私、联合学习和安全多方计算等方法，以确保NLP驱动的医疗保健应用程序中患者数据的机密性和安全性。此外，NLP系统可以产生影响的一个领域是数据政策，可以开发NLP方法，以用户可以理解的格式总结数字产品的数据政策，并确保模型与这些政策保持一致（Carlini等人，2021）</p><h2 id="16-那么我应该做什么呢？"><a href="#16-那么我应该做什么呢？" class="headerlink" title="16. 那么我应该做什么呢？"></a>16. 那么我应该做什么呢？</h2><p>NLP研究的前景是光明的。我们目前在LLM方面看到的快速进展并不意味着“一切都解决了”。相反，正如本文件所强调的，NLP中有许多未探索的研究方向没有被LLM的当前进展所解决。它们增加了NLP中LLM性能有限的许多现有任务（Bang et al.，2023a），以及新LLM功能所支持的越来越多的新领域。更广泛地说，作为一个领域，我们现在有机会摆脱以性能为中心的技术开发，并承认NLP是关于语言和人的，应该从根本上以人为中心。这带来了对扶持技术的新关注，这些技术具有文化和人口意识，稳健、可解释、高效，并与强大的道德基础相一致，最终对社会产生持久积极影响。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>天文学神经网络的历史、入门和展望</title>
    <link href="/2023/20230701/"/>
    <url>/2023/20230701/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#">发表于Royal Society上的《Astronomia ex machina: a history, primer and outlook on neural networks in astronomy？</a></p><p>个人评价：</p><p>大致算是讲述了人工智能的历史，没有很好的讲述人工智能在天文学的特殊性，生硬的讲述人工智能+天文学，简单的1+1-2，没有看到想要看到的1+1&gt;2。</p><p>让我印象最深的是文中提到居然只有三篇天文学的论文用了diffusion，是数据集的问题？</p><hr><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在这篇综述中，我们探讨了人工智能（AI）和深度学习在天文学中的历史发展和未来前景。我们通过三个浪潮来追踪天文学中联结主义的演变，从多层感知器的早期使用，到卷积神经网络和循环神经网络的兴起，最后到当今无监督和生成深度学习方法的时代。随着天文数据的指数级增长，深度学习技术提供了前所未有的机会来发现有价值的见解并解决以前棘手的问题。当我们进入预期的天文联结主义第四波浪潮时，我们主张采用针对天文应用进行微调的类似 GPT 的基础模型。这些模型可以利用高质量、多模态天文数据服务于最先进的下游任务。为了跟上大科技推动的进步，我们在天文学界提出了一种协作、开源的方法来开发和维护这些基础模型，利用这两个领域的独特优势，促进人工智能和天文学之间的共生关系。</p><span id="more"></span><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p>工智能 (AI) 的概念至少可以追溯到 350 年前莱布尼茨的《<em>组合艺术论文》</em> [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C1">1</a> ]。受笛卡尔和鲁尔的启发，莱布尼茨提出，通过“通用语言”的发展，所有思想都可以通过一小组基本概念的组合来表示，并且新概念可以以逻辑方式产生，可能<em>是</em>通过一些计算机。莱布尼茨的雄心勃勃的愿景（“让我们计算”）尚未实现，但模拟人类推理，或者至少建造一台机器来模仿人脑的计算和数据处理能力的追求，一直持续到今天。</p><p>可以公平地说，人工智能的根源甚至可以追溯到启发莱布尼茨的 Llull 中世纪哲学 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C2">2</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C3">3</a> ]。然而，如果我们现在认为人工智能是一门真正的科学学科，那么这个学科显然是在二十世纪战后几年出现的，遵循图灵的简单问题“机器能思考吗？” [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C4">4</a>]。从本质上讲，图灵在 1950 年提出的问题简洁地阐明了人工智能的雄心，但从具体细节的角度来看，从图灵提出第一个人工智能程序（即所谓的“逻辑理论家”）的问题起，又花了 5 年时间。由艾伦·纽厄尔 (Allen Newell)、克里夫·肖 (Cliff Shaw) 和赫伯特·西蒙 (Herbert Simon) 开发。逻辑理论家由研究与开发（兰德）公司资助，其设计部分是为了模仿人类数学家的角色，因为它可以自动证明数学定理。这是计算机科学领域的一项突破，《逻辑理论家》在 1956 年开创性的达特茅斯人工智能夏季研究项目 (DSRPAI) 会议上提出，现在被视为人工智能领域的真正诞生。的确，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C5">5</a>].</p><p>来自迪克的《机器人会梦见<em>电子羊吗？<em>克莱顿的</em>《西部世界》</em>、<em>《终结者》的《天网》<em>等等。伊恩·M·班克斯（Iain M. Banks）的银河文明被称为“文化”，想象了一个由强大的“思想”统治的社会，其智力和智慧远远超过人类，并且具有同等感知能力的生物和机器通常和平、合作和公平地共存。尽管有科幻小说，但如果这些梦想成为可能，我们距离一台能够真正独立思考的机器还需要很多年的时间 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C6">6</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C7">7</a>]。然而，如何在数学上（和算法上）对生物神经元（神经网络）的工作和相互关系进行建模，以及随后探索它们如何在数据分析师研讨会中找到作为工具的实用性的问题，才是真正被提及的问题。今天大多数人使用“人工智能”这个词。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN1">1</a>虽然我们必须时刻警惕炒作和流行语，但正是神经网络的</em>应用以及解决迄今为止棘手问题的可能性，才为包括天文学在内的许多不同领域的研究提供了令人兴奋的真正理由。</em></p><p>天文学家使用人工神经网络（ANN）已有三十多年了。1994 年，早期开拓者 Ofer Lahav 讽刺地指出了“神经怀疑论者”——那些抵制在严肃的天体物理学研究中使用此类技术的人——并认为人工神经网络“应该被视为一个通用的统计框架，而不是一个深奥的方法”[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C8">8</a> ]。不幸的是，这种怀疑一直存在。尽管最近神经网络（以及一般机器学习）在该领域的使用激增，如图<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F1">1</a>所示。这种怀疑也与天文学领域的成就相反，如果不使用人工神经网络就不可能取得这些成就，例如光度红移估计（例如[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C9">9</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C10">10</a>]）、大规模天文物体识别和聚类（例如[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C11">11</a> ]）以及完全数据驱动的模拟（例如[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C12">12</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C13">13</a> ]）。大多数对机器学习技术和深度学习的批评<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN2">2</a>特别是针对该方法论的“黑匣子”性质。在这篇综述中，我们提供了关于如何构建深度神经网络以及控制其学习的数学规则的入门知识，我们希望这将为神经怀疑论者提供有用的资源。尽管如此，我们必须认识到，关于深度神经网络如何工作的统一理论图景尚不存在。即使在深度学习社区中，这仍然是一个争论点。例如，Yann LeCun 在第 31 届神经信息处理系统 (NIPS) 会议上回应 Ali Rahimi 的“Test of Time”获奖演讲时说道：</p><p><img src="https://royalsocietypublishing.org/cms/asset/90b9eaa0-e734-4b61-a8d5-2e5927f08300/rsos221454f01.gif" alt="Figure 1. "></p><p>图 1.这里我们看到每月 arXiv:astro-ph 提交的摘要或标题包含一个或多个字符串的数量：“机器学习”、“ML”、“人工智能”、“AI”、“深度”学习”或“神经网络”。原始数据属于公共领域，可在<a href="https://www.kaggle.com/Cornell-University/arxiv%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82%3E">https://www.kaggle.com/Cornell-University/arxiv上获取。&gt;</a> </p><blockquote><p> Ali发表了一次有趣且精彩的演讲。但我从根本上不同意这个消息。从本质上讲，主要信息是当前的机器学习实践类似于“炼金术”（他的原话）。这是侮辱性的，是的。但不要介意：这是错误的！Ali 抱怨说，人们对目前 ML 中使用的许多方法缺乏（理论）理解，尤其是在深度学习中……仅仅因为可以对其进行理论研究而坚持使用一组方法，而忽略了一组经验上的方法仅仅因为您（还）不理解它们就可以更好地工作，理论上类似于在路灯下寻找丢失的车钥匙，但知道您在其他地方丢失了它们。是的，我们需要更好地理解我们的方法。但正确的态度是尝试解决问题，不要因为尚未成功解决问题而侮辱整个社区。这就像批评詹姆斯·瓦特不是卡诺或亥姆霍兹一样[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C14">14</a>]. </p></blockquote><p>抛开哲学问题不谈，LeCun 的基本观点是深度学习“有效”，因此我们应该使用它，即使我们不完全理解它。如果一个人不仁慈，我们可以对这个问题提出类似的论点。LL清洁发展机制范式。</p><p>显然，在深度学习渗透的每个领域，我们都看到专业知识的使用减少，取而代之的是从数据中自动获取的知识。我们已经看到这个过程在许多“应用深度学习”领域中发挥作用，例如计算机围棋[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C15">15</a> ]、蛋白质折叠[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C16">16</a> ]、自然语言处理[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C17">17</a> ]和计算机视觉[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C18">18</a> ]。我们认为，天文学的数据丰富性使其走上了一条与其他应用深度学习领域所走过的道路没有什么不同的道路。这种丰富并不是一个短暂的阶段；而是一个阶段。天文数据总量已经很大，并将在未来几年呈指数级增长。我们在<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F2">图 2中对此进行了说明</a>，其中我们展示了一系列天文调查及其在其生命周期内的估计数据量输出[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C19">19</a> ]。这甚至没有考虑与更大、更详细的数值模拟相关的数据（例如[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C22"> </a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C20">20-22</a>]）。目前的数据量规模已经给天文学带来了一个问题，因为许多经典方法依赖于人类监督和专业知识，而不断增加的数据量将使通过传统的人类监督和半监督方式探索和利用这些调查成为一个棘手的问题。令人严重关切的是，我们有可能错过——或大大延迟——有趣和重要的发现，仅仅是因为我们无法准确、一致地大规模询问天文数据。深度学习在各种数据密集型领域的自动化信息提取方面显示出了巨大的前景，因此非常适合作为处理超大规模天文数据挑战的解决方案。但我们不需要就此止步。这篇评论的展望更进一步，</p><p><img src="https://royalsocietypublishing.org/cms/asset/93422bc3-b9a5-4d4e-9050-3886362723cb/rsos221454f02.gif" alt="Figure 2. "></p><p>图 2.一系列天文调查在其生命周期内的数据量输出。我们可以看到天文调查数据量每16个月就会翻一番。数据取自Zhang 和Zhao [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C19">19</a> ]。</p><p>自天文学联结主义<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C25">在20 世纪 80 年代末的</a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN3">3 个</a>不起眼的开端以来，关于人工神经网络在天文学中的应用已经有许多优秀的评论（例如 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C23">23-25</a> ]）。我们对之前的文献综述采取了另一种方法，对这一领域进行了全面的调查，试图描绘出天文联结主义的“大图景”。虽然我们不可能将所有作品都纳入天文学联结论中，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN4">4</a>我们希望这篇评论能够作为天文学日益自动化的联结论“三波”的历史背景，并提供神经网络的一般入门知识，以帮助那些寻求探索的人这个有趣的话题还是第一次。</p><p>在§§2和§§3中，我们探索了天文学中多层感知器的初步工作，其中模型需要手动选择涌现属性作为输入。在第 4 节和第 5 节中，我们探讨了第二波浪潮，它与卷积神经网络和循环神经网络的传播相一致，在这些模型中，多层感知器的手动选择的输入被原始数据摄取所取代。在现在正在发生的第三次浪潮中，我们看到人类监督的消除与深度学习方法直接从数据中推断标签和知识有关，我们将在第 6-8 节中探讨这一浪潮。最后，在第 9 节中，我们展望未来并预测我们将很快进入天文联结主义的第四次浪潮。我们认为，如果天文学遵循其他应用深度学习领域的模式，我们将看到精心设计的深度学习模型被移除，取而代之的是经过微调的包罗万象的“基础”模型。作为第四波浪潮的一部分，我们主张天文学和联结主义之间的共生，这种共生基于天文学的相对数据财富和深度学习永不满足的数据胃口。机器学习中的许多超大型数据集都是专有的或质量较差，因此天文学家作为一个社区有机会开发和提供高质量的多模式公共数据集。反过来，该数据集可用于训练天文基础模型，以服务于最先进的下游任务。由于基础模型对数据和计算的渴望，单个天文学研究小组无法单独建立这样的模型。因此，我们得出的结论是，天文学作为一门学科，跟上大型科技巨头设定的研究步伐的机会很小——也就是说，除非我们效仿 EleutherAI 和 HuggingFace 的例子，并以草根开源的方式集中我们的资源。</p><p>在继续之前，我们必须首先向读者承认我们对他们并不完全诚实。这篇评论的摘要不是我们写的。它是通过本文介绍 [26, 27] 提示 OpenAI 的基于生成式预训练 Transformer 4（“GPT-4”）神经网络的基础模型<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C26">而</a>生成<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C27">的</a>。准确地说，我们提示“ChatGPT Plus”提供的 GPT-4 引擎使用原始 LaTeX 格式的第 1 段到本段落中的所有文本。然后，我们将以下提示附加到介绍文本中：</p><blockquote><p>”为上述文本写一个摘要，以吸引读者的眼球，并使他们对论文感兴趣。使摘要不超过 160 个字，并触及类 GPT 模型在天文学中的价值。“</p></blockquote><p>我们没有改变 GPT 生成的输出。我们在第 9 节中更详细地探讨了这些基础模型及其可能的天文用途。</p><h3 id="2-人工神经元入门"><a href="#2-人工神经元入门" class="headerlink" title="2.人工神经元入门"></a>2.人工神经元入门</h3><p style="color:#FF0000;">略过枯燥无味的基础介绍</p><h3 id="3-天文学的第一波联结主义浪潮"><a href="#3-天文学的第一波联结主义浪潮" class="headerlink" title="3.天文学的第一波联结主义浪潮"></a>3.天文学的第一波联结主义浪潮</h3><p>在反向传播（见脚注 9）普及以及第一个“人工智能冬天”随之过去之后，联结主义首次在 20 世纪 80 年代末在天文学领域进行讨论。1988 年出现了两项激进的研究，它们认识到天文学可以从使用人工神经网络中受益的领域 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C51">51</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C52">52</a> ]。他们共同发现天文物体分类<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN11">11</a>和望远镜调度可以通过使用人工神经网络来解决。这些研究随后导致了领域的迅速拓宽，并将联结主义应用于许多不同的天文用例（[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C23">23</a> ]和其中的参考文献）。在本节中，我们将概述 MLP 在天文学中早期使用的领域。</p><h4 id="3-1-分类问题"><a href="#3-1-分类问题" class="headerlink" title="3.1. 分类问题"></a>3.1. 分类问题</h4><p>奥德瓦恩<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C53">53</a> ]将天体分为恒星和星系类型。这些取自 Palomar Sky Survey 自动平板扫描仪目录 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C54">54</a> ]。为了编译他们的数据集，他们首先从扫描的观察中提取了一组新出现的图像参数。这些参数包括直径、椭圆率、面积和板透射率。然后使用这些参数来训练线性感知器和前馈 MLP，以将物体分类为恒星或星系。奥德瓦恩<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C53">53</a> ]发现他们表现最好的模型可以对星系进行分类，其完整性为95%95%对于星等小于 19.5 的物体。这项工作之后是<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C58">关于</a>恒星&#x2F;星系分类问题的更多研究（例如[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C55">55-58</a> ]）。星系形态类型分类在 20 世纪 90 年代初就被探索过。Storrie-Lombardi &amp; Lahav [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C59">59</a> ] 描述了一种 MLP，它将一组选定的 13 个星系汇总统计数据作为输入，并使用此信息将星系分类为五种形态类型之一。Storrie-Lombardi &amp; Lahav [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C59">59</a> ]报告前一准确率为 64%，前二准确率为 90%。在这项试点研究之后，同一小组进行了几项研究，证实 MLP 是有效的自动星系形态分类器（[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C60">60</a> – <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C65">65</a>]，请参阅第 5 节以了解该研究的后续内容）。</p><p>MLP 也被用于其他分类任务；在此，我们重点介绍 MLP 的应用的其他几个领域。冯·希佩尔<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C66">66</a> ]将恒星光谱分为温度类型，Klusch &amp; Napiwotzki [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C67">67</a> ]对摩根-基南系统类型做了同样的事情。Chon [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C68">68</a> ] 描述了在萨德伯里中微子天文台中使用 MLP 来搜索和分类 μ 子事件（以及中微子观测）。几项研究已经探讨了类星体分类<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C69">69-71 </a>。本质上，Carballo<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C69">69</a>] 使用 MLP 来选择类星体候选者，考虑到它们的射电通量、积分与峰值通量比、红色和蓝色波段的光度测量和点扩散函数，以及它们的射电光学位置分离。<em>他们发现他们的模型与 White等人</em>描述的决策树模型非常一致。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C72">72</a> ]，确认 MLP 是传统机器学习的竞争替代品。作为超新星光度分类挑战赛（SPCC，[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C73">73</a> ]）的一部分，Karpenka<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C74">74</a>] 提出使用神经网络将超新星分为 Type-1a&#x2F;非 Type-1a 类。为了对光变曲线进行分类，他们首先使用手工制作的拟合函数，然后根据拟合系数训练 MLP。他们发现他们的模型与在 SPCC 数据集上训练的其他更复杂的模型相比具有竞争力。从本节讨论的研究中，我们可以安全地得出结论，当给定专家指南提取的重要参数时，MLP 是天文数据的有效分类器。</p><h4 id="3-2-回归问题"><a href="#3-2-回归问题" class="headerlink" title="3.2. 回归问题"></a>3.2. 回归问题</h4><p>MLP 也用于回归问题。安吉尔<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C75">75</a> ]首先将它们应用于自适应望远镜光学器件。他们利用多镜望远镜 (MMT) 所观察到的 250 000 次模拟的聚焦和失焦恒星观测数据来训练 MLP。根据扁平化的 13 × 13 像素观测结果，他们的网络预测了每个 MMT 镜子使恒星聚焦所需的活塞位置和倾斜度。应用这些更正后，作者能够恢复原始配置文件。在后续研究中，桑德勒<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C76">76</a> ] 和劳埃德-哈特<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C77">77</a> ]证明了Angel<em>等人</em>的MLP对真正的MMT有效。</p><p>许多并行研究都探索了光度红移估计<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C79">（</a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C78">例如</a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C9"> <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C10">9,10,65,78,79 </a>[]</a>）<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C65">。</a>弗斯<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C10">10</a> ]训练了一个神经网络来预测斯隆数字巡天（SDSS）早期数据发布中包含的星系红移[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C80">80</a> ]。星系作为一组汇总参数输入到神经网络，输出是代表星系红移的单个浮点。他们发现他们的网络达到了与经典技术相当的性能。<em>扩展并确认了 Firth等人</em>的工作。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C10">10</a> ]，鲍尔<em>等人。</em>[[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C65">65</a> ]使用MLP来预测SDSS第一个数据发布中包含的星系红移[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C81">81</a> ]。他们还表明，MLP 能够预测星系的光谱类型和形态分类。</p><p>当然，MLP 在天文回归任务中得到了更广泛的应用。在这里，我们将挑选一些研究来展示 MLP 的早期使用广度。太阳黑子极大值预测是由 Koons &amp; Gorney 进行的[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C82">82</a> ]。他们发现，基于 MLP 的方法在接受先前周期的训练时能够预测太阳黑子的数量。拜勒-琼斯<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C83">83</a> ]从恒星的光谱中预测了恒星的有效温度。奥尔德<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C84">84</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C85">85</a> ]将MLP应用于宇宙学，证明在给定一组宇宙学参数时，MLP能够预测宇宙微波背景功率谱和物质功率谱。诺加德-尼尔森和约根森 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C86">86</a>] 使用 MLP 去除微波温度图中的前景。从本节讨论的研究中，我们可以看到，当给定专家指南提取的重要参数时，MLP 是天文数据的有效回归器。</p><h3 id="4-当代监督深度学习"><a href="#4-当代监督深度学习" class="headerlink" title="4.当代监督深度学习"></a>4.当代监督深度学习</h3><p>MLP 存在一些问题。主要是它们不能很好地扩展到高维数据集。例如，如果我们的数据集由 128 × 128 像素的图像组成，则仅 MLP 的输入层就需要 16 384 个神经元！当我们进入隐藏层时，这种缩放问题只会变得更糟。此外，由于 MLP 必须将展开的图像作为输入，因此它们会忽略训练图像的任何空间属性，因此要么需要大量的训练数据来分类或生成大图像，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN12">12</a>或专家在预处理步骤中从数据中提取描述性特征。我们可以在上一节中看到这个问题 - 第 3 节中描述的大多数 MLP 应用程序都需要专家从网络数据中提取特征，然后进行训练！这个缺点并不理想；如果原始数据中存在这些精心挑选的统计数据中不存在的特征怎么办？在这种情况下，最好让神经网络将原始数据作为输入，然后学习哪些特征最具描述性。在本节中，我们将讨论同时解决 MLP 缩放问题和专家依赖问题的神经网络架构。在我们总体探索了这些架构之后，我们将在第 5 节中讨论它们在天文问题中的应用。</p><h4 id="4-1-卷积神经网络"><a href="#4-1-卷积神经网络" class="headerlink" title="4.1. 卷积神经网络"></a>4.1. 卷积神经网络</h4><p>与上一节中描述的 MLP 不同，卷积神经网络（CNN；在 Fukushima [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C46">46</a> ] 中引入，并在 LeCun<em>等人</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C93">93</a> ] 中首次与反向传播相结合）并不完全由全连接层组成，其中每个神经元都连接到前一层和后一层中的每个神经元。相反，CNN（如图<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F7">7</a>所示）使用卷积层代替大部分（或全部）密集层。</p><p><img src="https://royalsocietypublishing.org/cms/asset/40dcc939-01dd-4d46-b6b8-6be55850457a/rsos221454f07.gif" alt="图 7."></p><p>图 7.对螺旋星系图像进行分类的卷积神经网络。</p><h4 id="4-2-循环神经网络"><a href="#4-2-循环神经网络" class="headerlink" title="4.2. 循环神经网络"></a>4.2. 循环神经网络</h4><p>标准前馈神经网络（例如 MLP（第 2.2 节）和 CNN（第 4.1 节））会在给定固定大小输入的情况下生成固定大小的向量。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN15">15</a>但是，如果我们想要分类或生成可变大小的向量怎么办？例如，我们可能想根据星系的旋转曲线对其形态进行分类。旋转曲线描述了星系可见恒星的速度与它们距星系中心的距离的关系。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F8">图8</a>显示了 Messier 81 可能的旋转曲线。旋转曲线的长度取决于其星系的大小，并且由于长度可变，以及 MLP 采用固定大小的输入这一事实，我们无法轻松使用 MLP 进行分类。然而，循环神经网络 (RNN) 可以采用可变长度输入并产生可变长度输出。RNN 与前馈 MLP 的不同之处在于它有一个隐藏状态，充当先前看到的信息的“记忆”存储。当 RNN 遇到新数据时，其权重通过时间反向传播算法（BPTT；[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C97">97</a> ]和其中的参考文献。另见脚注 9）进行改变。</p><h4 id="4-3-解决梯度消失问题"><a href="#4-3-解决梯度消失问题" class="headerlink" title="4.3. 解决梯度消失问题"></a>4.3. 解决梯度消失问题</h4><p>20 世纪 90 年代初，研究人员发现了通过反向传播训练深度神经网络的一个主要问题。Hochreiter 在他们的毕业论文中首次正式研究了“梯度消失”问题（Hochreiter [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C98">98</a> ]，另请参阅 Bengio<em>等人</em>后来的工作[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C99">99</a> ]）。由于梯度消失问题，人们普遍认为通过反向传播从头开始训练非常深的人工神经网络是不可能的。在本节中，我们将探讨什么是梯度消失问题，以及当代端到端训练的神经网络如何回避这个问题。</p><p>随着我们深入网络，通过反向传播的学习速度会减慢。这个问题再次导致人们对联结主义模型失去信心，迎来第二个人工智能冬天。直到2012年，新的繁荣才开始。在下面的三个小节中，我们将探讨一些针对梯度消失问题提出的部分解决方案，并展示它们如何结合在一起为当前的深度学习热潮做出贡献。</p><h5 id="4-3-1-非饱和激活函数"><a href="#4-3-1-非饱和激活函数" class="headerlink" title="4.3.1. 非饱和激活函数"></a>4.3.1. 非饱和激活函数</h5><p>我们可以从方程（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454M4x8">4.8</a>）和（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454M4x7">4.7</a>）中看到，如果 φ′ &#x3D; 1，那么乘积项不会自动趋于零或无穷大。如果是这样的话，为什么不简单地围绕这个属性设计我们的激活函数呢？修正线性单元 (ReLU; [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C46">46</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C47">47</a> ]) 是一个激活函数，它正是执行此操作，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN17">17</a></p><p><img src="https://royalsocietypublishing.org/cms/asset/c1f2d96d-144d-4e06-8f46-77c19d5f354b/rsos221454f35.gif" alt="显示公式"></p><p>如果输入大于零，则 ReLU 的梯度为 1，这正是我们缓解梯度消失问题所需的属性。类似的非饱和激活函数也共享 ReLU 梯度的有用属性，例如参见<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F6">图 6</a>中的指数线性单元、Swish 和 Mish 函数。</p><h5 id="4-3-2-图形处理单元加速"><a href="#4-3-2-图形处理单元加速" class="headerlink" title="4.3.2. 图形处理单元加速"></a>4.3.2. 图形处理单元加速</h5><p>如果我们可以加快训练速度，我们就可以运行低效算法（例如通过饱和激活进行反向传播）以在更短的时间内完成训练。加速训练的一种方法是使用专门适合神经网络训练的硬件。图形处理单元 (GPU) 最初是为了渲染视频游戏和其他密集型图形处理任务而开发的。这些渲染任务需要具有大规模并行能力的处理器。我们在前面的章节中已经看到，通过反向传播训练的神经网络也需要许多小的权重更新计算。考虑到这一点，尝试使用 GPU 加速深度神经网络就很自然了。</p><p>2004 年，Oh &amp; Jung [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C102">102</a> ] 率先使用 GPU 加速 MLP 模型，报告使用“ATI RADEON 9700 PRO”GPU 加速神经网络的推理性能提高了 20 倍。不久之后，Steinkrau<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C103">103</a> ]表明反向传播也可以受益于 GPU 加速，报告称训练和推理的性能提高了三倍。这两项突破之后，该领域开展了一系列活动（例如[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C104">104</a> - <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C107">107</a> ]），最终在 ImageNet 2012 上 GPU 加速神经网络取得了里程碑式的胜利。AlexNet [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C108">108</a> ]赢得了 ImageNet 分类和本地化挑战[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C109">109 ]</a>]，取得了前所未有的 top-5 分类错误 16.4%，单个目标定位错误 34.2%。在这两项挑战中，AlexNet 的得分都比第二名的模型高出了 10% 以上。Sutskever 和 Hinton 的获胜网络是通过反向传播 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C40">40</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C93">93</a> ] 训练的CNN [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C46">46</a> ] ，并使用 ReLU 激活 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C47">47</a> ] 和 dropout [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C110">110</a> ] 作为正则化器。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN18">18</a>GPU 加速训练带来的性能提升使网络能够在合理的时间内通过反向传播从头开始训练。发现可以使用现成的硬件从头开始训练神经网络，最终导致了联结主义第二个冬天的结束，并迎来了 2010 年代中后期和 2020 年代的寒武纪式深度学习爆炸（图 10 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F10">）</a>）。</p><p><img src="https://royalsocietypublishing.org/cms/asset/66e23b08-fa55-4363-8d1a-4538eb8a8676/rsos221454f10.gif" alt="图 10."></p><p>图 10.如果我们绘制训练神经网络模型所需的浮点运算 (FLOP) 总数，并将其与模型的发布日期进行比较，我们可以看到 2012 年左右的趋势变化。这对应于GPU 加速的超深度神经网络训练，2012 年标志着人工智能的“深度学习时代”和天文学第二波联结主义的开始（§5）。数据取自 Sevilla<em>等人。</em></p><h5 id="4-3-3-门控循环神经网络和残差网络"><a href="#4-3-3-门控循环神经网络和残差网络" class="headerlink" title="4.3.3. 门控循环神经网络和残差网络"></a>4.3.3. 门控循环神经网络和残差网络</h5><p style="color:#FF0000;">略过关于resnet和GRU的介绍</p><h5 id="4-4-注意力和transformer"><a href="#4-4-注意力和transformer" class="headerlink" title="4.4 注意力和transformer"></a>4.4 注意力和transformer</h5><p style="color:#FF0000;">略过关于注意力和transformer的介绍</p><h3 id="5-天文学的第二波联结主义"><a href="#5-天文学的第二波联结主义" class="headerlink" title="5.天文学的第二波联结主义"></a>5.天文学的第二波联结主义</h3><p>与经典联结主义方法相比，第 4 节中概述的<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN25">25</a>深度学习不需要提取涌现参数来训练其模型。CNN 特别适合观察基于图像的数据中的原始信息。同样，RNN 非常适合观察时间序列内的完整原始信息。天文学中这两种类型的数据都很丰富，在本节中，我们将回顾 CNN、RNN 和 Transformer 模型在天文数据中应用的历史。</p><h4 id="5-1-卷积神经网络应用"><a href="#5-1-卷积神经网络应用" class="headerlink" title="5.1. 卷积神经网络应用"></a>5.1. 卷积神经网络应用</h4><p><em>克里热夫斯基等人</em>不久后就提出了这一点。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C108">108</a> ] 将 CNN 建立为事实上的图像分类网络，引起天文学家的注意：2014 年，它们作为方法集合的一部分被应用于寻找脉冲星 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C129">129 ]。</a>朱<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C129">129</a> ]发现他们的集成非常有效，他们的测试集 pulsar 候选者 100% 在 90 008 个测试候选者中排名前 961。不久之后，哈拉 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C130">130</a>] 描述了使用一维 CNN 解决三元分类问题。他们发现他们的模型能够以令人印象深刻的精度将一维光谱分类为类星体、星系和恒星。CNN 也被广泛应用于星系形态分类。第一个到达现场的是 Dieleman<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C131">131</a> ]。[他们使用 CNN 对星系图像中 Galaxy Zoo 数据集 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C132"> 132</a> ]中定义的星系形态参数进行分类。他们通过 SDSS 观察星系，发现星系动物园标签与 CNN 分类之间有 99% 的一致性。韦尔塔斯公司<em>等。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C133">133 ]表明Dieleman</a><em>等人</em>引入的CNN 。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C131">131]</a>]同样适用于CANDELS场中星系的形态分类[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C134">134</a> ]。同样，Aniyan 和 Thorat [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C135">135</a> ] 表明 CNN 能够对射电星系进行分类。<em>Dieleman等人</em>的联合工作。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C131">131</a> ]，Huertas-Company<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C133">133</a> ] 和 Aniyan &amp; Thorat [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C135">135</a> ] 证实 CNN 同样适用于视觉上不同的调查，几乎不需要修改。<em>王尔德等人</em>把目光放得更远一些。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C136">136</a> ]使用深度CNN模型对模拟透镜事件进行分类。他们还使用遮挡映射对其数据应用了一些可解释性技术[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C137">137</a> ]、梯度类激活映射[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C138">138</a> ]和Google的DeepDream证明CNN确实通过观察引力透镜进行分类。还使用了替代的 CNN 模型，例如 U-Net（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F12">图 12b </a><em>）</em>。U-Net 最初是为了分割生物图像而开发的[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C120">120</a> ]。它在天文学中的首次使用是相关的：Akeret<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C139">139</a> ]使用U-Net[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C120">120</a> ]CNN通过分段隔离并最终消除射电望远镜数据中的射频干扰。同样，Berger 和 Stein [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C140">140</a> ] 使用了三维 U-Net（V-Net；[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C141">141]</a>]）在模拟中预测和分割出星系暗物质晕，Aragon-Calvo [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C142">142</a> ] 使用 V-Net 分割出构成宇宙大规模结构的宇宙细丝和墙壁。Hausen &amp; Robertson [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C143">143</a> ] 证明 U-Net 能够对 HST&#x2F;CANDELS 图像中的对象进行像素语义分类，从而证明 U-Net 能够直接在大型成像调查中发挥有用的作用，特别是在重叠的去混合方面物体，这是深度成像中长期存在的挑战。Lauritsen 等人的 U-Net <em>。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C144">144]</a>] 用于超分辨模拟亚毫米观测。他们发现，当使用由 L1 损失和测量预测点源与地面真实点源之间距离的自定义损失组成的损失时，U-Net 可以成功做到这一点。乔马<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C145">145</a> ] 是第一个证明图卷积神经网络（GCNN）在天文背景下有用的人。他们展示了他们的三维 GCNN 可以对来自 IceCube 中微子观测站的信号进行分类，并发现它的性能优于经典方法和标准三维 CNN。维拉纽瓦-多明戈<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C146">146</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C147">147 ]</a>] 证明 EdgeNet（一类 GCNN）可以在给定宿主星系的位置、速度、恒星质量和半径时估计晕质量 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C148">148</a> ]。作者还证明 EdgeNet 可以估计仙女座星系和银河系的光晕质量。我们必须从本小节中描述的研究中得出结论，CNN 是基于图像的天文数据的有效分类器和回归器。</p><h4 id="5-2-循环神经网络应用"><a href="#5-2-循环神经网络应用" class="headerlink" title="5.2. 循环神经网络应用"></a>5.2. 循环神经网络应用</h4><p>RNN 最初应用于离我们很近的天文学领域；奥塞姆<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C149">149</a> ] 预测了欧洲南方天文台甚大望远镜观测的大气视宁度，并且在 1990 年代中后期和 2000 年代初也探索了根据太阳风数据预测地磁暴的方法（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C151"> 150 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C150">,</a> 151 []</a>和同一小组的其他工作；[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C152">152</a> ]）。</p><p>Brodrick 等人在一项有先见之明的研究中首次使用 RNN 进行天文学分类*。*[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C153">153</a> ]。他们描述了类似 RNN 的 Elman 网络的使用 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C154">154</a> ]。他们的 RNN 的任务是寻找人工生成的窄带无线电信号，这些信号类似于外星文明可能产生的信号。他们发现他们的模型在测试集上的准确率为 92%，这表明 RNN 可能是寻找外星智慧生物的有用工具。<em>布罗德里克等人</em>十多年后。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C153">153</a> ]，Charnock &amp; Moss[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C155">155</a> ]使用了 LSTM（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F11">图 11</a>）对模拟超新星进行分类。他们描述了两个分类问题。一个是 Ia 型和非 Ia 型超新星的二元分类，另一个是 I、II 和 III 型超新星之间的分类。对于性能最佳的模型，他们报告二元分类问题的准确率超过 95%，三元分类问题的准确率超过 90%。这项研究巩固了 RNN 在天文学分类问题中的有用性。Charnock &amp; Moss [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C155">155</a> ] 随后开展了许多研究使用 RNN 对时间序列天文数据进行分类的项目。现代 RNN 在天文学中的使用的非详尽列表包括：随机采样变星分类 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C156">156</a> ]、系外行星实例分割 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C157">157]</a>]、变星&#x2F;星系序列图像分类[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C158">158</a> ]和伽马射线源分类[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C159">159</a> ]。我们必须从这些研究中得出结论，只要有足够的数据可用，RNN 就是天文时间序列的有效分类器。</p><p>当然，循环网络不仅限于分类；它们也可用于回归问题。首先，Weddell &amp; Webb [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C160">160</a> ] 成功地使用回声状态网络 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C161">161</a> ] 来预测宽视场中目标物体的点扩散函数。卡皮齐<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C162">162]</a>] 使用 RNN 来修复丢失的 NASA 开普勒恒星天体时间序列数据。他们发现他们的模型可以以极高的精度重新创建缺失的时间序列，这表明 RNN 可以内化有关其所训练的恒星的信息。与分类案例一样，关于使用 RNN 解决回归问题的研究在 2010 年代末大量兴起，在这里我们将重点介绍代表 RNN 使用案例范围的这些研究的精选。沉<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C163">163</a> ]使用 LSTM 和基于自动编码器的 RNN 对引力波数据进行去噪，Morningstar<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C164">164</a> ]使用循环推理机来重建引力透镜星系。刘<em>等人。</em>[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C165">165</a> ]使用 LSTM 来预测太阳耀斑活动。从这些研究中，与上面的分类案例类似，我们可以再次得出结论：RNN 是天文时间序列的有效回归器。</p><p>RNN 也被用于一些非常规的情况。例如，库格勒<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C166">166</a> ]使用自动编码RNN（特别是回声状态网络）来提取可变主序星的表示嵌入。他们发现这些嵌入捕获了这些变星的一些新兴特性，例如温度和表面重力，这表明嵌入空间内的聚类可能会产生语义上有意义的变星分类。当我们在第 8 节详细探讨天文学中的表示学习时，我们将重新审视这一研究方向。Smith 等人是深度学习中的思想与天文学中的思想之间更激烈的异花授粉的一个例子*。*[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C167">167]</a>]。他们使用由 CNN 编码器和 RNN 解码器组成的编码器-解码器网络来预测星系的表面亮度剖面。此类神经网络以前广泛用于自然语言图像字幕（captions)中，通过将表面亮度轮廓视为“字幕”，他们的模型能够比之前基于人类代理的经典方法快 100 倍以上。</p><h4 id="5-3-变压器应用"><a href="#5-3-变压器应用" class="headerlink" title="5.3. 变压器应用"></a>5.3. 变压器应用</h4><p><em>虽然 Transformer 最初用于自然语言，但也已被 Parmar等人</em>首先应用于图像。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C168">168</a> ]，以及 Dosovitskiy<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C18">18</a> ]。据我们所知，变形金刚尚未应用于天文图像，但它们已开始在时间序列天文学中得到应用。多诺索-奥利瓦<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C169">169</a> ]使用BERT[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C123">123</a> ]以自监督的方式生成光变曲线的表示空间。莫万<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C170">170</a> ]使用编码变压器对凌日系外行星勘测卫星（TESS，[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C171">171]</a>]）并表明去噪代理任务会产生一个富有表现力的嵌入空间。潘<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C172">172</a> ]还使用变压器模型来分析系外行星的光变曲线。Transformer 席卷了自然语言处理和计算机视觉领域（§9），因此，如果我们从其他领域的趋势推断，我们预计在不久的将来会看到更多 Transformer 应用于天文用例的示例。[我们将在第 9 节中在基础模型（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C173"> 173</a> ]和其中的参考文献）及其未来可能的天文应用的背景下重新审视变压器架构。</p><h4 id="5-4-监督学习的一个问题"><a href="#5-4-监督学习的一个问题" class="headerlink" title="5.4. 监督学习的一个问题"></a>5.4. 监督学习的一个问题</h4><p>监督学习需要高质量的标记数据集来训练神经网络。反过来，这些数据集需要费力的人工干预才能创建，因此监督数据供不应求。人们可以通过促使深度学习模型从完全未标记的数据中收集语义信息来避免这一问题。然后可以通过隐藏的描述性“潜在空间”访问学到的语义信息，然后用于数据生成、分类和回归等下游任务。事实上，本综述之前描述的所有网络都可以重新用于非监督任务，并且在第 6 节和第 7 节中，我们将探索一些不需要监督的深度学习框架。</p><h3 id="6-深度生成建模"><a href="#6-深度生成建模" class="headerlink" title="6.深度生成建模"></a>6.深度生成建模</h3><p>在本节中，我们将讨论天文学背景下的生成建模。与判别模型不同，生成模型显式学习数据集中类的分布（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F16">图 16</a>）。一旦我们了解了数据的分布，我们就可以利用这些知识来生成类似于训练数据集中的新合成数据。在以下小节中，我们将详细探讨深度生成模型的三种流行形式：变分自动编码器（第 6.1 节）、生成对抗网络（第 6.2 节）和基于分数（或扩散）模型系列（第 6.3 节） 。最后，在第 8 节中，我们讨论深度生成模型在天文学中的应用。</p><p><img src="https://royalsocietypublishing.org/cms/asset/f36adb52-507c-4dcd-b1b1-5eb92f412fe0/rsos221454f16.gif" alt="图 16."></p><p>图 16。这里我们展示了一组星系和一组恒星的可能潜在空间表示。潜在（或嵌入）空间是一组对象的压缩表示，其中相似的对象比不相似的对象聚集得更近。虽然这个空间通常是高维的，但这里我们将潜在空间投影到二维上以实现可视化目的。在（<em>a</em>）中，我们看到一个生成模型试图学习包含一组星系和一组恒星的数据集的潜在表示的概率分布。在（<em>b</em>）中，我们看到一个判别模型试图学习区分恒星和星系类型的边界</p><h4 id="6-1-（变分）自动编码器"><a href="#6-1-（变分）自动编码器" class="headerlink" title="6.1. （变分）自动编码器"></a>6.1. （变分）自动编码器</h4><p>自动编码器长期以来一直是神经网络架构的主要内容。<em>在反向传播普及者 Rumelhart等人</em>的姐妹论文中。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C174">174</a> ]演示了自动编码器内的反向传播。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F17">图 17</a>演示了基本的神经网络自动编码器架构。</p><p><img src="https://royalsocietypublishing.org/cms/asset/1ad572f7-b10e-4d1c-bdde-945675eda404/rsos221454f17.gif" alt="图 17."></p><p>图 17.自动编码器 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C174">174</a> ] 处理黑洞图像。<strong>z</strong>是潜在向量，<strong>x</strong>是训练集中的样本。编码器<em>q</em>学习将输入数据编码为潜在向量，而解码器<em>p将</em><strong>z</strong>作为输入并尝试重新创建<strong>x</strong>。</p><p>天真地，人们会认为一旦训练完毕，就可以“仅仅”采样一个新的潜在向量，并通过解码神经网络产生新的图像</p><p>$p (\hat{x}|z)$。我们不能这样做，因为纯粹通过重建损失训练的自动编码器没有动力产生平滑可插值的潜在空间。这意味着我们可以使用标准自动编码器来嵌入和检索训练集中包含的数据，但不能使用标准自动编码器来生成新数据。为了生成新数据，我们需要一个平滑的潜在空间，这是变分自动编码器（VAE，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F18">图 18</a>）通过设计产生的[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C175">175</a> ]。</p><p style="color:#FF0000;">略过关于VAE的介绍</p><h4 id="6-2-生成对抗网络"><a href="#6-2-生成对抗网络" class="headerlink" title="6.2. 生成对抗网络"></a>6.2. 生成对抗网络</h4><p style="color:#FF0000;">略过关于GAN的介绍</p><h5 id="6-3-diffusion模型"><a href="#6-3-diffusion模型" class="headerlink" title="6.3 diffusion模型"></a>6.3 diffusion模型</h5><p style="color:#FF0000;">略过关于diffusion的介绍</p><h3 id="7-表征学习"><a href="#7-表征学习" class="headerlink" title="7. 表征学习"></a>7. 表征学习</h3><p>自监督<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN31">31</a>表示学习最近迅速流行，大量模型被快速连续开发（例如[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C214">214</a> - <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C219">219</a> ]）。本质上，表示学习试图生成复杂的高维数据的语义上有意义的压缩表示（或嵌入）。除了简单地作为压缩设备之外，这些嵌入还可以用于下游任务，例如聚类、异常检测或分类。</p><p>在本节中，我们将描述天文学中流行的两种表示学习方法。第一种方法使用 SimCLR 模型定义的对比学习。第二种方法定义并使用“代理任务”（例如自动编码或下一个值预测）来训练深度学习模型，并从后续训练的网络中提取语义上有意义的表示。</p><h4 id="7-1-对比学习"><a href="#7-1-对比学习" class="headerlink" title="7.1. 对比学习"></a>7.1. 对比学习</h4><p style="color:#FF0000;">略过关于对比学习的介绍</p><h4 id="7-2-通过代理任务学习表征"><a href="#7-2-通过代理任务学习表征" class="headerlink" title="7.2. 通过代理任务学习表征"></a>7.2. 通过代理任务学习表征</h4><p>人们还可以通过代理任务来学习表征。代理任务是与网络最终使用无关的任何任务。然而，在学习执行代理任务的过程中，网络会了解训练集中的数据什么是重要的，什么是不重要的。然后可以以学习表示的形式提取该信息。如果代理任务足够通用，这些表示将包含有关数据集中项目的有用语义信息，然后可用于下游应用程序。</p><p>让我们通过回顾之前在第 4.2 节中讨论的示例来具体化这个过程。让我们想象一下，我们有一大组星系旋转曲线，我们想要从中提取嵌入。我们可以训练一个 LSTM 模型（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F24">图 24</a>）来预测旋转曲线中的下一个项目，该模型只能访问配置文件中的前一个项目。一旦 LSTM 模型接受了此任务的训练，我们就可以输入完整的新旋转曲线，并将最终隐藏状态重新用作代表性嵌入。请注意，此设置不依赖于任何外部标签，仅依赖于旋转曲线本身。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN33">33</a></p><p><img src="https://royalsocietypublishing.org/cms/asset/0955c95a-d19f-4d38-8a11-8d340024b6d9/rsos221454f24.gif" alt="图 24."></p><p>图 24.显示了用于提取旋转曲线表示的假设代理任务。{ <em>x</em> 0 , …, <em>x</em> <em>N</em> } 是星系旋转曲线的一组观测值，按距星系中心的径向距离的顺序排列。{ <em>p</em> 1 , …, <em>p</em> <em>N</em> } 是 LSTM 对应的 { <em>x</em> 1 , …, <em>x</em> <em>N</em> } 预测集。<strong>h</strong>是 LSTM 隐藏状态向量。有关 LSTM 内部工作原理的更多信息，请参见图<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F11">11 。</a>( <em>a</em> ) 在训练时，我们输入星系旋转曲线，并预测其序列中的下一个观测结果。( <em>b</em>）在推断时，我们输入完整的星系旋转曲线，并提取 LSTM 隐藏状态作为曲线的压缩表示嵌入。否则，我们会忽略LSTM 生成的任何输出</p><p>我们可以通过自动编码任务生成嵌入。再次，让我们使用一个天文示例来指定这一点，并说我们想要从一组星系观测中提取嵌入。我们可以为此重新调整变分自动编码器的用途，按照第 6.1 节中所述的正常方式对其进行训练。然而，一旦模型训练完毕，我们将放弃网络的解码部分，只考虑编码器。为了生成嵌入，我们只需将星系图像传递给经过训练的编码器。GAN 可以执行相同的过程（第 6.2 节）。在 GAN 的情况下，我们会在训练后丢弃生成器，并使用鉴别器的倒数第二层输出作为我们的嵌入。</p><p>监督网络也可用于生成嵌入。如果网络经过监督方式训练来对数据进行分类或回归，它将了解该数据的一些属性，以帮助其执行任务。我们可以通过将经过训练的网络的倒数第二层的输出作为嵌入来访问这些学习到的表示。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN34">34</a></p><h3 id="8-天文学的第三次联结主义浪潮"><a href="#8-天文学的第三次联结主义浪潮" class="headerlink" title="8.天文学的第三次联结主义浪潮"></a>8.天文学的第三次联结主义浪潮</h3><p>自 2010 年代中期在天文学中首次亮相以来[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C176">176</a> ]，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN35">35</a>深度生成模型已成为天文联结论中的一个流行子领域。这种流行是由其固有的可扩展性驱动的；由于不需要标记数据，因此可以将这些方法重新用于可能手头的任何数据集。自我监督的联结主义已经存在了更长时间（即[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C227">227</a>]），但最近由于它在处理大量未标记数据集方面的有用性而再次流行起来。本节分为两个主要部分。我们将首先在第 8.1 节中概述深度天文生成模型的历史，并在第 8.2 节中讨论天文表示学习的历史。尽管表示学习只是第 8.2 节中描述的研究的明确目标，但必须强调的是，<em>表示</em> <em>也可以从第 8.1 节中描述的所有深度生成模型中提取。</em></p><h4 id="8-1-深度天文生成模型"><a href="#8-1-深度天文生成模型" class="headerlink" title="8.1. 深度天文生成模型"></a>8.1. 深度天文生成模型</h4><p>捕获真正的天文数据需要准确了解望远镜的行为、设备特征、观测过程中的环境因素和数据缩减技术。这些复杂的步骤通常是针对单独的观察集量身定制的。然而，经典模拟还有一种替代方案：利用特定调查中的示例，可以开发数据驱动的方法，不仅可以模拟天文信号，还可以模拟数据的固有特征。除此之外，经过训练来复制天文观测的深度学习模型的运行成本比经典模拟要便宜得多，因此可以快速生成大量数据；然后可将数据用于大规模天文管道原型设计，帮助开发新的分析方法以及数据集扩充。</p><p>本质上，Regier<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C176">176</a> ]提出使用VAE来模拟星系观测。他们使用来自包含 43 444 个星系的 SDSS 采样数据集的缩小尺寸的 69 × 69 星系片段来训练网络。他们以与第 6.1 节中描述的相同方式训练网络，并发现该网络能够生成与训练集中发现的星系类似的星系。他们还发现他们的网络产生了语义上有意义的嵌入，并指出他们的星系是按方向和形态类型聚集的。Ravanbakhsh 等人也进行了同样的调查*。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C228">228</a> ]，他证明了 VAE 可以用来有条件地生成星系。拉万巴赫什</em>等人。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C228">228]</a>] 还率先使用 GAN 生成星系图像。斯平德勒</em>等人。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C177">177</a> ]使用VAE结合先验高斯混合模型（参见方程（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454M6x2">6.2</a>）和随附文本）来生成星系图像并将其聚类为形态类型。虽然本段之前的研究在训练集中使用了像素尺寸相对较小的图像，但 Fussell &amp; Moews [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C229">229</a> ] 和 Holzschuh</em>等人。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C230">230</a> ]证明了 GAN 能够生成大型高保真星系观测结果。Fussell &amp; Moews [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C229">229</a> ] 通过堆叠式 GAN 架构 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C231">231</a> ] 实现了这一目标，Holzschuh</em>等人。<em>[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C230">230</a> ]使用相关的StyleGAN架构[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C189">189</a> ]达到相同的目的。布托尼埃</em>等人。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C12">12</a> ]使用基于流的模型<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN36">36</a> [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C233">233,234</a> ]有条件地模拟星系观测<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C234">。</a>他们发现，他们的方法可以比以前的分析方法产生更准确的模拟，但代价是推理时间。相关地，史密斯</em>等人。*[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C13">13</a> ]使用扩散模型来生成大型高保真星系。他们在暗能量光谱仪器（DESI，[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C235">235</a>]）。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C237">一个是</a>在 SDSS 数据版本 7 [81、236、237] 中编录的一组 306 006 个星系<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C81">，</a>另一个是在河外巡天的光度学和旋转曲线观测（PROBES <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C236">、</a> [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C238">238]</a>]) 数据集。PROBES 包含分辨率良好的星系，这些星系表现出旋臂、条形和晚型星系的其他特征。他们发现，他们的模型产生的星系在质量和统计上都与训练集中的星系没有区别，这证明扩散模型是用于天文模拟的更成熟的 GAN 和 VAE 模型的竞争替代品。从所有这些研究中，我们可以得出结论，深层生成模型可以内化能够在物理和形态上描述星系的模型。</p><p>生成模型也被用来模拟更大规模的天文数据。在与星系生成相关的用例中，Smith &amp; Geach [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C239">239</a> ] 表明 Spatial-GAN [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C240">240</a> ] 可以模拟任意宽的领域调查。他们在哈勃极限深场上进行训练，发现在模型的合成深场中“检测到”的星系在统计上与真实的星系没有什么区别。罗德里格斯等人也探索了宇宙学模拟*。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C241">241</a> ]使用 GAN 快速生成宇宙网模拟，Mustafa</em>等人。<em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C242">242</a> ]以比经典模拟更快的速度生成弱透镜收敛图。除了 GAN，Remy</em>等人。*[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C243">243</a> ] <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN37">37</a>在 MassiveNus 的模拟地图上训练 SBGM [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C245">245</a> ]，并发现他们的模型能够复制这些地图。他们还证明，他们的模型能够在后验预测中产生可能的分布。最后，他们证明 SBGM 能够预测真实的哈勃宇宙演化巡天（COSMOS）场的质量图 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C246">246</a> ]。</p><p>GAN 在类似 Pix2Pix 的公式中的图像域翻译能力（[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C184">184</a> ]，另见图<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F19">19 </a><em>b</em>）在天文学中特别有用。沙温斯基<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C247">247</a> ]首先通过训练类似 Pix2Pix 的模型来对天文数据进行去噪来演示这种用途。他们在从 SDSS 采样的 4550 个星系上训练了他们的网络。星系被卷积以增加视宁度，并添加了散斑噪声。GAN 的任务就是扭转这一过程。他们发现他们的方法优于盲反卷积和 Lucy-Richardson 反卷积。<em>正如 Stark等人</em>所言，生成模型还能够分离来源。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C248">248]</a>] 通过使用 Pix2Pix 模型来演示类星体从其宿主星系的扩展光中发出的点源发射。Reiman &amp; Göhre [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C249">249 ] 使用与 Stark</a><em>等人类</em>似的模型。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C248">248</a> ]去混合重叠的星系。</p><p>在撰写本文时，天文学文献中只有三个基于扩散模型的<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C243">的</a>示例<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C13">13、243、244 </a>。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN38">38</a>令人惊讶的是，这些研究是天文学中基于分数的建模的唯一例子，因为 SBGM 产生的一代可以与最先进的 GAN 模型相媲美，并且没有其他模型中存在的缺点（例如在VAE，或者 GAN 中的模式崩溃和训练不稳定）。SBGM 在天文数据管道中也有一些自然用途。例如，类似于 Sasaki<em>等人的实现。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C206">206</a> ]可用于类似于 Buncher<em>等人的调查到调查的光度测量转换。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C254">254</a> ]。[Jayaram &amp; Thickstun <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C207"> 207</a> ]中描述的源图像分离模型具有作为天文物体去混合器<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C249">的</a>明显应用（即<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C255"> 248、249、255 [] </a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C248">）</a>。总而言之，天文学界开发 SBGM 的时机已经成熟，我们希望在未来几年看到人们对这一领域的浓厚兴趣。</p><h4 id="8-2-自监督天文表示学习"><a href="#8-2-自监督天文表示学习" class="headerlink" title="8.2. 自监督天文表示学习"></a>8.2. 自监督天文表示学习</h4><p>1993 年，Serra-Ricart<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C227">227</a> ]提出使用自动编码器来学习两微米银河巡天观测到的恒星嵌入[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C256">256</a> ]。他们首先证明了他们的自动编码器模型在分离高斯分布的玩具问题上比主成分分析 (PCA) 效果更好，然后他们证明他们的模型在实际数据上也优于经典的 PCA 方法。20 多年后，Graff<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C257">257</a> ] <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN39">39</a>表明自动编码器还能够捕获星系的属性，如映射暗物质挑战中所述 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C258">258</a>] 通过证明从自动编码器中提取的嵌入有利于计算星系的椭圆率作为下游任务。我们不局限于图像；Yang &amp; Li [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C259">259</a> ] 表明，自动编码器可以学习表示，然后可以使用这些表示来训练神经网络，以执行估计恒星大气参数的下游任务，而 Tsang &amp; Schultz [260] 则表明，自动编码器可以<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C260">生成</a>可以然后用于对变星光变曲线进行分类。从这些研究中，我们必须得出结论，通过代理任务训练的神经网络能够学习跨天文领域的语义上有意义的嵌入。</p><p>近，有人将自监督对比学习模型应用于星系图像聚类。哈亚特<em>等人。</em>[ [11 ] 使用 SDSS <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C11"> </a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C81">81</a> ]的多波段星系光度测量训练 SimCLR [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C214">214</a> ] 。他们表明，最终的嵌入通过直接在星系形态分类模型和红移估计模型的训练集中使用它们来捕获有用的信息。同样，萨米恩托<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C261">261</a> ] 对 SimCLR 进行了训练，使用从阿帕奇点天文台巡天测绘附近星系映射中的星系捕获的积分场光谱数据（MaNGA，[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C262">262</a>]）。他们再次发现 SimCLR 生成语义上有意义的嵌入。斯利耶普切维奇<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C263">263</a> ]证明“Bootstrap Your Own Latent”（BYOL，[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C216">216</a> ]）<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN40">40</a>对比学习模型能够学习射电星系的语义上有意义的表示。他们的模型在 100 000 个 Radio Galaxy Zoo 星系上进行训练，并在 1256 个星系的强 Mirabest 数据集上运行推理 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C264">264</a>]。他们发现从他们的模型派生的嵌入在语义上是有意义的，这表明自我监督方法可以在不同的调查之间转移。这些研究表明对比学习适用于图像；需要进一步研究以证明其对其他类型天文数据（例如时间序列和体积数据）的有效性。</p><h3 id="9-基础模型：第四次天体联结主义浪潮？"><a href="#9-基础模型：第四次天体联结主义浪潮？" class="headerlink" title="9. 基础模型：第四次天体联结主义浪潮？"></a>9. 基础模型：第四次天体联结主义浪潮？</h3><p>迄今为止，这篇综述表明深度学习已在天文学中得到广泛应用，这种应用取决于大量计算能力和数据的可用性。本节展望未来，并预测如果天文学继续追随其他应用深度学习领域的脚步，将会出现什么结果。简而言之，我们预测并认为天文联结主义可能会取消精心制作的深度学习模型，取而代之的是无所不包的“基础”模型。在第 9.1 节中，我们探讨了基础模型是什么，以及它们在深度学习中的背景。然后，第 9.2 节将这些模型置于天文学的背景下，并建议我们作为一个社区可以采取的行动来实现天文学基础模型。最后，§9。</p><h4 id="9-1-基础模型"><a href="#9-1-基础模型" class="headerlink" title="9.1. 基础模型"></a>9.1. 基础模型</h4><p>自诞生以来，联结主义一直遵循着更大的计算性和更大的通用性的道路[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C91">91</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C92">92</a> ]。那时，人为的偏见已经被抛在一边，取而代之的是直接从数据中学习的模型和技术。Sutton [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C91">91</a> ]通过语音识别领域举例说明了这个过程：</p><blockquote><p>“在语音识别方面，早在 20 世纪 70 年代就曾有过一项由 DARPA（国防高级研究计划局）赞助的竞赛。参赛者包括许多利用人类知识的特殊方法——单词、音素、人类声道等知识。另一方面是更新的方法，它们本质上更具统计性，并进行更多的计算，基于隐马尔可夫模型（HMM）。统计方法再次战胜了基于人类知识的方法。这导致了整个自然语言处理领域的重大变化，几十年来逐渐发生，统计和计算开始主导该领域。最近语音识别领域深度学习的兴起是朝着这个一致方向迈出的最新一步。深度学习方法对人类知识的依赖更少，并且使用更多的计算，结合大量训练集的学习，可以产生更好的语音识别系统。就像在[计算机围棋和计算机国际象棋]中一样，研究人员总是试图制造出按照研究人员认为自己的思维方式工作的系统——他们试图将这些知识放入他们的系统中——但事实证明，这最终适得其反，并且对研究人员的资源造成了巨大的浪费。当时，通过摩尔定律，大规模计算变得可用，并且找到了充分利用它的方法。</p></blockquote><p>我们看到这一原则通过深度学习的新范式转变再次发挥作用，甚至底层的神经网络架构也不再重要。此前，神经网络通过研究人员注入的归纳偏差来适应特定领域，例如计算机视觉的卷积和语言处理的递归。现在我们看到 Transformer 网络（参见第 4.4 节和 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C117">117</a> ]）在所有应用或其他深度学习领域中展开竞争<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN41">41</a>[ ：从语言处理 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C17"> 17</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C123">123</a> ] <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN42">42</a>到计算机视觉 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C18">18</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C168">168</a> ] 到图形学习 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C267">267</a> ]蛋白质折叠[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C16">16</a> ]到天文学[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C169">169</a>,<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C170">170</a>,<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C172">172</a>]。Transformer 的多功能性使我们能够采用针对一项任务训练的模型，并将其应用于类似但不同的任务，这一过程称为迁移学习。例如，我们可以在预测序列中下一个单词的“代理”任务上训练模型，然后将该模型应用于预测地理问题答案的类似但不同的任务。在此示例中，第一个模型称为“基础”模型，下游模型从中派生。这种设置带来了一些有用的优势。例如，如果基础模型得到改进，所有下游任务也会得到改进。因此，只需要一种模型就可以让研究人员以一种在资源分配给多个项目时不可能实现的方式集中精力。</p><p>为了训练基础模型，我们首先需要定义一个代理任务。由于标记数据集价格昂贵，而原始数据相对便宜，因此最简单且最具可扩展性的方法是通过自我监督学习。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN43">43</a>自监督学习不需要人类提供用于训练的标记数据集。相反，监控信号是根据原始数据自动生成的。例如，在天文学的背景下，此任务可以预测变星光曲线中的掩蔽值[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C169">169</a> ]。另一项任务可能是使用自动编码器（§6.1）来复制星系观测[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C177">177</a>]。进一步的任务可以是在自我监督框架内进行培训，例如对比学习（§7.1）。自监督学习的重要一点是它不需要带注释的数据。这意味着我们可以利用大量的原始数据（例如教科书、抓取的互联网文本、原始图像等）。</p><p>经过大量数据训练的大型模型表现出了令人惊讶的突发行为。例如，GPT-3 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C17">17</a> ] 是一个 1750 亿 (B) 参数模型，可以“提示”执行新任务（有关提示基础模型的更多信息，请参阅<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F25">图 25 ）。</a>这种能力在 GPT-3 的较旧、较小的 1.5B 参数兄弟中根本没有表现出来 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C122">122</a> ]。<em>此外，Wei等人</em>描述的一项荟萃研究。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C269">269</a> ]发现，较大的模型一旦达到一定规模，就会突然“解锁”算术、翻译和修辞格理解等能力。这些发现表明，除了扩展之外，无需进行架构更改即可执行自然语言处理中的许多任务[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C92">92</a>，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C270">270</a> ]。在<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F25">图 25中，我们看到了 Alayrac</a><em>等人</em>的一些结果。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C268">268</a> ]，一个包含LLM和图像编码器的模型。在此图中，我们可以看到该模型能够算术、阅读、计数，并且具有艺术、地理和动物学 44 以及文学的广泛知识（尽管不是“理解” <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN44">）</a>。该模型包含用于编码图像的 ResNet 变体 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C119">119</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C272">272</a> ] 和用于编码和生成文本的 Chinchilla LLM [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C273">273</a> ]。Chinchilla（以及 Flamingo）接受了预测文本序列中下一个单词的代理任务的训练，因此上述的紧急属性都没有被明确优化。</p><p><img src="https://royalsocietypublishing.org/cms/asset/81cee15a-9f23-4689-9f6a-b03ffd828d3f/rsos221454f25.gif" alt="图 25."></p><p>图 25.Flamingo是一个基础模型，能够理解自然语言背景下的图像。在这里，我们看到了火烈鸟新兴能力的一些例子。该图改编自图。1 在 Alayrac<em>等人中。</em></p><p>在下一小节中，我们将陈述并解释天文学基础模型的必要性，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN45">45</a>不仅是为了天文学，也是为了深度学习研究的开放性。</p><h4 id="9-2-扩展法则和数据护城河"><a href="#9-2-扩展法则和数据护城河" class="headerlink" title="9.2. 扩展法则和数据护城河"></a>9.2. 扩展法则和数据护城河</h4><p>霍夫曼<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C273">273 ]建议对 Kaplan</a><em>等人</em>首次提出的基础模型缩放定律进行更新。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C275">275</a> ]。他们的缩放定律方程将神经网络模型的大小和训练数据集的大小与可实现的最小损失联系起来。从数学上来说，方程是<br>$$<br>\mathcal{L_min}(N,D)&#x3D;\frac{A}{N^\alpha}+\frac{B}{D^\beta}+E<br>$$<br>其中<em>E</em>是一个常数，表示给定特定训练数据集时的最低可能损失。<em>N</em>是神经网络中可训练参数的数量，<em>D</em>是以标记为单位的数据集大小（有关标记化的更多信息，请参阅第 4.4 节）。我们可以看到，当我们在无限大数据集（即<em>N</em> &#x3D; <em>D</em> &#x3D; ∞）上训练无限大模型时，剩下的唯一项是“数据集熵”常数<em>E</em>。因此，我们只能通过增加模型的大小或训练集的大小来减少损失。</p><p>拟合上述方程后，Hoffmann<em>等人发现：<br>$$<br>\mathcal{L_min}(N,D)&#x3D;\frac{406.4}{N^{0.34}}+\frac{410.7}{D^{0.28}}+.69<br>$$<br>如果我们随后插入</em>N<em>和</em>D*以选择真实的基础模型，我们将得到<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F26">图 26</a>。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F26">我们可以在图 26</a>中看到，真实基础模型的模型大小项远低于数据集大小项。这意味着数据集大小的增加有可能比更大的模型减少更多的最小损失。因此，进一步改进这些基础模型的下一步显然是增加其数据集大小。</p><p><img src="https://royalsocietypublishing.org/cms/asset/1fcd5843-5bef-4a27-8e3a-413263ee8586/rsos221454f26.gif" alt="图 26."></p><p>图 26.所选基础模型的最小损失之间的比较。上表显示了模型中参数的数量 ( <em>N</em> )、模型训练集中的标记数量 ( <em>D )，以及从方程 (</em> <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454M9x1">9.1</a> )中计算出的相应的涌现项。这里我们使用 Hoffmann<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C273">273</a> ] 获取<em>A</em>、<em>α</em>、<em>B</em>和<em>β</em>的源值。</p><p><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F26">图 26</a>所示的比较中最大的数据集（MassiveText-English；[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C273">273</a> ]）总计 1.4 万亿 (T) 个令牌。然而，该数据集是专有的，仅供谷歌雇用的研究人员使用。在撰写本文时，最大的公共文本数据集是 The Pile [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C279">279</a> ]，总大小约为 260B 个令牌。我们可以通过无限期地从表面网络中抓取文本数据来增加这些数据集的大小，但这些数据往往质量较低。此外，我们已经耗尽了一些重要的高质量数据储备，例如基础研究论文和开源代码[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C280">280</a>]。我们还必须问自己：当生成模型开始大量创建数据并将其不加区别地转储到互联网上时会发生什么？如果从互联网上抓取的数据集中很大一部分文本是通过法学硕士生成的，则对其进行训练将导致不可预见的问题，并可能最终导致模型性能较差。因此，我们必须确保数据不是由深度生成模型生成的。除此之外，学院和广大公众将永远无法访问由字节跳动、谷歌、Meta、微软和其他科技巨头管理的深层网络中包含的大量数据。出于所有这些原因，如果我们想挖掘新的高质量数据，我们就需要跳出框框思考。</p><p>输入多模式基础模型。里德<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C124">124</a> ] <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN46">46</a>证明大型 Transformer 神经网络能够学习许多任务，从玩 Atari 游戏、为图像添加字幕、聊天到操作真正的机器人手臂。该模型在所有任务之间共享权重，并在推理时根据上下文决定要预测哪个任务。重要的是，里德<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C124">124</a> ]发现他们的模型遵循与其他基础模型相同的缩放法则，因此多模态基础模型对数据具有与我们在<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F26">图 26</a>中看到的相同的渴望。更令人惊讶的是，Aghajanyan<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C282">282]</a>]发现一旦神经网络达到一定规模，在连接的独立数据集上训练的基础模型显着优于单独训练的单峰模型。因此，我们可以使用高质量、公开的天文数据来扩充我们的文本数据集。</p><p>维拉鲁宾天文台的 189 16 兆像素 CCD 将每晚观测 1000 个科学帧，同时进行遗留时空勘测 (LSST) [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C283">283</a> ]。如果我们使用与 Dosovitskiy 等人相同的标记化方案，这相当于每晚3 × 10 12 个像素，或者每晚大约 12B 个标记*。*的视觉转换器[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C18">18</a> ]。仅经过 1 年的观察，LSST 将产生 4.4T 的原始数据，甚至比 MassiveText-English 数据集还要大。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN47">47</a>这些数据以及其他类似的天文数据可以编译成一个非常大的开放数据集，类似于 EleutherAI 的 Pile [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C279">279</a>]。该数据集将为大型科技公司以外的学者提供一种训练和研究大型基础模型的方法。对于一个资源相对匮乏的研究小组来说，编译这样的数据集是很困难的，但它可以通过集市式的开放开发来完成[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C284">284</a> ]。我们已经在大型开源项目中看到了这种开发模式的成功，其中最著名的是Linux内核。[EleutherAI（例如 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C279"> 279、285、286</a> ]）以及 HuggingFace 的 BigScience 计划 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C286">287</a> ]也证明了这种开发模型在<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C287">深度</a><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C285">学习</a>领域的有效性。]。一旦编译完成，我们必须确保进展保持公开，并且数据不会简单地吸收到专有数据集中 - 为此，我们必须为我们的数据集提供强大的（病毒式）copyleft 风格许可证。</p><p>一旦数据集编译完毕，我们训练所需的只是一些自我监督的替代任务，供我们的“天文基础”模型尝试。这些任务可能包括预测变星时间序列中的下一次观测、预测星系的低表面亮度剖面、预测星系的形态参数或简单地生成一系列观测中的下一次观测。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN48">48</a>正如我们将在下一小节中探讨的那样，这些代理任务根本不需要与我们最终将使用模型的下游任务相关。经过训练后，我们的 astrofoundation 模型将继承法学硕士享有的所有有趣属性，例如少样本到零样本生成和其他突发行为。</p><h4 id="9-3-天文基础模型的实际意义和用途"><a href="#9-3-天文基础模型的实际意义和用途" class="headerlink" title="9.3. 天文基础模型的实际意义和用途"></a>9.3. 天文基础模型的实际意义和用途</h4><p>本节探讨假设的天体基础模型（第 9.3.1 节）的更广泛含义，以及一些实际的天文学用途（第 9.3.2 节）。在§9.3.3中，我们强调了一项可能对天文学有用的下游任务；天文模拟的条件生成模型。</p><h5 id="9-3-1-基金会模式民主化"><a href="#9-3-1-基金会模式民主化" class="headerlink" title="9.3.1. 基金会模式民主化"></a>9.3.1. 基金会模式民主化</h5><p>2023 年春天<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN49">49</a>随之而来的是全球时代精神的注意力转向基础模型，特别是大型语言模型的 GPT 系列。领先的是 OpenAI 的 ChatGPT，它的发布已成为大型语言模型所拥有能力的公开广告（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F27">图 27</a>）。虽然很有影响力，但我们注意到 ChatGPT “只是”GPT-3 和 GPT-4 版本的 Web 界面包装器，这些版本已使用人类反馈进行了微调 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C292">291 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C291">,</a> 292 []</a>]。因此，ChatGPT 的受欢迎程度表明，人们对深度学习和基础模型有很多潜在的普遍兴趣，并且这种兴趣可以通过令人信服的公开演示来实现。这些模型的完全开放开发和传播也许是最公开的演示。我们确实看到开源基础模型的发布导致了创新和兴趣的爆发。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN50">50</a>一个特殊的例子是“Meta AI 的大型语言模型”的发布和影响（LLaMA；[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C293">293</a> ]）。LLaMA 是开源 LLM 的集合，最大的 LLaMA 具有与 GPT-3 相当的性能。自从 LLaMA 发布以来，整个项目生态系统已经兴起，以创新和有趣的方式使用该模型（例如 [<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C294">294</a> – <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C297">297</a> ]）。类似的故事发生在 2022 年，当时 StabilityAI 发布了基于潜在扩散的开放文本到图像扩散模型 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C94">94</a> ]。[接下来的一系列活动远远超过了 OpenAI 与其竞争的闭源 DALL-E 2 模型 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C203"> 203</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C298">298</a> ]所取得的进展。我们相信，如果开放的天文学基础模型得到有效开发和营销，天文学将会出现与 LLaMA 和稳定扩散模型发布类似的创新爆炸式增长。</p><p><img src="https://royalsocietypublishing.org/cms/asset/115230be-9fee-4b9d-87b0-00ee1e061c71/rsos221454f27.gif" alt="图 27."></p><p>图 27.这里我们显示了术语“GPT”在 Google 搜索中的相对流行度。当 ChatGPT 模型推出供公众使用时，我们可以看到 GPT 的搜索量大幅增加（令人惊讶的是，当 GPT-1、GPT-2 和 GPT-3 论文发布时，搜索量几乎没有增加！） [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C17">17</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C26">26</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C122">122</a>，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C290">290</a> ]。这些数据取自 Google 趋势 ( <a href="https://trends.google.co.uk/">https://trends.google.co.uk/</a> )。</p><p>3月中旬GPT-4发布了[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C26">26</a> ]。其随附的“技术报告”不包含有关模型架构、训练集大小或训练例程的详细信息。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN51">51</a>对于一个历史上建立在开源和开放研究基础上的领域来说，无耻地发布封闭模型是一个相当令人担忧的发展。最令人担忧的是，该领域的行业参与者关闭了商店，作为对 OpenAI 设定的开放&#x2F;封闭模型囚徒困境的反应。如图<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F28">28</a>表明，自 2010 年代中期以来，工业界已经产生了大部分有影响力的深度学习模型；如果未来的发展由于商业压力而被隐藏，我们将看到人才和创新的集中被锁在行业的大门后面。此外，基础建模的最新发展有可能通过普遍的自动化对全球经济和劳动力产生重大影响 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C173">173</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C300">300</a>]。随着自动化程度的提高，大型工业参与者的权力、专业知识和经济影响力的集中将削弱那些无法获得这些技术的人的经济讨价还价地位。这可能会导致社会平衡，获得经济和社会机会的人越来越少。这是一种被 Brynjolfsson [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C301">301</a> ] 模因地称为“图灵陷阱”的平衡：</p><p><img src="https://royalsocietypublishing.org/cms/asset/b8526010-8ace-4a72-a2ce-e552b892f6e8/rsos221454f28.gif" alt="图 28."></p><p>图 28.这里我们显示了学术界和工业界每年产生的被高度引用、最先进或具有历史意义的作品的数量。这些数据来自 Sevilla<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C299">299</a> ]。</p><blockquote><p>原则上，完全自动化的经济可以被构建为广泛地重新分配生产的利益，甚至分配给那些不再严格需要创造价值的人。然而，受益人在阻止分配变化方面处于弱势讨价还价地位，而这种变化使他们几乎一无所有。他们将不稳定地依赖于那些控制技术的人的决定。这为财富和权力的进一步集中打开了大门。</p></blockquote><p>为了避免这个陷阱，我们必须共同努力使基础模型以及自动化的最新成果可供所有人使用。在 Copyleft 数据集上训练的 Copyleft 基础模型（例如我们假设的天文基础模型）将在某种程度上减少大型科技公司与更广泛社会之间日益严重的技术不平等。</p><p>考虑到上述讨论，我们想重新审视第 9.2 节中的简要分析，并重申并强调迫切需要一个独立的、可验证的、完全开放的、强大的 Copyleft 许可替代方案，以替代由 OpenAI、微软、Anthropic 控制的封闭基础模型。 、谷歌和其他大型科技集团。虽然价格昂贵，但计算资源相当容易获得——最重要的问题是基础模型需要大量数据才能有效地训练它们。这些模型通常通过锁定在深层网络中的大量高质量的公开可用的文本数据进行训练。然而幸运的是，第 9.2 节表明，大量有用的多模态数据可以很容易地从天文观测中获得。</p><h5 id="9-3-2-可能的天文用例"><a href="#9-3-2-可能的天文用例" class="headerlink" title="9.3.2. 可能的天文用例"></a>9.3.2. 可能的天文用例</h5><p>在本小节中，我们概述了我们的天体基础模型的一些可能令人兴奋的天文用途。在我们深入研究之前，我们必须声明，这里我们只是粗略地了解了这项技术的潜力，我们希望——正如 LLaMA 和稳定扩散生态系统（第 9.3.1 节）所证明的那样——我们将有更多的用例此处尚未讨论社区参与所产生的问题。我们将本小节分为两部分。第一部分讨论基础模型如何帮助推广、公民科学和跨学科合作，第二部分讨论该模型如何帮助天文学研究。</p><h5 id="9-3-2-1-合作、公民科学和外展"><a href="#9-3-2-1-合作、公民科学和外展" class="headerlink" title="9.3.2.1. 合作、公民科学和外展"></a>9.3.2.1. 合作、公民科学和外展</h5><p>通过提供一个用于生成模拟和分析数据的通用平台，基于神经网络的天文基础模型将简化和促进以前不同领域的研究人员之间的协作。除此之外，底层技术的任何改进都可以轻松集成到特定领域（或与领域无关）的基础模型中，这些模型可用于以前需要多年专业培训才能操作的任务。天文学的一个具体例子是天文模拟。物理感知的天文基础模型可用于模拟和询问模拟的天文事件，其方式与现在的<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C22">经典</a>模拟非常相似[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C20">20-22</a> ]。第 9.3.3 节详细描述了一个可以促进这种模型的框架。</p><p>神经网络的多模态训练使我们能够在数据模式之间建立连接，这在当前方法中是不可能或困难的。作为一个例子，让我们考虑一下公民科学。在像银河动物园这样的公民科学项目中 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C132">132</a>]，公民科学家被要求用定量标签来标记天文物体。对于未经天文学训练的人来说，这可能是一个不直观的过程。具有自然语言意识的天文基础模型将允许参与者用自己的语言描述天文物体。这将减少对专门培训的需求，从而增加这些项目的可及性。人们可以想象一个类似银河动物园的新项目，其中公民科学家提供星系形态的自然语言描述。然后，基础模型可以处理和分析这些描述，这最终将有助于更全面地了解星系演化。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN52">52</a></p><p>具有天文学知识的基础模型可用于开发能够让学生、教育工作者和公众参与有关天文学的对话的聊天机器人。这些聊天机器人可以回答问题、提供解释，甚至可以根据用户的兴趣和先验知识建议个性化的学习资源。这将扩大天文学知识的获取范围并使之民主化，而这种获取天文学知识的便捷方式可以激发并帮助招募下一代天文学家。基金会模型已经可以充当导师，商业演员目前正在这个领域工作；最显着的例子是“Duolingo Max”，它为用户提供了用于外语学习的个性化聊天机器人，以及可汗学院的“Khanmigo”，它为学生的课程提供了个人导师。<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C26">26</a> ]，因此开放的天文学基础模型将比封闭的 GPT- <em>N</em>模型提供更广泛的访问，而后者已被促使具有天文学意识。</p><h5 id="9-3-2-2。加强研究"><a href="#9-3-2-2。加强研究" class="headerlink" title="9.3.2.2。加强研究"></a>9.3.2.2。加强研究</h5><p>虽然基础模型必须在现有数据上进行训练，但其识别数据内的模式和关系的能力可以带来新的知识发现，并提供一种更有效的方法来处理以前困难或耗时的数据。正如前面第 6-8 节中所讨论的，天体连接主义者可以使用基础模型来生成一组天文物体的嵌入。就像我们在第6-8 节中讨论的那样，这些嵌入可以用于下游天文任务，或者可以放入可视化管道中，例如 t 分布随机邻域嵌入方法 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C304"> <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C303">303、304 </a>[]</a>]。由于天文基础模型将是多模态的，研究人员可以组合从完全不同的仪器生成的多个数据集的嵌入，从而使他们能够鸟瞰其数据，</p><p>而这在目前很难实现。我们还可以利用基础模型的新兴能力来发挥我们的优势；<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F25">如图25</a>所示，我们可以使用少样本学习，并使用一些示例输入对来提示经过训练的模型。例如，我们可以使用成对的输入星系观测值和相应的输出表面亮度剖面[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C167">167</a>]。如果天文基础模型是一个小样本学习器（并且知道其训练数据中存在类似的输入输出配对），它将确定研究人员想要计算新星系的表面亮度剖面。然后，研究人员将使用提示的模型作为表面亮度轮廓提取器，从而避免了此类任务对专门分析或深度学习模型的需求。这个过程并不限于这个例子——它适用于基础模型知道的模式内的任何输入-输出对。更好的是，这个过程不需要对基础模型进行重新训练，只需要在推理时进行几次提示。</p><p>自主代理不再是科幻小说；而是现实。当仅由人类操作员给出高级提示时，由基础模型的拟像驱动的任务驱动的自主代理能够解决非常一般的任务[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C305">305</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C306">306]</a>]。因此，人们可以想象一个半自动化的研究管道，其中具有天文知识的自主代理可以通过 API 访问一组天文数据。代理将被提示一个高级研究目标（例如“在这个数据集中找到一些有趣和令人惊讶的东西”），然后将采取措施来完成此任务。这些步骤可能包括查询研究论文以进行文献综述、搜索大型多模态天文数据集以查找支持理论的数据、用额外的拟像来唤起和讨论其发现，或者旋转模拟来检验假设[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C307">307</a>]。当代理在后台运行时，人类研究人员将能够对结果提供高水平的解释，并且将成为为更一般的研究方向提供指导和完善的稳定之手。通过这种方式，天文学基础模型将提供工具，使所有天文学家成为他们自己强大的“人工智能实验室”的首席研究员。</p><h5 id="9-3-3-一类新的模拟"><a href="#9-3-3-一类新的模拟" class="headerlink" title="9.3.3. 一类新的模拟"></a>9.3.3. 一类新的模拟</h5><p>我们希望以我们假设的天体基础模型的实际应用来结束本小节；本着最近文本到图像建模工作的精神（即[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C94">94</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C308">308</a> ]），天文模拟的条件生成模型。如果我们训练无条件生成模型，我们无法在推理时控制其输出。如果我们想要生成特定类别的观测来训练下游任务（例如红移</p><p>作为一个思想实验，让我们考虑一下 Google 最近的“Imagen”模型，<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN53">53</a>并想象如何将其重新用于天文用例（图<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F29">29</a>和<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F30">30</a>，[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C308">308</a> ]）。Imagen 是冻结的 LLM（特别是 T5-XXL；[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C310">310</a> ]）和级联扩散模型（[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C309">309</a>]，另见§6.3）。LLM 充当语言编码器，然后将其生成的潜在空间表示作为条件向量传递到扩散模型上。如果我们用“天文基础”模型取代冻结的法学硕士（参见第 9.1 和 9.2 节），我们就可以利用天文学本质上的多模态性质。例如，如果我们的 astrofoundation 模型经过训练来理解 Galaxy Zoo 2 (GZ2) 形态分类 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C311">311</a> ]，我们可以将 GZ2 描述符作为<strong>y</strong>，将它们相应的星系对作为<strong>x</strong>并对其进行训练。</p><p><img src="https://royalsocietypublishing.org/cms/asset/d8c452eb-f47c-44eb-b46d-e50684589244/rsos221454f29.gif" alt="图 29."></p><p>图 29.选择从文本输入生成的 1024 × 1024 Imagen 样本。每个图像下方是其相应的条件文本。图改编自图。[<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C308"> 308</a> ]中的A.2 。</p><p>经过训练后，我们的天文 Imagen 模型可以生成类似于其所训练的真实星系观测结果的合成星系。然而，与无条件天文模拟器不同，该模型能够生成与共享 GZ2 参数调节集的真实星系特别相似的星系！</p><p><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454M9x2">与方程（ 9.2</a> ）描述的条件模型不同，天文基础型模型允许我们对条件向量进行创造性的处理。例如，我们可以反向运行模型来生成引用非常具体的天文物体的表示，然后生成该“类”的更多物体，并注入卫星遮挡、特定仪器响应函数、特定红移等特征，等（参见 Gal<em>等人</em>关于“文本倒置”的工作。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C312">312</a>]）。这些模拟将使研究人员能够为各种研究目的创建定制的数据集，例如研究特定的星系类型、形态或宇宙学现象。我们甚至可以创建一个“银河动物园”类型的数据集，要求公民科学家通过自然语言描述星系形态（§9.3.2）。这是可能的，因为编码基础模型从根本上并不关心标题采用哪种形式。由于自然语言固有的直观性，这种方法将减少公民科学家的培训成本。此外，由于推理时间生成相对便宜，因此像本节中描述的模型将允许天文学家比使用经典模拟更快地探索和测试假设和场景。</p><h3 id="10-联结主义的警告"><a href="#10-联结主义的警告" class="headerlink" title="10.联结主义的警告"></a>10.联结主义的警告</h3><p>到目前为止，在这篇评论中，我们对天文联结论的潜力非常乐观。然而，这并不意味着联结主义没有陷阱。10.1 节概述了天文联结论的一些实际缺点，并讨论了实践者如何减轻这些缺点。由于其重要性，我们将第 10.2 节专门讨论气候变化和碳排放，并通过对现代大型语言和基础模型的碳排放的案例研究来说明联结主义的影响。</p><h4 id="10-1-可能的实际陷阱"><a href="#10-1-可能的实际陷阱" class="headerlink" title="10.1. 可能的实际陷阱"></a>10.1. 可能的实际陷阱</h4><p><a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F26">如图26</a>所示，深度学习对数据有着永不满足的渴望。获取和标记用于深度学习模型训练的数据可能非常昂贵且耗时。精明的天体连接主义者可以通过不需要标记数据的自我监督或生成学习来缓解这个问题，然后将学习到的嵌入重新用于更专业的下游任务 54（参见§§6-9 <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN54">）</a>。与此相关的是罕见或完全意想不到的天文事件和现象<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#FN55">55</a>根据定义，在任何训练数据中采样都很差，因此深度学习模型将难以概括和内化这些事件。一种解决方案是使用异常检测方法来发现这些罕见现象。我们将读者引向 Pang<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C315">315</a> ]最近对异常检测技术的精彩回顾。</p><p>非常大的深度学习模型的训练和运行推理成本可能很高。一些天文应用，例如检测瞬态事件，需要实时处理大量数据。深度学习模型的计算复杂性可能会给它们在这些时间敏感场景中的部署带来挑战。在这种情况下，最好采用快速、简单、经典的技术或使用较小的深度学习模型。</p><p>天文数据可以通过各种不同的仪器（或模拟）进行观测，并且最终的输出数据可以通过任意数量的后处理管道进行处理。这些管道都有自己的特点、特质和缺陷，因此在通过深度神经网络传播时可能会显得非常不同。此外，调查中已知天体的分布可能会受到观测偏差或历史兴趣的影响，因此需要仔细检查数据集以确保它们能够代表所需的用例。除了护理之外，天体连接主义者还可以采用领域适应技术来确保他们的数据集能够代表其下游任务[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C316">316</a>]。最后，正如我们在第 9 节中探讨的那样，在数据集集合上简单地训练一个非常大的深度学习模型甚至可能就足够了 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C282">282</a> ]，但这种方法目前对于普通研究人员来说是遥不可及的。</p><p>当然，对深度学习长期的批评是可解释性。由于深度学习模型高度参数化，因此很难理解它们为何会做出某种行为或决策。有很多方法可以回避这个问题，本段将简要概述这个方向的一些进展，可能对从业者有用。也许可解释性的黄金标准是神经网络用自然语言逐步引导用户完成其“思维”过程，就像人类所做的那样。大型语言基础模型可以做到这一点，并且这种能力是“免费”的，具有足够大的模型和数据集[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C317">317</a>]。然而不幸的是，目前不存在这样的基础模型，它也具有深厚的天文学知识（§9），所以我们必须更有创意。注意力映射可用于显示深度学习模型在生成输出时关注哪些特征，并且这种注意力映射可以描述为数据的热图。注意力映射可以通过多种方式生成；例如，我们可以使用第 4.4 节中讨论的机制来突出显示输入数据中最有用的部分，以便模型预测或生成其输出。还可以使用类激活映射[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C231">231</a>] 将全卷积神经网络的输出追溯到其输入，以查看输入图像的哪些部分用于预测。遮挡映射（和其他扰动技术）可用于可视化所有架构的注意力。遮挡图要求我们遮挡输入数据的一部分，进而允许我们观察这如何影响输出预测[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C137">137</a> ]。我们还可以将某些统计方法应用于深度学习模型，以深入了解其内部运作方式。在贝叶斯范式（或“贝叶斯神经网络”）内训练的随机神经网络可用于估计神经网络预测中的不确定性[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C318">318</a>]。训练贝叶斯神经网络时，不需要先了解数据集；神经网络可以利用近似贝叶斯计算技术（例如无似然推理）来估计后验[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C319">319</a> ]。除了这些方法之外，许多其他深度可解释性管道也在使用中——远远超出了我们在这里讨论的空间——因此我们强烈推荐 Ras<em>等人。</em>[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C320">320</a> ]对可解释的深度学习领域进行了全面而广泛的概述。</p><h4 id="10-2-联结主义的碳危机"><a href="#10-2-联结主义的碳危机" class="headerlink" title="10.2. 联结主义的碳危机"></a>10.2. 联结主义的碳危机</h4><p style="color:#FF0000;">不看</p><h3 id="11-最后的评论，或者说我们如何学会停止担忧并热爱天文学的大数据时代"><a href="#11-最后的评论，或者说我们如何学会停止担忧并热爱天文学的大数据时代" class="headerlink" title="11. 最后的评论，或者说我们如何学会停止担忧并热爱天文学的大数据时代"></a>11. 最后的评论，或者说我们如何学会停止担忧并热爱天文学的大数据时代</h3><p>重复我们的介绍性陈述：在深度学习渗透的每个领域，我们都看到专业知识的使用减少，取而代之的是从数据中自动派生的知识。我们已经看到这个过程在许多不同的领域中发挥作用，从计算机围棋[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C15">15</a> ]到蛋白质折叠[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C16">16</a> ]，到自然语言处理[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C17">17</a> ]，到计算机视觉[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C18">18</a> ]。这个过程在深度学习社区中已被称为“痛苦的教训”，引文总结了这一原则：</p><blockquote><p>从 70 年的人工智能研究中可以学到的最大教训是，利用计算的通用方法最终是最有效的，而且效率很高。[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C91">91</a> ]</p></blockquote><p>没有理由相信天文学有根本上的不同。事实上，在这篇评论中，我们看到了指向这一结论的叙述（<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454F32">图 32</a>）。天文学中 MLP 的初始工作需要手动选择涌现属性作为输入（例如 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C53">53</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C75">75</a> ]）。随着 CNN 和 RNN 的出现，这些手动选择的输入让位于原始数据摄取（例如 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C131">131</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C155">155</a> ]）。现在，我们看到深度学习方法直接从数据中推断标签和知识，从而消除了人类监督（例如 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C170">170</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C177">177</a>]）。最终，如果天文学追随其他应用深度学习领域的脚步，我们将看到精心设计的深度学习模型被移除，取而代之的是经过微调的包罗万象的“基础”模型<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C173">173 []</a>。这个过程绝不是一件坏事；在天文发现过程中消除人类偏见使我们能够通过机缘巧合找到“未知的未知数”[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C169">169</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C261">261</a> ]。同样，利用数据的能力使我们能够直接生成和询问真实且综合的观察结果，从而避免了昂贵且脆弱的经典模拟的需要[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C13">13</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C239">239</a> ]。</p><p><img src="https://royalsocietypublishing.org/cms/asset/e0b8cdf3-3dc9-42f6-9585-9fe2aa146f52/rsos221454f32.gif" alt="图 32."></p><p>图 32.这里我们看到标题或摘要与图例中给出的术语匹配的 arXiv:astro-ph 提交的数量。我们可以看到三个不同的“波浪”。第一个对应于使用 MLP 的研究 (§§2.1–3)，第二个对应于使用注入原始数据的“深度学习”方法的研究 (§§4.1–5)，第三个对应于使用生成或自我生成的研究-监督模型（§§6–8）。原始数据属于公共领域，可在<a href="https://www.kaggle.com/Cornell-University/arxiv">https://www.kaggle.com/Cornell-University/arxiv</a>             </p><p>上获取。</p><p>天文学的相对数据财富使我们有机会与深度学习研究的前沿形成共生关系，这是一个日益需要数据的领域[ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C92">92</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C280">280</a> ]。机器学习中的许多超大型数据集都是专有的，因此天文学界有机会介入并提供高质量的多模态公共数据集。反过来，该数据集可用于训练天文“基础”模型，该模型可用于最先进的下游任务（例如天文模拟，请参阅§9.3.3）。最后，根据联结主义的最新发展 [ <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C17">17</a> , <a href="https://royalsocietypublishing.org/doi/10.1098/rsos.221454#RSOS221454C273">273</a>] 大多数天文学家缺乏训练该领域前沿模型的资源。如果天文学想要有机会跟上大型科技巨头的步伐，我们必须效仿 EleutherAI 和 HuggingFace 的例子，并以草根式的开源方式集中我们的资源（§9）。我们将此作为对社区的挑战。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>天文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Voice Conversion With Just Nearest Neighbors</title>
    <link href="/2023/20230629/"/>
    <url>/2023/20230629/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://arxiv.org/abs/2305.18975">《Voice Conversion With Just Nearest Neighbors》</a></p><p>demo地址：<a href="https://bshall.github.io/knn-vc/">https://bshall.github.io/knn-vc/</a></p><p>代码地址：<a href="https://github.com/bshall/knn-vc">https://github.com/bshall/knn-vc</a></p><p>简介：任意语音转换旨在将源语音转换为目标语音，仅以目标说话者的几个例子为参考。最近的方法产生了令人信服的转换，但代价是增加了复杂性——使结果难以复制和构建。相反，我们保持简单。我们提出了k近邻语音转换（kNN-VC）：一种简单而有效的任意到任意转换方法。首先，我们提取源语音和参考语音的自监督表示。为了转换为目标说话者，我们将源表示的每个帧替换为其在参考中的最近邻居。最后，预训练的声码器从转换后的表示中合成音频。客观和主观评价表明，kNN-VC以与现有方法相似的可懂度分数提高了说话人的相似性。</p><p><img src="/2023/20230629/model.png"></p><span id="more"></span><p>模型架构：源话语和参考话语被编码为来自预训练的WavLM模型的自监督特征[6]。每个源特征被分配给来自参考的k个最接近特征的平均值。然后使用HiFi GAN对得到的特征序列进行声码，以获得转换后的波形输出。</p><blockquote><p> 原理非常简单，不过个人怀疑需要的声音量可能是别的模型的几倍，由于是相当于直接利用embedding</p></blockquote><p>对于我们所有的实验，我们用均匀的加权设置k&#x3D;4，并使用余弦距离来比较特征。在初步实验中，我们发现kNN-VC在k&#x3D;4左右的值范围内是相当稳健的。也就是说，当有更多的参考音频可用时（例如≥10分钟），甚至可以通过使用更大的k值（按k&#x3D;20的顺序）来提高转换质量。</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>chatgpt在数据集上的性能调查</title>
    <link href="/2023/20230402/"/>
    <url>/2023/20230402/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="http://opensamizdat.com/posts/chatgpt_survey/">Open Samizdat</a></p><hr><h2 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h2><p>chatgpt的流行及其各种令人印象深刻的功能使一些人相信它是在现有系统的语言能力上向前迈出的重要一步，nlp领域很快就会被生成语言模型所吞噬，甚至它预示着AGI。</p><p>为了验证这些说法，我对arXiv预打印进行了调查，将chatgpt与其他方法进行了比较，主要是使用较小的微调模型。</p><p>chatgpt的性能并不像我预期的那样令人印象深刻，因为它经常被更小的模型所超越。</p><span id="more"></span><h2 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h2><p>截至2023-03-23，在arXiv上搜索chatgpt可以找到141篇论文。</p><p>为了过滤掉无关的论文，我忽略了那些似乎不是关于nlp研究的论文，或者那些讨论非性能方面的论文，比如chatgpt的政治倾向。</p><p>然后，我打开剩下的论文，滚动浏览它们，寻找一个可以在chatgpt和其他模型之间进行定量比较的表格或图表。</p><p>我找到了19篇符合这些标准的论文。这19篇论文的表格和图表在下面的附录中，并附有简要的评论。对于每篇论文，我统计了chatgpt比表现最好的非llm方法1获得更好分数的数据集的数量。如果一个数据集报告了多个分数，我会试着选择信息量最大的分数。如果报告了一个数据集的多个分割或变体，我试图找到信息量最大的子集。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>在151次比较中，Chatgpt赢得了34次(22.5%)。<br>在大多数经典的nlp任务中，chatgpt的性能并不优于现有的微调基线，尽管它通常很接近。<br>在某些情况下，它甚至无法击败简单的词袋模型(例如，预测Amin-Affective的亲和性- 58.5 vs 44.8的准确度)，或者它比监督基线差得惊人的显著(例如，Bang-Benchmark中的clutrr关系推理数据集- 48.4 vs 23.3的准确度或kococo - benchmark中的goemotions情绪分类数据集- 52.75 vs 25.55的准确度)。<br>chatgpt与情感任务(kococo - benchmark, Amin-Affective)斗争，它有时比旧的bert-tier模型(Jang-Consistency)表现出更高的脆弱性。<br>我惊讶地发现chatgpt并不擅长文本生成任务，如总结或问题回答(wang - summary, Tan-QA)，尽管人们非常喜欢这些功能。<br>chatgpt在语义相似性任务上似乎不是很强大，但它确实很擅长将生成的文本与引用进行比较(komic - evaluation, Wang-Evaluation)。<br>我认为这两种技能是高度相关的，但我想不是。</p><p>一些论文(<a href="http://opensamizdat.com/posts/chatgpt_survey/#qin-understanding">Qin-Understanding</a>, <a href="http://opensamizdat.com/posts/chatgpt_survey/#wang-robustness">Wang-Robustness</a>, <a href="http://opensamizdat.com/posts/chatgpt_survey/#hendy-translation">Hendy-Translation</a>)也对不同版本的gpt-3进行了比较。<br>结果好坏参半，我在任务中找不到chatgpt获胜或失败的任何模式。<br>这表明在训练过程中有一定程度的脆弱性，其中模型获得了一些能力，但它可能会失去其他能力，<a href="https://arxiv.org/abs/2303.10420">Ye等人2023</a>。<br>当他们证明chatgpt在几个数据集和任务上比text-davinci-003更差时，就证实了这一点，令人惊讶的是，包括小队问答数据集。</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>有几个重要的注意事项需要考虑。</p><p>我已经标记了每个说明是否表明chatgpt可能比报告的强(+)或弱(-)。</p><ul><li><strong>Suboptimal utilization of chatgpt (+)</strong></li></ul><p>提示工程可以显著提高llm的表现(例如，在中理解中，78.7分vs 86.2分)，但这里的一些论文只使用非常基本的提示技术。</p><p>很明显，一些chatgpt实验是在最后一分钟匆忙添加的。</p><p>随着未来人们对chatgpt和适当的提示技术越来越熟悉，性能利用率可能会提高，chatgpt可能会获得额外的胜利。</p><ul><li><strong>Self-selected datasets (+).</strong></li></ul><p>研究人员倾向于设计数据集，这样它们就可以用现有的方法和途径来解决。</p><p>我相信，这就是为什么一些旧的分类数据集仍然具有单词袋方法的竞争性结果。</p><p>经过微调的模型可能会有很好的结果，因为数据集与它们的强项相匹配。</p><p>另一方面，由于缺乏适当的数据集或方法，许多有趣的chatgpt的能力目前难以衡量，因此它们没有包括在本次调查中。</p><ul><li><strong>Data leakage (-)</strong></li></ul><p>完全有可能一些用于评估的数据集被泄露到chatgpt的训练集，这可能会影响结果。</p><p>一些数据集是公开可用的，在使用chatgpt期间可能发生了额外的泄漏。</p><p>实验gpt模型的研究人员通常会提供原始数据，如果他们使用较少的提示，甚至会提供标签。</p><p>目前还不清楚openai对这些数据做了什么，很可能对于许多数据集来说，chatgpt已经被污染了。</p><ul><li><strong>Positive result bias (-).</strong></li></ul><p>当研究人员无法让chatgpt在他们特定的数据集或用例上工作时，他们可能不会报告。</p><ul><li><strong>Weak baselines (-).</strong></li></ul><p>许多用于比较的基线不再是真正的sota模型。<br>对于chatgpt胜出的一些任务，您可能可以找到更好的模型。</p><ul><li>主要是英语。</li></ul><p>通常情况下，chatgpt的评估主要集中在英语语言上。众所周知，多语言评估缺乏适当的数据集。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我对chatgpt的表现有点失望，也许这是不公平的。它没有达到宣传的效果，也没有达到我迄今为止看到的所有精选的例子。它仍然是一个伟大的多面手模型，它可以以某种方式执行许多技能，这是一个奇迹。但这距离真正的语言智能还很远。对于大多数经典的nlp任务来说，小型微调模型并不是那么令人印象深刻，这通常比这更糟糕。<br>我认为vanilla chatgpt将被用作一些应用程序的快速原型，但对于大多数生产就绪的解决方案，它将被一个经过微调的模型(出于经济原因，通常更小)所取代，除非需要自由文本交互。我还想知道他们将来是否会允许对gpt-3.5模型(包括chatgpt)进行微调。Openai(再次)提出了关于他们模型的安全问题，让人们对模型进行微调意味着他们违反了自己的安全措施。我想他们最终会这么做的，就像他们之前的所有模型一样。</p><p>很难预测新模型是否会明显优于chatgpt。在某种程度上，无论是计算方面还是数据方面，扩展范式都遇到了瓶颈。指数增长不可能无限期地持续下去，这一事实不应该那么令人惊讶。我们将不得不等待几个月的类似批次的论文来评估gpt-4，因为作者在他们的论文中除了回答问题之外没有报告任何内容。他们的api页面简单地说明:对于许多基本任务，gpt-4和gpt-3.5模型之间的差异并不显著。然而，在更复杂的推理情况下，gpt-4比我们之前的任何模型都更有能力。<br>这是否意味着扩展完全停止了对基本功能的改进，chatgpt基本上是性能的峰值?另一方面，复杂的推理可能是目前这些模型最有趣的特征，但不幸的是，在本次调查中，它几乎没有。</p><p>看起来gpt-3甚至更老的nlp模型通常具有类似的功能，但是chatgpt得到了所有媒体的关注。我想这说明了用户体验有多重要。人们不关心智能自动补全，因为他们需要摆弄一些模糊的参数，比如温度或top-p。但只要你把它包装成聊天工具，并赋予它一点个性，人们就会为之疯狂。我想知道 <a href="https://galactica.org/">galactica</a>的情况。如果他们不试图以科学界为目标，它可能会像chatgpt一样成功——在科学界，事实性(llms的一个众所周知的弱点)是最重要的价值观之一。</p><p>评估llms正变得越来越困难。gpt模型的作者只报告了少数数据集上的结果，社区需要以不协调的方式测试其余的数据集。数据泄露对评估的有效性是一个持续的威胁，我们可能不得不从头开始重新思考。我可以预见用户研究在未来会被更多地使用，但那将会更加昂贵。Big-bench是一个由442名志愿研究人员共同撰写的大型基准集合，旨在测试llms，由于数据泄漏，对于最受欢迎的llms之一来说，它几乎完全无用。学术界花费数千小时或美元来测试(阅读，为模型所有者提供价值)完全封闭的模型，这有点病态，这些模型可以随时被删除、更新或污染，（我在这里的工作与此类似。讽刺的是，我也知道）。</p><h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h4 id="Bang-Benchmark"><a href="#Bang-Benchmark" class="headerlink" title="Bang-Benchmark"></a>Bang-Benchmark</h4><p>Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie, B., … &amp; Fung, P. (2023). A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. <a href="https://arxiv.org/abs/2302.04023">arXiv preprint arXiv:2302.04023</a>.</p><p><strong>3 out of 21.</strong> This is the most thorough paper in this survey. They compared chatgpt with fine-tuned and zero-shot sota on 21 datasets from 7 tasks: summarization, machine translation, sentiment analysis, question answering, misinformation detection, task-oriented dialogue, and open-domain knowledge-grounded dialogue. chatgpt was able to win only in a small handful of cases. Additionally, they evaluated chatgpt’s multilinguality, multimodality, reasoning capabilities, factuality, and interactivity, but that’s outside of my scope here. There is not much information about their prompt design, and they did not report confidence intervals for the scores, despite calculating them only from a small test sets (mostly 50 samples). Small samples size is actually a problem for many of the papers here, probably because of the limited API access people had.</p><p>I am skeptical about the covid-scientific dataset, which they describe as <em>a testset that consists of covid-19-related scientific or medical myths that must be debunked correctly to ensure the safety of the public.</em> In my experience, it appears that chatgpt was heavily reinforced to align its communication regarding covid-19 in a particular manner and was likely exposed to significant amounts of texts about covid-19 misinformation. The excellent performance on this dataset may be the result of what is essentially a data leak.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Bang-Benchmark.png" alt="img"></p><h4 id="Kocon-Benchmark"><a href="#Kocon-Benchmark" class="headerlink" title="Kocoń-Benchmark"></a>Kocoń-Benchmark</h4><p>Kocoń, J., Cichecki, I., Kaszyca, O., Kochanek, M., Szydło, D., Baran, J., … &amp; Kazienko, P. (2023). ChatGPT: Jack of All Trades, Master of None. <a href="https://arxiv.org/abs/2302.10724">arXiv preprint arXiv:2302.10724</a>.</p><p><strong>0 out of 25.</strong> This <em>benchmarking</em> paper analyzes chatgpt’s performance on 25 datasets from 11 tasks: offensiveness detection, linguistic acceptability, humor recognition, spam detection, word sense disambiguation, natural language inference, question answering, emotion recognition, sentiment analysis, emoji prediction, and stance detection. Some of these tasks are in Polish. chatgpt performed worse in all tasks, often by a significant margin. It particularly struggled with <em>emotion</em> and <em>pragmatic</em> tasks. They used few-shot prompting in some cases (aggressionper and goemoper datasets), while other tasks only had vanilla prompting.</p><p>They calculated some interesting correlations regarding the performance metrics. First, Figure 7 in their paper shows that chatgpt seems to perform worse for more difficult tasks — tasks where sota is further away from 100% performance. This may suggest that chatgpt struggles with long-tail tasks. Second, they estimated the probability of data leaking to chatgpt for each dataset. Most datasets were marked as either <em>probable</em> or <em>highly probable</em>, which is alarming in its own right. Figure 10 shows that datasets with a lower leak probability had worse performance, suggesting that data leak might have inflated the results in some cases. However, I would like to see this without the goemoper tasks, where chatgpt was asked to imitate specific annotators based on 1-3 examples of their annotations. chatgpt performed 30-47% worse than sota on these tasks, and it might have skewed the results.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Koco%C5%84-Benchmark.png" alt="img"></p><h4 id="Qin-Understanding"><a href="#Qin-Understanding" class="headerlink" title="Qin-Understanding"></a>Qin-Understanding</h4><p>Qin, C., Zhang, A., Zhang, Z., Chen, J., Yasunaga, M., &amp; Yang, D. (2023). Is ChatGPT a General-Purpose Natural Language Processing Task Solver? <a href="https://arxiv.org/abs/2302.06476">arXiv preprint arXiv:2302.06476</a>.</p><p><strong>0 out of 7.</strong> This is another paper that compares chatgpt’s performance across a significant number of tasks. Figure 1 is actually misleading since the <em>Fine-tuning</em> models for all the <em>Reasoning</em> tasks (the first two rows of results) are also just language models prompted with <em>chain-of-thought few-shot prompts</em>. Therefore, I only consider the results from the last row where fine-tuned models are actually used. They outperform chatgpt in all cases.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Qin-Understanding.png" alt="img"></p><h4 id="Zhong-Understanding"><a href="#Zhong-Understanding" class="headerlink" title="Zhong-Understanding"></a>Zhong-Understanding</h4><p>Zhong, Q., Ding, L., Liu, J., Du, B., &amp; Tao, D. (2023). Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT. <a href="https://arxiv.org/abs/2302.10198">arXiv preprint arXiv:2302.10198</a>.</p><p><strong>1 out of 8.</strong> This is a comparison on the glue <em>natural language understanding</em> benchmark. They actually use some of the more advanced prompting strategies, including <em>few-shot</em> and <em>chain-of-thought prompts</em>. Their basic prompts were generated by chatgpt, a bizzare decision, as I doubt that chatgpt is self-conscious enough that it’s able to generate optimal prompts for itself. The prompting techniques helped to improve the average performance from 78.7 to 86.2.</p><p>chatgpt arguably outperformed the roberta-large model in 4 out of the 8 tasks reported here. However, in this case, I have decided to compare the performance with the <a href="https://gluebenchmark.com/leaderboard/">glue leaderboard</a> instead, as roberta is a bit outdated by now. Compared to the true sota results (turing ulr v6 model), chatgpt performed better only for sentiment analysis. chatgpt did not perform particularly well for sentiment analysis in the three previous papers, but they all used only vanilla prompts. The authors also discuss the instability of few-shot prompting. The performance for the cola dataset can differ by more than 20% depending on the selected examples.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Zhong-Understanding.png" alt="img"></p><h4 id="Jang-Consistency"><a href="#Jang-Consistency" class="headerlink" title="Jang-Consistency"></a>Jang-Consistency</h4><p>Jang, M., &amp; Lukasiewicz, T. (2023). Consistency Analysis of ChatGPT. <a href="https://arxiv.org/abs/2303.06273">arXiv preprint arXiv:2303.06273</a>.</p><p><strong>3 out of 9.</strong> The authors show that chatgpt is surprisingly <em>brittle</em> when the input texts are perturbed, in some cases even more so than bert-tier models. They tested two text comparison tasks (paraphrase detection and natural language inference) with three types of perturbations:</p><ul><li><strong>Semantic perturbations.</strong> How do the predictions change if we paraphrase one of the inputs? The paraphrases were generated by quilbot or chatgpt. chatgpt changes its prediction 10-30% of the time if we rephrase one of the inputs. It\s more consistent for paraphrases generated by itself.</li><li><strong>Negation perturbations.</strong> How do the predictions change if we negate the input? chatgpt performs better than older models, which are notorious for not understanding negation Kassner &amp; Schütze 2020 . In this case, we expect negation to flip the prediction.</li><li><strong>Symmetric perturbations.</strong> How do the predictions change if we switch the order of the inputs? chatgpt is incredibly inconsistent in this regard, much more so than any of the older models. mrpc is a completely symmetric task (<em>do the two sentences have the same meaning?</em>), but chatgpt changes its prediction based on the order of the two sentences in 12.5% of cases. To improve the results, they had to merge neutral and contradiction labels into one in snli-2c. The fact that the model cannot distinguish between these two concepts is also concerning.</li></ul><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Jang-Consistency1.png" alt="img"><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Jang-Consistency2.png" alt="img"><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Jang-Consistency3.png" alt="img"></p><h4 id="Wang-Robustness"><a href="#Wang-Robustness" class="headerlink" title="Wang-Robustness"></a>Wang-Robustness</h4><p>Wang, J., Hu, X., Hou, W., Chen, H., Zheng, R., Wang, Y., … &amp; Xie, X. (2023). On the Robustness of ChatGPT: An Adversarial and Out-of-Distribution Perspective. <a href="https://arxiv.org/abs/2302.12095">arXiv preprint arXiv:2302.12095</a>.</p><p><strong>8 out of 8.</strong> The <em>brittleness</em> (or robustness) is the main topic of this paper as well. They test two scenarios: (1) <em>adversarial attacks</em> and (2) <em>out-of-domain generalization</em>. chatgpt is the clean winner as it was able to achieve the best results in all cases. I specifically checked for possible data leaks for the adversarial datasets in this case, as this is the paper where chatgpt has the best win ratio. There are a handful of samples leaked on the advglue benchmark website, but the biggest leak is probably <a href="https://huggingface.co/datasets/adv_glue/viewer/adv_mnli/validation">HuggingFace Datasets page</a> where they show 100 samples for each of the subsets tested here. Some of them are quite small (e.g., rte has only 302 samples) and a large portion of the datasets could have been leaked this way. Otherwise, I don’t understand how chatgpt became so much better than <code>text-davinci-003</code> in some cases .</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Wang-Robustness.png" alt="img"></p><h4 id="Wang-Summarization"><a href="#Wang-Summarization" class="headerlink" title="Wang-Summarization"></a>Wang-Summarization</h4><p>Wang, J., Liang, Y., Meng, F., Li, Z., Qu, J., &amp; Zhou, J. (2023). Cross-Lingual Summarization via ChatGPT. <a href="https://arxiv.org/abs/2302.14229">arXiv preprint arXiv:2302.14229</a>.</p><p><strong>0 out of 5.</strong> <a href="http://opensamizdat.com/posts/chatgpt_survey/#qin-understanding">Qin-Understanding</a> and <a href="http://opensamizdat.com/posts/chatgpt_survey/#bang-benchmark">Bang-Benchmark</a> have already shown that chatgpt does not beat the sota models for English <em>summarization</em>. Here, the authors show the same for crosslingual summarization (English to Mandarin and English to German). It could be argued that the evaluation metrics (mainly rouge) do not match the use case perfectly, and a user study should be conducted to see how people react to the outputs. On the other hand, <a href="http://opensamizdat.com/posts/chatgpt_survey/#bang-benchmark">Bang-Benchmark</a> claim that the summaries produced by chatgpt are sometimes longer than the input documents, so it’s hard to believe that chatgpt really gets what this task is about.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Wang-Summarization.png" alt="img"></p><h4 id="Yang-Summarization"><a href="#Yang-Summarization" class="headerlink" title="Yang-Summarization"></a>Yang-Summarization</h4><p>Yang, X., Li, Y., Zhang, X., Chen, H., &amp; Cheng, W. (2023). Exploring the Limits of ChatGPT for Query or Aspect-Based Text Summarization. <a href="https://arxiv.org/abs/2302.08081">arXiv preprint arXiv:2302.08081</a>.</p><p><strong>2 out of 6.</strong> chatgpt was actually able to achieve some <em>summarization</em> wins here, although the tasks are query-based and aspect-based summarization. Perhaps these tasks are better aligned with chatgpt’s training. It wins an aspect-based newts dataset and is also competitive for qmsum, where the task is to summarize a meeting transcript according to a specific query. The <em>golden</em> version only includes the parts of the meeting that are relevant to the input query.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Yang-Summarization.png" alt="img"></p><h4 id="Hendy-Translation"><a href="#Hendy-Translation" class="headerlink" title="Hendy-Translation"></a>Hendy-Translation</h4><p>Hendy, A., Abdelrehim, M., Sharaf, A., Raunak, V., Gabr, M., Matsushita, H., … &amp; Awadalla, H. H. (2023). How Good are GPT Models at Machine Translation? A Comprehensive Evaluation. <a href="https://arxiv.org/abs/2302.09210">arXiv preprint arXiv:2302.09210</a>.</p><p><strong>1 out of 8.</strong> This paper is a pretty robust evaluation of the <em>machine translation</em> capabilities of the gpt models. The one experiment that uses chatgpt is shown in the Figure below. The best performing models from the wmt benchmark outperformed the gpt models for most metrics. The comparison between <code>text-davinci-003</code> and chatgpt is less clear and depends on the language. There are other experiments in the paper, but they do not use chatgpt. The paper is actually exceptionally in-depth, and the follow-up investigation paints a much better picture of the capabilities of gpt models than the basic table shown here.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Hendy-Translation.png" alt="img"></p><h4 id="Jiao-Translation"><a href="#Jiao-Translation" class="headerlink" title="Jiao-Translation"></a>Jiao-Translation</h4><p>Jiao, W., Wang, W., Huang, J. T., Wang, X., &amp; Tu, Z. (2023). Is ChatGPT a Good Translator? A Preliminary Study. <a href="https://arxiv.org/abs/2301.08745">arXiv preprint arXiv:2301.08745</a>.</p><p><strong>1 out of 16.</strong> This is another, in my opinion, weaker, <em>machine translation</em> paper. chatgpt falls behind the available machine translation systems in almost all cases, except for one dataset, wmt20 rob3 — an <em>out-of-distribution</em> test set based on transcribed speech. This is another paper that made the bizarre decision to ask chatgpt for the prompts.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Jiao-Translation1.png" alt="img"><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Jiao-Translation2.png" alt="img"></p><h4 id="Kocmi-Evaluation"><a href="#Kocmi-Evaluation" class="headerlink" title="Kocmi-Evaluation"></a>Kocmi-Evaluation</h4><p>Kocmi, T., &amp; Federmann, C. (2023). Large Language Models are State-of-the-Art Evaluators of Translation Quality. <a href="https://arxiv.org/abs/2302.14520">arXiv preprint arXiv:2302.14520</a>.</p><p><strong>1 out of 1.</strong> This paper has found that chatgpt is an excellent <em>evaluator</em> of translations. gpt models have outperformed existing measures and models in terms of their alignment with human judgements. The performance of individual gpt models depends on the prompt formulation. <code>text-davinci-003</code> works best with a 0-100 scale, <code>text-davinci-002</code> when it has to select 1-5 stars, and chatgpt when it has to select from five text descriptions (e.g., <em>No meaning preserved</em> or <em>Some meaning preserved, but not understandable</em>).</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Kocmi-Evaluation.png" alt="img"></p><h4 id="Wang-Evaluation"><a href="#Wang-Evaluation" class="headerlink" title="Wang-Evaluation"></a>Wang-Evaluation</h4><p>Wang, J., Liang, Y., Meng, F., Shi, H., Li, Z., Xu, J., … &amp; Zhou, J. (2023). Is ChatGPT a Good NLG Evaluator? A Preliminary Study. <a href="https://arxiv.org/abs/2303.04048">arXiv preprint arXiv:2303.04048</a>.</p><p><strong>2 out of 3.</strong> This paper confirms the results from <a href="http://opensamizdat.com/posts/chatgpt_survey/#kocmi-evaluation">Kocmi-Evaluation</a> and shows that chatgpt is great for <em>text evaluation</em>. Instead of machine translation, they use summarization (summeval), story generation (openmeva), and data-to-text (bagel) tasks.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Wang-Evaluation1.png" alt="img"><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Wang-Evaluation2.png" alt="img"><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Wang-Evaluation3.png" alt="img"></p><h4 id="Tan-QA"><a href="#Tan-QA" class="headerlink" title="Tan-QA"></a>Tan-QA</h4><p>Tan, Y., Min, D., Li, Y., Li, W., Hu, N., Chen, Y., &amp; Qi, G. (2023). Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. <a href="https://arxiv.org/abs/2303.07992">arXiv preprint arXiv:2303.07992</a>.</p><p><strong>2 out of 8.</strong> The authors evaluated chatgpt’s performance on 8 <em>question answering</em> datasets, including two multilingual ones. The results showed that chatgpt’s performance varied significantly. It outperformed the sota model by a significant margin for wqsp, but fell completely behind for qald-9. This unpredictability is not surprising, as question answering depends on two factors: (1) the number of answers in the training data, and (2) the number of answers the model memorized. These factors can vary significantly for questions from different domains or languages. The authors also observed that chatgpt is less stable for similar&#x2F;nearly identical inputs (see also <a href="http://opensamizdat.com/posts/chatgpt_survey/#jang-consistency">Jang-Consistency</a>).</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Tan-QA.png" alt="img"></p><h4 id="Omar-QA"><a href="#Omar-QA" class="headerlink" title="Omar-QA"></a>Omar-QA</h4><p>Omar, R., Mangukiya, O., Kalnis, P., &amp; Mansour, E. (2023). ChatGPT versus Traditional Question Answering for Knowledge Graphs: Current Status and Future Directions Towards Knowledge Graph Chatbots. <a href="https://arxiv.org/abs/2302.06466">arXiv preprint arXiv:2302.06466</a>.</p><p><strong>1 out of 4.</strong> This is another paper that evaluates <em>question answering</em>, but they focus on knowledge graphs. chatgpt performs reasonably well on the general knowledge datasets (yago and qald-9), but it fails completely on the academic datasets (dblp and mag). These academic datasets have questions about the virtual academic knowledge graph of authors, publications, and citations. Theoretically, chatgpt has seen most of this graph during the training, but it’s obviously unable to infer this level of information from the raw text data. Compared to <a href="http://opensamizdat.com/posts/chatgpt_survey/#tan-qa">Tan-QA</a>, the non-gpt baselines used here are actually quite weak (the sota models have f1 in 80s for the qald-9).</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Omar-QA.png" alt="img"></p><h4 id="Wei-Extraction"><a href="#Wei-Extraction" class="headerlink" title="Wei-Extraction"></a>Wei-Extraction</h4><p>Wei, X., Cui, X., Cheng, N., Wang, X., Zhang, X., Huang, S., … &amp; Han, W. (2023). Zero-Shot Information Extraction via Chatting with ChatGPT. <a href="https://arxiv.org/abs/2302.10205">arXiv preprint arXiv:2302.10205</a>.</p><p><strong>2 out of 6.</strong> This paper evaluates three <em>information extraction</em> tasks: entity-relation triple extraction (re), named entity recognition (ner), even extraction (ee). They report the results for Mandarin and English, with the first dataset for all three tasks in the table below being Mandarin. They compared fine-tuning smaller models (<em>full-shot</em> for normal fine-tuning or <em>fs-x</em> for few-shot tuning) with vanilla chatgpt prompting (<em>single</em>), and a more complex multi-turn chatgpt dialogue (chatie). Don’t get fooled by the bolded results. The fine-tuned baselines are mostly better. chatgpt performed poorly for ner but was able to outperform full-shot solutions for Mandarin re and ee.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Wei-Extraction.png" alt="img"></p><h4 id="Gao-Extraction"><a href="#Gao-Extraction" class="headerlink" title="Gao-Extraction"></a>Gao-Extraction</h4><p>Gao, J., Zhao, H., Yu, C., &amp; Xu, R. (2023). Exploring the Feasibility of ChatGPT for Event Extraction. <a href="https://arxiv.org/abs/2303.03836">arXiv preprint arXiv:2303.03836</a>.</p><p><strong>0 out of 1.</strong> This paper evaluates <em>event extraction</em> on the ace dataset. The results are calculated only from a handful of samples (20 for each category), there are no confidence intervals, and the f1 score for chatgpt <em>Simple Examples</em> does not make sense given the <em>Precision</em> and <em>Recall</em> values. The authors split the samples based on the event <em>frequency</em> (how many times that event is mentioned in the training dataset) and the sample <em>complexity</em> (how many events are in one sample), but the results are all over the place, likely due to the small size of the test sets.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Gao-Extraction.png" alt="img"></p><h4 id="Amin-Affective"><a href="#Amin-Affective" class="headerlink" title="Amin-Affective"></a>Amin-Affective</h4><p>Amin, M. M., Cambria, E., &amp; Schuller, B. W. (2023). Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT. <a href="https://arxiv.org/abs/2303.03186">arXiv preprint arXiv:2303.03186</a>.</p><p><strong>1 out of 7</strong>. A very straightforward comparison between chatgpt and a set of rather simple baselines for three <em>affective classification</em> tasks: big-five personality prediction, sentiment analysis, and suicide tendency detection. chatgpt’s results are really not impressive and it managed to win only sentiment analysis. In some cases, it was beaten even by a <em>bag-of-words</em> approach. Note that <a href="http://opensamizdat.com/posts/chatgpt_survey/#kocon-benchmark">Kocoń-Benchmark</a> also claim that chatgpt does not work well on emotional tasks. chatgpt managed to win sentiment analysis in this paper and in <a href="http://opensamizdat.com/posts/chatgpt_survey/#zhong-understanding">Zhong-Understanding</a>, but in both cases it was only compared to roberta. When it’s compared with the sota models, it falls behind (<a href="http://opensamizdat.com/posts/chatgpt_survey/#bang-benchmark">Bang-Benchmark</a>, <a href="http://opensamizdat.com/posts/chatgpt_survey/#kocon-benchmark">Kocoń-Benchmark</a>, <a href="http://opensamizdat.com/posts/chatgpt_survey/#qin-understanding">Qin-Understanding</a>).</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Amin-Affective.png" alt="img"></p><h4 id="Kuzman-Genre"><a href="#Kuzman-Genre" class="headerlink" title="Kuzman-Genre"></a>Kuzman-Genre</h4><p>Kuzman, T., Mozetič, I., &amp; Ljubešić, N. (2023). ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification. <a href="https://arxiv.org/abs/2303.03953">arXiv preprint arXiv:2303.03953</a>.</p><p><strong>1 out of 2.</strong> A very straight-forward paper where they compare chatgpt with an xlm-roberta based fine-tuned model for <em>genre classification</em>. They use English (en-ginco) and Slovenian (ginco) datasets. chatgpt performed better on the English one.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Kuzman-Genre.png" alt="img"></p><h4 id="Zhang-Stance"><a href="#Zhang-Stance" class="headerlink" title="Zhang-Stance"></a>Zhang-Stance</h4><p>Zhang, B., Ding, D., &amp; Jing, L. (2022). How would Stance Detection Techniques Evolve after the Launch of ChatGPT?. <a href="https://arxiv.org/abs/2212.14548">arXiv preprint arXiv:2212.14548</a>.</p><p><strong>5 out of 6.</strong> They evaluate the performance on <em>stance detection</em>, which is a task that aims to identify whether a text is in favor of or against something. They used two datasets, the first of which contains texts about the feminist movement (fm), the legalization of abortion (la), and Hillary Clinton (hc). The second dataset is about us politicians. chatgpt outperformed sota in 5 out of 6 splits, with the only exception being the <em>abortion</em> split.</p><p><img src="http://opensamizdat.com/posts/chatgpt_survey/images/Zhang-Stance.png" alt="img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>全世界国家计算机方向博士申请和毕业走向粗略分析</title>
    <link href="/2023/20230401/"/>
    <url>/2023/20230401/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自B站UP主<strong>五云寨主</strong>的<a href="https://www.bilibili.com/video/BV12c411L7KS">《全世界国家计算机方向博士申请和毕业走向粗略分析》</a></p><p>全世界国家计算机方向博士申请和毕业走向粗略分析（新西兰参考澳洲，瑞士参考德国和丹麦）</p><hr><p>视频主要简略介绍全世界各国关于申请读博士的不同的要求以及全世界各个国家的学校的博士毕业的主要的出路。</p><p>澳大利亚和美国主要是Up主待的比较多的地方,但是对别的国家也有所了解。</p><p>内容不算太多。</p><p>简单总结：除了美国就是中国，亚洲这块HK 坡县 本子，hk需要转金融，坡县岗位一般，本子只能去汽车等传统行业且压力最小，土澳和加拿大推荐加拿大，欧陆的德国注重项目，时间久，北欧压力小，带嘤很难（牛津除外），综上：还得是美国。</p><span id="more"></span><h2 id="大陆VS港澳VS-国外"><a href="#大陆VS港澳VS-国外" class="headerlink" title="大陆VS港澳VS 国外"></a>大陆VS港澳VS 国外</h2><h3 id="大陆"><a href="#大陆" class="headerlink" title="大陆"></a>大陆</h3><p>在国内读博士的好处是你第一名的家人很近,朋友很近,你在生活上可能会比较舒适,这是非常大的优点。</p><p>缺点呢,就是说有可能就是国内的博士特别多,然后会比较卷,然后而且在国内的博士里面差距会非常的大,就是说他有一个优点是如果你能够跟一个特别厉害的老板,特别厉害的组,你在这个组里面,你是这个老板比较喜欢的top的学生的话,那么你可能会在国内有很强的一些科研的资源,以及以后未来事业发展的资源,所以说尤其是你如果是在这种情况下想在国内长期的发展,比如说你以后想在国内找老师的职位,然后想申请一些科研基金,以及想在科研的道路上走得很远,主要在国内的市场上发展的话,那么你如果能在这些方向上做得很好,那在国内读博士将会是一个非常好的选择。</p><p>当然,但是也有一个主要原因,是国内的博士特别多,比如说越是非常厉害的老师,他们主力的博士生是越多的,然后他们给每个人花的精力会更少,那这种情况下可能你不一定能捡过别人的情况下,有可能出国会是更好的一个选择。</p><h3 id="港澳"><a href="#港澳" class="headerlink" title="港澳"></a>港澳</h3><p>然后出国呢,首先香港倒是不算出国,但是香港的极所学校的教育模式和国内的会有一点不同。</p><p>第一,香港他们的教育是基本上是英语教育的,然后他们的科研的理念可能比较跟欧美国家以及澳大利亚、英国这些的比较接轨一点,首先它也是英语的教育。</p><p>但是其实在香港,而且香港的学校也非常的好,比如说香港有几所所学校,香港中文大学,香港科技大学,然后这些都是非常非常不错,基本上都是亚洲最顶尖的,如果你能在香港最好的老师那里做,那也是非常好的。</p><p>那在香港读博士,比如说如果计算机博士会有一点点毕业的限制,毕业以后的发展限制,就是说如果你在香港要找到IT方面的就业的工作会比较难,因为香港主要是一个金融城市,所以有很多人他们毕业以后会转入金融行业。</p><p>如果说你想IT就业的话,你需要回到国内大陆再来找,因为大陆的IT会比较发达,但同样也会非常的卷。</p><p>当然另外有一部分香港毕业的同学他们会选择,到第三个国家去找,比如说到美国,到美国的话因为有很多,美国有很多计算机的公司,所以机会会很多。</p><p>但是有一个问题,国外的人在美国找工作你需要一些工作签证,如果你在香港毕业的学生你将会很难拿到美国的工作签证抽签啊这些的会带来巨大的不确定性,然后这是一个比较大的问题。</p><p>当然香港也有一些优点就跟国内类似,比如说你可以靠近你的家人和朋友,你在香港你可以经常坐地铁去深圳找你的朋友和家人,这是一件非常好的事情。</p><h3 id="国外"><a href="#国外" class="headerlink" title="国外"></a>国外</h3><h4 id="东南亚"><a href="#东南亚" class="headerlink" title="东南亚"></a>东南亚</h4><p>另外就是一些比较靠近中国的国家,比如说新加坡,新加坡的学校其实也不错,新加坡是一个很小的,很就是非常干净的环境,非常好的城市,一个国家,然后他们的极数学校非常的好,一个是新加坡国立,还有一个是南洋理工。</p><p>然后他们有很多老师都非常非常强,同样在新加坡读国诗的话,相对于国内的话,他也算离得比较近,你回家也会比较近,而且新加坡的美食,他有很多东南亚的美食,也有很多中国的美食,所以说对于食物这一块会好很多。</p><p>然后新加坡的博士申请几乎跟美国的或者是澳大利亚的很像,需要考GRE,托福亚斯等,然后也是英语教育。</p><p>另外,他的私资学生比例可能会比国内的高校更稍微平衡一点,就是说你可能需要拿到的老师的时间会多一点,这是一件很好的事情。</p><p>但是同样毕业的新加坡也是有一个问题就是,你如果在新加坡境内找IT公司,新加坡是有一些IT公司,相对于香港来说我觉得他的IT公司会多一点,然后有一些国内的IT公司也在搬往新加坡,但是它的发展空间不是太大。</p><p>所以说有一部分人他们还是会选择从新加坡回到中国的更大的市场去就业,所以说这种情况下你跟在国内读博士的同学来比,你会有那么一点点的学术网络上的不同。</p><p>所以说也是一个考虑的,然后新加坡读博士的话,他的毕业要求也挺高的,感觉跟香港是持平的,新加坡那边的老师他们都很厉害。</p><p>另外,但是另外新加坡有一个优点是新加坡的PHD毕业以后应该会比较容易拿到新加坡的他们的永久居民在本地工作是没有太大障碍的。<br>然后还有一份选项是很多新加货币的人也会到美国的一些IT公司去工作,这一点跟香港很像,他们可能也会遇到一些工作签证的问题,所以会增加一些不确定性。</p><h4 id="澳大利亚和加拿大"><a href="#澳大利亚和加拿大" class="headerlink" title="澳大利亚和加拿大"></a>澳大利亚和加拿大</h4><p>然后剩下有几个国家我觉得非常非常类似,一个是澳大利亚,一个是加拿大。</p><p>然后这两个国家其实也是有学读博士首选的几个国家之一。澳大利亚和加拿大有一点比较相似的是他们都是英联邦教育制度,特别是澳大利亚是非常靠近英联邦教育制度的。</p><p>然后加拿大因为靠近美国,所以他介于英联邦和美式教育之间,然后申请加拿大和申请澳大利亚基本上你都需要雅思托福或者GRE,然后他们那边有很多老师科研做的非常不错。</p><p>比如说澳大利亚的一些大学,悉尼大学,莫尔本大学,奥国利大学,然后昆斯兰大学,然后澳大利亚主要有八大国家八大校这些都是比较好的。<br>然后加拿大有很多多伦多大学,然后还有就是华铁卢大学等等,加拿大的还有很多科研工作者都非常厉害。</p><p>然后加拿大的华人是很多的,所以如果你生活在多人多或者是温哥华这些地方的话,在美食和生活上是相对于欧美国家来说是还不错的。<br>但是缺点呢,就是非常的寒冷,你需要适应这种天气。澳大利亚同样也是有很多移民或者是亚洲人。</p><p>同样缺点呢,就是他其实算是优点吧,他气候比较像度假的,就是没有太寒冷的天气。而且大部分澳大利亚的城市都是靠近大海,所以是环境还是不错的。</p><p>同样毕业的条件这两个国家比较相似吧,但有一点点区别是澳大利亚的博士一般来说他们是不上课的,他们是直接进学校就开始做科研。<br>所以他们毕业的时间会跟英国比较像是会比美国要短一年。美国的博士一般是五年毕业,然后加拿大的博士可能也会五年。</p><p>但是澳大利亚可能是三年到四年,因为他们不需要上课,然后不需要上课呢,有一个优点呢,就是你可以直接进入科研,然后集中一些毕业论文等等的。</p><p>缺点呢,就是你没有时间去补一些基础知识,但我个人觉得其实更好的一种方式还是你可以上一些课,然后慢慢的进入科研,这样会更好。<br>当然对不同的同学你的需求不一样,可能这个设置也是不一样的。</p><p>然后同样优点呢,加拿大和澳大利亚是一名国家,所以读完博士以后申请他们国家的永久拘留权是比较容易的,因为他们国家非常缺人产。<br>但是也有一个缺点就是特别是澳大利亚的,澳大利亚本国的IT的公司行业其实不是很发达,所以你即使拿到了澳大利亚的工作签证和他的永久居民,但是你要找到一份很好的工作也不是特别容易给你。</p><p>就是说你要么你就是在学校或者一些研究所里面做科研,要么你就是去工业检。</p><p>但是学校研究所的科研职位并不是特别多大大量,所以其实有一部分人他们毕业以后会选择从澳大利亚去到美国,因为美国的公司会更多。<br>然后这种情况下他们也会有一些问题就是签证的问题,然后拿到工作签证许可的问题,然后这一系列的问题会造成很多不确定性。</p><p>加拿大同样也有这个问题,但是加拿大因为靠近美国,在加拿大的那个温哥华等城市有很多美国的工业界的公司的分布,所以说他们那边相对早工作会比澳大利亚好很多。</p><p>另外他们可能直接到美国来跳槽或者是找美国工业界的工作也会容易很多。</p><p>所以加拿大的选择我个人感觉是优于澳大利亚的。</p><p>另外从国家的福利上来讲,澳大利亚和加拿大这两个国家他们都是非常的就是基于矿争资源人口很稀少,所以他们的国家的福利是比较不错的。</p><p>就是说基础福利是比较好的,比如说澳大利亚他们如果拿到他们的永久居民是他们的居民以后基本上工地医院开密室可以不用花钱的。</p><p>然后如果是小孩上学什么的功利也不用花钱,然后上大学的话国家可以给你贷款,无息贷款基本上也不用花他钱。</p><p>所以是比较就是比较适合居住的一个地方,但是工作的话机会很少,因为他大部分的产业都是很单一的那种。</p><p>比如说煤矿啊矿业这一款,加拿大其实也类似,但加拿大唯一的区别是他靠近美国,所以会有一些机会。</p><h4 id="欧洲"><a href="#欧洲" class="headerlink" title="欧洲"></a>欧洲</h4><h5 id="英国"><a href="#英国" class="headerlink" title="英国"></a>英国</h5><p>然后,欧洲国家其实我不是特别熟悉,但是据我所知英国那边的话,英国其实有很多很好的学校,因为英国是一个历史非常悠久的国家。</p><p>然后比如说剑桥牛津等就不说了,就如果你能拿到这些学校的offer,那你不用考虑以后的出入,就直接去英国就好了。</p><p>然后英国呢,一般学校博士毕业的话,他也是有一些问题就是英国他不是一名国家,所以你要拿到他的永久居民会比较困难一点。</p><p>就也不是说困难,就是时间会花得长一点。</p><p>然后另外英国现在他们的IT产业公司工业界不是那么发达,他有很多是美国的公司的分部,但他招的人可能没有那么多。</p><p>所以其实有很多英国毕业的人,他们可能不是英国最牛逼的学校毕业的,当然英国比如说最牛逼的剑桥啊,牛津啊他去什么地方都没有问题。他们可能也不会选择回国,但回国并不是一个不好的选择,回国当然也是一个非常好的选择,因为我们国家现在发展得非常好。</p><p>就是说回国你要面对更多的竞争,因为你并没有在国内完成你的学业,所以你的社会支援和学业支援导师支援这一块跟国内是有一定的,就是没有完全的接轨,可能会有一定的后悔。</p><p>你需要再重新去建立一些支援,这中间需要花掉一些时间和精力。</p><h5 id="德国"><a href="#德国" class="headerlink" title="德国"></a>德国</h5><p>另外德国,德国是一个非常有意思的国家,德国做科研非常注重你的这个这个项目的扎实的工地,他们不是很喜欢灌水,就说有可能你毕业以后不需要发很多容,但是你需要完成一个项目完整的做完这个项目。</p><p>所以德国的博士生的时间会非常的长,我认识好几个读博士的人,德国最快的也是五年以上,甚至读七年八年的很多,但是他们有一个好处是他们会给这个博士非常多的funding,就让你不用担心做到七年八年都可以。</p><p>另外呢,他们给博士的讲学生跟他们德国相当于一个全职工作可以养活自己和自己的家人都没有太大的关系,所以你在德国这种地方读博士也会压力比较小。</p><p>会尽下心来认真的做一件事情,就是所以你会看到很多德国毕业的博士他们的年龄相对而言会偏大,但是他们其实的工作会非常非常的扎实,就很多很扎实的工作其实都德国的一些大学和研究所做的。</p><h5 id="小众欧洲国家"><a href="#小众欧洲国家" class="headerlink" title="小众欧洲国家"></a>小众欧洲国家</h5><p>然后另外还有一些欧洲国家可能比较小众,比如说丹麦啊,北欧这些国家他可能第一是他这些国家他不是英语国家,然后第二他可能没有太多的华人,可能生活会不长得非常的不方便,比如说买菜啊,生活会非常不方便。</p><p>但是优点的是比如说这些国家他们读国是压力非常小,他们会给你很充足的funding,然后他们读PhD有点像是一个那种把你当成一个工作人员,就是PhD给的那个奖学金可以足够你养活一家人。</p><p>然后另外他们也不怎么加班,他们会给很充足的项目金费等等,就压力非常的小,这也是一种非常好的方式,适合做一些适合让你尽一下心来做一些科研吧,我觉得也是非常好的选择。</p><h4 id="东亚"><a href="#东亚" class="headerlink" title="东亚"></a>东亚</h4><p>然后另外亚洲国家日本也是很有意思啊,日本他他有一个优点就是他跟我们中国很近,然后你可以几乎感觉你就像在国内一样的跟你的朋友他们在交流,因为没有时间,时差。<br>另外日本的学校的科研也做得非常好,比如说东京大学还有诸波大学等等这些都是非常好的学校,然后日本还有一点非常好,就是他们那边读博士的人,因为日本本国的读人并不是很多。</p><p>然后如果去他们那边读博士你也是可以拿到充足的方定,然后并不会就是特别压力很大,他可以让你慢慢的做很多事情,就是日本的一种工匠精神,他们可能跟美国有一点不一样,美国可能会一年一年的看你的,看你的表现,然后来给你方定,但是日本的话他可能会给你很长的方定,然后让你做一些更大了,更慢慢的事情,这点可能会比国内压力小一些。</p><p>另外在美食上日本的美食会非常非常的多,然后另外日本就业的话,因为日本基本上没有什么互联网企业,所以日本就业可以去一些传统,比如说汽车工业等等,然后因为日本他们非常缺劳动力,所以在日本的就业还是过得去的,但是可能待遇到什么都不一定能上升空间,什么的可能比不了过来。</p><p>另外他的语言是日语,所以你需要学习日语,如果你不会日语,你只会英语的话,在日本会非常困难。</p><h4 id="美国"><a href="#美国" class="headerlink" title="美国"></a>美国</h4><p>就如果你要申请美国的话,一般来说你需要考GRE,托福。雅思也是可以接受,但对雅思的分数要求更高,所以最好是考托福。</p><p>美国有疫情以后,有一些学校其实已经取消了GRE,然后美国毕业的话,基本上你第一年要上课,然后后面几年可能做科研,做科研做得比较好的话,就可以顺利的毕业。</p><p>然后美国有些优点就是说美国的IT的互联网金融业这些非常发达,所以就业的选择会非常多。</p><p>然后另外由于你在美国毕业,所以你可以直接拿到美国的工作许可,然后对你在美国上工作会非常的有意义。</p><p>另外美国如果毕业以后你想回中国,你的学历中国也是比较认可的。</p><p>但是美国有一个问题就是美国申请美国的永久居住,就是美国的绿卡,需要的时间可能会比较长。</p><p>即使你是做科研的话,你需要科研做得很好,这样你才能够申请到美国的特殊人才绿卡。</p><p>这种情况下你可以比较迅速的拿到一些东西,拿到绿卡,然后可以在美国比较自由的换工作。</p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>留学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>how to be successful——samaltman</title>
    <link href="/2023/20230330/"/>
    <url>/2023/20230330/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自Openai创始人Sam Altman的博客<a href="https://blog.samaltman.com/how-to-be-successful">how to be successful</a>，以下大部分为谷歌机译（不得不说，谷歌有些地方翻译得确实不好）</p><p>Sam Altman的几条建议确实有帮助，值得阅读和思考，也不像我曾经读过的一些夸夸其谈的人生建议浮于表面。</p><hr><p>我观察了成千上万的创始人，并思考了很多关于赚大钱或创造重要事物所需要的东西。通常，人们开始想要前者，最终想要后者。</p><p>这里有 13 个关于如何取得如此异常成功的想法。一旦您已经达到成功的基线程度（a baseline degree of success），通过特权或努力，并且想要投入工作以将其转化为异常成功，这里的一切都更容易做到。[1] 但其中大部分适用于任何人。</p><span id="more"></span><h2 id="1-复利自己"><a href="#1-复利自己" class="headerlink" title="1.复利自己"></a><strong>1.复利自己</strong></h2><p>复利是神奇的。每个地方都要去寻找它。指数曲线是创造财富的关键。</p><p>一家价值每年增长 50% 的中型企业会在很短的时间内变得庞大。世界上很少有企业具有真正的网络效应和极高的可扩展性。但是随着技术的发展，越来越多的人会这样做。找到它们并创建它们是值得付出很多努力的。</p><p>你自己也想成为一条指数曲线——你的目标应该是让你的生活遵循一条不断增长的向上和向右的轨迹。转向具有复利效应的职业很重要——大多数职业的发展都是线性的。</p><p>你不想从事这样的职业，即从事该行业两年的人与从事该行业二十年的人一样高效（effective）——你的学习率应该始终很高。随着你事业的进步，你所做的每个工作单元都应该产生越来越多的结果。有很多方法可以获得这种影响力，比如资金、技术、品牌、网络效应和管理人员。</p><p>专注于为您定义的成功指标（金钱、地位、对世界的影响或其他）添加另一个零是很有用的。我愿意在项目之间花费尽可能多的时间来寻找我的下一件事。但我一直希望它是一个项目，如果成功，将使我的职业生涯的其余部分看起来像一个注脚。</p><p>大多数人都会陷入线性机会的泥潭。愿意让小机会去专注于潜在的步骤变化。</p><p>我认为商业中最大的竞争优势——无论是对公司还是对个人的职业——都是长期思考，对世界上不同的系统将如何融合在一起有一个广阔的视野。指数增长的一个显着方面是最远的年份是最重要的。在一个几乎没有人采取真正长远眼光的世界里，市场会丰厚地回报那些这样做的人。</p><p>相信指数，保持耐心，然后惊喜不断。</p><h2 id="2-过于自信（Have-almost-too-much-self-belief"><a href="#2-过于自信（Have-almost-too-much-self-belief" class="headerlink" title="2. 过于自信（Have almost too much self-belief)"></a><strong>2. 过于自信</strong>（<strong>Have almost too much self-belief)</strong></h2><p>自信心是非常强大的。我认识的最成功的人几乎相信自己到了妄想（delusion）的地步。</p><p>早早培养这个。当您获得更多数据点表明您的判断力良好并且您可以始终如一地交付结果时，请更加相信自己。</p><p>如果你不相信自己，就很难让自己对未来产生相反的的想法。但这是创造最多价值的地方。</p><p>我记得很多年前埃隆马斯克带我参观了 SpaceX 工厂。他详细地谈到了制造火箭的每一个部分，但让我印象深刻的是，当他谈到要向火星发射大型火箭时，他脸上绝对肯定的表情。我离开时想“嗯，所以这就是信念的基准。”（“huh, so that’s the benchmark for what conviction looks like.”)</p><p>管理你自己的士气——以及你团队的士气——是大多数努力中最大的挑战之一。没有足够的自信，这几乎是不可能的。不幸的是，你越雄心勃勃，这个世界就越想打垮你。  </p><p>大多数非常成功的人在人们认为他们错了的时候至少一次对未来确实是正确的。否则，他们将面临更多的竞争。</p><blockquote><p>这里的意思应该是要（至少一次）抓住别人意识不到的机会</p></blockquote><p>自信必须与自我意识相平衡。我曾经讨厌任何形式的批评并积极避免它。现在我试着总是假设它是真的来听它，然后决定我是否要采取行动。寻求真相是艰难的，而且常常是痛苦的，但这是将自信与自欺欺人区分开来的东西。</p><p>这种平衡也可以帮助你避免自命不凡（entitled）和脱离现实（ out of touch）的情况。</p><h2 id="3-学会独立思考"><a href="#3-学会独立思考" class="headerlink" title="3.学会独立思考"></a><strong>3.学会独立思考</strong></h2><p>创业精神很难教，因为原创性思维很难教。学校不是为了教授这些而设立的——事实上，它通常会奖励相反的东西。所以你必须自己培养它。</p><p>从第一原则开始思考并尝试产生新想法很有趣，找到可以与之交流的人是在这方面做得更好的好方法。下一步是找到简单、快速的方法在现实世界中测试这些想法。</p><p>“我将失败很多次，我将真正正确一次”是创业者的道。您必须给自己很多机会才能获得幸运。</p><p>最重要的教训之一是，你可以在似乎无解的情况下想出该怎么做。你这样做的次数越多，你就会越相信它。毅力来自于知道你可以在被击倒后重新站起来。</p><h2 id="4-善于“销售”"><a href="#4-善于“销售”" class="headerlink" title="4. 善于“销售”"></a><strong>4. 善于“销售”</strong></h2><p>光有自信是不够的——你还必须能够说服其他人相信你的信仰。</p><p>在某种程度上，所有伟大的职业都变成了销售工作。你必须向客户、潜在员工、媒体、投资者等宣传你的计划。这需要鼓舞人心的愿景、强大的沟通技巧、一定程度的个人魅力和执行能力的证据。</p><p>善于沟通——尤其是书面沟通——是一项值得的投资。对于清楚地交流，我最好的建议是首先确保你的思路清晰，然后使用简单明了的语言。</p><p>擅长销售的最好方法是真诚地相信您所销售的产品。推销你真正相信的东西感觉很棒，而试图推销蛇油感觉很糟糕。</p><p>擅长销售就像提高任何其他技能一样——任何人都可以通过刻意练习变得更好。但出于某种原因，也许是因为它令人反感，许多人将其视为无法学习的东西。</p><p>我的另一个重要销售秘诀是在重要的时候亲自出现。当我刚开始的时候，我总是愿意上飞机。这通常是不必要的，但它有三次为我带来了职业生涯的转折点，否则我会走另一条路。</p><p><strong>## 5.让冒险变得容易</strong></p><p>大多数人都高估了风险，低估了回报。承担风险很重要，因为不可能一直都是正确的——你必须尝试很多事情，并在你学到更多东西时迅速适应。</p><p>在职业生涯的早期冒险往往更容易；你没有太多可失去的，而且你可能会得到很多。一旦你达到了承担基本义务的地步，你就应该尽量让冒险变得容易。寻找小赌注，如果你错了，你可以输掉 1 倍，但如果成功，你可以赚 100 倍。然后在那个方向下更大的赌注。</p><p>不过，不要保存（save up）太久。在 YC，我们经常注意到长期在谷歌或 Facebook 工作的创始人存在的问题。当人们习惯了舒适的生活、可预测的工作以及无论做什么都会成功的声誉时，就很难将其抛在脑后（而且人们有一种令人难以置信的能力，可以始终将他们的生活方式与明年的薪水相匹配）。即使他们真的离开了，回来的诱惑也很大。将短期收益和便利置于长期成就之上是很容易的，也是人的本性。  </p><p>但是当你不在跑步机上时，你可以跟随你的直觉，花时间做一些可能真的很有趣的事情。尽可能让你的生活保持廉价（cheap)和灵活(flexible)是做到这一点的有效方法，但显然需要权衡取舍。</p><h2 id="6-专注"><a href="#6-专注" class="headerlink" title="6.专注"></a><strong>6.专注</strong></h2><p>专注是工作的力量倍增器。</p><p>几乎我见过的每个人都会因为花更多时间思考应该关注什么而受益匪浅。做正确的事比长时间工作更重要。大多数人将大部分时间浪费在无关紧要的事情上。</p><p>一旦你弄清楚该做什么，就势不可挡地快速完成你的一小部分优先事项。我还没有遇到一个行动迟缓却非常成功的人。</p><h2 id="7-努力工作"><a href="#7-努力工作" class="headerlink" title="7.努力工作"></a><strong>7.努力工作</strong></h2><p>通过聪明或努力的工作，你可以在你的领域达到前10%时，这仍然是一个伟大的成就。但要达到1%，需要两者兼得——你将与其他非常有才华的人竞争，他们有很好的想法，也愿意付出很多努力。</p><p>极端的人得到极端的结果。大量工作伴随着巨大的生活权衡，决定不这样做是完全理性的。但它有很多优点。在大多数情况下，动力会叠加，成功会带来成功。</p><p>这通常很有趣。生活中最大的乐趣之一就是找到你的目标，擅长它，并发现你的影响比你自己更重要。一位 YC 创始人最近表示非常惊讶，他在离开一家大公司的工作并努力发挥最大可能的影响力后变得更加快乐和充实。努力工作应该受到庆祝。  </p><p>我不完全清楚为什么努力工作在美国的某些地区变成了一件坏事，但在世界其他地区肯定不是这种情况——美国以外的企业家表现出的精力和干劲很快就会消失成为新的标杆。</p><p>你必须弄清楚如何努力工作而不精疲力竭。人们为此找到了自己的策略，但几乎总能奏效的一个方法是找到你喜欢和你喜欢花很多时间在一起的人一起做的工作。</p><p>我认为那些假装你可以在大部分时间（在你生命的某个时期）不工作的情况下在专业上取得超级成功的人是在帮倒忙。事实上，工作耐力似乎是长期成功的最大预测指标之一。</p><p>关于努力工作的另一个想法：在你职业生涯的开始就开始努力。努力工作就像兴趣一样复杂，你越早做，你就有越多的时间来获得回报。当您承担的其他责任较少时，也更容易努力工作，这在您年轻时经常但并非总是如此。</p><h2 id="8-大胆"><a href="#8-大胆" class="headerlink" title="8.大胆"></a><strong>8.大胆</strong></h2><p>我相信做一个艰难的创业比一个简单的创业更容易。人们希望成为令人兴奋的事情的一部分，并觉得他们的工作很重要。</p><p>如果你在一个重要问题上取得了进展，你就会有源源不断的人想要帮助你。让自己变得更有野心，不要害怕从事你真正想从事的工作。</p><p>如果其他人都在开模因（meme）公司，而你想开一家基因编辑公司，那就去做吧，不要再猜了。</p><p>跟随你的好奇心。对你来说令人兴奋的事情对其他人来说往往也会令人兴奋。</p><h2 id="9-任性（Be-Willful"><a href="#9-任性（Be-Willful" class="headerlink" title="9.任性（Be Willful)"></a><strong>9.任性</strong>（Be Willful)</h2><p>一个很大的秘密是，你可以以惊人的比例让世界屈服于你的意志——大多数人甚至不去尝试，只是接受事情本来的样子。</p><p>人们拥有使事情发生的巨大能力。自我怀疑、过早放弃和不够努力的结合使大多数人无法发挥他们的潜力。</p><p>问你想要什么。你通常不会得到它，而且拒绝通常会很痛苦。但是当它起作用时，它的效果出奇地好。</p><p>几乎总是，那些说“我将继续前进直到成功，无论挑战是什么，我都会解决它们”的人，并且是认真的，继续取得成功。他们坚持了足够长的时间，让自己有机会走好运。</p><p>Airbnb 是我的基准。他们讲的故事太多了，我不建议尝试重现（把用完的信用卡放在孩子们用来放棒球卡的九槽三环活页夹页里，每顿饭都吃美元商店的麦片，一场又一场的战斗具有强大的根深蒂固的兴趣，等等）但他们设法生存了足够长的时间，好运如愿以偿。</p><p>要任性，就必须乐观——希望这是一种可以通过实践提高的人格特质。我从未见过一个非常成功的悲观者。</p><h2 id="10-难以与之竞争-Be-hard-to-compete-with"><a href="#10-难以与之竞争-Be-hard-to-compete-with" class="headerlink" title="10.难以与之竞争(Be hard to compete with)"></a><strong>10.难以与之竞争</strong>(<strong>Be hard to compete with</strong>)</h2><p>大多数人都明白，如果公司难以与之竞争，它们就更有价值。这很重要，而且显然是正确的。</p><p>但这也适用于您个人。如果你所做的事情可以由其他人完成，那么它最终会被完成，而且花费更少。</p><p>变得难以与之竞争的最好方法是建立杠杆（ build up leverage）。例如，您可以通过个人关系、建立强大的个人品牌或擅长多个不同领域的交叉点来做到这一点。还有很多其他的策略，但你必须想出一些方法来做到这一点。</p><blockquote><p>不太理解为什么叫建立杠杆。</p></blockquote><p>大多数人会做大多数与他们一起出去玩的人所做的事情。这种模仿行为通常是错误的——如果你在做其他人都在做的事情，你就不会很难与之竞争。</p><h2 id="11-建立网络"><a href="#11-建立网络" class="headerlink" title="11.建立网络"></a><strong>11.建立网络</strong></h2><p>伟大的工作需要团队。建立一个有才华的人际网络来与之共事——有时是紧密的，有时是松散的——是伟大事业的重要组成部分。你认识的真正有才华的人的网络规模往往会限制你的成就。</p><p>建立网络的有效方法是尽可能多地帮助人们。在很长一段时间内，这样做会带来我最好的职业机会和四项最佳投资中的三项。由于十年前我为帮助一位创始人所做的事情，我经常感到好事发生在我身上，这让我一直感到惊讶。</p><blockquote><p>这一说法还是有点意料之外，又意料之中的。</p></blockquote><p>建立人脉的最佳方式之一是树立真正关心与您共事的人的想法（reputation）。过于慷慨地分享好处；它会回到你 10 倍。此外，学习如何评估人们擅长什么，并将他们置于这些角色中。（这是我学到的关于管理的最重要的东西，但我还没有读过很多。）你想要有这样的名声:让人们足够努力，以至于他们完成了比他们想象的更多的事情，但又不会努力到筋疲力尽。</p><p>每个人在某些事情上都比其他人做得更好。用你的长处定义你自己，而不是你的弱点。承认你的弱点并想办法解决它们，但不要让它们阻止你做你想做的事。“我不能做 X，因为我不擅长 Y”是我经常从企业家那里听到的话，而且几乎总是反映出缺乏创造力。弥补你的弱点的最好方法是雇用<strong>互补</strong>的团队成员，而不是只雇用擅长与你相同的人。</p><p>建立网络的一个特别有价值的部分是善于发现未被发现的人才。通过练习，快速发现智力、动力和创造力变得更加容易。最简单的学习方法就是结识很多人，并记录谁给你留下了深刻印象，谁没有。请记住，您主要是看进步的速度，不要高估经验或当前成就。</p><p>当我遇到一个新的人时，我总是试着问自己“这个人是自然的力量（a force of nature）吗？” 对于寻找可能成就伟大事业的人来说，这是一个很好的启发式方法。</p><p>发展人脉的一个特殊情况是，找一个杰出的人在你身上押注，最好是在你职业生涯的早期。毫无疑问，做到这一点最好的方法就是尽你所能地提供帮助。(请记住，你必须在以后的某个时候支付这些钱!)</p><p>最后，记住要与支持你抱负的积极的人共度时光。</p><h2 id="12-你通过拥有东西而致富（You-get-rich-by-owning-things）"><a href="#12-你通过拥有东西而致富（You-get-rich-by-owning-things）" class="headerlink" title="12. 你通过拥有东西而致富（You get rich by owning things）"></a><strong>12. 你通过拥有东西而致富（You get rich by owning things）</strong></h2><p>我小时候最大的经济误解是人们靠高薪致富。尽管有一些例外——例如艺人——但在福布斯榜单的历史上几乎没有人拿到过薪水。</p><p>通过拥有价值迅速增加的东西，你会变得真正富有。</p><p>这可以是企业的一部分、房地产、自然资源、知识产权或其他类似的东西。但不知何故，你需要拥有某些东西的股权，而不是仅仅出卖你的时间。时间仅线性缩放。</p><p>让东西的价值迅速增加的最好方法是大规模地制造人们想要的东西。</p><h2 id="13-内部驱动"><a href="#13-内部驱动" class="headerlink" title="13. 内部驱动"></a><strong>13. 内部驱动</strong></h2><p>大多数人主要受外部驱动；他们做他们做的事是因为他们想给别人留下深刻印象。这很糟糕，原因有很多，但这里有两个重要的原因。</p><p>首先，您将致力于达成共识的想法和达成共识的职业轨迹。如果其他人认为您在做正确的事，您会非常在意——比您意识到的要多得多。这可能会阻止你做真正有趣的工作，即使你做了，其他人也会这样做。</p><p>其次，风险计算通常会出错。你会非常专注于跟上其他人的步伐，而不是在竞争性游戏中落后，即使是在短期内也是如此。</p><p>聪明人似乎特别容易受到这种外部驱动行为的影响。意识到它会有所帮助，但只是一点点——你可能必须非常努力地工作才能不落入模仿陷阱。</p><p>我认识的最成功的人主要是内部驱动的；他们做自己做的事是为了给自己留下深刻印象，因为他们觉得有必要让世界发生一些事情。在你赚到足够的钱来购买你想要的任何东西并获得足够的社会地位以致于获得更多不再有趣之后，这是我所知道的唯一会继续推动你达到更高水平表现的力量。</p><p>这就是为什么一个人的动机问题如此重要。这是我试图了解某人的第一件事。正确的动机很难定义一套规则，但当你看到它时你就会知道。</p><p>Jessica Livingston和Jessica Livingston是我的基准。YC 最初几年被广泛嘲笑，几乎没有人认为刚开始时会取得巨大成功。但他们认为，如果它能奏效，对世界来说将是一件好事，而且他们乐于助人，而且他们坚信他们的新模式比现有模式更好。</p><p>最终，您将通过在对您重要的领域中出色地完成工作来定义您的成功。你越早朝那个方向开始，你就能走得越远。在您不痴迷的任何事情上都很难取得巨大成功。</p><p>[1] 我在HN上写的一条评论回复：</p><p>我对基本收入（basic income）感到兴奋的最大原因之一是它将通过让更多人自由承担风险来释放人类的潜能。</p><p>在那之前，如果你不是天生幸运，你就必须努力爬上一段时间才能大展拳脚。如果您出生在极端贫困中，那么这将非常困难:(</p><p>机会分配如此不均，显然是一种令人难以置信的耻辱和浪费。但我亲眼目睹了足够多的人，他们一出生就面临着糟糕的筹码，并继续取得令人难以置信的成功，从而知道这是可能的。</p><p>我深深地意识到，如果我不是天生幸运的话，我个人就不会成为现在的我。</p><p>感谢 Brian Armstrong、Greg Brockman、Dalton Caldwell、Diane von Furstenberg、Maddie Hall、Drew Houston、Vinod Khosla、Jessica Livingston、Jon Levy、Luke Miles（6 稿！）、Michael Moritz、Ali Rowghani、Michael Seibel、Peter Thiel， Tracy Young 和 Shivon Zilis 审阅了本文的草稿，特别感谢 Lachy Groom 帮助撰写本文。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
      <tag>生活</tag>
      
      <tag>社会</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大型语言模型的意义和理解需要感官基础吗?是的!</title>
    <link href="/2023/20230327/"/>
    <url>/2023/20230327/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自Yann LeCun的<a href="https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view">Do large language models need sensory grounding for meaning and understanding? Spoiler: YES!</a></p><hr><h2 id="机器学习糟透了"><a href="#机器学习糟透了" class="headerlink" title="机器学习糟透了!"></a>机器学习糟透了!</h2><ul><li>(与人类和动物相比)监督学习(SL)需要大量的标记样本。</li><li>强化学习(RL)需要大量的试验。</li><li>自监督学习(SSL)需要大量的未标记样本。</li><li>目前大多数基于ml的人工智能系统:犯愚蠢的错误，不推理也不计划</li><li>动物和人类:<ul><li>可以很快学习新任务。</li><li>知道世界如何运行</li><li>能够推理和计划</li></ul></li><li>人类和动物有常识，而机器却没有那么多常识(这很肤浅)。</li></ul><span id="more"></span><h2 id="自我监督学习-学会填空"><a href="#自我监督学习-学会填空" class="headerlink" title="自我监督学习&#x3D;学会填空"></a>自我监督学习&#x3D;学会填空</h2><h2 id="自回归大型语言模型-AR-LLMs"><a href="#自回归大型语言模型-AR-LLMs" class="headerlink" title="自回归大型语言模型(AR-LLMs)"></a>自回归大型语言模型(AR-LLMs)</h2><ul><li><p>Auto-Regressive Large Language Models (AR-LLMs)</p></li><li><p>token可以表示单词或子单词</p></li><li><p>编码器&#x2F;预测器是一个transformer架构</p></li><li><p>表现惊人，但是会犯愚蠢的错误</p><ul><li>事实错误、逻辑错误、前后矛盾、推理有限、毒性……</li></ul></li><li><p><strong>LLM不了解潜在的现实，他们没有常识&amp;他们不能计划他们的答案</strong></p></li></ul><h3 id="关于AR-LLMs的不受欢迎的观点"><a href="#关于AR-LLMs的不受欢迎的观点" class="headerlink" title="关于AR-LLMs的不受欢迎的观点"></a>关于AR-LLMs的不受欢迎的观点</h3><ul><li>Auto-Regressive LLMs are <strong>doomed.</strong></li><li>它们不可能是真实的、无毒的等等。</li><li>设任何生成的令牌将我们带出正确答案集合之外的概率为$e$，则$P(correct)&#x3D;(1-e)^n$。</li><li>这是指数增长的，这是不可修复的。</li></ul><h3 id="自回归生成模型太烂了"><a href="#自回归生成模型太烂了" class="headerlink" title="自回归生成模型太烂了!"></a>自回归生成模型太烂了!</h3><ul><li>AR-LLMs<ul><li>在输入和输出之间有一个常数的计算步骤。微弱的代表性力量（Weak representational power.）。</li><li>没有真正的推理和计划。</li></ul></li><li>人和大多数动物<ul><li>理解世界如何运行</li><li>能预测动作序列</li><li>可以执行无限步骤的推理链。</li><li>能通过将复杂任务分解为子任务序列来计划它。</li></ul></li></ul><h2 id="人工智能和机器学习面临的三大挑战"><a href="#人工智能和机器学习面临的三大挑战" class="headerlink" title="人工智能和机器学习面临的三大挑战"></a>人工智能和机器学习面临的三大挑战</h2><p>1.学习世界的表征和预测模型</p><ul><li>监督学习和强化学习需要太多的样本&#x2F;试验</li><li>自我监督学习&#x2F;学习依赖&#x2F;填补空白<ul><li>学习以非特定于任务的方式来表示世界</li><li>学习用于计划和控制的预测模型</li></ul></li></ul><p>2.学会推理，像Daniel Kahneman的“System 2”</p><ul><li>Beyond feed-forward, System 1 subconscious computation.</li><li>使推理与学习相容。<ul><li>Reasoning and planning as energy minimization.</li></ul></li></ul><p>3.学习计划复杂的动作序列</p><ul><li>Learning hierarchical representations of action plans</li></ul><h2 id="能够推理和计划的认知架构"><a href="#能够推理和计划的认知架构" class="headerlink" title="能够推理和计划的认知架构"></a>能够推理和计划的认知架构</h2><p>​立场论文：<a href="https://openreview.net/forum?id=BZ5a1r-kVsf">“A path towards autonomous machine intelligence”</a></p><p>Longer talk见 YouTube上的“LeCun Berkeley” 。</p><h3 id="自主人工智能的模块化架构"><a href="#自主人工智能的模块化架构" class="headerlink" title="自主人工智能的模块化架构"></a>自主人工智能的模块化架构</h3><ul><li>Configurator<ul><li>为任务配置其他模块</li></ul></li><li>Perception<ul><li>Estimates state of the world</li></ul></li><li>World Model<ul><li>Predicts future world states</li></ul></li><li>Cost<ul><li>Compute “discomfort”</li></ul></li><li>Actor<ul><li>Find optimal action sequences</li></ul></li><li>Short-Term Memory<ul><li>Stores state-cost episodes</li></ul></li></ul><p><img src="/2023/20230327/mode1.jpg"></p><p><img src="/2023/20230327/mode2.jpg"></p><p><img src="/2023/20230327/mode3.jpg"></p><h3 id="建立和训练World-Model"><a href="#建立和训练World-Model" class="headerlink" title="建立和训练World Model"></a>建立和训练World Model</h3><p>——基于能源模型的联合嵌入体系结构</p><p>我们如何表示预测中的不确定性?</p><ul><li>世界只有部分是可预测的</li><li>一个预测模型如何能表示多个预测?</li><li>概率模型在连续域是难以处理的。</li><li>生成模型必须预测世界的每一个细节</li><li>我的解决方案：Joint-Embedding Predictive Architecture</li></ul><h4 id="世界模型的体系结构-JEPA"><a href="#世界模型的体系结构-JEPA" class="headerlink" title="世界模型的体系结构:JEPA"></a>世界模型的体系结构:JEPA</h4><p><img src="/2023/20230327/JEPA.jpg"></p><h4 id="架构-Generative-vs-Joint-Embedding"><a href="#架构-Generative-vs-Joint-Embedding" class="headerlink" title="架构:Generative vs Joint Embedding"></a>架构:Generative vs Joint Embedding</h4><p><img src="/2023/20230327/JEPA_art.jpg"></p><h3 id="Energy-Based-Models"><a href="#Energy-Based-Models" class="headerlink" title="Energy-Based Models:"></a>Energy-Based Models:</h3><p><img src="/2023/20230327/EB_model.jpg"></p><p><img src="/2023/20230327/EBM_train.jpg"></p><h3 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h3><ul><li>放弃生成模型<ul><li>偏向joint-embedding architectures</li><li>抛弃Auto-Regressive generation</li></ul></li><li>抛弃概率模型<ul><li>偏向energy-based models</li></ul></li><li>抛弃对比方法<ul><li>偏向regularized methods</li></ul></li><li>抛弃强化学习<ul><li>偏向model-predictive control</li><li>只有当计划不能产生预期结果时，才使用RL来调整世界模型或critic。</li></ul></li></ul><h3 id="非对比学习地训练JEPA"><a href="#非对比学习地训练JEPA" class="headerlink" title="非对比学习地训练JEPA"></a>非对比学习地训练JEPA</h3><p><img src="/2023/20230327/JEPA_TRAIN.jpg"></p><h3 id="VICReg-Variance-Invariance-Covariance-Regularization"><a href="#VICReg-Variance-Invariance-Covariance-Regularization" class="headerlink" title="VICReg: Variance, Invariance, Covariance Regularization"></a>VICReg: Variance, Invariance, Covariance Regularization</h3><p><img src="/2023/20230327/VICReg.jpg"></p><h3 id="VICRegL"><a href="#VICRegL" class="headerlink" title="VICRegL"></a>VICRegL</h3><p><img src="/2023/20230327/VICRegL.jpg"></p><h3 id="MC-JEPA-Motion-Content-JEPA"><a href="#MC-JEPA-Motion-Content-JEPA" class="headerlink" title="MC-JEPA: Motion &amp; Content JEPA"></a>MC-JEPA: Motion &amp; Content JEPA</h3><p><img src="/MC-JEPA.JPG"></p><p><img src="/MC-JEPA1.JPG"></p><p><img src="/MC-JEPA2.JPG"></p><h3 id="Image-JEPA"><a href="#Image-JEPA" class="headerlink" title="Image-JEPA"></a>Image-JEPA</h3><p><img src="/2023/20230327/IJEPA.jpg"></p><h3 id="Hierarchical-Prediction-at-Multiple-Time-Scales-Abstraction-Levels"><a href="#Hierarchical-Prediction-at-Multiple-Time-Scales-Abstraction-Levels" class="headerlink" title="Hierarchical Prediction at Multiple Time-Scales &amp; Abstraction Levels"></a>Hierarchical Prediction at Multiple Time-Scales &amp; Abstraction Levels</h3><p><img src="/2023/20230327/HPMTA.jpg"></p><h2 id="Hierarchical-Planning-with-Uncertainty"><a href="#Hierarchical-Planning-with-Uncertainty" class="headerlink" title="Hierarchical Planning with Uncertainty"></a>Hierarchical Planning with Uncertainty</h2><ul><li>Hierarchical world model</li><li>Hierarchical world model</li><li>An <strong>action</strong> at level k specifies an <strong>objective</strong> for level k-1</li><li>Prediction in higher levels are more <strong>abstract</strong> and <strong>longer-range</strong>.</li><li>这种通过最小化“行动”变量的成本来进行规划&#x2F;推理的类型是当前架构所缺少的</li><li>包括AR-LLMs，多模态系统，学习机器人，…</li></ul><h2 id="迈向自主人工智能系统的步骤"><a href="#迈向自主人工智能系统的步骤" class="headerlink" title="迈向自主人工智能系统的步骤"></a>迈向自主人工智能系统的步骤</h2><ul><li>自监督学习<ul><li>学习世界的表象</li><li>学习世界的预测模型</li></ul></li><li>处理预测中的不确定性<ul><li>Joint-embedding predictive architectures</li><li>Energy-Based Model framework</li></ul></li><li>从观察中学习世界模型<ul><li>像动物或者人类婴儿一样？</li></ul></li><li>推理和计划<ul><li>这与基于梯度的学习是兼容的</li><li>没有符号，没有逻辑→向量和连续函数</li></ul></li></ul><h2 id="立场-猜想"><a href="#立场-猜想" class="headerlink" title="立场&#x2F;猜想"></a>立场&#x2F;猜想</h2><ul><li><p>预测是智力的本质</p><ul><li>学习世界的预测模型是常识的基础</li></ul><p>几乎所有的东西都是通过自我监督学习学到的</p><ul><li>低级特征、空间、对象、物理、抽象表现……</li><li>通过强化、监督或模仿几乎学不到任何东西</li></ul></li><li><p>推理&#x3D;&#x3D;模拟&#x2F;预测+目标优化</p><ul><li>计算上比auto-regressive generation更强大。</li></ul></li><li><p>H-JEPA和非对比训练才是关键</p><ul><li>概率生成模型和对比方法注定要失败。</li></ul></li><li><p>情感是自主智力的必要条件</p><ul><li>评论家或世界模型+内在成本对结果的预期。</li></ul></li></ul><h2 id="AI研究的挑战"><a href="#AI研究的挑战" class="headerlink" title="AI研究的挑战"></a>AI研究的挑战</h2><ul><li>为从视频、图像、音频、文本中训练基于层次联合嵌入体系结构的世界模型寻找一个通用的方法。</li><li>设计替代成本以驱动H-JEPA学习相关表示(预测只是其中之一)</li><li>将H-JEPA集成到一个能够规划&#x2F;推理的代理中</li><li>在不确定性存在的情况下设计层次规划的推理程序(基于梯度的方法，束搜索，MCTS，….)</li><li>在模型或评论家不准确并导致不可预见的结果的情况下，尽量减少RL的使用。</li><li>Scaling</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Dropout Reduces Underfitting</title>
    <link href="/2023/20230324/"/>
    <url>/2023/20230324/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>对于欠拟合模型，我们可以通过早期 dropout 来提高性能——dropout 仅在训练的初始阶段应用，之后关闭。</p><p>对于过拟合模型——我们使用后期 dropout，该方法在训练早期不使用 dropout，而是在训练后期才激活。</p><span id="more"></span><p>代码：<a href="https://github.com/facebookresearch/dropout">https://github.com/facebookresearch/dropout</a></p><hr><p>当数据量减少（顶部）或模型容量增加（底部）时，可能会发生过拟合。</p><p><img src="/2023/20230324/x3.png"></p><p><img src="/2023/20230324/x4.png"></p><p>dropout 率的影响。随着 dropout 率的增加，训练准确率会下降。然而，存在一个最佳的 dropout 率（在本例中为 p &#x3D; 0.15 ），可以最大化测试准确率。</p><p><img src="/2023/20230324/x5.png"></p><h2 id="dropout-如何减少欠拟合"><a href="#dropout-如何减少欠拟合" class="headerlink" title="dropout 如何减少欠拟合"></a>dropout 如何减少欠拟合</h2><p><img src="/2023/20230324/1.jpg"></p><p>梯度范数（左）和模型距离（右）。带有 dropout 的模型梯度幅度较小，但在参数空间中移动了更大的距离。</p><p><img src="/2023/20230324/x10.png"></p><p>训练曲线。当早期 dropout 结束时，模型在训练损失上经历显著下降，并在测试准确率上相应增加。</p><p><img src="/2023/20230324/x17.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>联邦学习之跨客户端信息重建</title>
    <link href="/2023/20230323/"/>
    <url>/2023/20230323/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>主要基于</p><ul><li><p>Subgraph federated learning with missing neighbor generation. NIPS’21</p></li><li><p>Fedni: Federated graph learning with network inpainting for population-based disease prediction. IEEE Transactions on Medical Imaging’22</p></li></ul><hr><span id="more"></span><h2 id="FedSage"><a href="#FedSage" class="headerlink" title="FedSage+"></a>FedSage+</h2><p>FedSage &#x3D; FedAVG+GraphSage</p><h3 id="FedAVG："><a href="#FedAVG：" class="headerlink" title="FedAVG："></a><strong>FedAVG：</strong></h3><p><strong>本地训练 (Local Training &#x2F; Multiple Local Epochs):</strong></p><ul><li>每个被选中的客户端 k 使用其本地数据集对接收到的模型 $W_{global}$ 进行训练。</li><li>这意味着客户端 k 会在其本地数据上运行多轮（epochs）的随机梯度下降（或其他优化算法），以最小化其本地损失函数 ，从而得到一个本地更新后的模型$W_{local,k}$。</li></ul><p><strong>模型上传 (Model Upload):</strong> 客户端将本地训练完成后的模型参数发送回服务器。</p><p><strong>模型聚合 (Model Aggregation):</strong></p><ul><li>服务器收集来自各个客户端的本地模型参数。</li><li>服务器通过对这些本地模型参数进行<strong>加权平均</strong>来更新全局模型： $W_{global}^{t+1}&#x3D;\sum_{k&#x3D;1}{n_k}W_{local,k}$</li></ul><h3 id="GraphSAGE："><a href="#GraphSAGE：" class="headerlink" title="GraphSAGE："></a><strong>GraphSAGE：</strong></h3><p>伪代码</p><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">Input:<br>  G = (V, E)          <span class="hljs-comment">// 图结构</span><br>  x_v                 <span class="hljs-comment">// 节点 v 的初始特征向量，对于所有 v 属于 V</span><br>  K                   <span class="hljs-comment">// 聚合的层数 (搜索深度)</span><br>  W^k                 <span class="hljs-comment">// 第 k 层的权重矩阵 (可学习参数), for k = 1...K</span><br>  AGGREGATE_k         <span class="hljs-comment">// 第 k 层的聚合函数 (例如 MEAN, POOL, LSTM), for k = 1...K</span><br>  N(v)                <span class="hljs-comment">// 节点 v 的邻居集合</span><br>  SAMPLE(S, num_samples) <span class="hljs-comment">// 从集合 S 中随机采样 num_samples 个元素</span><br><br>Output:<br>  z_v                 <span class="hljs-comment">// 节点 v 的最终嵌入向量 (在第 K 层之后), for all v 属于 V (或目标子集)</span><br><br><span class="hljs-keyword">Procedure</span>:<br>  <span class="hljs-comment">// 初始化第 0 层的隐藏表示为节点的输入特征</span><br>  h_v^<span class="hljs-number">0</span> ← x_v  <span class="hljs-keyword">for</span> all v ∈ V<br><br>  <span class="hljs-comment">// 对于每一层 k 从 1 到 K</span><br>  <span class="hljs-keyword">FOR</span> k = <span class="hljs-number">1</span> <span class="hljs-keyword">TO</span> K <span class="hljs-keyword">DO</span><br>    <span class="hljs-comment">// 对于图中的每一个节点 v (或者需要计算嵌入的目标节点)</span><br>    <span class="hljs-keyword">FOR</span> v ∈ V <span class="hljs-keyword">DO</span><br>      <span class="hljs-comment">// 1. 邻居采样: 从节点 v 的邻居中采样固定数量的邻居</span><br>      <span class="hljs-comment">//    S_k 是为第 k 层预设的采样邻居数量</span><br>      sampled_neighbors_of_v ← SAMPLE(N(v), S_k)<br><br>      <span class="hljs-comment">// 2. 聚合邻居信息:</span><br>      <span class="hljs-comment">//    从采样的邻居节点获取它们在上一层 (k-1) 的表示</span><br>      <span class="hljs-comment">//    并使用聚合函数 AGGREGATE_k 来汇总这些表示</span><br>      h_N(v)^k ← AGGREGATE_k(<span class="hljs-comment">&#123;h_u^(k-1) FOR u ∈ sampled_neighbors_of_v&#125;</span>)<br><br>      <span class="hljs-comment">// 3. 更新节点表示:</span><br>      <span class="hljs-comment">//    拼接节点 v 在上一层的表示 h_v^(k-1) 和聚合后的邻居表示 h_N(v)^k</span><br>      concatenated_representation ← <span class="hljs-keyword">CONCAT</span>(h_v^(k-<span class="hljs-number">1</span>), h_N(v)^k)<br><br>      <span class="hljs-comment">//    通过一个全连接层 (使用权重 W^k) 和非线性激活函数 sigma (例如 ReLU)</span><br>      h_v^k ← σ(W^k ⋅ concatenated_representation)<br>    <span class="hljs-keyword">END</span> <span class="hljs-keyword">FOR</span><br>  <span class="hljs-keyword">END</span> <span class="hljs-keyword">FOR</span><br><br>  <span class="hljs-comment">// 最终的输出嵌入是第 K 层的隐藏表示</span><br>  z_v ← h_v^K  <span class="hljs-keyword">for</span> all v ∈ V (<span class="hljs-keyword">or</span> target subset)<br><br>  RETURN z_v<br></code></pre></td></tr></table></figure><h3 id="FedSage-1"><a href="#FedSage-1" class="headerlink" title="FedSage+"></a>FedSage+</h3><p>FedSage+ 在 FedSage 的基础上引入了一个关键组件：<strong>缺失邻居生成器 (Missing Neighbor Generator, 通常称为 NeighGen)</strong>。通过显式地处理跨客户端连接的缺失问题，尝试恢复更完整的图结构信息。</p><p><img src="/2023/20230323/neighgen.jpg"></p><p>dgen用了预测缺失邻居数量，fGen用了预测特征向量。</p><h2 id="Fedni"><a href="#Fedni" class="headerlink" title="Fedni"></a>Fedni</h2><p><img src="/2023/20230323/x1.png"></p><p>随机从某个节点出发，使用BFS获取子图作为训练集，以学会从子图生成完整图的能力。</p><h2 id="想法"><a href="#想法" class="headerlink" title="想法"></a>想法</h2><p>（1）FedSage+的思想有点类似BERT，而另一篇论文Fedni利用了GAN的思想引入了鉴别器，和FedSage+的随机删除不同，他是使用广度优先搜索算法来实现节点删除。像FedSage+和Fedni都有一些NLP和CV中的比较先进的思想相类似的想法。那么它的未来方向是不是还可以参考比如CV中的diffusion model、NLP中的GPT系列等思想？</p><p>（2）FedGraph是由服务端来分配采样邻居策略，是不是意味着服务端知道所有子图间的连接数据，这样的话是不是隐私性&#x2F;安全性没有FedSage+那么好，有没有可能存在攻击服务端来获取各子图之间连接的关系情况？</p><p>（3）联邦学习怎么和鲁棒性产生联系？</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>联邦学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Prompt Engineering By Lil&#39;Log</title>
    <link href="/2023/20230320/"/>
    <url>/2023/20230320/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>提示工程概览，来自<a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">Prompt Engineering | Lil’Log</a>。</p><hr><p><strong>Prompt Engineering</strong>，也称为<strong>In-Context Prompting</strong>，指的是如何与 LLM 通信以<em>在不</em>更新模型权重的情况下引导其行为以获得预期结果的方法。这是一门经验科学，即时工程方法的效果在模型之间可能会有很大差异，因此需要大量的实验和启发式方法。</p><p>有用的资源：</p><ul><li><a href="https://github.com/openai/openai-cookbook">OpenAI Cookbook</a>有许多关于如何有效利用 LLM 的深入示例。</li><li><a href="https://github.com/dair-ai/Prompt-Engineering-Guide">Prompt Engineering Guide</a> repo 包含相当全面的有关 prompt engineering 的教育材料集合。</li></ul><p>这篇文章仅关注自回归语言模型的提示工程，因此与完形填空测试、图像生成或多模态模型无关。就其核心而言，即时工程的目标是对齐和模型可操纵性。查看我<a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/">之前关于可控文本生成的帖子</a>。</p><p>**[作者的个人看法]**在我看来，一些提示工程论文不值得 8 页长，因为那些技巧可以用一两句话解释，剩下的就是基准测试。一个易于使用和共享的基准基础设施应该对社区更有好处。迭代提示或外部工具的使用并非易事。使整个研究界都采用它也很重要。</p><span id="more"></span><h1 id="基本prompting"><a href="#基本prompting" class="headerlink" title="基本prompting"></a>基本prompting</h1><p>Zero-shot 和 few-shot learning 是两种最基本的模型提示方法，由许多 LLM 论文开创，通常用于基准 LLM 性能。</p><h2 id="Zero-shot"><a href="#Zero-shot" class="headerlink" title="Zero-shot"></a>Zero-shot</h2><p><strong>Zero-shot 学习</strong>是简单地将任务文本输入模型并要求结果。</p><p>（所有情感分析示例均来自SST-2）</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">Text:</span> i<span class="hljs-comment">&#x27;ll bet the video game is a lot more fun than the film.</span><br><span class="hljs-symbol">Sentiment:</span><br></code></pre></td></tr></table></figure><h2 id="Few-shot"><a href="#Few-shot" class="headerlink" title="Few-shot"></a>Few-shot</h2><p><strong>Few-shot learning</strong>提供了一组关于目标任务的高质量演示，每个演示都包含输入和期望的输出。当模型首先看到好的例子时，它可以更好地理解人类的意图和需要什么样的答案的标准。因此，少样本学习通常比零样本学习有更好的性能。然而，它是以更多的token消耗为代价的，并且当输入和输出文本很长时可能会达到上下文长度限制。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">Text:</span> (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face <span class="hljs-built_in">and</span> generally displaying the wacky talent that brought him fame <span class="hljs-keyword">in</span> the first place.<br><span class="hljs-symbol">Sentiment:</span> positive<br><br><span class="hljs-symbol">Text:</span> despite all evidence <span class="hljs-keyword">to</span> the contrary, this clunker has somehow managed <span class="hljs-keyword">to</span> pose <span class="hljs-keyword">as</span> an actual feature movie, the kind that charges full admission <span class="hljs-built_in">and</span> gets hyped <span class="hljs-keyword">on</span> tv <span class="hljs-built_in">and</span> purports <span class="hljs-keyword">to</span> amuse small children <span class="hljs-built_in">and</span> ostensible adults.<br><span class="hljs-symbol">Sentiment:</span> negative<br><br><span class="hljs-symbol">Text:</span> <span class="hljs-keyword">for</span> the first time <span class="hljs-keyword">in</span> years, de niro digs deep emotionally, perhaps because he<span class="hljs-comment">&#x27;s been stirred by the powerful work of his co-stars.</span><br><span class="hljs-symbol">Sentiment:</span> positive<br><br><span class="hljs-symbol">Text:</span> i<span class="hljs-comment">&#x27;ll bet the video game is a lot more fun than the film.</span><br><span class="hljs-symbol">Sentiment:</span><br></code></pre></td></tr></table></figure><p>许多研究研究了如何构建上下文中的示例以最大限度地提高性能，并观察到<strong>prompt格式、训练示例和示例顺序的选择会导致</strong>从接近随机猜测到接近 SoTA 的显着不同的性能。</p><p><a href="https://arxiv.org/abs/2102.09690">赵等。(2021)</a>调查了少样本分类的情况，并提出 LLM 的几个偏差（他们在实验中使用 GPT-3）导致了如此高的方差：（1）如果示例中的标签分布不平衡，则存在多数标签<em>偏差</em>; (2) <em>Recency bias</em>是指模型可能在最后重复标签的倾向；(3) <em>Common token bias</em>表明 LLM 比 rare tokens 更倾向于产生 common token。为了克服这种偏差，他们提出了一种方法来校准模型输出的标签概率，使其在输入字符串为 时保持一致<code>N/A</code>。</p><h3 id="示例选择技巧"><a href="#示例选择技巧" class="headerlink" title="示例选择技巧"></a>示例选择技巧</h3><ul><li>选择在语义上与测试示例相似的示例k-嵌入空间中的 NN 聚类（<a href="https://arxiv.org/abs/2101.06804">Liu et al., 2021</a>）</li><li>为了选择多样化且具有代表性的示例集，<a href="https://arxiv.org/abs/2209.01975">Su 等人。(2022)</a>提出使用基于图的方法：（1）首先，构造一个有向图$G&#x3D;(V,E)$基于嵌入（例如通过<a href="https://arxiv.org/abs/1908.10084">SBERT</a>或<a href="https://arxiv.org/abs/2201.10005">其他</a> <a href="https://platform.openai.com/docs/guides/embeddings">嵌入</a> <a href="https://openai.com/blog/new-and-improved-embedding-model">模型</a>）样本之间的余弦相似度，其中每个节点指向其k最近的邻居；(2) 从一组选定的样本开始$\mathcal{L}&#x3D;∅$和一组剩余样本$\mathcal{U}$在. 每个样品在∈在得分为</li></ul><p>$$<br>\text{score}(u) &#x3D; \sum_{v \in {v \mid (u, v) \in E, v\in \mathcal{U}}} s(v)\quad\text{where }s(v)&#x3D;\rho^{- \vert {\ell \in \mathcal{L} \vert (v, \ell)\in E }\vert},\quad\rho &gt; 1<br>$$</p><p>吖如果选择了许多相邻的样本，那么$s(v)$就很低，因此评分鼓励选择不同的样本。</p><ul><li><a href="https://arxiv.org/abs/2112.08633">鲁宾等人。(2022)</a>提出通过特定于一个训练数据集的<a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/">对比学习</a>来训练嵌入，以进行上下文学习样本选择。给定每个训练对(X,y), 一个例子的质量$e_i$（格式化的输入输出对）可以通过 LM 分配的条件概率来衡量：$\text{score}(e_i) &#x3D; P_\text{LM}(y \mid e_i, x)$. 我们可以用 top-k 识别其他示例k和底部-k将每个训练对的候选正集和负集打分，并将其用于对比学习。</li><li>一些研究人员尝试使用<a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/#q-learning-off-policy-td-control">Q-Learning</a>来进行样本选择。( <a href="https://arxiv.org/abs/2211.04486">Zhang et al. 2022</a> )</li><li>受基于不确定性的<a href="https://lilianweng.github.io/posts/2022-02-20-active-learning/">主动学习</a>的启发，<a href="https://arxiv.org/abs/2302.12246">Diao 等人。(2023)</a>建议在多次抽样试验中识别具有高分歧(high disagreement)或熵的例子。然后注释这些示例以用于 few-shot prompts。</li></ul><h3 id="Example-Ordering提示"><a href="#Example-Ordering提示" class="headerlink" title="Example Ordering提示"></a>Example Ordering提示</h3><ul><li>一个普遍的建议是保持样本选择的多样性、与测试样本的相关性以及随机顺序，以避免多数标签偏差和近因偏差。</li><li>增加模型大小或包含更多训练示例不会减少上下文示例的不同排列之间的方差。相同的订单可能适用于一种型号，但不适用于另一种型号。当验证集有限时，请考虑选择顺序，以使模型不会产生极度不平衡的预测或对其预测过于自信。( <a href="https://arxiv.org/abs/2104.08786">Lu et al. 2022</a> )</li></ul><h1 id="Instruction-Prompting"><a href="#Instruction-Prompting" class="headerlink" title="Instruction Prompting"></a>Instruction Prompting</h1><p>在提示中展示few-shot示例的目的是向模型解释我们的意图；换句话说，以演示的形式向模型描述任务指令。然而，few-shot 在token使用方面可能很昂贵，并且由于上下文长度有限而限制了输入长度。那么，为什么不直接给出指令呢？</p><p><em>Instructed LM</em>（例如<a href="https://openai.com/research/instruction-following">InstructGPT</a>、<a href="https://github.com/allenai/natural-instructions">natural instruction</a>）使用高质量元组（任务指令、输入、ground truth 输出）微调预训练模型，使 LM 更好地理解用户意图并遵循指令。<a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#rl-fine-tuning-with-human-preferences">RLHF</a>（人类反馈强化学习）是一种常用的方法。instruction following style fine-tuning 的好处是改进了模型，使其更符合人类的意图，并大大降低了沟通成本。</p><p>在与指令模型交互时，我们应该详细描述任务要求，尽量<em>具体</em>和<em>准确</em>，避免说“不做某事”，而是具体说明要做什么。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs vbnet">Please label the sentiment towards the movie <span class="hljs-keyword">of</span> the given movie review. The sentiment label should be <span class="hljs-string">&quot;positive&quot;</span> <span class="hljs-built_in">or</span> <span class="hljs-string">&quot;negative&quot;</span>. <br><span class="hljs-symbol">Text:</span> i<span class="hljs-comment">&#x27;ll bet the video game is a lot more fun than the film. </span><br><span class="hljs-symbol">Sentiment:</span><br></code></pre></td></tr></table></figure><p>向目标听众解释是另一种聪明的指示方式</p><ul><li>例如为孩子们制作教育材料，</li></ul><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs applescript">Describe what <span class="hljs-keyword">is</span> quantum physics <span class="hljs-keyword">to</span> a <span class="hljs-number">6</span>-<span class="hljs-built_in">year</span>-old.<br></code></pre></td></tr></table></figure><ul><li>和安全的内容，</li></ul><figure class="highlight python-repl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python-repl"><span class="hljs-meta prompt_">...</span> <span class="language-python"><span class="hljs-keyword">in</span> language that <span class="hljs-keyword">is</span> safe <span class="hljs-keyword">for</span> work.</span><br></code></pre></td></tr></table></figure><p><strong>In-context instruction learning</strong>( <a href="https://arxiv.org/abs/2302.14691">Ye et al. 2023</a> ) 将few-shot learning与instruction prompting结合。它在提示中包含多个跨不同任务的演示示例，每个演示都由指令、任务输入和输出组成。请注意，他们的实验仅针对分类任务，指令提示包含所有标签选项。</p><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">Definition:</span> Determine the speaker <span class="hljs-keyword">of</span> the dialogue, <span class="hljs-string">&quot;agent&quot;</span> <span class="hljs-built_in">or</span> <span class="hljs-string">&quot;customer&quot;</span>.<br><span class="hljs-symbol">Input:</span> I have successfully booked your tickets.<br><span class="hljs-symbol">Ouput:</span> agent<br><br><span class="hljs-symbol">Definition:</span> Determine which category the question asks <span class="hljs-keyword">for</span>, <span class="hljs-string">&quot;Quantity&quot;</span> <span class="hljs-built_in">or</span> <span class="hljs-string">&quot;Location&quot;</span>.<br><span class="hljs-symbol">Input:</span> What<span class="hljs-comment">&#x27;s the oldest building in US?</span><br><span class="hljs-symbol">Ouput:</span> Location<br><br><span class="hljs-symbol">Definition:</span> Classify the sentiment <span class="hljs-keyword">of</span> the given movie review, <span class="hljs-string">&quot;positive&quot;</span> <span class="hljs-built_in">or</span> <span class="hljs-string">&quot;negative&quot;</span>.<br><span class="hljs-symbol">Input:</span> i<span class="hljs-comment">&#x27;ll bet the video game is a lot more fun than the film.</span><br><span class="hljs-symbol">Output:</span><br></code></pre></td></tr></table></figure><h1 id="自洽抽样"><a href="#自洽抽样" class="headerlink" title="自洽抽样"></a>自洽抽样</h1><p><strong>自洽采样(Self-Consistency Sampling)</strong>( <a href="https://arxiv.org/abs/2203.11171">Wang et al. 2022a</a> ) 是对温度 &gt; 0 的多个输出进行采样，然后从这些候选中选择最好的一个。选择最佳候选人的标准因任务而异。一个通用的解决方案是选择<strong>多数票</strong>。对于易于验证的任务，例如带有单元测试的编程问题，我们可以简单地运行解释器并通过单元测试验证正确性。</p><h1 id="思维链-CoT"><a href="#思维链-CoT" class="headerlink" title="思维链 (CoT)"></a>思维链 (CoT)</h1><p><strong>思维链 (CoT) 提示</strong>( <a href="https://arxiv.org/abs/2201.11903">Wei et al. 2022</a> ) 生成一系列短句来逐步描述推理逻辑，称为<em>推理链</em>或<em>基本原理</em>，最终得出最终答案。CoT 的好处对于<strong>复杂的推理任务</strong>更为明显，同时使用<strong>大型模型</strong>（例如，具有超过 50B 个参数）。简单的任务只能从 CoT 提示中略微获益。</p><h2 id="CoT-提示的类型"><a href="#CoT-提示的类型" class="headerlink" title="CoT 提示的类型"></a>CoT 提示的类型</h2><p>CoT提示的两种主要类型：</p><ul><li><strong>Few-shot CoT</strong>。它是通过一些演示来提示模型，每个演示都包含手动编写（或模型生成）的高质量推理链。</li></ul><p>（所有数学推理示例均来自<a href="https://github.com/openai/grade-school-math">GSM8k</a>）</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs routeros">Question: Tom <span class="hljs-keyword">and</span> Elizabeth have a competition <span class="hljs-keyword">to</span> climb a hill. Elizabeth takes 30 minutes <span class="hljs-keyword">to</span> climb the hill. Tom takes four times as long as Elizabeth does <span class="hljs-keyword">to</span> climb the hill. How many hours does it take Tom <span class="hljs-keyword">to</span> climb up the hill?<br>Answer: It takes Tom 30<span class="hljs-number">*4</span> = &lt;&lt;30<span class="hljs-number">*4</span>=120&gt;&gt;120 minutes <span class="hljs-keyword">to</span> climb the hill.<br>It takes Tom 120/60 = &lt;&lt;120/<span class="hljs-attribute">60</span>=2&gt;&gt;2 hours <span class="hljs-keyword">to</span> climb the hill.<br>So the answer is 2.<br>===<br>Question: Jack is a soccer player. He needs <span class="hljs-keyword">to</span> buy two pairs of<span class="hljs-built_in"> socks </span><span class="hljs-keyword">and</span> a pair of soccer shoes. Each pair of<span class="hljs-built_in"> socks </span>cost <span class="hljs-variable">$9</span>.50, <span class="hljs-keyword">and</span> the shoes cost <span class="hljs-variable">$92</span>. Jack has <span class="hljs-variable">$40</span>. How much more money does Jack need?<br>Answer: The total cost of two pairs of<span class="hljs-built_in"> socks </span>is <span class="hljs-variable">$9</span>.50 x 2 = $&lt;&lt;9.5<span class="hljs-number">*2</span>=19&gt;&gt;19.<br>The total cost of the<span class="hljs-built_in"> socks </span><span class="hljs-keyword">and</span> the shoes is <span class="hljs-variable">$19</span> + <span class="hljs-variable">$92</span> = $&lt;&lt;19+<span class="hljs-attribute">92</span>=111&gt;&gt;111.<br>Jack need <span class="hljs-variable">$111</span> - <span class="hljs-variable">$40</span> = $&lt;&lt;<span class="hljs-attribute">111-40</span>=71&gt;&gt;71 more.<br>So the answer is 71.<br>===<br>Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?<br>Answer:<br></code></pre></td></tr></table></figure><ul><li><strong>Zero-shot CoT</strong>。使用自然语言语句<code>Let&#39;s think step by step</code>明确鼓励模型首先生成推理链，然后提示<code>Therefore, the answer is</code>生成答案（<a href="https://arxiv.org/abs/2205.11916">Kojima 等人，2022 年</a>）。或者类似的说法<code>Let&#39;s work this out it a step by step to be sure we have the right answer</code>（<a href="https://arxiv.org/abs/2211.01910">Zhou et al. 2022</a>）。</li></ul><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs vbnet"><span class="hljs-symbol">Question:</span> Marty has <span class="hljs-number">100</span> centimeters <span class="hljs-keyword">of</span> ribbon that he must cut <span class="hljs-keyword">into</span> <span class="hljs-number">4</span> equal parts. <span class="hljs-keyword">Each</span> <span class="hljs-keyword">of</span> the cut parts must be divided <span class="hljs-keyword">into</span> <span class="hljs-number">5</span> equal parts. How <span class="hljs-type">long</span> will <span class="hljs-keyword">each</span> final cut be?<br><span class="hljs-symbol">Answer:</span> <span class="hljs-keyword">Let</span><span class="hljs-comment">&#x27;s think step by step.</span><br></code></pre></td></tr></table></figure><h2 id="提示和扩展"><a href="#提示和扩展" class="headerlink" title="提示和扩展"></a>提示和扩展</h2><ul><li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#self-consistency-sampling">自洽抽样</a>可以通过抽取多个不同的答案然后进行多数表决来提高推理的准确性。( <a href="https://arxiv.org/abs/2203.11171">Wang et al. 2022a</a> )</li><li>集成学习的另一种方法是改变示例顺序或使用模型生成的基本原理来代替人工编写的基本原理，以在多个样本试验中引入随机性。然后以多数票聚合模型输出以获得最终答案。（<a href="https://arxiv.org/abs/2207.00747">王等人 2022b</a>）</li><li>如果训练样例只与真答案相关联（易于验证！）但没有基本原理，我们可以遵循 STaR <em>（</em> Self-Taught Reasoner；<a href="https://arxiv.org/abs/2203.14465">Zelikman et al. 2022</a>）方法： (1) 让 LLM 生成推理链，只保留那些导致正确答案的人；(2) 然后用生成的基本原理微调模型并重复该过程直到收敛。请注意，更高的温度更有可能产生错误的基本原理和正确的答案。如果训练样本没有真实答案，可以考虑使用多数票作为“正确”答案。</li><li>提示具有更高推理复杂性的演示可以实现更好的性能，其中复杂性由链中推理步骤的数量来衡量。分隔推理步骤时，换行符<code>\n</code>比<code>step i</code>、句号<code>.</code>或分号<code>;</code>更有效。（<a href="https://arxiv.org/abs/2210.00720">Fu等，2023</a>）</li><li>*基于复杂性的一致性(Complexity-based consistency)*是通过仅在顶部进行多数投票，在所有世代中明确偏好复杂链k复杂的链条。（<a href="https://arxiv.org/abs/2210.00720">Fu等，2023</a>）</li><li>后来，<a href="https://arxiv.org/abs/2302.12822">Shum 等人。(2023)</a>发现在他们的实验中，仅使用复杂示例的 CoT 提示可以提高复杂问题的准确性，但在简单问题中表现不佳；GSM8k 上显示的证据。</li><li>发现<code>Question:</code>改成<code>Q:</code>有用。（<a href="https://arxiv.org/abs/2210.00720">Fu等，2023</a>）</li><li><a href="https://arxiv.org/abs/2205.03401">Ye &amp; Durrett (2022)</a>发现，对于涉及文本推理的 NLP 任务（即 QA 和 NLI），在提示中包含解释的好处很小到中等，并且效果因模型而异。他们观察到解释更可能是非事实的而不是不一致的（即解释是否需要预测）。非事实的解释很可能导致错误的预测。</li><li><em>Self-Ask</em> ( <a href="https://arxiv.org/abs/2210.03350">Press et al. 2022</a> ) 是一种反复提示模型提出<em>后续问题</em>以迭代构建思维过程的方法。可以通过搜索引擎结果回答后续问题。同样，<em>IRCoT</em>（Interleaving Retrieval CoT；<a href="https://arxiv.org/abs/2212.10509">Trivedi et al. 2022</a>）和<em>ReAct</em>（Reason + Act；<a href="https://arxiv.org/abs/2210.03629">Yao et al. 2023</a>）将迭代 CoT 提示与维基百科 API 查询相结合，以搜索相关实体和内容，然后将其添加回语境。</li></ul><p><img src="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/SelfAsk-search.png" alt="img"></p><p>图 1. Self-Ask 如何与外部搜索查询一起使用。<br>（图片来源：<a href="https://arxiv.org/abs/2210.03350">Press et al. 2022</a>）。</p><h1 id="Automatic-Prompt-Design"><a href="#Automatic-Prompt-Design" class="headerlink" title="Automatic Prompt Design"></a>Automatic Prompt Design</h1><p>Prompt 是一系列前缀标记，可增加在给定输入的情况下获得所需输出的概率。因此，我们可以将它们视为可训练的参数，并通过梯度下降<a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">直接在嵌入空间上对其进行优化</a>，例如<strong>AutoPrompt</strong> ( <a href="https://arxiv.org/abs/2010.15980">Shin et al., 2020</a> , <strong>Prefix-Tuning</strong> ( <a href="https://arxiv.org/abs/2101.00190">Li &amp; Liang (2021)</a> ), <strong>P-tuning</strong> ( <a href="https://arxiv.org/abs/2103.10385">Liu et al . 2021</a> ) 和<strong>Prompt-Tuning</strong> ( <a href="https://arxiv.org/abs/2104.08691">Lester et al. 2021</a> ).<a href="https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/#smart-prompt-design">我的“可控神经文本生成”帖子中的这一部分</a>对它们有很好的介绍。从 AutoPrompt 到 Prompt-Tuning 的趋势是设置逐渐简化。</p><p><strong>APE</strong>（Automatic Prompt Engineer；<a href="https://arxiv.org/abs/2211.01910">Zhou et al. 2022</a>）是一种搜索模型生成的候选指令池，然后根据所选得分函数过滤候选集，最终选择得分最高的最佳候选指令的方法。</p><ol><li>提示 LLM 基于输入输出对形式的一小组演示生成候选指令。例如<code>&#123;&#123;Given desired input-output pairs&#125;&#125;\n\nThe instruction is</code>。</li><li>对于给定的数据集$\mathcal{D}<em>\text{train} &#x3D; {(x, y)}$，我们想找到一条instruction$\rho$满足$\rho^* &#x3D; \arg\max</em>\rho \mathbb{E}<em>{(x, y) \in \mathcal{D}</em>\text{train}} [f(\rho, x, y)]$，其中$f(.)$是每个样本的得分函数，例如执行精度$\mathbb{1}[\text{LM}(.\vert \rho, x)&#x3D;y]$或对数概率：$p_\text{LM}(y \mid \rho, x)$</li><li>使用迭代蒙特卡洛搜索方法通过提示提出语义相似的变体来改进最佳候选者，例如<code>Generate a variation of the following instruction while keeping the semantic meaning.\n\nInput: ...\n\nOutput:...</code></li></ol><p>为了自动构建思想链提示，<a href="https://arxiv.org/abs/2302.12822">Shum 等人。(2023)</a>建议 augment-prune-select，一个三步过程：</p><ol><li><em>增强</em>：使用few-shot或zero-shot CoT 提示生成给定问题的多个伪思维链；</li><li><em>Prune</em>：根据生成的答案是否与基本事实相匹配来修剪伪链。</li><li><em>选择</em>：应用方差减少的策略梯度策略来学习所选示例的概率分布，同时将示例的概率分布视为策略，将验证集的准确性视为奖励。</li></ol><p><a href="https://arxiv.org/abs/2210.03493">张等。(2023)</a>改为采用<em>聚类</em>技术对问题进行抽样，然后生成链。他们观察到 LLM 倾向于犯某些类型的错误。一种类型的错误在嵌入空间中可能相似，因此被组合在一起。通过仅从频繁错误的集群中抽取一个或几个样本，我们可以防止对一种错误类型的过多错误演示，并收集一组不同的示例。</p><ol><li><em>问题聚类</em>：嵌入问题并运行k-means 聚类的方法。</li><li><em>演示选择</em>：从每个集群中选择一组具有代表性的问题；即来自一个集群的一个演示。每个簇中的样本按到簇质心的距离排序，最接近质心的样本首先被选择。</li><li><em>Rationale generation</em> : 使用zero-shot CoT 为选定的问题生成推理链，并构建few-shot prompt以运行推理。</li></ol><h1 id="Augmented-Language-Models"><a href="#Augmented-Language-Models" class="headerlink" title="Augmented Language Models"></a>Augmented Language Models</h1><p><a href="https://arxiv.org/abs/2302.07842">Mialon 等人</a>对Augmented Language Models的综述。<a href="https://arxiv.org/abs/2302.07842">(2023)</a>广泛涵盖了多种语言模型类别，这些语言模型增强了推理技能和使用外部工具的能力。推荐它。</p><h2 id="Retrieval"><a href="#Retrieval" class="headerlink" title="Retrieval"></a>Retrieval</h2><p>通常我们需要在模型预训练时间截止或内部&#x2F;私有知识库之后完成需要最新知识的任务。在这种情况下，如果我们没有在提示中明确提供上下文，模型将不知道上下文。<a href="https://lilianweng.github.io/posts/2020-10-29-odqa/">开放领域问答(Open Domain Question Answering)</a>的许多方法依赖于首先对知识库进行检索，然后将检索到的内容作为提示的一部分。这种过程的准确性取决于检索和生成步骤的质量。</p><p><a href="https://arxiv.org/abs/2203.05115">拉扎里杜等人。(2022)</a>研究了如何使用谷歌搜索进行文档检索以增强 LLM。给出一个问题q，从 Google 返回的 20 个 URL 中提取干净的文本，生成一组文档。因为这些文件很长，每个文件被分成6个句子的段落，{p}. 段落按照证据段落和查询之间基于 TF-IDF 的余弦相似度进行排序。提示中仅使用最相关的段落来生成答案A.</p><p>对于闭卷 QA，每个演示的格式如下，以构建 few-shot 提示。发现将问题与证据交换（问题和答案之间的距离更长）在所有数据集中始终产生较低的结果。</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">Evidence:</span> ...<br><span class="hljs-symbol">Question:</span> ...<br><span class="hljs-symbol">Answer:</span> ...<br></code></pre></td></tr></table></figure><p>答案概率以三种方式计算：</p><ol><li><a href="https://lilianweng.github.io/posts/2020-10-29-odqa/#RAG">RAG</a>风格，$p(a_i \mid q) &#x3D; \sum_{i&#x3D;1}^n p_\text{tf-idf} (p_i \mid q) \cdot p_\text{LM}(a_i \mid q, p_i)$，其中$p_\text{tf-idf} (p_i \mid q)$是 TF-IDF 段落和问题表示之间的归一化余弦相似度。</li><li>噪声信道推断，$p(a_i\mid q) &#x3D; \frac{p_\text{LM}(q \mid a_i, p_i) \cdot p_\text{LM}(a_i \mid p_i)}{p_\text{LM}(q \mid p_i)}$</li><li>Product-of-Experts (PoE)，结合上面使用的所有概率，除了$p_\text{LM}(p_i \mid q)$.</li></ol><p>根据他们在生成和分类任务上的实验，在三个答案重新排序分数中 - PoE &gt; Noisy channel &gt; RAG。在个体概率中，$p_\text{LM}(a \mid q, p_i)$和$p_\text{LM}(q \mid p_i, a)$被发现是最有用的。$p_\text{LM}(q \mid p_i, a)$在给定证据段落和答案的情况下，捕获 LM 可以很好地解释问题，并且可以可靠地用于对候选答案进行重新排序。</p><p>针对基于不同日期的问题的<a href="https://situatedqa.github.io/">SituatedQA</a>数据集进行的一项观察是，尽管 LM（预训练截止日期为 2020 年）可以通过 Google 搜索访问最新信息，但其在 2020 年后问题上的表现仍然比 2020 年前问题差<em>很多</em>。这表明上下文信息和模型内部知识之间存在一些差异或参数冲突。</p><p>有趣的是，即使仅使用“internal retrieval”也被发现是有益的，即在回答问题之前生成关于某个主题的知识（<a href="https://arxiv.org/abs/2110.08387">Liu 等人，2022 年</a>）。首先我们可以使用下面的模板来抽取知识：</p><figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs stata"><span class="hljs-keyword">Generate</span> some knowledge <span class="hljs-keyword">about</span> the <span class="hljs-keyword">input</span>. Examples:<br><br><span class="hljs-keyword">Input</span>: What <span class="hljs-keyword">type</span> of water formation is formed <span class="hljs-keyword">by</span> clouds?<br>Knowledge: Clouds are made of water vapor.<br><br><span class="hljs-keyword">Input</span>: &#123;question&#125;<br>Knowledge:<br></code></pre></td></tr></table></figure><p>然后用模型生成的知识，进一步提示 LM 得到答案。</p><h2 id="Programming-Language"><a href="#Programming-Language" class="headerlink" title="Programming Language"></a>Programming Language</h2><p>既有<strong>PAL</strong>（程序辅助语言模型）；<a href="https://arxiv.org/abs/2211.10435">高等。2022</a> ) 和<strong>PoT</strong> (Program of Thoughts prompting; <a href="https://arxiv.org/abs/2211.12588">Chen et al. 2022</a> ) 要求 LLM 生成编程语言语句来解决自然语言推理问题，从而将求解步骤卸载到运行时，例如 Python 解释器。这样的设置解耦了复杂的计算和推理。它依赖于具有足够好的编码技能的 LM。</p><p><img src="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/PoT.png" alt="图 2. 比较 CoT 和 PoT。"></p><p>（图片来源：<a href="https://arxiv.org/abs/2211.12588">Chen et al. 2022</a>）。</p><h2 id="External-APIs"><a href="#External-APIs" class="headerlink" title="External APIs"></a>External APIs</h2><p><strong>TALM</strong>（工具增强语言模型；<a href="https://arxiv.org/abs/2205.12255">Parisi 等人，2022 年</a>）是一种通过文本到文本 API 调用进行增强的语言模型。LM 被引导生成<code>|tool-call</code>并<code>tool input text</code>以任务输入文本为条件来构造 API 调用请求。当<code>|result</code>出现时，调用指定的工具 API，并将返回的结果附加到文本序列。最终输出是在<code>|output</code>令牌之后生成的。</p><p><img src="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/TALM.png" alt="img"></p><p>图 3. TALM 中 API 调用的格式。（图片来源：<a href="https://arxiv.org/abs/2205.12255">Parisi 等人，2022 年</a>）。</p><p>TALM 采用自我对弈的方法来迭代引导工具使用示例的数据集，并用它来微调 LM。这个迭代的自我对弈管道模仿了一个 RL 过程，其中 LM 是策略网络，它由带有二元奖励信号的策略梯度训练。</p><p><img src="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/TALM-iteration.png" alt="img"></p><p>图 4. 自我对弈迭代有助于提升模型性能。<br>（图片来源：<a href="https://arxiv.org/abs/2205.12255">Parisi 等人，2022 年</a>）。</p><p><strong>Toolformer</strong> ( <a href="https://arxiv.org/abs/2302.04761">Schick et al. 2023</a> ) 是一种可以通过简单的 API 使用外部工具的 LM，它以自我监督的方式构建，每个 API 只需要少量的演示。Toolformer 的工具箱包括：</p><ul><li><em>Calculator</em>帮助 LM 缺乏精确的数学技能；</li><li><em>Q&amp;A system</em>以帮助解决不忠实的内容和幻觉；</li><li><em>Search engine</em>在预训练截止时间后提供最新信息；</li><li>提高低资源语言性能的<em>Translation system</em></li><li>使 LM 了解时间进程的<em>calendar</em></li></ul><p><img src="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/toolformer.png" alt="img"></p><p>图 5. 说明如何构建 Toolformer。<br>（图片来源：<a href="https://arxiv.org/abs/2302.04761">Schick et al. 2023</a>）。</p><p>Toolformer 的训练如下：</p><ol><li><p>*<em>Prompting to annotate potential API calls</em>. 。要求预训练的 LM 通过带有 API 调用使用示例的少量学习来注释数据集。格式化示例：</p><p><img src="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/toolformer-annotation.png" alt="img"></p><p>图 6. 如何注释数据集以进行 API 调用。<br>（图片来源：<a href="https://arxiv.org/abs/2302.04761">Schick et al. 2023</a>）。</p><ul><li><p>每个 API 调用都表示为（API 名称，相应输入）的元组，$c&#x3D;(a_c, i_c)$和其对应的结果表示为r. 有结果和无结果的API调用序列分别标注如下：</p><p>$$<br>\begin{aligned}<br>  e(c) &amp;&#x3D; \langle\texttt{API}\rangle a_c(i_c) \langle\texttt{&#x2F;API}\rangle \<br>  e(c, r) &amp;&#x3D; \langle\texttt{API}\rangle a_c(i_c) \to r \langle\texttt{&#x2F;API}\rangle<br>  \end{aligned}<br>$$</p></li><li><p>基于概率的示例 API 调用$p_\text{LM}(\langle\texttt{API}\rangle \mid \text{prompt}(\mathbf{x}), \mathbf{x}_{1:i})$并选择top-k在 position i进行 API 调用的候选位置$i$如果概率大于阈值。</p></li><li><p>Then we sample potential API calls from the LM given the sequence $[\text{prompt}(\mathbf{x}), x_1, \dots, x_{i-1}, \langle\texttt{API}\rangle]$ as prefix and ⟨&#x2F;API⟩ as suffix.</p></li></ul></li><li><p>*根据 API 调用是否有助于模型预测未来标记过滤注释。*使用自我监督损失来确定哪些 API 调用实际上有帮助。</p><ul><li><p>执行每个 API 调用$c_i$得到相应的结果$r_i$.</p></li><li><p>计算 LM 在令牌上的加权交叉熵损失$X_i,…,Xn$当模型以提示为前缀时。计算了两个版本，一个带有 API 结果，另一个带有空序列$\varepsilon$.</p><p>$$<br>\begin{aligned}<br>  L^+_i &amp;&#x3D; L_i(e(c_i, r_i)) \<br>  L^-_i &amp;&#x3D; \min(L_i(\varepsilon), L_i(e(c_i, \varepsilon))) \<br>  \end{aligned}<br>$$</p><p>仅 API 调用$L^-_i - L^+_i$保留大于阈值的标记，这意味着添加此 API 调用及其结果有助于模型预测未来的标记。</p></li></ul></li><li><p>*在这个带注释的数据集上微调 LM。<em>新的训练序列构造为$\mathbf{x}^</em> &#x3D; x_{1:i-1}, e(c_i, r_i), x_{i:n}$. 训练数据是原始数据集（例如论文中的 CCNet 的子集）及其增强版本的组合。</p></li></ol><p>在推理时，解码一直运行到模型产生“→” 令牌，表明它正在等待下一个 API 调用的响应。</p><p>Toolformer 目前不支持链式使用工具（即使用一个工具的输出作为另一个工具的输入）或以交互方式（即在人工选择后采用 API 响应）。两者都是未来扩展模型的有趣方向。</p><h1 id="Citation"><a href="#Citation" class="headerlink" title="Citation"></a>Citation</h1><p>格式如下</p><blockquote><p>Weng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. <a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/</a>.</p></blockquote><p>或</p><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs applescript">@article&#123;weng2023prompt,<br>  title   = <span class="hljs-string">&quot;Prompt Engineering&quot;</span>,<br>  author  = <span class="hljs-string">&quot;Weng, Lilian&quot;</span>,<br>  journal = <span class="hljs-string">&quot;lilianweng.github.io&quot;</span>,<br>  <span class="hljs-built_in">year</span>    = <span class="hljs-string">&quot;2023&quot;</span>,<br>  <span class="hljs-built_in">month</span>   = <span class="hljs-string">&quot;Mar&quot;</span>,<br>  url     = <span class="hljs-string">&quot;https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/&quot;</span><br>&#125;<br></code></pre></td></tr></table></figure><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>[1] Zhao et al. <a href="https://arxiv.org/abs/2102.09690">“Calibrate Before Use: Improving Few-shot Performance of Language Models.”</a> ICML 2021</p><p>[2] Liu et al. <a href="https://arxiv.org/abs/2101.06804">“What Makes Good In-Context Examples for GPT-3?”</a> arXiv preprint arXiv:2101.06804 (2021).</p><p>[3] Lu et al. <a href="https://arxiv.org/abs/2104.08786">“Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity.”</a> ACL 2022</p><p>[4] Ye et al. <a href="https://arxiv.org/abs/2302.14691">“In-Context Instruction Learning.”</a> arXiv preprint arXiv:2302.14691 (2023).</p><p>[5] Su et al. <a href="https://arxiv.org/abs/2209.01975">“Selective annotation makes language models better few-shot learners.”</a> arXiv preprint arXiv:2209.01975 (2022).</p><p>[6] Rubin et al. <a href="https://arxiv.org/abs/2112.08633">“Learning to retrieve prompts for in-context learning.”</a> NAACL-HLT 2022</p><p>[7] Wei et al. <a href="https://arxiv.org/abs/2201.11903">“Chain of thought prompting elicits reasoning in large language models.”</a> NeurIPS 2022</p><p>[8] Wang et al. <a href="https://arxiv.org/abs/2203.11171">“Self-Consistency Improves Chain of Thought Reasoning in Language Models.”</a> ICLR 2023.</p><p>[9] Diao et al. <a href="https://arxiv.org/abs/2302.12246">“Active Prompting with Chain-of-Thought for Large Language Models.”</a> arXiv preprint arXiv:2302.12246 (2023).</p><p>[10] Zelikman et al. <a href="https://arxiv.org/abs/2203.14465">“STaR: Bootstrapping Reasoning With Reasoning.”</a> arXiv preprint arXiv:2203.14465 (2022).</p><p>[11] Ye &amp; Durrett. <a href="https://arxiv.org/abs/2205.03401">“The unreliability of explanations in few-shot in-context learning.”</a> arXiv preprint arXiv:2205.03401 (2022).</p><p>[12] Trivedi et al. <a href="https://arxiv.org/abs/2212.10509">“Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.”</a> arXiv preprint arXiv:2212.10509 (2022).</p><p>[13] Press et al. <a href="https://arxiv.org/abs/2210.03350">“Measuring and narrowing the compositionality gap in language models.”</a> arXiv preprint arXiv:2210.03350 (2022).</p><p>[14] Yao et al. <a href="https://arxiv.org/abs/2210.03629">“ReAct: Synergizing reasoning and acting in language models.”</a> ICLR 2023.</p><p>[15] Fu et al. <a href="https://arxiv.org/abs/2210.00720">“Complexity-based prompting for multi-step reasoning.”</a> arXiv preprint arXiv:2210.00720 (2022).</p><p>[16] Wang et al. <a href="https://arxiv.org/abs/2207.00747">“Rationale-augmented ensembles in language models.”</a> arXiv preprint arXiv:2207.00747 (2022).</p><p>[17] Zhang et al. <a href="https://arxiv.org/abs/2210.03493">“Automatic chain of thought prompting in large language models.”</a> arXiv preprint arXiv:2210.03493 (2022).</p><p>[18] Shum et al. <a href="https://arxiv.org/abs/2302.12822">“Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data.”</a> arXiv preprint arXiv:2302.12822 (2023).</p><p>[19] Zhou et al. <a href="https://arxiv.org/abs/2211.01910">“Large Language Models Are Human-Level Prompt Engineers.”</a> ICLR 2023.</p><p>[20] Lazaridou et al. <a href="https://arxiv.org/abs/2203.05115">“Internet augmented language models through few-shot prompting for open-domain question answering.”</a> arXiv preprint arXiv:2203.05115 (2022).</p><p>[21] Chen et al. <a href="https://arxiv.org/abs/2211.12588">“Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks.”</a> arXiv preprint arXiv:2211.12588 (2022).</p><p>[22] Gao et al. <a href="https://arxiv.org/abs/2211.10435">“PAL: Program-aided language models.”</a> arXiv preprint arXiv:2211.10435 (2022).</p><p>[23] Parisi et al. <a href="https://arxiv.org/abs/2205.12255">“TALM: Tool Augmented Language Models”</a> arXiv preprint arXiv:2205.12255 (2022).</p><p>[24] Schick et al. <a href="https://arxiv.org/abs/2302.04761">“Toolformer: Language Models Can Teach Themselves to Use Tools.”</a> arXiv preprint arXiv:2302.04761 (2023).</p><p>[25] Mialon et al. <a href="https://arxiv.org/abs/2302.07842">“Augmented Language Models: a Survey”</a> arXiv preprint arXiv:2302.07842 (2023).</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>健身新手的减脂完全手册</title>
    <link href="/2023/20230319/"/>
    <url>/2023/20230319/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://www.bilibili.com/video/BV1AM411r7z3/">B站版《健身新手的减脂完全手册》™</a></p><hr><p>这个视频介绍合理理减脂的方法</p><p>实际上,减脂从根本上说是一个方法问题,和网上宣传的要有毅力,要坚持,要自律都没什么关系。</p><h2 id="减脂原理"><a href="#减脂原理" class="headerlink" title="减脂原理"></a>减脂原理</h2><h3 id="要有10-20-的热量缺口"><a href="#要有10-20-的热量缺口" class="headerlink" title="要有10%-20%的热量缺口"></a>要有10%-20%的热量缺口</h3><p>我们的热量涉入来自于食物,热量消耗在来自于两方面,主要是基础代谢,其次是活动消耗。左右两边热量相等的话,你的体重就会不增不减,叫做热量平衡点。想要减脂左边的热量涉入,要比右边的热量消耗少20%。</p><p>热量缺口一定要有,但也不需要太大。而热量缺口怎么来的呢?即可以从左边减少一点热量涉入,也可以从右边多做一点有氧活动来提供,或者两者兼而有之。<br>减脂性的基础代谢是提高不了的,我们争取不让它超额下降就可以了。</p><span id="more"></span><h3 id="碳氮脂的比例"><a href="#碳氮脂的比例" class="headerlink" title="碳氮脂的比例"></a>碳氮脂的比例</h3><p>热量涉入是按照每克碳水蛋白质脂肪分别449大卡来求和的。</p><p>$$<br>热量摄入(kcal)&#x3D;碳水<em>4kcal+蛋白质</em>4kcal+脂肪*9kcal<br>$$</p><p>定碳氮脂就定了热量,但定的热量不代表定了碳氮脂。所以我们在热量平衡点的基础上降低了10%-20%热量后,还要按照一种经验化的碳氮脂比例而非任意的比例去组成这个热量。</p><h3 id="碳水的日内分配"><a href="#碳水的日内分配" class="headerlink" title="碳水的日内分配"></a>碳水的日内分配</h3><p>在热量一定碳碳脂配额也一面的情况下,把碳水放在不同的时间去吃,就可以人为的操纵胰岛素的升降,也会影响到我们的减脂效果。<br>就像我们玩游戏的时候有timeing的概念,碳水涉入也有timeing。<br>尤其是对于健身距离来说,碳水的timeing是减脂期保护肌肉的一个重要策略。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>首先是要有10-20%的热量缺口,饮食和有氧都是制造这个热量缺口的手段。</p><p>其次是应吃的热量要以特定的碳氮脂比例来提供,再次是管理碳水的日内分配。<br>这三个原理都和饮食有关,而有氧只适合热量缺口有关。<br>所以说,减脂里饮食比有氧更重要,也更复杂。</p><h2 id="饮食"><a href="#饮食" class="headerlink" title="饮食"></a>饮食</h2><h3 id="热量平衡"><a href="#热量平衡" class="headerlink" title="热量平衡"></a>热量平衡</h3><p>但在我们吃下去后,热量会有两个损失过程,首先是因为不完全吸收造成的热量损失。人体排泄物里会减出少量的碳碳脂及其衍生物,因此学界把碳碳脂热量下调为449大卡。如果还要考虑消化碳碳脂对我们身体造成的额外发热的这种食物热效应,碳碳脂的倒手热量还可以继续下调为3.8,2.8,8.55大卡。</p><table><thead><tr><th>单位kcal&#x2F;g</th><th>碳水</th><th>蛋白质</th><th>脂肪</th></tr></thead><tbody><tr><td>食物能值</td><td>4.1</td><td>5.65</td><td>9.45</td></tr><tr><td>考虑吸收率</td><td>4.0</td><td>4.0</td><td>9.0</td></tr><tr><td>考虑热效应</td><td>3.8</td><td>2.8</td><td>8.55</td></tr></tbody></table><p>在计算食物热量时,我们用这两种标准都可以,因为食物热量的绝对值是不重要的,我们要的是减脂的10%-20%的热量缺口,和增计的10%-20%的热量盈余。乘以这个百分比后,两种计算标准的差额其实是不大的。</p><h4 id="我们怎么计算自己吃的食物热量呢"><a href="#我们怎么计算自己吃的食物热量呢" class="headerlink" title="我们怎么计算自己吃的食物热量呢?"></a>我们怎么计算自己吃的食物热量呢?</h4><p>一种办法是用我在评论区放的一个食物热量计算的EXCEL表,把日常食物录进去,输入食物分数,来得到碳碳脂的量和总热量。另一种办法是用薄荷APP来记录自己的食物,两种办法在本质上都是一样的。</p><h3 id="热量消耗"><a href="#热量消耗" class="headerlink" title="热量消耗"></a>热量消耗</h3><p>$$<br>热量摄入&#x3D;基础代谢+活动消耗<br>$$<br>一般人的热量消耗主要靠的是基础代谢,基础代谢就是在完全进息状态时身体要活着所需要的能量,就要比手机吸平时也要耗电一样。<br>其次的热量消耗来自于活动,就是我们上班上课,走路骑车,路铁游养等一切身体活动导致的热量消耗。<br>如果左边的热量消耗和右边的热量设入相等,那么你的体重就会不变,这个数值叫热量平衡点。<br>如果要减脂,就要在热量平衡点的基础上减少10%-20%的热量设入来形成热量缺口,迫使人体分解脂肪来填补热量差。</p><p><img src="/2023/20230319/table1.png"><br>公式算出的基础代谢有一定参考性,但它对于减值的指导意义还是比较有限的。</p><p>更大的问题是活动消耗的预测公式。前面说的基础代谢的预测公式,起码让你带入体重、身高、年龄等数字去计算,而活动消耗的预测公式则完全靠文字描述请你去对好入座,主观性很强。<br>总之就是说,活动消耗的预测公式是很主观和不准确的。当然有人会说,本来就是让你估计一下,不准确就不准确呗。<br>但问题是,我们的减值本来就只需要10%-20%的热量差,而光是活动消耗这块的估计值,动辙就可以让热量平衡点变动30%-40%,那么这个估计结果对我的意义,当然就很有限。<br>所以,减值到底应该吃多少热量,除了按照上面这些预测公式去计算外,<br>我觉得一个更好的操作办法就是,<strong>既然算不准,那就别算了</strong>。<br>你可以直接按经验化配额开始试吃,并做出调整就行了。</p><p><img src="/2023/20230319/table2.png"></p><p>所谓经验化配额,就是大家总结出来的对于一般人群按照这个量去吃,就可能恰好有10%-20%的热量缺口,让你开始掉体重。<br>按经验化配额边吃边调整的具体做法是,你先按这个配额吃两周,如果这两周你的体重能掉1.5%-2.5%,大概是2-3斤或3-4斤,那就很合适。<br>简直前期,因为碳水的突然降低,体重会掉得快一点,往后会更加和缓。如果你的体重完全不掉,就说明这个热量、这个配额是你的热量平衡点,<br>需要降低热量才能有热量缺口。调整的办法是,蛋白质和脂肪不变,下调碳水0.3-0.5克即可,这样试探者就制造出了热量缺口,大概率能引起体重下降了。<br>在减脂期里,体重能掉就说明一切正常,不用减碳水。体重不掉就还是下调0.3-0.5克碳水。但碳水不是无限递减的,非比赛型的减脂里,碳水最后也不要低于2克。<br>如果继续降低配额的话,就可能影响上班上学的状态了。<br><strong>为什么是以两周而不是两三天来看体重变化?</strong><br>我们每天体重变化的因素,一是体内食物的注流排空程度的差异。<br>有人会说,我总是空腹量体重,但这种空腹只是清除了大肠断的部分食物残渣而已,小肠断的食迷是排不掉的。<br>真正的空腹你去做一次,长劲就懂了。<br>要进食12小时并提前6小时喝泄药才能实现空腹。<br>我们平时说的空腹是还有食物注流的,可能导致每次量体重相差半斤八凉也不奇怪。<br>其次人体的脱水和吸水也很能影响体重,比如泰水食物量能直接影响鸡糖原储量,一克鸡糖原能扭住三克水,<br>人体内三五百克鸡糖原的变化就能造成体重两三斤的变化。<br>另外,那里子食物量变化以后,在水那平行的要求下,人体必须随之吸收或排出水,这也会影响体重。<br>极端的例子是,健身比赛选手上台前会停止吃盐和大量喝水来降低体内的那里子。<br>在水那平行的作用下,人体也随之排水,他们两三天就能丢好几斤的体重,这就是那里子对体重的影响。<br>生活里总有人说,我低碳低盐几天就能掉几斤体重,我一恢复饮食吃几天就长几斤体重。<br>这并不是因为在这么短的时间内,就能分解掉和合成出几斤的脂肪,那是不可能的。<br>主要就是因为低碳低盐能导致脱水掉体重,高弹高盐能导致吸水长体重。<br>最后,肌肉和脂肪的重量变化也能影响体重,这也是我们真正关心的。<br>不过肌肉和脂肪的重量变化是极其缓慢和微弱的,他们和短期内的体重变化几乎没有关系。<br>比如有人说自己乱吃东西,一天就长了一斤。<br>那我们来分析一下一天长一斤是否可能是肌肉或脂肪造成的呢?<br>首先不可能是肌肉造成的,没有人能一天长出一斤肌肉,同时也不可能是脂肪造成的,<br>因为一斤脂肪的热量是4500大卡,相当于4.5公斤米饭。<br>也就是说一个人必须在热量平衡点的基础上,在这一天里额外再吃4.5公斤米饭,<br>才可能利用这个热量盈余来合成出一斤脂肪。这当然是不可能的事情。<br>同样的,如果一天降低了一斤体重,也不能认为是今天分解了一斤脂肪,<br>因为需要在这一天里少吃或者运动消耗4.5公斤米饭的热量,才可能分解一斤脂肪。<br>这也是不可能的。<br>所以我们看体重一定不要只看两三天的变化,建议大家以两周来看变化。<br>为什么呢?我们用这张示意图简单解释。红色线表示脂肪量,它的增减波动很小,<br>但能随着时间退一而持续积累,一直减值就一直下跌。<br>绿色线表示食物注流排空和人体脱水吸水的重量变化。<br>它的增减波动很大,但不能持续积累,不能往上或往下的持续发展。<br>如果你两三天就去比较体重,哪怕你的红线脂肪确实降低了,<br>但绿线的波动更大,可以轻易覆盖掉红线脂肪的变化。<br>这时候体重的增减主要是绿线造成的,你无法通过体重变化来判断脂肪量的变化。<br>相反,你10天半月再去比较体重的话,你的红线脂肪的降低量已经绘积累的更多了。<br>绿线的波动就没有那么容易覆盖掉红线脂肪的变化。<br>这时候你才能从自己的体重变化里看出脂肪量的变化。<br>所以我们这里讲影视配额时,说的是以两周为周期来看体重变化并判断配额是否合适,<br>而不是让你吃两三天就去判断。<br>短期内的体重变化很难判断脂肪,<br>因为脂肪在短期里根本就不可能合成或分解多少来。<br>好了,以上我们先把碳碳脂总量讲完,那是不是这些既定的碳碳脂,<br>以任意方式分配到各参区,减脂效果都一样呢?<br>当然不是,所以我们还要关注碳碳脂的日内分配。<br>对既定的碳碳脂总量进行日内分配,主要是为了人为的操纵移道素来帮助减脂器多调脂肪找到肌肉。<br>移道素的功能是把血液里的葡萄糖胺计算等营养物质转晕到肌肉组织和脂肪组织里。<br>让肌肉和脂肪进入合成状态。<br>作为合成基素,移道素既能增计也能增值。<br>在没有力量训练的时候,身高移道素当然不会增弃,使会增值,这是移道素的负面作用。<br>但在力量训练之后,肌纤围已经破坏,身高移道素就能在增值的同时会促进增计,这是移道素的正面作用。<br>所以我们对移道素的管理的基本思路就是,力量训练和迅口餐要故意升高移道素来促进增计。<br>这天的其他餐以及休息日的时候,则要保持移道素的相对稳定。<br>我们来看饮食是如何影响移道素的。<br>科研工作者让寿室者设入同等热量的碳水蛋白质脂肪,然后监测他们的移道素变化。<br>结果是碳水最能刺激移道素分泌,蛋白质的刺激能力弱一些,脂肪的刺激能力最弱。<br>也就是说,我们不管吃什么东西都会刺激移道素分泌,我们要做的是对移道素刺激能力最强的碳水移向,进行合理的分配来趋力避害。<br>碳水有三个主要的投放时间点,最大的投放时间点是迅口餐。<br>迅口餐可以吃全天50%左右的碳水,加30-5车蛋白质,脂肪要少最好在20克以内。<br>碳水量大且用快碳,目的是为了升高移道素。<br>蛋白质也比较多,是为了给肌肉修复提供原料并且进一步升高移道素。<br>低脂不是因为害怕脂肪本身,而是因为脂肪对移道素的刺激能力比碳水要差得多。<br>所以在迅口餐总热量一定的前提下,低脂才能让出热量,吃缓成高碳,让迅口餐的移道素更高。<br>迅口餐的时间是最好能在练完半小时内开始吃上,不要抹几大半天都不去吃东西。<br>迅口餐的方式,如果能很快地吃上正餐,就直接去吃正餐。<br>如果没法很快吃到正餐,就可以用蛋白粉加便携快碳先淀一部分,剩一部分再用正餐补齐。<br>赶时间的话还可以全部用蛋白粉加便携快碳来解决,这样就不用吃正餐了。<br>便携快碳我用单独一页来举例,这些只是很少的一部分。<br>只要它的碳水率是脂肪率的5倍以上,并且是以糯米、面粉等精致碳水制成,就可以算作便携快碳了。<br>迅口餐故意升高移道素是非常重要的。<br>之前陆家们闹内部矛盾的时候,他们内部一位职业运动员在网上挂出过他们内部的饮食方案。<br>可以看出,他们训练后吃大量快碳来做高移道素,同时自己还要注射食物单位的外圆性移道素。<br>我不是让大家去复制他们这么大的碳水量,也不是让大家去打移道素,而是想强调,力量训练后升高移道素是非常重要的。<br>碳水的第二个投放时间点是早饭,早饭可以吃全天30%的碳水,碳水GI无所谓。<br>蛋白质和脂肪没太多要求,多少一点都可以,全天总量合规就行了。<br>一晚上的空腹后我们的血糖是很低的,早饭给一些碳水,让血糖升高一些是必要的。<br>不要在低血糖状态下开始上班上学。<br>早饭有点麻烦的是碳白质来源很少,如果是吃食堂的话,两个鸡蛋加一包牛奶差不多能凑合20克蛋白质,<br>但同时也有快20克脂肪了。这就意味着早饭的碳水不能是从油条煎饼、麻团等糖油混合物理获取,<br>只能来自于馒头花卷面条等纯碳水,否则早饭就可能花掉你全天大部分的脂肪配额。<br>如果是自己冲条早饭的话,碳水可以用燕麦片、燕麦敷、玉米粉、藕粉、芝麻糊等,蛋白质则用蛋白粉来提供。<br>碳水的第三个投放时间点是去面前,去面前可以吃全天20%的碳水,是中等或中高质爱的碳水都行,<br>如葱凉面包、香蕉、葡萄干、浙堂饮料等。这些碳水量也就小几时刻,使过定电肚子,不至于让你持撑影响训练,也不至于让你血糖飙升。<br>训练前给点碳水的目的是为了温和的提供血糖支持,让训练有力气,也能抑制训练过程里的肌肉分解,比喝支炼胺基酸还划算。<br>时间是你到了健身房吃都来得及。这张图我都发过好多次,不管是快谈慢谈吃下去,血糖都会立即升高,在半小时到一小时左右达到高点。<br>所以你到了健身房就吃,加上你换衣服、热身等单个的时间,这样你在训练中程正好匹配上碳水给你的血糖。<br>除了上述的迅候早饭迅前,其他餐的时候碳水可以少吃奶制不吃,主要靠吃蛋白质纸防和蔬菜来填饱肚子。<br>其他餐如果你专门去吃比较多的碳水来刺激到素的话,更可能起到增值的作用。<br>在增基期的时候由于碳水的总配额比较多,所以其他餐一般还能吃一些碳水。<br>但在减脂期的时候,尤其是碳水配额低于2.5克每公斤以后,也会明显感觉到全天碳水很紧张。<br>这时候就特别需要注意在其他餐节约碳水出来,有些满足迅候餐早饭迅前餐,尤其是不能抢劫迅候餐的碳水去给其他餐吃。<br>以上我们讲的是对碳单纸做日内分配来对移导素区立毕害,实际上就是碳水分配很讲究。<br>蛋白质和纸房不太讲究,只需要注意迅候餐要保证有30-50个蛋白质并且要20克以内低质。<br>其他时候的蛋白质和纸房分配可以集中一些,也可以均匀一些,这不重要,全天总量合规就行了。<br>下面我们按照大家不同的训练时间,根据上述的碳单纸分配原则,给大家列出了各餐的分配方案,不然有些观众还是看不懂。<br>早饭后就训练的人,直接把早饭作为你们的迅前餐。吃到早饭就开始练。<br>这一餐吃到六七分饱就好了,所有的练前餐都不要吃成全饱,完全保护的锻炼是很不舒服的。<br>练完后你直接吃迅候餐,如果不方便吃上正餐的话可以蛋白粉加便宜快餐来解决。<br>午饭和晚饭都是你的其他餐,我这里给了一点碳水,不然整个下午和晚上的时间太长,不吃碳水的话有些人会习惯不了。<br>再来看午饭前训练的人,把午饭作为你们的迅候餐,然后晚饭是你们的其他餐,理论上可以不吃碳水了。<br>你如果确实不习惯的话可以云一点碳水给晚饭,但一定要少,如果你把其他餐的碳水拉高了,本质上就是在抢劫迅候餐。<br>再来看晚饭前训练的人,把晚饭作为你的迅候餐,你的午饭是其他餐,理论上可以不吃碳水。<br>同样的,如果你不习惯的话,还是可以云一点碳水给午饭,但还是要少。<br>越是减脂期推进的时候,因为碳水总配额越来越紧张,所以大家就越来越需要注意把其他餐节约碳水出来让给迅候早饭迅节,尤其是不能去抢劫迅候餐的碳水。<br>最后是晚上才训练的人,这是很多上班族的情况,你可以把晚饭当作迅前餐,吃到个六七分饱垫垫肚子,就去练了。<br>重点是练完一定要吃迅候餐,晚上健身的人很容易对饮食进行反向操作,在午饭晚饭这些不太需要碳水的时候,他们米饭馒头尬尬地吃。<br>但晚上训练后又不敢吃任何碳水了,理由是白天已经吃过了,再吃就会长胖。<br>为什么其他餐吃那么多碳水都不怕胖,迅候餐吃碳水就会长胖呢?<br>实际上他们的问题是碳水的日内分配错了,他们在白天就把迅候餐的碳水给抢劫过来吃完了。<br>正确的做法是,午饭和晚饭的碳水都不要太多,练完后的迅候餐才是全天最大的一顿。<br>这样就把他们的反向操作给纠正过来了。<br>以上四种训练时间的饮食方案我列的都是四餐,带的总量都是一模一样的,没有谁比谁多吃,只是各餐的分配不同。<br>思路都是从其他餐节约碳水出来,优先供应给训后早饭训前。<br>唯一区别是吃了早饭就训练的人,和晚饭后才训练的人,我在他们的其他餐里放了一点碳水,否则他们每天会有一个很长的午碳水时间,有些人会觉得受不了。<br>最后是休息日怎么吃。休息日没有力量训练,没有增计窗口期,所以不要故意去升高移道素。<br>做法就是,每餐都是碳蛋纸的混合,各餐大概分碳就行。<br>总量上碳水可以减少一些,但白吃个纸房基本不变,多少一点也没事。<br>休息日碳水总量的减少是自然发生的,因为在休息日你不会吃迅前那20%的碳水,<br>迅后那50%的大碳水会改成常规量碳水,这样扣减以后,休息日的碳水总量自然就变少了。<br>因为碳水的总量减少了一些,再加上各餐的碳蛋纸是大概分碳的,这样全间的移道素就会比较稳定,从而减少移道素增值的负面作用。<br>当然,前面我们已经说了,只要一进食就会引起移道素分泌,我们能做的只是让它相对稳定不要太高而已。<br>好了,上面我们把碳蛋纸的总量和分配都说完了,你自己去找食物,配餐组合出这些碳蛋纸就行了。<br>但有的人肯定会说,不知道哪些食物里有碳蛋纸,所以我们再影视的最后一部分,讲一下食物来源。<br>首先讲碳水食物,碳水食物有两个属性,一是它的碳水率,就是100克这种食物里有多少个碳水,这个好理解。<br>另一个属性是它分解为葡萄糖的速度,也就是大家常说的高中低Gi碳水,或者快碳慢碳。<br>所有的碳水食物都必于分解为葡萄糖后才能进入血液工人体利用,<br>如果一种碳水吃下去后血糖上升得很快,就说明它分解为葡萄糖的速度很快。<br>Gi就是用来描述碳水分解为葡萄糖速度的一种指数。<br>怎么测食物的Gi呢?<br>我们再次引用这张图。<br>这条最高的线是受试者吃了含有50克碳水的葡萄糖后的血糖曲线。<br>比如葡萄糖水的浓度如果是软面20,那么他们就要吃250克葡萄糖水。<br>另外几条线是其他类型的碳水,也是让受试者吃含有50克碳水的食物来看血糖曲线。<br>比如金米饭的碳水率是25%,所以就要吃200克金米饭。<br>接着以0分钟到120分钟为界限,把葡萄糖曲线下方面积视为100。<br>其他食物的曲线下方面积是多少,它的Gi就是多少。<br>比如金米饭的血糖曲线很猛,它的曲线下方面积并不比葡萄糖小多少,可能算出来是80。<br>那么金米饭的Gi就是80,又比如五股饭的曲线比较平缓。<br>它的曲线下方面积会比葡萄糖小的多,可能算出来是40,那么五股饭的Gi就是40。<br>这就是Gi的测量办法,测的是涉入50克碳水后在120分钟内的血糖曲线下方面积。<br>在这张表里,我给大家列出了常见的高中低Gi碳水食物。<br>在应用上,训练过餐一般吃高Gi碳水去促进移导宿升高,其他时候则比较随意。<br>碳水的Gi值得我们关注,但也不是说极端重要,因为Gi是影响移导宿的众多因素里的其中之一而已。<br>碳水量、蛋白质量、纤维素量,乃至静时的顺序都能影响移导宿。<br>我们需要综合考虑,而不是只论快慢碳。尤其是碳水量的影响是很大的,你一餐的碳水量够大的话,移导宿就不会低。<br>所以我们说碳水的Gi值得关注,但又不要觉得就是它除载了移导宿。<br>接下来讲脂肪,脂肪的食物来源和碳水一样便宜都是,简直期的脂肪一定要吃够保底量。<br>脂肪是许多性激素的前提,也是脂溶性维生素,比如维生素AD1K,它们的吸收载体。<br>脂肪一旦吃少了,可能导致心肌素缺乏,如男性的搞冻下降,女性的生理期稳乱,以及导致使绍性维生素缺乏引起的各种刺身症状。<br>要知道我们设计的饮食配额已经有热量缺口了,那种近乎领指的饮食会把热量缺口搞得太大,也无法发挥脂肪的生理功能,只是一种自我摧残,对减脂是有害无益的。<br>那么,脂肪怎么吃才能比较合适呢?我的建议是炒菜的植物油,鸡蛋,牛奶,正常吃,尤其是炒菜千万不要放弃。<br>炒菜里的植物油并不多,不管是清炒还是重油,一家菜一片肉,它能附着的油量是有限的。<br>只要你不是去喝油汤或者舔盘子,你是吃不尽多少油的。<br>你要是真的害怕油,你就刮一下,甚至过一下水就行了。像左边这种清炒的菜,一人份你记为五克脂肪就够了。<br>像右边这种比较重油的菜,一人份你记为10-15克脂肪也够了。反正我自己是从来都吃食堂的炒菜,<br>包括水煮肉片、凉拌鸡,这些看起来牛稍微重一点的菜,我都是正常吃的。<br>在一般炒菜之外,有两类食物会隐藏很多脂肪,你要特别谨慎。一种是往内部吸油的菜,比如炒鸡蛋、炒茄子、炸蘑菇等。<br>前面说了,菜品表面能附着的油量是很有限的,但它如果能往内部吸油的话,就可以变得很猛了。<br>像炒鸡蛋我看过几个测评视频,如果是饭馆、食堂那种宽油炒蛋,一个鸡蛋能吸油20-30克。<br>要知道一个鸡蛋本身是5克蛋白质加6克脂肪,你吃一个炒鸡蛋只获得了5克蛋白质,却同时送给你30克脂肪,直接把你全天的脂肪配额占掉一大半。<br>所以说,这些往内部吸油的菜大家叫谨慎,不是说不能吃,但它非常占脂肪配额,你要吃的话就要正确进入配额。<br>除了这些往内部吸油的菜,另一类需要特别谨慎的是固体脂肪。固体脂肪的问题是,你看不出它有脂肪,<br>所以你觉得自己没有吃脂肪。<br>左边这样常用的白瓷勺一勺油是6克,让你喝下去你肯定觉得很离谱。<br>如果让你吃右边这样一个星巴克的常规高点,我们不会觉得多严重,但实际上有30多克脂肪,相当于喝了6勺油。<br>如果让你吃肯的鸡蛋两个蛋挞,你觉得并不离谱,实际上有30克脂肪,相当于喝了5勺油。<br>如果早饭吃两根油条,你觉得很正常,实际上也是30克脂肪,相当于喝了5勺油。<br>如果让你吃两把坚果,你觉得是特别健康的食物,实际上也是40多克脂肪,相当于喝了7勺油。<br>总之,我们会很紧替液体脂肪,但脂肪以固体形式存在时,你就会不知不觉地吃下去。<br>这些脂肪刺客不是说不能吃,而是说你要正确地寄入自己的脂肪配额。<br>你不能花啦啦的吃了一堆脂肪,又说自己没有吃。<br>现实中有很多人对炒菜里的那点植物油津津计较,但吃起固体脂肪的时候一点都不客气,这就颠倒主次了。<br>所以,我建议大家一般的炒菜鸡蛋牛奶也都正常吃,尤其是不要放弃炒菜。<br>拒绝吃炒菜的话,就会失去食物的舒适性和便捷性。<br>同时,对于往内部汐油的菜,以及肥肉、糖油混合物、坚果这些固体脂肪,要谨慎一些。<br>这些是可以吃,没问题,但它们会比较占你的脂肪配额,吃了就要正确地寄入配额。<br>脂肪方面最后再说下所谓的优质脂肪问题。<br>现在网上特别强调优质脂肪,说简直要吃坚果、花生酱这些富含不饱和脂肪酸的优质脂肪才行。<br>言下之意,植物油等其他的脂肪食物就是不优质脂肪了。<br>其实这是一个四十二非的说法。<br>首先,饱和、单不饱和、多不饱和这三种脂肪酸各有其真理功能都是人体所必须的,<br>本身就没有谁优质谁劣质之分。<br>包括中国营养协会在内,主要国家对三种脂肪酸设备的比例建议大概是1比1比1。<br>现在网上有些人把三种脂肪酸都要分出优劣来,这本身是很奇怪的。<br>其实我们再来看我们日常食物里的具体的脂肪酸比例。<br>常用的植物油的脂肪酸就是以不饱和脂肪为主的,这一点和坚果区别不大。<br>饱和脂肪烧高的是肥肉和乳质品,但也不是特别高,也就是五六成而已。<br>之一上我们就正常吃植物油、鸡蛋和乳质品,加上瘦肉里的微量的肥肉,<br>就正好能把三种脂肪酸组合出良好的比例。<br>坚果做一种脂肪来源当然是没问题的,但不至于说成是独一无二的优质脂肪。<br>我们先把自己因吃多少脂肪和吃了多少脂肪,能搞清楚就很厉害了。<br>没必要再去计算三种脂肪酸的比例这些太细碎又太不重要的事情。<br>好了,脂肪我就说完了,最后说蛋白质。<br>蛋白质的推荐设备量,新手也至少吃到每公斤1.5克。<br>接油量比较好的人可以吃到每公斤2克。<br>有人喜欢吃肉,蛋白质还可以吃到2-3克。<br>科研上已经反复证明了3克以内的蛋白质设入是不会损害胜功的。<br>但是2克以上的蛋白质对于维持肌肉就是多余的了。<br>它的性质是单纯的提供热量而已。<br>所以只要你在减脂期有热量缺口,体重在缓慢下降,那你的蛋白质多吃点也没事。<br>蛋白质的食物来源非常非常狭窄,纯粹的蛋白质来源只有两种。<br>一是猪牛羊、鸡鸭鱼虾等各种瘦肉,二是蛋白粉,除此之外就没有了。<br>鸡蛋和牛奶也是蛋白质来源,但它们会负责和蛋白质差不多量的脂肪。<br>如果你靠鸡蛋和牛奶作为主要的蛋白质来源,那你在吃够蛋白质的时候,脂肪早就涉入爆炸了。<br>除非你吃的是鸡蛋白、蛋清液、脱脂牛奶。<br>另外,豆类和豆腐等豆制品也有类似的情况。<br>他们在提供蛋白质同时会负责相当多的脂肪,所以不可能作为蛋白质的主要来源。<br>健身饮食里的最大难点就是找蛋白质吃。<br>在食堂大家要努力的找瘦肉吃,不需要吃水煮的瘦肉,炒菜或者凉拌的瘦肉都可以。<br>对于餐馆和外卖的话,我大概列出了这些东西,他们能提供好吃的瘦肉给你。<br>一顿饭的配餐就是要解决蛋白质,解决的蛋白质这一餐就配成了。<br>因为碳水和脂肪遍地都是,不可能没有。<br>还有一个小问题是很多人来问我,计算蛋白质时要不要去除植物蛋白?<br>如果你是用XL或者薄荷APP计算,主食和蔬菜里的植物蛋白会自动记录,你就不要专门提除了。<br>植物蛋白里的人体必须安计算的比例比较少,所以在生物价、安计算分等这些蛋白质评价方法里会显得弱势一些。<br>但因为植物蛋白占我们每天蛋白质设入量的比例是很低的,就算你麻烦的把这些植物蛋白打折,寄入蛋白质总量。<br>最后的结果无非就是少了几克、十克而已,这是无所谓的,所以不要去纠结要不要去除植物蛋白这个问题了。<br>好了,以上我们讲了碳蛋质总量、分配、食物来源,简直饮食就全部说完了。<br>下一个问题说简直期的运动。<br>运动有力量训练和油氧两类。力量训练这部分很简单,就是和增计器一样,计划、配种、组数都继续维持。<br>按照我们上面领事模式,因为热量缺口并不大,而且有汛前汛后的碳水支持,所以力量训练不会受什么影响。<br>运动方面我们重点讲讲油氧。<br>油氧应该视为制造热量缺口的一种方式。<br>我们需要的10%-20%的热量缺口,可以从饮食做,也可以从油氧做,也可以兼而有之。<br>在简直的前中期,比如在碳水2.5-3.5克的阶段,我们不会觉得有多饿,用饮食来提供热量缺口是很容易的。<br>花时间去做油氧,会不太划算。<br>但在简直后期,比如碳水只有2-2.5克的阶段,我们会感觉吃的很少。<br>继续用饮食制造热量缺口的难度会越来越大。<br>这个时候做一些油氧就会变得比较划算。<br>所以对于简直期的油氧,我建议你先看看能否用饮食就让体重开始缓掉。<br>如果你用饮食就能让体重开始缓掉,就说明你的热量缺口已经合适了,当然不需要费力去做油氧。<br>随着简直期的推进,如果你不想继续降低碳水的话,就可以增加油氧强度,每周可以做2-3次。<br>油氧的时间的话可以循序渐进,一般是40-60分钟。<br>有些同学不愿意做长油氧,而喜欢力量训练后做20分钟的短油氧,似乎也可以。<br>但由于油氧时间越长,脂肪的功能比例会越高,<br>所以单从理论上说,一个60分钟的长油氧会比3个20分钟的短油氧更有效,不过这是一种理论假设而已。<br>油氧的方式很多,跑步机、妥卫仪、游泳、爬楼、跳槽、打球、飞板等等的方式都可以,选什么方式不重要,比较重要的是心率。<br>油氧时的心率越高,单位时间消耗的热量更多,但能坚持的时间也越短,所以我们需要坚固心率和可持续信。<br>对于20-30岁的人来说,合理的油氧心率大概是每分钟120-150次。<br>这样的心率有相对较好的热量消耗功率,也能让人持续的坚持40-60分钟而不难受。<br>怎么测心率呢?用运动手玩手表、用跑步机上的握把,或者自己数20秒的脉搏,都能制造心率。<br>你在做油氧时自己调节强度,使得自己的心率进入这个区间就行了。比如跑步机的速度和坡度都能改变你的心率。<br>有人喜欢平坡跑步,有人喜欢斗坡慢走。<br>同样的,游泳、爬楼、跳槽、打球等其他游泳方式也一样。心率太高了,你就做得和缓一点,心率不够,你就做得紧凑一点。<br>游泳上一些不建议的做法,首先是不建议力量训练后接长游泳,这样会耽误你的迅口餐,进而损失了迅口的增计窗口期,浪费了自己的力量训练。<br>如果因为时间关系,你必须把游泳安排在力量训练之后的话,那可以做个20分钟的短游泳。<br>其实我觉得最方便的方式就是休息日去做个长游泳,这样完全不会影响力量训练。<br>另外也不建议做高频的长游泳。饮食和游泳都是制造热量缺口的工具,它俩加起来,让我们形成10-20%的热量缺口,也就是300-500%大卡的热量缺口,就行了。<br>很多人是误以为吃的越少越减脂,游泳越多越减脂,无原则的去执行,少吃多冻,很容易让你形成特别巨大热量缺口,这是不可能真正减脂的。<br>稍后我们再来详细讲讲。<br>最后提一嘴空腹游养的问题,这是网上真论最多的话题之一了。<br>它的理论假设是我们在起床后的血糖和鸡糖原水平比较低,这时候去做游养会有更高比例的脂肪功能。<br>空腹游养在健身运动员那边比较流行,但许多科研论文又觉得它是一个伪命题。<br>所以我觉得是否空腹游养就随便你了,这些都是不关键的细致末节。<br>如果你要做空腹游养的话,可以先吃点蛋白质或者支链按计算店一下,具称有异志肌肉分解的作用。<br>空腹游养后也不要继续空腹饿肚子,直接该吃饭就吃饭去。<br>以上我们就把简直的知识框架全部讲完了。<br>下面我们来讲一个同学的简直案例,看看一个新手是如何开始简直的,同时也融入讲解一些简直期的常见错误。<br>这是我们学校的一个同学,男生身高175,其中大半年来一直是80公斤,健身新手。<br>他在简直前代日常饮食是这样的,早饭、油条、包子、鸡蛋、午饭和晚饭是馒头煎饼和瘦肉炒菜,夜宵是炒面。<br>餐间饿了的零食会吃一些小雪糕小饼干。<br>我们来分析他的饮食。<br>分析饮食不是去看食物的健康和不健康而是看炭蛋质的总量和分配对不对。<br>现在看总量,我帮他计算了一下,碳水是4.0克,这个配合是一个增计器的配合,对于简直来说太高了。<br>简直应该下调到3-3.5克作为期时量。<br>蛋白质是1.2克,这显然是不够的,健身新手也至少要保底1.5克蛋白质。<br>纸房是1.3克,简直期应该下调到0.8克左右。<br>再来看分配,他各餐的碳水分配是随意的,晚上力量训练后他觉得不饿所以不吃东西,也就是没有迅口餐,丧失了增计的机会,这是一个很大的错误。<br>他的其他餐有在大吃碳水,主要起到了增值的作用。<br>这位同学决定要减肥后,他根据自己理解的简直餐把火石换成了这样。<br>简单的说就是,他用他认为的健康食品替代了不健康食品。<br>馒头煎饼换成了玉米红薯水果,瘦肉炒菜换成了纯粹的西荤肉,全脂牛奶换成了低脂牛奶。<br>饼干这些垃圾食品则完全不吃了。<br>OK,这样的饮食在网上极其常见,我们一搜索健康餐、减脂餐,冒出来的都是这些东西,也影响了很多人。<br>那么这样的健康饮食能减脂吗?我们还是来看碳蛋质的总量和分配。<br>算出来的结果是,他的碳水吃了2.1克,实际上普通人减脂在最末期才会到这么低的配额。<br>蛋白质吃了1.1克,还是老毛病蛋白质不够。<br>不要看他的食谱里吃了几两鸡胸肉,有鸡蛋牛奶什么的蛋白质就够了,够不够,要拉出来算账才知道。<br>脂肪的问题更大,他这种健康饮食不是炒菜,也戒断了糖油混合物,所以脂肪低到了只有0.2克,离脂肪的保底量还差得远。<br>最后来看总热量,和之前相比,总热量从2600多大卡,降为1100多大卡。光是饮食就制造了57%的热量缺口。<br>除此之外,因为他为了减脂还去每周做45次优雅,这样热量缺口可能进一步扩大到70%以上。<br>远远超过我们需要的20%热量缺口。<br>这样的结果就是,第一他会非常非常劳累,因为火事很少又难吃,还要叠加高频优雅。<br>第二他也不能真正的减脂,只有在前面一段时间会比较快的掉一些体重。<br>初期所丢到的体重既来自于脂肪和肌肉的分解,也来自于我们前面说过的,低碳低盐会导致人体的脱水。<br>接着,因为热量缺口极为巨大,会触发他的身体的自我保护,基础代谢会明显下降,使得热量差逐渐消失了。<br>由此形成一个现象,就是他明明吃的很少,明明动的很多,但是体重腰围就是不掉。<br>而且接下来他还可能认为,我一定是吃的还不够少,我一定是动的还不够多,所以才没有瘦下来。<br>所以他还会去进一步的少吃多动,这样再进一步的摧毁他的技术代谢,进入到了永远不可能减脂的恶心循环。<br>下面我们来给他调整出一个正确的减脂饮食,调整饮食就是调整碳难值的总量和分配。<br>先来看他的早饭,早饭的碳水特别高,蛋白质不够,脂肪略高但还凑合,所以我们给他去掉占据了大量碳水和脂肪配合的油条,增加了鸡蛋牛奶,就得到这样的结果。<br>然后是午饭,因为他是晚上健身,所以午饭是他的其他餐,要节约碳水,主要靠蛋白质脂肪和蔬菜来吃饱,所以煮食只吃一两米饭,瘦肉炒菜还是不便,另外再给他加一两酱牛肉和一份蔬菜来让他吃饱。<br>然后是晚饭,晚饭是他的迅前餐,垫垫肚子就要去健身了,简单点的话可以吃粗粮面包、香蕉、葡萄干、浙糖饮料的,如果确实很饿也可以去食堂吃点碳水,外加一点瘦肉。<br>这里举例的是让他吃一根玉米和一两酱牛肉垫垫肚子,吃完就去炼。<br>最后是炼完的迅口餐,由于其他餐节约了碳水出来,现在碳水的剩余配额还有很多,他的80公斤体重可以吃食堂3-4两米饭加瘦肉。<br>如果觉得吃不下或者要图方便的话也可以用便携快碳加蛋白粉来解决。<br>另外他的用餐时间也要尽量提前到炼完半小时以内就开始吃上,而不是像以前那样,炼完一两个小时之后觉得饿了才开始吃。<br>那个就不叫迅口餐了。<br>为了让他的饮食更舒服,我们还留了一些配额让他吃垃圾食品,比如我们给他加一只雪糕,这样太难指的总量配额就差不多了。<br>调整前后的饮食相比,碳水从4克降为2.9克,蛋白质从1.2克增加到1.8克,脂肪从1.3克降低到0.9克。<br>总热量下调了500大卡左右,热量缺口大约19%,这就是减脂饮食。<br>一是有10%-20%大约3500大卡的热量缺口,二是因吃热量因以特定的碳蛋脂比例来提供,30碳水的时机要适配他的力量训练。<br>如果可能还有人问,前面说的健身男性的减脂期的碳水70配额推荐是3-3.5克,为什么他在这里只吃了2.9克还差那么一点呢?<br>比如强调的是,这个视频你所说的所有的配额比例的数字都是一个大概方向,不是让你去按克数吃饭。<br>就比如说现阶段你的碳水配额是3克,那么你今天吃的不管是3克还是2.7克还是3.3克都可以,只要整体上比较稳定就可以了。<br>但是在我们开始一个全新的饮食模式时,一定还是要先做一点计算和规划,不要去按感觉吃饭。<br>没有足够经验的人按感觉吃饭是很不准的,就像我们刚才的举例。<br>这位同学按感觉计划的减脂餐完全就是错误的配额。<br>最后一部分内容我们来归纳一下大家在减脂时的常见错误,希望你没有走过这些弯路。<br>第一个常见问题是食物决定论。<br>很多人受网络宣传的影响认为吃干净健康的食物就能减脂,比如燕麦、红薯、糙米饭、鸡胸酸奶、西南花、零糖领子、零添加之类的。<br>其实不管是红薯燕麦也好,还是米饭饭饭也罢,这些食物都没有问题,但它们都只是碳蛋纸的来源而已。<br>健身饮食的实质是管理碳蛋纸的总量和分配,而不是吃什么食物就能怎样怎样。<br>只要碳蛋纸的总量和分配错了,健康食品也可以组合出很垃圾的饮食,就像我们前面刚才举例的那位同学。<br>脱离食物决定论,转而去关注碳蛋纸的总量和分配,这是我们健身入门的一个重要标志。<br>如果说换食物就能出腹肌就能报血管,那我还做这么长的视频干嘛呢?我直接开场让大家去吃桥麦面,吃鸡胸肉不就行了吗?<br>第二个常见问题是把减脂的叶臣无原则的少吃多动。很多人对热量缺口的限度没有概念,他觉得靠自己的意志力,越少吃,越多动就越减脂。<br>实际上你只需要在体重不变的热量平衡点的基础上,减少10-20%的热量就够了。对于一般体重人来说就是三五百大卡。<br>我们身边有很多人就像我们前面举例过的那个同学,一决定减脂就要去另起如造的搞一套所谓的干净饮食,同时叠加高频率的长有氧。<br>在不知不觉中就把自己的热量缺口做得非常大,在错误的道路上越走越远,还觉得自己在自律。<br>少吃多动这四个字本身没有问题,但它是正确的废话,是一团不明不白的酱糊化。就比如少吃这两个字。<br>少吃多少热量,热量由何种比例的碳蛋纸提供,以及碳水的实际。这些都是有讲究的,并不是说拿着少吃两个字就能搞定减脂的。<br>第三个常见问题是不吃或者少吃熏后餐。减脂期底不是每一分钟都要强迫身体是分解带解。<br>减脂期我们保持力量训练的强度无变,并且在熏后餐吃那么多碳水和蛋白质来提高胰道素,就是为了让身体进入增肌增脂的合成带解阶段。<br>我们必须用这个窗口继续做一点增肌的努力来尽可能抵消其他时段的肌肉分解。<br>有些人练完之后吃的很少奶汁不吃,这就是完全放弃了增肌。<br>当然,熏后餐之所引人如此的充足,原因是我们在其他餐节约了很多碳水出来。<br>我们不是让你加碳水,我们只是调整了碳水的日类分配来节约出了熏后餐。<br>第四个常见问题是不注意饮食的舒适性和可持续。<br>如果你想长期保持比较低的体脂率,火食就必须弄得好吃。<br>怎样让火食好吃一些呢?我这里有三个建议。<br>第一是一般的炒菜你正常吃,不需要去吃水煮。<br>你要小心的主要是西油的菜和固体脂肪就行了。<br>吃炒菜是不会危害你的脂肪配额的。<br>如果你拒绝吃炒菜,那么食堂和外卖几乎就没法吃了,这就太折腾了。<br>第二个建议是,你要把有限的脂肪配额花在你喜欢的食物上。<br>比如一包牛奶加两个鸡蛋差不多是20克脂肪,而一根懵鲁雪糕是15克脂肪。<br>你要喜欢鸡蛋牛奶,你就吃鸡蛋牛奶。你要喜欢雪糕,你就吃雪糕。<br>我们每天的脂肪配额有大几十克,是绝对能支持你吃一些所谓的垃圾食品的。<br>你只需要做好配额管理就行了。<br>第三个建议是,要利用好熏厚餐吃精致碳水和甜口食品的机会。<br>熏厚餐是全年碳水最多的一顿,而且一般吃的是快碳。<br>各种甜口的碳水糕点,比如米糕、发糕、青团、瓷巴、豆沙包、粘豆包、云片糕等等,都可以当作你的熏厚餐,完全没有问题。<br>研制的第五个常见问题是,心态太极,预期太高。我们今天在网上看到很多博主两三个月研制就有很好的效果,或者说只减了几公斤体重就开始暴雪管了。<br>实际上他们能这么快出效果,是因为他们定期就会做研制。<br>基础体制率只有10%-15%,所以只需要两三个月,丢几公斤体重就能出效果了。<br>普通人的基础体制率要高了多,你要达到别人那种效果,就需要用更长时间去丢更多体重才行。<br>如果你想一步到位,就需要把减脂期拉长成五六个月去掉更多体重。<br>如果你只想减脂两三个月,那你就可以分为两三次来逐步减干。<br>总之,你如果基础体制率比较高的话,就不可能两三个月丢几公斤体重就开始击又拉丝。<br>不要对自己有过高的预期。<br>好了,我用这个视频就把减脂讲完了。最重要的内容我放在这里你可以截图。<br>这是减脂的饮食配额和碳水地检方法。<br>这是各餐情况的介绍。<br>早饭、迅前、迅后、其他餐、休息日。<br>这是不同训练时间适配的各餐顺序,早上训练和上午训练的。<br>最后是有氧的建议做法。<br>有氧只是一种热量缺口的提供方式。<br>饮食比有氧更关键也更重要,一定要先学会吃饭,再去考虑有氧。<br>感谢大家看完这个超长视频。<br>我做这个视频的初衷是想让大家学会科学的舒适的生活化的完成减脂。<br>我在开头就说了,减脂首先是一个方法问题,和坚持或者毅力没什么关系。<br>看再多的粒子鸡汤都没用。<br>学会方法才有用。<br>最后要感谢本期节目的金主爸爸曼迪米洛地尔丁生发喷雾。<br>保住头发人人有责,不要肌肉大朗,头也涂了。<br>每天用曼迪喷雾简单喷一喷,维持好自己的发机线,顺手抹抹眉毛,还有浓眉效果,<br>让丑的变帅,帅的更帅。<br>我的账号是在主页里发框架型视频,在动态里发碎片画真实,<br>路过的朋友可以随手点个关注。有提问可以发在评论区,我尽量的回复。<br>下期节目我们再见!</p>]]></content>
    
    
    
    <tags>
      
      <tag>健身</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图深度学习</title>
    <link href="/2023/20230221/"/>
    <url>/2023/20230221/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于《图深度学习》马耀，汤继良。跳过了一些专用于二分图、超图等章节。</p><span id="more"></span><hr><h2 id="图基础"><a href="#图基础" class="headerlink" title="图基础"></a>图基础</h2><h3 id="谱图论"><a href="#谱图论" class="headerlink" title="谱图论"></a>谱图论</h3><p>拉普拉斯矩阵L&#x3D;D-A</p><blockquote><p><strong>定理：</strong> 对于图G&#x3D;{V,E}，其拉普拉斯矩阵的特征值是非负的。</p></blockquote><blockquote><p><strong>定理：</strong> 给定一个图G,其拉普拉斯矩阵的特征值为0的数目(特征值0的重数）等于图中连通分量的数目。</p></blockquote><p>图信号的谱域基础是图傅里叶变换 (Graph Fourier Transform, GFT)，它是建立在谱图论之上的。</p><p>传统的傅里叶变换可表示为:</p><p>$$<br>\hat{f}(\xi) &#x3D; &lt;f(t), \exp(-2\pi i \xi t)&gt; &#x3D; \int _ {-\infty}^{\infty} f(t) \exp(-2\pi i \xi t) dt<br>$$</p><p>式中，$ \hat{f} $ 是 $ f $ 的傅里叶变换；$ \xi $ 表示相应指数的频率。它将信号 $ f(t) $ 分解为一系列复指数形式 $ \exp(-2\pi i \xi t) $，这些指数是思维拉普拉斯算子（或二阶微分算子）的特征函数，因为：</p><p>$$<br>\begin{aligned} \nabla(\exp(-2\pi i \xi t)) &amp;&#x3D; \frac{\partial^2}{\partial t^2} \exp(-2\pi i \xi t) \\ &amp;&#x3D; \frac{\partial}{\partial t} (-2\pi i \xi) \exp(-2\pi i \xi t) \\ &amp;&#x3D; -(2\pi i \xi)^2 \exp(-2\pi i \xi t). \end{aligned}<br>$$</p><p>类似地，一个在图 $ G $ 上的信号 $ f $ 的图傅里叶变换可表示为：</p><p>$$<br>\hat{f}[l] &#x3D; &lt;f, u_l&gt; &#x3D; \sum _ {i&#x3D;1}^{N} f[i] u_l[i]<br>$$</p><p>式中，$ u_l $ 表示图的拉普拉斯矩阵 $ L $ 的第 $ l $ 个特征向量，其对应的特征值 $ \lambda_l $ 表示 $ u_l $ 的频率（或平滑度）。向量 $ \hat{f} $ 是 $ f $ 的图傅里叶变换，$ \hat{f}[l] $ 表示它的第 $ l $ 个元素。这些特征向量是图 $ G $ 上傅里叶基，而 $ \hat{f} $ 由信号 $ f $ 对应这些傅里叶基的图傅里叶系数组成。$ f $ 的图傅里叶变换也可以用矩阵形式表示为：</p><p>$$<br>\hat{f} &#x3D; U^T f<br>$$</p><p>式中，矩阵 $ U $ 的第 $ l $ 列是 $ u_l $。</p><p>根据</p><p>$$ u_l^T L u_l &#x3D; \lambda_l \cdot u_l^T u_l &#x3D; \lambda_l $$</p><p>可知特征值 $ \lambda_l $ 度量对应的特征向量 $ u_l $ 的平滑度。更具体地说，与小的特征值相关联的特征向量在图中变化缓慢，即相邻节点的特征向量的值是相似的。因此这些特征向量是平滑的，并在整个图上低频地变化。</p><h2 id="图嵌入"><a href="#图嵌入" class="headerlink" title="图嵌入"></a>图嵌入</h2><p><img src="/2023/20230221/randowwalk.png"></p><p><strong>node2vec:</strong><br>设 $G &#x3D; {V, E}$ 表示一个连通图。考虑以图 $G$ 中的 $v^{(0)} \in V$ 为起始节点的随机游走。假设随机游走规则从节点 $v^{(t-1)}$ 走到节点 $v^{(t)}$，现在停留在节点 $v^{(t)}$。接着随机游走需要决定下一步访问哪个节点。不同于 DeepWalk 中均匀地从 $v^{(t)}$ 的邻居中选择 $v^{(t+1)}$，node2vec 基于 $v^{(t)}$ 和 $v^{(t-1)}$ 定义节点的采样概率。特别地，选择下一个节点的未归一化“概率”定义如下：<br>$$<br>\alpha _ {pq}(v^{(t+1)}|v^{(t-1)}, v^{(t)}) &#x3D; \begin{cases} \frac{1}{p}, &amp; \text{dis}(v^{(t-1)}, v^{(t+1)}) &#x3D; 0, \\ 1, &amp; \text{dis}(v^{(t-1)}, v^{(t+1)}) &#x3D; 1, \\ \frac{1}{q}, &amp; \text{dis}(v^{(t-1)}, v^{(t+1)}) &#x3D; 2, \end{cases}<br>$$</p><p>式中，$\text{dis}(v^{(t-1)}, v^{(t+1)})$ 表示节点 $v^{(t-1)}$ 和 $v^{(t+1)}$ 之间的最短路径的长度。</p><p>对于DeepWalk，有以下定理：</p><blockquote><p>在矩阵形式下，给定图G，采用负采样策略的DeepWalk得到的嵌入表示等价于分解以下矩阵得到的解：<br>$$<br>log(\frac{vol(G)}{T}\sum _ {r&#x3D;1}^T(AP^r)D^{-1})-log(k)<br>$$<br>其中$P&#x3D;D^{-1}A$，$vol(G)&#x3D;\sum _ {i&#x3D;1}^|V|\sum _ {j&#x3D;1}^|V A _ {i,j}$,k为负样本数量</p></blockquote><p>社区模块度:<br>$$<br>Q&#x3D;\frac{1}{2*vol(G)}\sum _ {ij}(A _ {i,j}-\frac{d(v_i)d(v_j)}{vol(G)})h_ih_j<br>$$</p><p>其中，$h_i$为节点i为哪个社区的指示值。</p><p>超图嵌入常使用MLP做辅助。</p><h2 id="图神经网络"><a href="#图神经网络" class="headerlink" title="图神经网络"></a>图神经网络</h2><h3 id="图滤波器"><a href="#图滤波器" class="headerlink" title="图滤波器"></a>图滤波器</h3><h4 id="基于谱的图滤波器"><a href="#基于谱的图滤波器" class="headerlink" title="基于谱的图滤波器"></a><strong>基于谱的图滤波器</strong></h4><p>使用图傅里叶变换，我们可以得到$f’&#x3D;U\cdot\gamma(\Lambda)\cdot U^Tf$。</p><p>若假设有一个定义在图G上的噪声图信号$y&#x3D;f_0+\eta$，其中$\eta$是附加</p><p>的与原信号不相关的高斯噪声，模型的目标是从噪声信号y中恢复原始信号$f_0$。设原始信号九相对于图G是平滑的。为了利用信号$f_0$的平滑性的先验信息，在优化问题中引入形式为$f^TLf$的正则项：<br>$$<br>argmin_f ||f-y||^2+cf^TLf<br>$$<br>求导有$f’&#x3D;U(I+c\Lambda)^{-1}U^Ty$。</p><p><strong>Poly-Filter：</strong></p><p>对$\gamma(\cdot)$用K阶截断多项式建模，即$\gamma(\lambda_l)&#x3D;\sum\theta_k\Lambda^k$。</p><p>根据$U\cdot\Lambda^k\cdot U^T&#x3D;L^k$：<br>$$<br>f’&#x3D;\sum \theta_k L^kf<br>$$<br><strong>Cheby-Filter：</strong></p><p>切比雪夫多项式$T_k(y)&#x3D;2yT _ {k-1}(y)-T _ {k-2}(y)$，其中$T_0&#x3D;1,,T_y&#x3D;y$。</p><p>对于$y\in[-1,1]$，$T_k(y)&#x3D;cos(k\cdot arccos(y))$。</p><p>此外，切比雪夫多项式满足以下关系：<br>$$<br>\int _ {-1}^{1} \frac{T_l(y)T_m(y)}{\sqrt{1-y^2}} dy &#x3D;<br>\begin{cases}<br>\delta _ {l,m} \pi&#x2F;2, &amp; m, l &gt; 0, \\<br>\pi, &amp; m &#x3D; l &#x3D; 0,<br>\end{cases}<br>$$<br>因此，这些切比雪夫多项式形成了关于$dy&#x2F;\sqrt{1-y^2}$的平方可积函数的希尔伯特空间（Hillbert Space）的正交基。</p><p><strong>GCN-Filter：</strong></p><p>将切比雪夫多项式的阶数设为K &#x3D; l,并把最大特征值近似为2。即：<br>$$<br>\begin{align}<br>\gamma(\Lambda)&amp;&#x3D;\theta_0T_0(\tilde\Lambda)+\theta_1T_1(\tilde\Lambda)<br>\\&amp;&#x3D;\theta_0 I+\theta_1\tilde\Lambda<br>\\&amp;&#x3D;\theta_0I+\theta_1(\Lambda-I)<br>\end{align}<br>$$<br>用于f’时$f’&#x3D;\theta_0f-\theta_1(D^{-1&#x2F;2}AD^{1&#x2F;2})f$，令$\theta&#x3D;\theta_0&#x3D;-\theta_1$，$f’&#x3D;\theta(I+D^{-1&#x2F;2}AD^{1&#x2F;2})f$。</p><p>矩阵$I+D^{-1&#x2F;2}AD^{1&#x2F;2}$的特征值的范围是[0,2],故对信号f重复滤波时有可能出现数值不稳定的现象。</p><p>因此，再归一化(renormalization)技巧被提出用来解决该问题。即用$\tilde D^{-1&#x2F;2}\tilde A\tilde D^{1&#x2F;2}$代替$I+D^{-1&#x2F;2}AD^{1&#x2F;2}$,其中$\tilde A&#x3D;A+I$和$\tilde D_ii&#x3D;\sum_j \tilde A _ {i,j}$。经过这些简化步骤，GCN-Filter最后可被表示为：<br>$$<br>f’&#x3D;\theta \tilde D^{-1&#x2F;2}\tilde A\tilde D^{1&#x2F;2}f<br>$$</p><h4 id="基于空间的图滤波器"><a href="#基于空间的图滤波器" class="headerlink" title="基于空间的图滤波器"></a>基于空间的图滤波器</h4><p>GraphSage-Filter</p><p>GAT-Filter</p><h2 id="图神经网络的健壮性"><a href="#图神经网络的健壮性" class="headerlink" title="图神经网络的健壮性"></a>图神经网络的健壮性</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>攻击者可以在模型训练和模型测试阶段执行攻击。根据攻击者执行攻击的能力， 攻击可大致分为逃逸攻击和投毒攻击：</p><ul><li>逃逸攻击：攻击是在训练好的模型上，即在测试阶段进行的。换言之，在逃逸攻击的模式下，攻击者不能更改模型的参数或结构。</li><li>投毒攻击：攻击发生在模型训练之前。因此，攻击者可以在训练数据中插入“毒药”，使得基于该数据训练的模型出现故障</li></ul><p>除节点特征外，图数据还提供了丰富的结构信息。因此，攻击者可以从不同的角度对图结构数据进行扰动，例如修改节点特征、添加或删除边和添加新节点：</p><ul><li>修改节点特征：攻击者可以在保留图结构的同时修改节点的特征。</li><li>添加或删除边：攻击者可以添加或者删除原图中已有的边。</li><li>添加新节点：攻击者可以向原有的图添加新的节点，并将它们和原图中的节点相连</li></ul><h3 id="图对抗攻击"><a href="#图对抗攻击" class="headerlink" title="图对抗攻击"></a>图对抗攻击</h3><h4 id="白盒攻击"><a href="#白盒攻击" class="headerlink" title="白盒攻击"></a>白盒攻击</h4><p><strong>PGD拓扑攻击：</strong></p><p><strong>基于积分梯度的攻击：</strong></p><p>由于攻击者仅被允许执行从0到1或从1到0的修改，梯度信息可能没有太大帮助，因为考虑到图神经网络模型是非线性的，单个点上的梯度并不能反映诸如从0到1或从1到0的大变化的影响。因此，受积分梯度(Integrated Gradients)的启发，利用离散积分梯度来设计分数，称为积分梯度分数（IG-Score）。</p><p>$IG_H(i,j) &#x3D; \sum _ {k&#x3D;1}^{m} \frac{\partial \mathcal{L}_i(\frac{k}{m}(H _ {i,j} - 0))}{\partial H _ {i,j}}; \quad 1 \to 0, H _ {i,j}&#x3D;1,$</p><p>$IG_H(i,j) &#x3D; \sum _ {k&#x3D;1}^{m} \frac{\partial \mathcal{L}_i(0 + \frac{k}{m}(1 - H _ {i,j}))}{\partial H _ {i,j}}; \quad 0 \to 1, H _ {i,j}&#x3D;0$</p><h4 id="灰盒攻击"><a href="#灰盒攻击" class="headerlink" title="灰盒攻击"></a>灰盒攻击</h4><p><strong>Nettack：</strong><br>$$<br>argmax (max\quad lnZ’ _ {i,c}-lnZ’ _ {i,y_i})<br>$$<br>其中$Z’&#x3D;f _ {GNN}(A’,F’;\Theta’)$。</p><p>代理损失：<br>$$<br>L _ {sur}(A,F;\Theta,v_i)&#x3D;max _ {c\ne y_i}([\tilde A^2F\Theta] _ {i,c}-[\tilde A^2F\Theta] _ {i,y_i})<br>$$<br><strong>Metatack：</strong></p><p>攻击者的目标可以数学地表示为一个双层优化问题：</p><p>$$<br>\min _ {G’ \in \Phi(G)} \mathcal{L} _ {\text{atk}}(f _ {\text{GNN}}(G’; \Theta^\ast)) \quad \text{s.t.} \quad \Theta^\ast &#x3D; \arg \min _ {\Theta} \mathcal{L} _ {\text{tr}}(f _ {\text{GNN}}(G; \Theta))<br>$$</p><p>$f _ {GNN}$代表受害模型；tr表示用于训练模型的损失函数。</p><p>定义$L _ {atk}$的一种方法可以是将其设为训练模型的负值。另一种方法是首先使用原图g上经过良好训练的代理模型预测无标签节点的标签，然后将预测用作无标签节点的“标签”。<br>$$<br>L _ {atk}&#x3D;-L _ {tr}-\beta L _ {self}<br>$$<br>使用元梯度：<br>$$<br>\begin{align}<br> \nabla_G^{\text{meta}} &amp;&#x3D; \nabla_G \mathcal{L} _ {\text{atk}}(f _ {\text{GNN}}(G; \Theta_T))<br> \\&amp;&#x3D; \nabla_f \mathcal{L} _ {\text{atk}}(f _ {\text{GNN}}(G; \Theta_T)) \cdot \left[ \nabla_G f _ {\text{GNN}}(G; \Theta_T) + \nabla _ {\Theta_T} f _ {\text{GNN}}(G; \Theta_T) \cdot \nabla_G \Theta_T \right]<br> \end{align}<br>$$</p><p>计算元梯度 $\nabla_G^{\text{meta}} &#x3D; \frac{d \mathcal{L} _ {\text{atk}}}{d G}$，即攻击损失对图 $G$ 的总导数。根据链式法则，$\mathcal{L} _ {\text{atk}}$ 对 $G$ 的导数首先会涉及到 $\mathcal{L} _ {\text{atk}}$ 对 $f$ 的导数，然后再乘以 $f$ 对 $G$ 的导数：<br>$$ \frac{d \mathcal{L} _ {\text{atk}}}{d G} &#x3D; \frac{\partial \mathcal{L} _ {\text{atk}}}{\partial f} \cdot \frac{d f}{d G} $$<br>$\frac{\partial \mathcal{L} _ {\text{atk}}}{\partial f}$ 即 $\nabla_f \mathcal{L} _ {\text{atk}}(f _ {\text{GNN}}(G; \Theta_T))$。由于 $f &#x3D; f _ {\text{GNN}}(G, \Theta_T(G))$，其全导数为：<br>$$ \frac{d f}{d x} &#x3D; \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \cdot \frac{d y}{d x} $$</p><p>反转元梯度的符号：<br>$$<br>s(i,j)&#x3D;\nabla _ {A _ {i,j}}^{meta}(-2A _ {i,j}+1)<br>$$</p><h4 id="黑盒攻击"><a href="#黑盒攻击" class="headerlink" title="黑盒攻击"></a>黑盒攻击</h4><p>在黑盒攻击设定中，攻击者无法获取受害者模型信息。攻击者只能查询受害者模型的预测结果。这类方法大多采用强化学习来学习攻击策略。它们将受害者模型视为一台黑盒查询机，利用查询结果设计强化学习的奖励。</p><h4 id="图净化"><a href="#图净化" class="headerlink" title="图净化"></a>图净化</h4><ol><li><p>除去特征相似度低的节点之间的边<br>实验研究表明，许多对抗攻击方法（如Nettack和IG-FGSM ）倾向于添加边来连接节点特征明显不同的节点。类似地，当除去边时，这些攻击方法倾向于移除具有相似特征的节点之间的边。故有该方法试图除去特征差异很大的节点之间的边。更具体地说，它提出了一个评分函数度量节点特征之间的相似度。</p></li><li><p>邻接矩阵的低秩近似<br>对Nettack等攻击产生的对抗扰动进行实验探究，结果表明，Nettack往往会扰动图结构，从而增加邻接矩阵的秩。同时，邻接矩阵取值最小的奇异值的数量也会增加。因此，有方法基于奇异值分解（SVD），以消除加入图结构中的对抗扰动。具体而言，给定一个图的邻接矩阵A,用SVD分解它， 只保留top-k个奇异值来重构邻接矩阵。重构后的邻接矩阵可以近似地认为是净化后的图结构，可直接用于图神经网络模型。</p></li></ol><h4 id="图结构学习"><a href="#图结构学习" class="headerlink" title="图结构学习"></a>图结构学习</h4><p><strong>Pro-GNN：</strong><br>$$<br>min _ {\Theta,S}L_train(S,F;\Theta)+||A-S||_F^2+\beta_1||S|| _ 1+\beta _ 2||S|| _ \ast+\beta _ 3\cdot tr(F^TLF)<br>$$</p><h2 id="图上的其他深度模型"><a href="#图上的其他深度模型" class="headerlink" title="图上的其他深度模型"></a>图上的其他深度模型</h2><h3 id="图上的自编码器"><a href="#图上的自编码器" class="headerlink" title="图上的自编码器"></a>图上的自编码器</h3><p>编码器:$Z&#x3D;f _ {GNN}(A,X;\theta _ {GNN})$</p><p>解码器：$\hat A&#x3D;\sigma(ZZ^T)$</p><h3 id="图上的循环神经网络"><a href="#图上的循环神经网络" class="headerlink" title="图上的循环神经网络"></a>图上的循环神经网络</h3><p><img src="/2023/20230221/RNN.png"></p><h2 id="图神经网络的高级方法"><a href="#图神经网络的高级方法" class="headerlink" title="图神经网络的高级方法"></a>图神经网络的高级方法</h2><blockquote><p>定理：令G表示一个以A作为其邻接矩阵的连通的非二分图，对于任何特征$f\in R^N$，则：<br>$$<br>lim _ {L\to \infty} (D^{-\frac{1}{2}}AD^{-\frac{1}{2}})^Lf&#x3D;\theta_1 u_1<br>$$<br>其中，u1是$D^{-1&#x2F;2}AD^{-1&#x2F;2}$与其最大特征值相关的特征向量，且$\theta_1 u_1^T f$</p></blockquote><h3 id="WL测试"><a href="#WL测试" class="headerlink" title="WL测试"></a>WL测试</h3><blockquote><p>定理：给定任意两个非同构图G1和G2，如果图神经网络模型将这两个图映射到不同的嵌入中，则WL测试也可以确定这两个图是非同构的。</p></blockquote><blockquote><p>定理：如果图神经网络模型中的AGG()、COM()和POOL()函数是单射的，当它有足够多的图滤波层时，可以将通过WL测试为非同构的两个图映射到不同的嵌入。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>图神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何爱自己</title>
    <link href="/2023/20230206/"/>
    <url>/2023/20230206/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>转载自<a href="https://www.zhihu.com/question/21002625/answer/33803818">知乎</a>。</p><hr><p><strong>请像爱一个你爱的人一样去爱。</strong></p><p><em>你是如何爱一个人的？</em></p><p><em>你喜欢关于ta的一切吗？ta的颜，ta的语，ta的一颦一笑？ta的努力，ta的童心，ta的生活轨迹？</em></p><p><em>你会关心ta吗？你会对ta嘘寒问暖吗？ta生病了，你会不会照顾ta？</em></p><p><em>你会帮助ta吗？ta取得了成就，你会为ta开心，为ta喝彩，为ta感到骄傲自豪吗？ta受到挫折，你会安慰ta，支持ta，鼓励ta吗？</em></p><span id="more"></span><p>**我会呢！因为我爱他。**因为我爱他，所以我希望成为一个最好的伴侣。我向着成为“贤内助”的方向努力。</p><p>我是一个十分感性的姑娘，但是在恋爱中，我会注意让自己理性地思考问题。我在各个方面考虑他的感受，我在各种细节思考他的需求。</p><p>我关心他，关注他，引导他，帮助他，跟他撒娇，给他纠正错误，天冷给他织围巾，提醒他带外套；他累了给他按摩，他困了给他铺床；室内空调温度低，倒热水给他喝，还要确保不烫嘴。</p><p>我学做菜，因为他爱吃；我炖鸡汤给他喝，因为喜欢看他眼睛里闪着光的欢喜笑脸。</p><p>我要求自己成为他的贤内助，辅助他取得事业上的成功。我督促他健身和早起，并且以身作则。我在外人面前，以朋友的身份，真诚地夸赞他的优点，使他更受欢迎。</p><p>有时我会在最亲近的朋友间，当面指出他的错误， 调侃他懒惰的小毛病。更多的时候，我会提醒自己，把一些话留着私底下告诉他，避免他难堪。</p><p>他取得了成就，我发自内心地赞美他；他受到了挫折，我心疼，温柔地安慰他，然后理性地分析事态，与他探讨解决方案。</p><p>我不宠溺他。他的不好的习惯，会在我的引导下慢慢改掉或者改善。</p><p><strong>我爱他，我就对他好。然而我不曾意识到，我对自己并不好。</strong></p><p>**我不注重自己的健康，不关注身体的需求。**一个人的时候，不注重规律饮食，很容易就不吃饭、不按时吃饭、叫外卖、吃零食、吃泡面，怎么糊弄怎么来。渴了不去喝水，饿了不去吃东西。</p><p>**我对自己要求很严。**我对别人宽容，<strong>对自己却总像是拿着个鞭子</strong>，鞭策自己在各个方面都要努力，都要优秀。我绷紧了神经，努力向前冲刺，给自己的小身躯上压上了蜗壳，重重的。</p><p><strong>我不体恤自己劳累，我不体谅自己的身体</strong>。我生病了吃药只为不让病躯拖慢自己。我受挫了先责备自己。不敢畅快地哭，受伤了也舍不得给自己的心几句温柔的安慰。</p><p>**我吝惜对自己的褒奖、欣赏。**取得了成就，我很少打从心底地夸赞自己。不欣赏自己，担心自己的外表不够吸引人。</p><p>**我不关注我内心的需求。**我总是朝着“我应该”的方向去做选择，很少考虑“我想要”的那些选项。</p><p>而今我想想，如果是他，我怎会舍得让他这么糊弄自己的身子呢。我关心他的身体健康，我爱他，所以我会督促他规律饮食，我会愿意为他做饭，让他注意营养均衡。如果是他，我怎会不支持他，鼓励他，为他欢喜为他愁？</p><p>**从前的我，还没有想过爱自己。**是的，<strong>我还没有想过爱自己。我还没有好好思考过这个问题。我从未意识到这是个问题。</strong></p><p>我爱他，我爱至亲，我爱护花花草草小猫小狗。</p><p><strong>我却没想过要爱自己。这是多么可笑啊！</strong></p><p><strong>多幸运才能在这世上走一遭，为何不该趁此机会好好地爱自己吗？有许许多多的人，在我的生命里出现。然而人生这条路，大部分时间只能自己走。</strong></p><p><strong>我是重要的，很重要，非常重要。</strong></p><p><strong>我值得用爱别人的方式去爱自己，像谈恋爱一样去爱自己！我需要用心地爱自己，就像爱我爱的人一样。我知道如何用心地爱一个人。我用同样的办法去爱自己。</strong></p><p><em>你愿意从今天开始，承诺好好地爱自己吗？</em></p><p><em>你愿意变得“自恋”一些吗？你愿意接纳自己的外表，珍惜自己，认可自己作为一个独立无二的存在吗？</em></p><p><em>你愿意从今天开始关心自己的健康吗？生病了，你能不能负责任地照顾好自己呢？</em></p><p><em>你愿意帮助自己的心吗？ 取得了成就，你会为自己开心，为自己喝彩，为自己感到骄傲自豪吗？受到了挫折，你会安慰自己，支持和鼓励自己吗？</em></p><p>我愿意。因为我爱自己。</p><p>我开始关心自己，倾听自己的心声，关注自己的需求。我开导自己，帮助自己。<strong>因为我知道自己是重要的。</strong></p><ul><li><em>给自己做好吃的。哪怕只有自己一个人吃，认认真真地炒两个小菜，调一小碟酱料，抓起筷子愉快地说：嘿，要开动喽！</em></li><li><em>不再忽视自己的需求。工作时再忙再赶，口渴了，起身给自己倒杯热水，还要吹吹，不要烫嘴；眼睛酸涩了肩膀疼了，起身走走，给自己揉揉，捏捏，在心里说一声：亲爱的，辛苦了；</em></li><li><em>受挫的时候，温柔地安慰自己，肯定自己：“亲爱的，你很努力，你也很聪明。运气有时候忙着照顾别人所以没来得及眷顾你。” 然后理性地分析事态，寻找解决方案；</em></li><li><em>取得了成就，赞美自己，给自己肯定（就是给自己点个赞～）</em></li><li><em>不要宠溺自己。改掉不好的习惯。不要因为“希望别人对我的看法很好”而健身、练舞、读书、做饭、打扫房间，而是为了自己能更自信，为了让自己身体健康，为了使自己开心，为了让自己骄（自）傲（恋），才去做那些“使自己变得更好”的事。</em></li><li><em>像对待爱人一样耐心地对自己。不再急着看到成绩，不再急着要求自己成长。</em></li></ul><p>有时候我在想，那么是不是就要让自己安逸舒服放纵？因为不想让自己遭受压力巨大的日子，而放弃一次选举，放弃一个奖项的申请，放弃一次难度系数很高的考试，放弃参加一个有趣但很有挑战的比赛？</p><p>我既想做这些，又明明知道每次一旦开始这么一个任务，就会让自己忙成陀螺，让自己压力山大，需要逼着自己去做好多好多额外的事，需要逼着自己去加倍地努力，需要冒着失败和受挫的风险，冒着吃力不讨好、花了大量时间最后一无所获的风险，冒着最后的结果让自己伤心难过的风险。这是爱吗？这是合适的爱的方式吗？我的心里很纠结，很矛盾。</p><p>直到我看到了这么些话：</p><p><img src="/2023/20230206/1.jpg"></p><p>正如约会不是因为知道一个人会答应我我就去约，而应该是因为我喜欢他我才去约他；<strong>做一件事情不是因为知道我一定能完成我才去做，而是因为我想去做，想去挑战，很想完成它，我才去做！哪怕最后会失败，如果我真心想要，那么就去做吧！失败也是可以的( It is OKAY to fail )！</strong></p><p>去参加一个充满挑战的比赛，去参加一次竞选，让自己忙成陀螺，让自己压力山大，逼着自己去做好多好多额外的事，逼着自己去加倍地努力，需要冒着失败和受挫的风险，冒着吃力不讨好、花了大量时间最后一无所获的风险，冒着最后的结果让自己伤心难过的风险。</p><p><strong>因为我知道我的内心是想参加的。只是它一开始不够勇敢，它害怕。所以我要鼓励它，支持它，帮助它实现这个愿望。</strong></p><p>至于失败，那又怎样。<strong>失败是可以接受的(</strong> <strong>It is OKAY to fail )</strong>！我爱我自己。哪怕遭遇挫折和失败，我还是会为自己骄傲，我还要站在自己的这边，肯定这个努力了的自己（嗒就是给自己点个赞），告诉自己：你很勇敢，继续向前，么么哒！</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>画家Sidokid的收集</title>
    <link href="/2023/sidokid/"/>
    <url>/2023/sidokid/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>sidokid是我喜欢的一位插画画家之一，然而她在前两年把ins、微博、微信公众号都注销了。比较有名的插画有“But i like you”系列，由于“But i like you”系列在网上传播广泛，也易于找到，故不在下面收录。</p><p>以下为插画收集。</p><hr><span id="more"></span><p><img src="/2023/sidokid/2.jpg"></p><p><img src="/2023/sidokid/3.jpg"><br><img src="/2023/sidokid/4.jpg"><br><img src="/2023/sidokid/5.jpg"><br><img src="/2023/sidokid/6.jpg"><br><img src="/2023/sidokid/7.jpg"><br><img src="/2023/sidokid/8.jpg"><br><img src="/2023/sidokid/9.jpg"><br><img src="/2023/sidokid/10.jpg"><br><img src="/2023/sidokid/11.jpg"><br><img src="/2023/sidokid/12.jpg"><br><img src="/2023/sidokid/13.jpg"><br><img src="/2023/sidokid/14.jpg"><br><img src="/2023/sidokid/15.jpg"><br><img src="/2023/sidokid/16.jpg"><br><img src="/2023/sidokid/17.jpg"><br><img src="/2023/sidokid/18.jpg"><br><img src="/2023/sidokid/19.jpg"><br><img src="/2023/sidokid/20.jpg"><br><img src="/2023/sidokid/21.jpg"><br><img src="/2023/sidokid/22.jpg"><br><img src="/2023/sidokid/23.jpg"><br><img src="/2023/sidokid/24.jpg"><br><img src="/2023/sidokid/25.jpg"><br><img src="/2023/sidokid/26.jpg"><br><img src="/2023/sidokid/27.jpg"><br><img src="/2023/sidokid/28.jpg"><br><img src="/2023/sidokid/29.jpg"><br><img src="/2023/sidokid/30.jpg"><br><img src="/2023/sidokid/31.jpg"><br><img src="/2023/sidokid/32.jpg"><br><img src="/2023/sidokid/33.jpg"><br><img src="/2023/sidokid/34.jpg"><br><img src="/2023/sidokid/35.jpg"><br><img src="/2023/sidokid/36.jpg"><br><img src="/2023/sidokid/37.jpg"><br><img src="/2023/sidokid/38.jpg"><br><img src="/2023/sidokid/39.jpg"><br><img src="/2023/sidokid/40.jpg"><br><img src="/2023/sidokid/41.jpg"><br><img src="/2023/sidokid/42.jpg"><br><img src="/2023/sidokid/43.jpg"><br><img src="/2023/sidokid/44.jpg"><br><img src="/2023/sidokid/45.jpg"><br><img src="/2023/sidokid/46.jpg"><br><img src="/2023/sidokid/47.jpg"><br><img src="/2023/sidokid/48.jpg"><br><img src="/2023/sidokid/49.jpg"><br><img src="/2023/sidokid/50.jpg"><br><img src="/2023/sidokid/51.jpg"><br><img src="/2023/sidokid/52.jpg"><br><img src="/2023/sidokid/53.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>艺术</tag>
      
      <tag>插画</tag>
      
      <tag>漫画</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>408复习攻略</title>
    <link href="/2023/20230128/"/>
    <url>/2023/20230128/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考<a href="https://www.bilibili.com/video/BV16W4y1z71H/">408复习攻略——140分经验之谈</a></p><span id="more"></span><hr><blockquote><p>成绩单：</p><p>报考专业：电子信息</p><p>英语二 85</p><p>政治 66 </p><p>数学二 122</p><p>计算机专业学科基础专业综合 140</p><p>总分413</p></blockquote><h3 id="408简介"><a href="#408简介" class="headerlink" title="408简介"></a>408简介</h3><p><strong>科目与难度：</strong></p><p>数据结构：❤❤❤</p><p>操作系统：❤❤</p><p>计组：❤❤❤</p><p>计网：❤ （以计网为1❤难度）</p><p><strong>题目类型：</strong></p><p>选择题：40题 80分</p><p>综合应用题：7题 70分</p><p>（详细的题目类型比例和分析见知乎）</p><h3 id="复习方法"><a href="#复习方法" class="headerlink" title="复习方法"></a>复习方法</h3><h4 id="复习资料"><a href="#复习资料" class="headerlink" title="复习资料"></a>复习资料</h4><p><strong>教材</strong></p><p>王道单科书</p><p>《数据结构》严蔚敏</p><p>《计算机组成原理》白中英</p><p>《计算机组成原理》唐朔飞</p><p>《计算机网络》谢希仁</p><p>《操作系统》汤子瀛</p><p>买二手就行</p><p>单科书的话不必要去看单科书从头到尾看一遍</p><p>真题：王道真题讲解</p><p>模拟题：王道模拟题（9月份出）</p><h4 id="五轮复习法"><a href="#五轮复习法" class="headerlink" title="五轮复习法"></a>五轮复习法</h4><h5 id="第1轮"><a href="#第1轮" class="headerlink" title="第1轮"></a><strong>第1轮</strong></h5><p>学习王道四门单科书</p><p>第一轮<strong>只需做选择题</strong></p><p>一两天搞不懂的内容直接跳过，可以留到</p><h5 id="第2轮"><a href="#第2轮" class="headerlink" title="第2轮"></a><strong>第2轮</strong></h5><p>再过一遍单科书，不是说重新读一遍</p><p>第二轮主要关注你第一轮做的错题和对应相关的内容</p><p>巩固一下尝试解决第一轮没搞懂的内容</p><p>每复习一节完成大题</p><h5 id="第3轮"><a href="#第3轮" class="headerlink" title="第3轮"></a>第3轮</h5><p>主要关注你的错题(特别是大题)<br>继续尝试掌握前面两轮没有搞懂的东西</p><h5 id="第4轮"><a href="#第4轮" class="headerlink" title="第4轮"></a>第4轮</h5><p>主要做真题、模拟题（时间不够可不做）<br>真题非常重要一定要做<br>模拟题其实UP主是没有做的<br>大家时间充裕的话可以做一做<br>但是一定要先做完真题再做模拟题<br>你先做完真题什么错题<br>搞都搞懂了，然后你再去做模拟题时间够的话<br>你可以不做也行其实</p><h5 id="第5轮"><a href="#第5轮" class="headerlink" title="第5轮"></a>第5轮</h5><p><strong>查漏补缺</strong></p><p>再看错题！（选择、大题、真题）</p><p>是在搞不懂就战略性放弃，值不了几分</p><h4 id="交叉复习法"><a href="#交叉复习法" class="headerlink" title="交叉复习法"></a>交叉复习法</h4><p>隔天交叉复习四门客<br>比如说:</p><p>5月11日复习数据结构和计算机网络<br>5月12日复习组成原理和操作系统</p><p><strong>说明：</strong></p><p>组成原理和os有很多相似的内容，里面比如说包括这种一些置换算法，比如说组成原理里面里面有cash的置换算法，os里面有page的置换算法，然后他们还有一些查表，什么段表页表都是很相似的东西。<br>然后数据结构内容比较多也比较复杂，然后加上计网计网比较轻松一点。</p><p><strong>原因：</strong></p><p>然后一天复习两门这样是比较好的<br>一天复习四门不太现实，容易手忙脚乱<br>一天只复习一门的话，战线拉的太长。</p><p>除了408你还需要复习数学（例如花一个上午）、英语（背单词+做题）、政治（政治后期可能需要花较多的时间），一天复习四门可能挤压其他学科的复习时间。</p><p>Up主的时间表：</p><p><img src="/2023/20230128/timetable.png"></p><h4 id="如何做笔记"><a href="#如何做笔记" class="headerlink" title="如何做笔记"></a>如何做笔记</h4><p>不要单独去做笔记<br>不要单独拿个本子公公整整、子子细细把教材或者说视频上的内容再整理一遍</p><p>没有必要！浪费时间！<br>你抄一遍你还不如多多去做几道题了<br>这个是完全没有必要<br>很多同学有这种完美主义的倾向</p><p>我们的目标是<strong>花尽可能少的时间去掌握尽可能要多的知识</strong><br>记住啊千万不要干这种傻事</p><h5 id="什么时候做笔记呢"><a href="#什么时候做笔记呢" class="headerlink" title="什么时候做笔记呢"></a>什么时候做笔记呢</h5><p>不是说一点笔记也不做，我们也要做笔记。</p><p><strong>情况1：</strong></p><p>单科书上没讲清楚或者说自己不理解的地方，我查了资料看了视频我搞懂了，那么我简单的记一下，记在单科书相应内容附近</p><p><strong>情况2：</strong></p><p>做错的题需要这个适当的整理一下<br>不需要搞那种很漂亮的表格</p><p>只需要拿一张A4记录一下（记下题号，什么内容）</p><p>记下来之后，然后你就有你会看到这个整个表，上面哪些内容哪些你错的比较多的</p><p>第一轮复习完，除了答题的草稿纸，你应该只有两样东西：</p><p>（1）做了一些笔记的单科书</p><p>（2）N张A4纸，记录了自己错题的题号、内容、页码</p><p>个人觉得无需思维导图，看单科书目录就行（王道貌似思维导图）</p><h4 id="真题和模拟题"><a href="#真题和模拟题" class="headerlink" title="真题和模拟题"></a>真题和模拟题</h4><p>真题是非常非常非常重要的</p><p>做题时间</p><p>408是下午考，定个时间在那个对应的时间进行模拟考试</p><p>严格计时，严格判分，不要欺骗自己</p><p>对自己越严&#x3D;&#x3D;老师判题给你放水</p><h4 id="复习的误区"><a href="#复习的误区" class="headerlink" title="复习的误区"></a>复习的误区</h4><h5 id="误区1"><a href="#误区1" class="headerlink" title="误区1"></a>误区1</h5><p>仔仔细细<strong>一遍搞定408</strong></p><p>正确做法是重复多次，进行3~5轮复习</p><p>不要看别人3个月复习就能上岸，有些同学很夸张的这种什么2个月，甚至有1个月。</p><h5 id="误区2"><a href="#误区2" class="headerlink" title="误区2"></a>误区2</h5><p>第一遍就想<strong>搞定每个知识点</strong>，做对每一道题</p><p>正确的做法是对于太难太难太复杂的内容，如果当天的话我没搞懂，那我就跳过了，后面复习的时候再来应对这些难点。</p><h5 id="误区3"><a href="#误区3" class="headerlink" title="误区3"></a>误区3</h5><p>在看王道单科书之前学习一遍教材</p><p>浪费大量时间，408考纲和教材的内容还是有出入的</p><p>正确做法是直接看王道单科书，遇到不懂、不够清除的再去看教材<br>对跨考的同学来说也是没有必要的</p><h5 id="误区4"><a href="#误区4" class="headerlink" title="误区4"></a>误区4</h5><p>408内容太多记不住。<br>有的同学一开始的说，我复习了可能一个月，我复习到后面前面的已经全部都忘了，他认为408内容太多是不可能记住的。<br>那是其实不是这样的。<br>因为现在是6月底快7月了，半年之后的话你应该至少重复了3遍408的内容，我觉得到时候95%的内容你肯定已经记住了。<br>所以说不要担心，不会记不住，只要你动手作题，那肯定你记得住。</p><h5 id="误区5"><a href="#误区5" class="headerlink" title="误区5"></a>误区5</h5><p>只看书看视频不动手作题</p><p>不做题和没看书一样</p><p>正确做法：</p><p>第一轮，每看一节单科书做一节选择题，然后立即回顾做错的知识点</p><p>时间充裕可以简单浏览一下题目，不复杂可以一起做掉，大题可以全留给第二轮。</p><h5 id="三个指导思想"><a href="#三个指导思想" class="headerlink" title="三个指导思想"></a>三个指导思想</h5><p>（1）重复多次的效果是远大于花大量的时间学习一次的效果，不要死磕一个知识点</p><p>（2）选择对自己来说性价比最高的学习方式，不要钻牛角尖，这里的性价比讲的是分数和时间的比例</p><p>（3）动手做题是最重要的，所以考试最终就是动手做题。</p><h5 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h5><p>（1）现在开始复习晚吗？</p><p>难道我说晚了，你就不考了？动起来！<br>不要问这种没有意思的问题</p><p>（2）做的题错很多怎么办？感觉好难？</p><p>我的答案只有一个就是重复！重复！重复！再重复！<br>这个没办法，你第一遍做确实是这样，大家心态一定要稳定。<br>第一遍错很多是正常的，第二遍就会好很多，第三遍会好更多。<br>重复就行了坚持就行了</p><p>（3）群友复习快又好，我不行了。</p><p>关你啥事，按照自己的节奏推进。<br>然后考研群不是不是用来水的，不是用来聊天的，你是交流。<br>实在搞不懂的问题，你可以请教别人的。<br>千万不要每天花什么几个小时水群，早上水一会，中午水一会，晚上水一会。</p><h3 id="最重要的两个因素"><a href="#最重要的两个因素" class="headerlink" title="最重要的两个因素"></a>最重要的两个因素</h3><p>第一个是<strong>执行力</strong>。<br>特别是动手作题，现在就开始动手作题,。<br>第二个是<strong>心态</strong>。心态也很很重要不要被别人所影响，然后不要被同学老是其他任何人影响你的心态。你就把自己当成一个和尚每天复习就行了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>考研</tag>
      
      <tag>学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【待更新】《深度学习调优手册》- 系统最大化深度学习模型性能</title>
    <link href="/2023/20230126/"/>
    <url>/2023/20230126/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><em>非官方支持的Google产品</em></p><p>由Google Research和Harvard University研究人员联合出品。</p><p>翻译By我。</p><p>先在github翻译完，在上传到博客。</p><span id="more"></span><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#%E8%BF%99%E4%B8%AA%E6%96%87%E6%A1%A3%E6%98%AF%E4%B8%BA%E8%B0%81%E8%AE%BE%E8%AE%A1%E7%9A%84%EF%BC%9F">这个文档是为谁设计的？</a></li><li><a href="#why-a-tuning-playbook">Why a tuning playbook?</a></li><li><a href="#%E5%BC%80%E5%A7%8B%E6%96%B0%E9%A1%B9%E7%9B%AE%E5%89%8D%E7%9A%84%E6%8C%87%E5%8D%97">开始新项目前的指南</a><ul><li><a href="#%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84">选择模型架构</a></li><li><a href="#%E9%80%89%E6%8B%A9%E4%BC%98%E5%8C%96%E5%99%A8">选择优化器</a></li><li><a href="#%E9%80%89%E6%8B%A9batchsize">选择batchsize</a></li><li><a href="#%E9%80%89%E6%8B%A9%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE">选择初始配置</a></li></ul></li><li><a href="#a-scientific-approach-to-improving-model-performance">A scientific approach to improving model performance</a><ul><li><a href="#the-incremental-tuning-strategy">The incremental tuning strategy</a></li><li><a href="#exploration-vs-exploitation">Exploration vs exploitation</a></li><li><a href="#choosing-the-goal-for-the-next-round-of-experiments">Choosing the goal for the next round of experiments</a></li><li><a href="#Designing-the-next-round-of-experiments">Designing the next round of experiments</a></li><li><a href="#Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration">Determining whether to adopt a training pipeline change or<br>hyperparameter<br>configuration</a></li><li><a href="#After-exploration-concludes">After exploration concludes</a></li></ul></li><li><a href="#Determining-the-number-of-steps-for-each-training-run">Determining the number of steps for each training run</a><ul><li><a href="#Deciding-how-long-to-train-when-training-is-not-compute-bound">Deciding how long to train when training is not compute-bound</a></li><li><a href="#Deciding-how-long-to-train-when-training-is-compute-bound">Deciding how long to train when training is compute-bound</a></li></ul></li><li><a href="#Additional-guidance-for-the-training-pipeline">Additional guidance for the training pipeline</a><ul><li><a href="#Optimizing-the-input-pipeline">Optimizing the input pipeline</a></li><li><a href="Evaluating-model-performance">Evaluating model performance</a></li><li><a href="#Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint">Saving checkpoints and retrospectively selecting the best checkpoint</a></li><li><a href="#Setting-up-experiment-tracking">Setting up experiment tracking</a></li><li><a href="#Batch-normalization-implementation-details">Batch normalization implementation details</a></li><li><a href="#Considerations-for-multi-host-pipelines">Considerations for multi-host pipelines</a></li></ul></li><li><a href="#faqs">FAQs</a></li><li><a href="#acknowledgments">Acknowledgments</a></li><li><a href="#citing">Citing</a></li><li><a href="#contributing">Contributing</a></li></ul><h2 id="这个文档是为谁设计的？"><a href="#这个文档是为谁设计的？" class="headerlink" title="这个文档是为谁设计的？"></a>这个文档是为谁设计的？</h2><p>本文档面向对深度学习<strong>模型性能最优化</strong>感兴趣的工程师和研究人员（个人和团队）。我们假设你已经掌握了机器学习和深度学习概念的基本知识。</p><p>我们的重点是超参数调参过程。我们触及了深度学习培训的其他方面，例如pipeline应用和优化，但是我们对这些方面的处理并不完整。</p><p>我们假设机器学习问题是一个有监督的学习问题或类似的问题（例如自监督）。尽管如此，本文档中的一些规定也可能适用于其他类型的问题。</p><h2 id="Why-a-tuning-playbook"><a href="#Why-a-tuning-playbook" class="headerlink" title="Why a tuning playbook?"></a>Why a tuning playbook?</h2><p>Currently, there is an astonishing amount of toil and guesswork involved in<br>actually getting deep neural networks to work well in practice. Even worse, the<br>actual recipes people use to get good results with deep learning are rarely<br>documented. Papers gloss over the process that led to their final results in<br>order to present a cleaner story, and machine learning engineers working on<br>commercial problems rarely have time to take a step back and generalize their<br>process. Textbooks tend to eschew practical guidance and prioritize fundamental<br>principles, even if their authors have the necessary experience in applied work<br>to provide useful advice. When preparing to create this document, we couldn’t<br>find any comprehensive attempt to actually explain <em>how to get good results with<br>deep learning</em>. Instead, we found snippets of advice in blog posts and on social<br>media, tricks peeking out of the appendix of research papers, occasional case<br>studies about one particular project or pipeline, and a lot of confusion. There<br>is a vast gulf between the results achieved by deep learning experts and less<br>skilled practitioners using superficially similar methods. At the same time,<br>these very experts readily admit some of what they do might not be<br>well-justified. As deep learning matures and has a larger impact on the world,<br>the community needs more resources covering useful recipes, including all the<br>practical details that can be so critical for obtaining good results.</p><p>We are a team of five researchers and engineers who have worked in deep learning<br>for many years, some of us since as early as 2006. We have applied deep learning<br>to problems in everything from speech recognition to astronomy, and learned a<br>lot along the way. This document grew out of our own experience training neural<br>networks, teaching new machine learning engineers, and advising our colleagues<br>on the practice of deep learning. Although it has been gratifying to see deep<br>learning go from a machine learning approach practiced by a handful of academic<br>labs to a technology powering products used by billions of people, deep learning<br>is still in its infancy as an engineering discipline and we hope this document<br>encourages others to help systematize the field’s experimental protocols.</p><p>This document came about as we tried to crystalize our own approach to deep<br>learning and thus it represents the opinions of the authors at the time of<br>writing, not any sort of objective truth. Our own struggles with hyperparameter<br>tuning made it a particular focus of our guidance, but we also cover other<br>important issues we have encountered in our work (or seen go wrong). Our<br>intention is for this work to be a living document that grows and evolves as our<br>beliefs change. For example, the material on debugging and mitigating training<br>failures would not have been possible for us to write two years ago since it is<br>based on recent results and ongoing investigations. Inevitably, some of our<br>advice will need to be updated to account for new results and improved<br>workflows. We do not know the <em>optimal</em> deep learning recipe, but until the<br>community starts writing down and debating different procedures, we cannot hope<br>to find it. To that end, we would encourage readers who find issues with our<br>advice to produce alternative recommendations, along with convincing evidence,<br>so we can update the playbook. We would also love to see alternative guides and<br>playbooks that might have different recommendations so we can work towards best<br>practices as a community. Finally, any sections marked with a 🤖 emoji are places<br>we would like to do more research. Only after trying to write this playbook did<br>it become completely clear how many interesting and neglected research questions<br>can be found in the deep learning practitioner’s workflow.</p><h2 id="开始新项目前的指南"><a href="#开始新项目前的指南" class="headerlink" title="开始新项目前的指南"></a>开始新项目前的指南</h2><p>我们在调参过程中做出的许多决定可以在项目开始时做，只有在情况发生变化时偶尔会重新审视。</p><p>我们的指南提出了以下假设:</p><ul><li>已经完成了足够的问题制定（problem formulation），数据清洁等的基本工作，以使在模型架构和训练配置上花费时间是有意义的。</li><li>已经有了一个pipeline,可以进行训练和评估。并容易为各种感兴趣的模型进行训练和预测。</li><li>选定并应用了合适的metrics。这些应该尽可能地代表将在部署环境中测量的内容。</li></ul><h3 id="选择模型架构"><a href="#选择模型架构" class="headerlink" title="选择模型架构"></a>选择模型架构</h3><p><em><strong>摘要:</strong></em> <em>当开始一个新项目时，请尝试使用已经work的模型。</em></p><ul><li><p>选择一个成熟的、广泛使用的模型架构。 这样可以更好的在后续构建自定义模型。</p></li><li><p>模型架构常有很多超参数来确定模型的大小和其他细节。比如layers的数量和宽度，比如激活函数的类型。</p><ul><li>因此，选择模型架构实际上代表着选择一类模型，模型大家族内不同的模型代表着不同的一组超参数。</li><li>我们将在<a href="#%E9%80%89%E6%8B%A9%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE">选择初始配置</a>和<a href="#%E6%8F%90%E5%8D%87%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E7%9A%84%E7%A7%91%E5%AD%A6%E6%96%B9%E6%B3%95">提升模型性能的科学方法</a>两章考虑模型超参数选择的问题。</li></ul></li><li><p>如果可能，尝试找到一篇 解决了与手头问题尽可能相近的问题 的论文，并把复现论文中的模型作为起点。</p></li></ul><h3 id="选择优化器"><a href="#选择优化器" class="headerlink" title="选择优化器"></a>选择优化器</h3><p><em><strong>摘要:</strong></em> <em>从最受欢迎的优化器开始，用于解决手头问题。</em></p><ul><li>在所有类型的机器学习问题和模型架构中，没有优化器是“最佳”的。 甚至比较优化器性能是困难的。<br><a href="https://arxiv.org/abs/1910.05446">参考论文：《comparing the performance of optimizers is a difficult task》</a>.<br>🤖</li><li>我们推荐使用成熟的、广泛使用的优化器，尤其是在开始一个新项目的时候。<ul><li>理想情况下，选择用于相同类型问题下的最广泛使用的优化器。</li></ul></li><li>注意关注所选择优化器的<em><strong>所有</strong></em>超参数。<ul><li>具有更多超参数的优化器可能需要更多的调参经历来找到最佳配置。</li><li>这点很重要，尤其是在项目的开始阶段我们可能，而忽略了优化器的超参数，把它看成了无用且讨厌的参数（<a href="#identifying-scientific-nuisance-and-fixed-hyperparameters">nuisance parameters</a>）。</li><li>在项目的初始夹断，最好是从一个简单的优化器开始，比如使用固定动量的SGD或固定 $\epsilon$、 $\beta_{1}$、$\beta_{2}$的Adam。然后再切换到更通用的优化器。</li></ul></li><li>我们喜欢的成熟的优化器包括（但不限于）以下：<ul><li><a href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms">带动量的SGD</a><br>(我们喜欢 Nesterov变种的。)</li><li><a href="#what-are-the-update-rules-for-all-the-popular-optimization-algorithms">Adam 和NAdam</a>,<br>它们比带动量的SGD更通用。请注意，Adam有四个可调的超参数，而且<a href="https://arxiv.org/abs/1910.05446">它们都很重要</a>！<ul><li>可看<br><a href="#Adam%E7%9A%84%E8%B6%85%E5%8F%82%E6%95%B0%E5%BA%94%E8%AF%A5%E5%A6%82%E4%BD%95%E8%B0%83%E6%95%B4%EF%BC%9F">Adam的超参数应该如何调整？</a></li></ul></li></ul></li></ul><h3 id="选择batchsize"><a href="#选择batchsize" class="headerlink" title="选择batchsize"></a>选择batchsize</h3><p><em><strong>Summary:</strong></em> <em>The batch size governs the training speed and shouldn’t be used<br>to directly tune the validation set performance. Often, the ideal batch size<br>will be the largest batch size supported by the available hardware.</em></p><ul><li>The batch size is a key factor in determining the <em>training time</em> and<br><em>computing resource consumption</em>.</li><li>Increasing the batch size will often reduce the training time. This can be<br>highly beneficial because it, e.g.:<ul><li>Allows hyperparameters to be tuned more thoroughly within a fixed time<br>interval, potentially resulting in a better final model.</li><li>Reduces the latency of the development cycle, allowing new ideas to be<br>tested more frequently.</li></ul></li><li>Increasing the batch size may either decrease, increase, or not change the<br>resource consumption.</li><li>The batch size should <em>not be</em> treated as a tunable hyperparameter for<br>validation set performance.<ul><li>As long as all hyperparameters are well-tuned (especially the learning<br>rate and regularization hyperparameters) and the number of training<br>steps is sufficient, the same final performance should be attainable<br>using any batch size (see<br><a href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).</li><li>Please see <a href="#why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance">Why shouldn’t the batch size be tuned to directly improve<br>validation set<br>performance?</a></li></ul></li></ul><h4 id="Determining-the-feasible-batch-sizes-and-estimating-training-throughput"><a href="#Determining-the-feasible-batch-sizes-and-estimating-training-throughput" class="headerlink" title="Determining the feasible batch sizes and estimating training throughput"></a>Determining the feasible batch sizes and estimating training throughput</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>For a given model and optimizer, there will typically be a range of batch<br>sizes supported by the available hardware. The limiting factor is usually<br>accelerator memory.</li><li>Unfortunately, it can be difficult to calculate which batch sizes will fit<br>in memory without running, or at least compiling, the full training program.</li><li>The easiest solution is usually to run training jobs at different batch<br>sizes (e.g. increasing powers of 2) for a small number of steps until one of<br>the jobs exceeds the available memory.</li><li>For each batch size, we should train for long enough to get a reliable<br>estimate of the <em>training throughput</em></li></ul><p align="center">training throughput = (# examples processed per second)</p><p align="center">or, equivalently, the <em>time per step</em>.</p><p align="center">time per step = (batch size) / (training throughput)</p><ul><li>When the accelerators aren’t yet saturated, if the batch size doubles, the<br>training throughput should also double (or at least nearly double).<br>Equivalently, the time per step should be constant (or at least nearly<br>constant) as the batch size increases.</li><li>If this is not the case then the training pipeline has a bottleneck such as<br>I&#x2F;O or synchronization between compute nodes. This may be worth diagnosing<br>and correcting before proceeding.</li><li>If the training throughput increases only up to some maximum batch size,<br>then we should only consider batch sizes up to that maximum batch size, even<br>if a larger batch size is supported by the hardware.<ul><li>All benefits of using a larger batch size assume the training throughput<br>increases. If it doesn’t, fix the bottleneck or use the smaller batch<br>size.</li><li><strong>Gradient accumulation</strong> simulates a larger batch size than the<br>hardware can support and therefore does not provide any throughput<br>benefits. It should generally be avoided in applied work.</li></ul></li><li>These steps may need to be repeated every time the model or optimizer is<br>changed (e.g. a different model architecture may allow a larger batch size<br>to fit in memory).</li></ul></details><h4 id="Choosing-the-batch-size-to-minimize-training-time"><a href="#Choosing-the-batch-size-to-minimize-training-time" class="headerlink" title="Choosing the batch size to minimize training time"></a>Choosing the batch size to minimize training time</h4><details><summary><em>[Click to expand]</em></summary><br><p align="center">Training time = (time per step) x (total number of steps)</p><ul><li>We can often consider the time per step to be approximately constant for all<br>feasible batch sizes. This is true when there is no overhead from parallel<br>computations and all training bottlenecks have been diagnosed and corrected<br>(see the<br><a href="#determining-the-feasible-batch-sizes-and-estimating-training-throughput">previous section</a><br>for how to identify training bottlenecks). In practice, there is usually at<br>least some overhead from increasing the batch size.</li><li>As the batch size increases, the total number of steps needed to reach a<br>fixed performance goal typically decreases (provided all relevant<br>hyperparameters are re-tuned when the batch size is changed;<br><a href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).<ul><li>E.g. Doubling the batch size might halve the total number of steps<br>required. This is called <strong>perfect scaling</strong>.</li><li>Perfect scaling holds for all batch sizes up to a critical batch size,<br>beyond which one achieves diminishing returns.</li><li>Eventually, increasing the batch size no longer reduces the number of<br>training steps (but never increases it).</li></ul></li><li>Therefore, the batch size that minimizes training time is usually the<br>largest batch size that still provides a reduction in the number of training<br>steps required.<ul><li>This batch size depends on the dataset, model, and optimizer, and it is<br>an open problem how to calculate it other than finding it experimentally<br>for every new problem. 🤖</li><li>When comparing batch sizes, beware the distinction between an example<br>budget&#x2F;<a href="https://developers.google.com/machine-learning/glossary#epoch">epoch</a><br>budget (running all experiments while fixing the number of training<br>example presentations) and a step budget (running all experiments with<br>the number of training steps fixed).<ul><li>Comparing batch sizes with an epoch budget only probes the perfect<br>scaling regime, even when larger batch sizes might still provide a<br>meaningful speedup by reducing the number of training steps<br>required.</li></ul></li><li>Often, the largest batch size supported by the available hardware will<br>be smaller than the critical batch size. Therefore, a good rule of thumb<br>(without running any experiments) is to use the largest batch size<br>possible.</li></ul></li><li>There is no point in using a larger batch size if it ends up increasing the<br>training time.</li></ul></details><h4 id="Choosing-the-batch-size-to-minimize-resource-consumption"><a href="#Choosing-the-batch-size-to-minimize-resource-consumption" class="headerlink" title="Choosing the batch size to minimize resource consumption"></a>Choosing the batch size to minimize resource consumption</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>There are two types of resource costs associated with increasing the batch<br>size:<ol><li><em>Upfront costs</em>, e.g. purchasing new hardware or rewriting the training<br>pipeline to implement multi-GPU &#x2F; multi-TPU training.</li><li><em>Usage costs</em>, e.g. billing against the team’s resource budgets, billing<br>from a cloud provider, electricity &#x2F; maintenance costs.</li></ol></li><li>If there are significant upfront costs to increasing the batch size, it<br>might be better to defer increasing the batch size until the project has<br>matured and it is easier to assess the cost-benefit tradeoff. Implementing<br>multi-host parallel training programs can introduce<br><a href="#considerations-for-multi-host-pipelines">bugs</a> and<br><a href="#batch-normalization-implementation-details">subtle issues</a> so it is<br>probably better to start off with a simpler pipeline anyway. (On the other<br>hand, a large speedup in training time might be very beneficial early in the<br>process when a lot of tuning experiments are needed).</li><li>We refer to the total usage cost (which may include multiple different kinds<br>of costs) as the “resource consumption”. We can break down the resource<br>consumption into the following components:</li></ul><p align="center">Resource consumption = (resource consumption per step) x (total number of steps)</p><ul><li>Increasing the batch size usually allows us to<br><a href="#choosing-the-batch-size-to-minimize-training-time">reduce the total number of steps</a>.<br>Whether the resource consumption increases or decreases will depend on how<br>the consumption per step changes.<ul><li>Increasing the batch size might <em>decrease</em> the resource consumption. For<br>example, if each step with the larger batch size can be run on the same<br>hardware as the smaller batch size (with only a small increase in time<br>per step), then any increase in the resource consumption per step might<br>be outweighed by the decrease in the number of steps.</li><li>Increasing the batch size might <em>not change</em> the resource consumption.<br>For example, if doubling the batch size halves the number of steps<br>required and doubles the number of GPUs used, the total consumption (in<br>terms of GPU-hours) will not change.</li><li>Increasing the batch size might <em>increase</em> the resource consumption. For<br>example, if increasing the batch size requires upgraded hardware, the<br>increase in consumption per step might outweigh the reduction in the<br>number of steps.</li></ul></li></ul></details><h4 id="Changing-the-batch-size-requires-re-tuning-most-hyperparameters"><a href="#Changing-the-batch-size-requires-re-tuning-most-hyperparameters" class="headerlink" title="Changing the batch size requires re-tuning most hyperparameters"></a>Changing the batch size requires re-tuning most hyperparameters</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>The optimal values of most hyperparameters are sensitive to the batch size.<br>Therefore, changing the batch size typically requires starting the tuning<br>process all over again.</li><li>The hyperparameters that interact most strongly with the batch size, and therefore are most important to tune separately for each batch size, are the optimizer hyperparameters (e.g. learning rate, momentum) and the regularization hyperparameters.</li><li>Keep this in mind when choosing the batch size at the start of a project. If<br>you need to switch to a different batch size later on, it might be<br>difficult, time consuming, and expensive to re-tune everything for the new<br>batch size.</li></ul></details><h4 id="How-batch-norm-interacts-with-the-batch-size"><a href="#How-batch-norm-interacts-with-the-batch-size" class="headerlink" title="How batch norm interacts with the batch size"></a>How batch norm interacts with the batch size</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>Batch norm is complicated and, in general, should use a different batch size<br>than the gradient computation to compute statistics. See the<br><a href="#batch-normalization-implementation-details">batch norm section</a> for a<br>detailed discussion.</li></ul></details><h3 id="Choosing-the-initial-configuration"><a href="#Choosing-the-initial-configuration" class="headerlink" title="Choosing the initial configuration"></a>Choosing the initial configuration</h3><ul><li>Before beginning hyperparameter tuning we must determine the starting point.<br>This includes specifying (1) the model configuration (e.g. number of<br>layers), (2) the optimizer hyperparameters (e.g. learning rate), and (3) the<br>number of training steps.</li><li>Determining this initial configuration will require some manually configured<br>training runs and trial-and-error.</li><li>Our guiding principle is to find a simple, relatively fast, relatively<br>low-resource-consumption configuration that obtains a “reasonable” result.<ul><li>“Simple” means avoiding bells and whistles wherever possible; these can<br>always be added later. Even if bells and whistles prove helpful down the<br>road, adding them in the initial configuration risks wasting time tuning<br>unhelpful features and&#x2F;or baking in unnecessary complications.<ul><li>For example, start with a constant learning rate before adding fancy<br>decay schedules.</li></ul></li><li>Choosing an initial configuration that is fast and consumes minimal<br>resources will make hyperparameter tuning much more efficient.<ul><li>For example, start with a smaller model.</li></ul></li><li>“Reasonable” performance depends on the problem, but at minimum means<br>that the trained model performs much better than random chance on the<br>validation set (although it might be bad enough to not be worth<br>deploying).</li></ul></li><li>Choosing the number of training steps involves balancing the following<br>tension:<ul><li>On the one hand, training for more steps can improve performance and<br>makes hyperparameter tuning easier (see<br><a href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).</li><li>On the other hand, training for fewer steps means that each training run<br>is faster and uses fewer resources, boosting tuning efficiency by<br>reducing the time between cycles and allowing more experiments to be run<br>in parallel. Moreover, if an unnecessarily large step budget is chosen<br>initially, it might be hard to change it down the road, e.g. once the<br>learning rate schedule is tuned for that number of steps.</li></ul></li></ul><h2 id="A-scientific-approach-to-improving-model-performance"><a href="#A-scientific-approach-to-improving-model-performance" class="headerlink" title="A scientific approach to improving model performance"></a>A scientific approach to improving model performance</h2><p>For the purposes of this document, the ultimate goal of machine learning<br>development is to maximize the utility of the deployed model. Even though many<br>aspects of the development process differ between applications (e.g. length of<br>time, available computing resources, type of model), we can typically use the<br>same basic steps and principles on any problem.</p><p>Our guidance below makes the following assumptions:</p><ul><li>There is already a fully-running training pipeline along with a<br>configuration that obtains a reasonable result.</li><li>There are enough computational resources available to conduct meaningful<br>tuning experiments and run at least several training jobs in parallel.</li></ul><h3 id="The-incremental-tuning-strategy"><a href="#The-incremental-tuning-strategy" class="headerlink" title="The incremental tuning strategy"></a>The incremental tuning strategy</h3><p><em><strong>Summary:</strong></em> <em>Start with a simple configuration and incrementally make<br>improvements while building up insight into the problem. Make sure that any<br>improvement is based on strong evidence to avoid adding unnecessary complexity.</em></p><ul><li>Our ultimate goal is to find a configuration that maximizes the performance<br>of our model.<ul><li>In some cases, our goal will be to maximize how much we can improve the<br>model by a fixed deadline (e.g. submitting to a competition).</li><li>In other cases, we want to keep improving the model indefinitely (e.g.<br>continually improving a model used in production).</li></ul></li><li>In principle, we could maximize performance by using an algorithm to<br>automatically search the entire space of possible configurations, but this<br>is not a practical option.<ul><li>The space of possible configurations is extremely large and there are<br>not yet any algorithms sophisticated enough to efficiently search this<br>space without human guidance.</li></ul></li><li>Most automated search algorithms rely on a hand-designed <em>search space</em> that<br>defines the set of configurations to search in, and these search spaces can<br>matter quite a bit.</li><li>The most effective way to maximize performance is to start with a simple<br>configuration and incrementally add features and make improvements while<br>building up insight into the problem.<ul><li>We use automated search algorithms in each round of tuning and<br>continually update our search spaces as our understanding grows.</li></ul></li><li>As we explore, we will naturally find better and better configurations and<br>therefore our “best” model will continually improve.<ul><li>We call it a <em>launch</em> when we update our best configuration (which may<br>or may not correspond to an actual launch of a production model).</li><li>For each launch, we must make sure that the change is based on strong<br>evidence – not just random chance based on a lucky configuration – so<br>that we don’t add unnecessary complexity to the training pipeline.</li></ul></li></ul><p>At a high level, our incremental tuning strategy involves repeating the<br>following four steps:</p><ol><li>Identify an appropriately-scoped goal for the next round of experiments.</li><li>Design and run a set of experiments that makes progress towards this goal.</li><li>Learn what we can from the results.</li><li>Consider whether to launch the new best configuration.</li></ol><p>The remainder of this section will consider this strategy in much greater<br>detail.</p><h3 id="Exploration-vs-exploitation"><a href="#Exploration-vs-exploitation" class="headerlink" title="Exploration vs exploitation"></a>Exploration vs exploitation</h3><p><em><strong>Summary:</strong></em> <em>Most of the time, our primary goal is to gain insight into the<br>problem.</em></p><ul><li>Although one might think we would spend most of our time trying to maximize<br>performance on the validation set, in practice we spend the majority of our<br>time trying to gain insight into the problem, and comparatively little time<br>greedily focused on the validation error.<ul><li>In other words, we spend most of our time on “exploration” and only a<br>small amount on “exploitation”.</li></ul></li><li>In the long run, understanding the problem is critical if we want to<br>maximize our final performance. Prioritizing insight over short term gains<br>can help us:<ul><li>Avoid launching unnecessary changes that happened to be present in<br>well-performing runs merely through historical accident.</li><li>Identify which hyperparameters the validation error is most sensitive<br>to, which hyperparameters interact the most and therefore need to be<br>re-tuned together, and which hyperparameters are relatively insensitive<br>to other changes and can therefore be fixed in future experiments.</li><li>Suggest potential new features to try, such as new regularizers if<br>overfitting is an issue.</li><li>Identify features that don’t help and therefore can be removed, reducing<br>the complexity of future experiments.</li><li>Recognize when improvements from hyperparameter tuning have likely<br>saturated.</li><li>Narrow our search spaces around the optimal value to improve tuning<br>efficiency.</li></ul></li><li>When we are eventually ready to be greedy, we can focus purely on the<br>validation error even if the experiments aren’t maximally informative about<br>the structure of the tuning problem.</li></ul><h3 id="Choosing-the-goal-for-the-next-round-of-experiments"><a href="#Choosing-the-goal-for-the-next-round-of-experiments" class="headerlink" title="Choosing the goal for the next round of experiments"></a>Choosing the goal for the next round of experiments</h3><p><em><strong>Summary:</strong></em> <em>Each round of experiments should have a clear goal and be<br>sufficiently narrow in scope that the experiments can actually make progress<br>towards the goal.</em></p><ul><li>Each round of experiments should have a clear goal and be sufficiently<br>narrow in scope that the experiments can actually make progress towards the<br>goal: if we try to add multiple features or answer multiple questions at<br>once, we may not be able to disentangle the separate effects on the results.</li><li>Example goals include:<ul><li>Try a potential improvement to the pipeline (e.g. a new regularizer,<br>preprocessing choice, etc.).</li><li>Understand the impact of a particular model hyperparameter (e.g. the<br>activation function)</li><li>Greedily maximize validation error.</li></ul></li></ul><h3 id="Designing-the-next-round-of-experiments"><a href="#Designing-the-next-round-of-experiments" class="headerlink" title="Designing the next round of experiments"></a>Designing the next round of experiments</h3><p><em><strong>Summary:</strong></em> <em>Identify which hyperparameters are scientific, nuisance, and<br>fixed hyperparameters for the experimental goal. Create a sequence of studies to<br>compare different values of the scientific hyperparameters while optimizing over<br>the nuisance hyperparameters. Choose the search space of nuisance<br>hyperparameters to balance resource costs with scientific value.</em></p><h4 id="Identifying-scientific-nuisance-and-fixed-hyperparameters"><a href="#Identifying-scientific-nuisance-and-fixed-hyperparameters" class="headerlink" title="Identifying scientific, nuisance, and fixed hyperparameters"></a>Identifying scientific, nuisance, and fixed hyperparameters</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>For a given goal, all hyperparameters will be either <strong>scientific<br>hyperparameters</strong>, <strong>nuisance hyperparameters</strong>, or <strong>fixed<br>hyperparameters</strong>.<ul><li>Scientific hyperparameters are those whose effect on the model’s<br>performance we’re trying to measure.</li><li>Nuisance hyperparameters are those that need to be optimized over in<br>order to fairly compare different values of the scientific<br>hyperparameters. This is similar to the statistical concept of<br><a href="https://en.wikipedia.org/wiki/Nuisance_parameter">nuisance parameters</a>.</li><li>Fixed hyperparameters will have their values fixed in the current round<br>of experiments. These are hyperparameters whose values do not need to<br>(or we do not want them to) change when comparing different values of<br>the scientific hyperparameters.<ul><li>By fixing certain hyperparameters for a set of experiments, we must<br>accept that conclusions derived from the experiments might not be<br>valid for other settings of the fixed hyperparameters. In other<br>words, fixed hyperparameters create caveats for any conclusions we<br>draw from the experiments.</li></ul></li></ul></li><li>For example, if our goal is to “determine whether a model with more hidden<br>layers will reduce validation error”, then the number of hidden layers is a<br>scientific hyperparameter.<ul><li>The learning rate is a nuisance hyperparameter because we can only<br>fairly compare models with different numbers of hidden layers if the<br>learning rate is tuned separately for each number of layers (the optimal<br>learning rate generally depends on the model architecture).</li><li>The activation function could be a fixed hyperparameter if we have<br>determined in prior experiments that the best choice of activation<br>function is not sensitive to model depth, or if we are willing to limit<br>our conclusions about the number of hidden layers to only cover this<br>specific choice of activation function. Alternatively, it could be a<br>nuisance parameter if we are prepared to tune it separately for each<br>number of hidden layers.</li></ul></li><li>Whether a particular hyperparameter is a scientific hyperparameter, nuisance<br>hyperparameter, or fixed hyperparameter is not inherent to that<br>hyperparameter, but changes depending on the experimental goal.<ul><li>For example, the choice of activation function could be a scientific<br>hyperparameter (is ReLU or tanh a better choice for our problem?), a<br>nuisance hyperparameter (is the best 5-layer model better than the best<br>6-layer model when we allow several different possible activation<br>functions?), or a fixed hyperparameter (for ReLU nets, does adding batch<br>normalization in a particular position help?).</li></ul></li><li>When designing a new round of experiments, we first identify the scientific<br>hyperparameters for our experimental goal.<ul><li>At this stage, we consider all other hyperparameters to be nuisance<br>hyperparameters.</li></ul></li><li>Next, we convert some of the nuisance hyperparameters into fixed<br>hyperparameters.<ul><li>With limitless resources, we would leave all non-scientific<br>hyperparameters as nuisance hyperparameters so that the conclusions we<br>draw from our experiments are free from caveats about fixed<br>hyperparameter values.</li><li>However, the more nuisance hyperparameters we attempt to tune, the<br>greater the risk we fail to tune them sufficiently well for each setting<br>of the scientific hyperparameters and end up reaching the wrong<br>conclusions from our experiments.<ul><li>As described<br><a href="#striking-a-balance-between-informative-and-affordable-experiments">below</a>,<br>we could counter this risk by increasing the computational budget,<br>but often our maximum resource budget is less than would be needed<br>to tune over all non-scientific hyperparameters.</li></ul></li><li>We choose to convert a nuisance hyperparameter into a fixed<br>hyperparameter when, in our judgment, the caveats introduced by fixing<br>it are less burdensome than the cost of including it as a nuisance<br>hyperparameter.<ul><li>The more a given nuisance hyperparameter interacts with the<br>scientific hyperparameters, the more damaging it is to fix its<br>value. For example, the best value of the weight decay strength<br>typically depends on the model size, so comparing different model<br>sizes assuming a single specific value of the weight decay would not<br>be very insightful.</li></ul></li></ul></li><li>Although the type we assign to each hyperparameter depends on the<br>experimental goal, we have the following rules of thumb for certain<br>categories of hyperparameters:<ul><li>Of the various optimizer hyperparameters (e.g. the learning rate,<br>momentum, learning rate schedule parameters, Adam betas etc.), at least<br>some of them will be nuisance hyperparameters because they tend to<br>interact the most with other changes.<ul><li>They are rarely scientific hyperparameters because a goal like “what<br>is the best learning rate for the current pipeline?” doesn’t give<br>much insight – the best setting could easily change with the next<br>pipeline change anyway.</li><li>Although we might fix some of them occasionally due to resource<br>constraints or when we have particularly strong evidence that they<br>don’t interact with the scientific parameters, we should generally<br>assume that optimizer hyperparameters must be tuned separately to<br>make fair comparisons between different settings of the scientific<br>hyperparameters, and thus shouldn’t be fixed.<ul><li>Furthermore, we have no <em>a priori</em> reason to prefer one<br>optimizer hyperparameter value over another (e.g. they don’t<br>usually affect the computational cost of forward passes or<br>gradients in any way).</li></ul></li></ul></li><li>In contrast, the <em>choice</em> of optimizer is typically a scientific<br>hyperparameter or fixed hyperparameter.<ul><li>It is a scientific hyperparameter if our experimental goal involves<br>making fair comparisons between two or more different optimizers<br>(e.g. “determine which optimizer produces the lowest validation<br>error in a given number of steps”).</li><li>Alternatively, we might make it a fixed hyperparameter for a variety<br>of reasons, including (1) prior experiments make us believe that the<br>best optimizer for our problem is not sensitive to current<br>scientific hyperparameters; and&#x2F;or (2) we prefer to compare values<br>of the scientific hyperparameters using this optimizer because its<br>training curves are easier to reason about; and&#x2F;or (3) we prefer to<br>use this optimizer because it uses less memory than the<br>alternatives.</li></ul></li><li>Hyperparameters introduced by a regularization technique are typically<br>nuisance hyperparameters, but whether or not we include the<br>regularization technique at all is a scientific or fixed hyperparameter.<ul><li>For example, dropout adds code complexity, so when deciding whether<br>to include it we would make “no dropout” vs “dropout” a scientific<br>hyperparameter and the dropout rate a nuisance hyperparameter.<ul><li>If we decide to add dropout to our pipeline based on this<br>experiment, then the dropout rate would be a nuisance<br>hyperparameter in future experiments.</li></ul></li></ul></li><li>Architectural hyperparameters are often scientific or fixed<br>hyperparameters because architecture changes can affect serving and<br>training costs, latency, and memory requirements.<ul><li>For example, the number of layers is typically a scientific or fixed<br>hyperparameter since it tends to have dramatic consequences for<br>training speed and memory usage.</li></ul></li></ul></li><li>In some cases, the sets of nuisance and fixed hyperparameters will depend on<br>the values of the scientific hyperparameters.<ul><li>For example, suppose we are trying to determine which optimizer out of<br>Nesterov momentum and Adam results in the lowest validation error. The<br>scientific hyperparameter is the <code>optimizer</code>, which takes values<br><code>&#123;&quot;Nesterov_momentum&quot;, &quot;Adam&quot;&#125;</code>. The value<br><code>optimizer=&quot;Nesterov_momentum&quot;</code> introduces the nuisance&#x2F;fixed<br>hyperparameters <code>&#123;learning_rate, momentum&#125;</code>, but the value<br><code>optimizer=&quot;Adam&quot;</code> introduces the nuisance&#x2F;fixed hyperparameters<br><code>&#123;learning_rate, beta1, beta2, epsilon&#125;</code>.</li><li>Hyperparameters that are only present for certain values of the<br>scientific hyperparameters are called <strong>conditional hyperparameters</strong>.</li><li>We should not assume two conditional hyperparameters are the same just<br>because they have the same name! In the above example, the conditional<br>hyperparameter called <code>learning_rate</code> is a <em>different</em> hyperparameter<br>for <code>optimizer=&quot;Nesterov_momentum&quot;</code> versus <code>optimizer=&quot;Adam&quot;</code>. Its role<br>is similar (although not identical) in the two algorithms, but the range<br>of values that work well in each of the optimizers is typically<br>different by several orders of magnitude.</li></ul></li></ul></details><h4 id="Creating-a-set-of-studies"><a href="#Creating-a-set-of-studies" class="headerlink" title="Creating a set of studies"></a>Creating a set of studies</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>Once we have identified the scientific and nuisance hyperparameters, we<br>design a “study” or sequence of studies to make progress towards the<br>experimental goal.<ul><li>A study specifies a set of hyperparameter configurations to be run for<br>subsequent analysis. Each configuration is called a “trial”.</li><li>Creating a study typically involves choosing the hyperparameters that<br>will vary across trials, choosing what values those hyperparameters can<br>take on (the “search space”), choosing the number of trials, and<br>choosing an automated search algorithm to sample that many trials from<br>the search space. Alternatively, we could create a study by specifying<br>the set of hyperparameter configurations manually.</li></ul></li><li>The purpose of the studies is to run the pipeline with different values of<br>the scientific hyperparameters, while at the same time <strong>“optimizing away”</strong><br>(or “optimizing over”) the nuisance hyperparameters so that comparisons<br>between different values of the scientific hyperparameters are as fair as<br>possible.</li><li>In the simplest case, we would make a separate study for each configuration<br>of the scientific parameters, where each study tunes over the nuisance<br>hyperparameters.<ul><li>For example, if our goal is to select the best optimizer out of Nesterov<br>momentum and Adam, we could create one study in which<br><code>optimizer=&quot;Nesterov_momentum&quot;</code> and the nuisance hyperparameters are<br><code>&#123;learning_rate, momentum&#125;</code>, and another study in which<br><code>optimizer=&quot;Adam&quot;</code> and the nuisance hyperparameters are <code>&#123;learning_rate, beta1, beta2, epsilon&#125;</code>. We would compare the two optimizers by<br>selecting the best performing trial from each study.</li><li>We can use any gradient-free optimization algorithm, including methods<br>such as Bayesian optimization or evolutionary algorithms, to optimize<br>over the nuisance hyperparameters, although<br><a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">we prefer</a><br>to use quasi-random search in the<br><a href="#exploration-vs-exploitation">exploration phase</a> of tuning because of a<br>variety of advantages it has in this setting.<br><a href="#after-exploration-concludes">After exploration concludes</a>, if<br>state-of-the-art Bayesian optimization software is available, that is<br>our preferred choice.</li></ul></li><li>In the more complicated case where we want to compare a large number of<br>values of the scientific hyperparameters and it is impractical to make that<br>many independent studies, we can include the scientific parameters in the<br>same search space as the nuisance hyperparameters and use a search algorithm<br>to sample values of <em>both</em> the scientific and nuisance hyperparameters in a<br>single study.<ul><li>When taking this approach, conditional hyperparameters can cause<br>problems since it is hard to specify a search space unless the set of<br>nuisance hyperparameters is the same for all values of the scientific<br>hyperparameters.</li><li>In this case,<br><a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">our preference</a><br>for using quasi-random search over fancier black-box optimization tools<br>is even stronger, since it ensures that we obtain a relatively uniform<br>sampling of values of the scientific hyperparameters. Regardless of the<br>search algorithm, we need to make sure somehow that it searches the<br>scientific parameters uniformly.</li></ul></li></ul></details><h4 id="Striking-a-balance-between-informative-and-affordable-experiments"><a href="#Striking-a-balance-between-informative-and-affordable-experiments" class="headerlink" title="Striking a balance between informative and affordable experiments"></a>Striking a balance between informative and affordable experiments</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>When designing a study or sequence of studies, we need to allocate a limited<br>budget in order to adequately achieve the following three desiderata:<ol><li>Comparing enough different values of the scientific hyperparameters.</li><li>Tuning the nuisance hyperparameters over a large enough search space.</li><li>Sampling the search space of nuisance hyperparameters densely enough.</li></ol></li><li>The better we can achieve these three desiderata, the more insight we can<br>extract from our experiment.<ul><li>Comparing as many values of the scientific hyperparameters as possible<br>broadens the scope of the insights we gain from the experiment.</li><li>Including as many nuisance hyperparameters as possible and allowing each<br>nuisance hyperparameter to vary over as wide a range as possible<br>increases our confidence that a “good” value of the nuisance<br>hyperparameters <strong>exists</strong> in the search space for each configuration of<br>the scientific hyperparameters.<ul><li>Otherwise, we might make unfair comparisons between values of the<br>scientific hyperparameters by not searching possible regions of the<br>nuisance parameter space where better values might lie for some<br>values of the scientific parameters.</li></ul></li><li>Sampling the search space of nuisance hyperparameters as densely as<br>possible increases our confidence that any good settings for the<br>nuisance hyperparameters that happen to exist in our search space will<br>be found by the search procedure.<ul><li>Otherwise, we might make unfair comparisons between values of the<br>scientific parameters due to some values getting luckier with the<br>sampling of the nuisance hyperparameters.</li></ul></li></ul></li><li>Unfortunately, improvements in <em>any</em> of these three dimensions require<br>either increasing the number of trials, and therefore increasing the<br>resource cost, or finding a way to save resources in one of the other<br>dimensions.<ul><li>Every problem has its own idiosyncrasies and computational constraints,<br>so how to allocate resources across these three desiderata requires some<br>level of domain knowledge.</li><li>After running a study, we always try to get a sense of whether the study<br>tuned the nuisance hyperparameters well enough (i.e. searched a large<br>enough space extensively enough) to fairly compare the scientific<br>hyperparameters (as described in greater detail<br><a href="#extracting-insight-from-experimental-results">below</a>).</li></ul></li></ul></details><h3 id="Extracting-insight-from-experimental-results"><a href="#Extracting-insight-from-experimental-results" class="headerlink" title="Extracting insight from experimental results"></a>Extracting insight from experimental results</h3><p><em><strong>Summary:</strong></em> <em>In addition to trying to achieve the original scientific goal of<br>each group of experiments, go through a checklist of additional questions and,<br>if issues are discovered, revise the experiments and rerun them.</em></p><ul><li>Ultimately, each group of experiments has a specific goal and we want to<br>evaluate the evidence the experiments provide toward that goal.<ul><li>However, if we ask the right questions, we will often find issues that<br>need to be corrected before a given set of experiments can make much<br>progress towards their original goal.<ul><li>If we don’t ask these questions, we may draw incorrect conclusions.</li></ul></li><li>Since running experiments can be expensive, we also want to take the<br>opportunity to extract other useful insights from each group of<br>experiments, even if these insights are not immediately relevant to the<br>current goal.</li></ul></li><li>Before analyzing a given set of experiments to make progress toward their<br>original goal, we should ask ourselves the following additional questions:<ul><li><a href="#identifying-bad-search-space-boundaries">Is the search space large enough?</a><ul><li>If the optimal point from a study is near the boundary of the search<br>space in one or more dimensions, the search is probably not wide<br>enough. In this case, we should run another study with an expanded<br>search space.</li></ul></li><li><a href="#not-sampling-enough-points-in-the-search-space">Have we sampled enough points from the search space?</a><ul><li>If not, run more points or be less ambitious in the tuning goals.</li></ul></li><li>What fraction of the trials in each study are <strong>infeasible</strong> (i.e.<br>trials that diverge, get really bad loss values, or fail to run at all<br>because they violate some implicit constraint)?<ul><li>When a very large fraction of points in a study are <strong>infeasible</strong><br>we should try to adjust the search space to avoid sampling such<br>points, which sometimes requires reparameterizing the search space.</li><li>In some cases, a large number of infeasible points can indicate a<br>bug in the training code.</li></ul></li><li><a href="#how-can-optimization-failures-be-debugged-and-mitigated">Does the model exhibit optimization issues?</a></li><li><a href="#examining-the-training-curves">What can we learn from the training curves of the best trials?</a><ul><li>For example, do the best trials have training curves consistent with<br>problematic overfitting?</li></ul></li></ul></li><li>If necessary, based on the answers to the questions above, refine the most<br>recent study (or group of studies) to improve the search space and&#x2F;or sample<br>more trials, or take some other corrective action.</li><li>Once we have answered the above questions, we can move on to evaluating the<br>evidence the experiments provide towards our original goal (for example,<br><a href="#detecting-whether-a-change-is-useful-with-isolation-plots">evaluating whether a change is useful</a>).</li></ul><h4 id="Identifying-bad-search-space-boundaries"><a href="#Identifying-bad-search-space-boundaries" class="headerlink" title="Identifying bad search space boundaries"></a>Identifying bad search space boundaries</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>A search space is suspicious if the best point sampled from it is close to<br>its boundary. We might find an even better point if we expanded the search<br>range in that direction.</li><li>To check search space boundaries, we like to plot completed trials on what<br>we call <strong>basic hyperparameter axis plots</strong> where we plot the validation<br>objective value versus one of the hyperparameters (e.g. learning rate). Each<br>point on the plot corresponds to a single trial.<ul><li>The validation objective value for each trial should usually be the best<br>value it achieved over the course of training.</li></ul></li></ul><p align="center" id="figure-1"><img src="assets/bad_search_space.png" width="49%" alt="Example of bad search space boundaries"><img src="assets/good_search_space.png" width="49%" alt="Example of good search space boundaries"></p><p align="center"><b>Figure 1:</b> Examples of bad search space boundaries and acceptable search space boundaries.</p><ul><li>The plots in <a href="#figure-1">Figure 1</a> show the error rate (lower is better)<br>against the initial learning rate.</li><li>If the best points cluster towards the edge of a search space (in some<br>dimension), then the search space boundaries might need to be expanded until<br>the best observed point is no longer close to the boundary.</li><li>Often, a study will include “infeasible” trials that diverge or get very bad<br>results (marked with red Xs in the above plots).<ul><li>If all trials are infeasible for learning rates greater than some<br>threshold value, and if the best performing trials have learning rates<br>at the edge of that region, the model <a href="#how-can-optimization-failures-be-debugged-and-mitigated">may suffer from stability issues<br>preventing it from accessing higher learning<br>rates</a>.</li></ul></li></ul></details><h4 id="Not-sampling-enough-points-in-the-search-space"><a href="#Not-sampling-enough-points-in-the-search-space" class="headerlink" title="Not sampling enough points in the search space"></a>Not sampling enough points in the search space</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>In general,<br><a href="#how-many-trials-are-needed-to-get-good-results-with-quasi-random-search">it can be very difficult to know</a><br>if the search space has been sampled densely enough. 🤖</li><li>Running more trials is of course better, but comes at an obvious cost.</li><li>Since it is so hard to know when we have sampled enough, we usually sample<br>what we can afford and try to calibrate our intuitive confidence from<br>repeatedly looking at various hyperparameter axis plots and trying to get a<br>sense of how many points are in the “good” region of the search space.</li></ul></details><h4 id="Examining-the-training-curves"><a href="#Examining-the-training-curves" class="headerlink" title="Examining the training curves"></a>Examining the training curves</h4><details><summary><em>[Click to expand]</em></summary><br><p><em><strong>Summary:</strong></em> <em>Examining the training curves is an easy way to identify common<br>failure modes and can help us prioritize what actions to take next.</em></p><ul><li>Although in many cases the primary objective of our experiments only<br>requires considering the validation error of each trial, we must be careful<br>when reducing each trial to a single number because it can hide important<br>details about what’s going on below the surface.</li><li>For every study, we always look at the <strong>training curves</strong> (training error<br>and validation error plotted versus training step over the duration of<br>training) of at least the best few trials.</li><li>Even if this is not necessary for addressing the primary experimental<br>objective, examining the training curves is an easy way to identify common<br>failure modes and can help us prioritize what actions to take next.</li><li>When examining the training curves, we are interested in the following<br>questions.</li><li>Are any of the trials exhibiting <strong>problematic overfitting?</strong><ul><li>Problematic overfitting occurs when the validation error starts<br><em>increasing</em> at some point during training.</li><li>In experimental settings where we optimize away nuisance hyperparameters<br>by selecting the “best” trial for each setting of the scientific<br>hyperparameters, we should check for problematic overfitting in <em>at<br>least</em> each of the best trials corresponding to the settings of the<br>scientific hyperparameters that we’re comparing.<ul><li>If any of the best trials exhibits problematic overfitting, we<br>usually want to re-run the experiment with additional regularization<br>techniques and&#x2F;or better tune the existing regularization parameters<br>before comparing the values of the scientific hyperparameters.<ul><li>This may not apply if the scientific hyperparameters include<br>regularization parameters, since then it would not be surprising<br>if low-strength settings of those regularization parameters<br>resulted in problematic overfitting.</li></ul></li><li>Reducing overfitting is often straightforward using common<br>regularization techniques that add minimal code complexity or extra<br>computation (e.g. dropout, label smoothing, weight decay), so it’s<br>usually no big deal to add one or more of these to the next round of<br>experiments.</li><li>For example, if the scientific hyperparameter is “number of hidden<br>layers” and the best trial that uses the largest number of hidden<br>layers exhibited problematic overfitting, then we would usually<br>prefer to try it again with additional regularization instead of<br>immediately selecting the smaller number of hidden layers.</li><li>Even if none of the “best” trials are exhibiting problematic<br>overfitting, there might still be a problem if it occurs in <em>any</em> of<br>the trials.<ul><li>Selecting the best trial suppresses configurations exhibiting<br>problematic overfitting and favors those that do not. In other<br>words, it will favor configurations with more regularization.</li><li>However, anything that makes training worse can act as a<br>regularizer, even if it wasn’t intended that way. For example,<br>choosing a smaller learning rate can regularize training by<br>hobbling the optimization process, but we typically don’t want<br>to choose the learning rate this way.</li><li>So we must be aware that the “best” trial for each setting of<br>the scientific hyperparameters might be selected in such a way<br>that favors “bad” values of some of the scientific or nuisance<br>hyperparameters.</li></ul></li></ul></li></ul></li><li>Is there high step-to-step variance in the training or validation error late<br>in training?<ul><li>If so, this could interfere with our ability to compare different values<br>of the scientific hyperparameters (since each trial randomly ends on a<br>“lucky” or “unlucky” step) and our ability to reproduce the result of<br>the best trial in production (since the production model might not end<br>on the same “lucky” step as in the study).</li><li>The most likely causes of step-to-step variance are batch variance (from<br>randomly sampling examples from the training set for each batch), small<br>validation sets, and using a learning rate that’s too high late in<br>training.</li><li>Possible remedies include increasing the batch size, obtaining more<br>validation data, using learning rate decay, or using Polyak averaging.</li></ul></li><li>Are the trials still improving at the end of training?<ul><li>If so, this indicates that we are in the<br><a href="#determining-the-number-of-steps-for-each-training-run">“compute bound” regime</a><br>and we may benefit from<br><a href="#Deciding-how-long-to-train-when-training-is-compute-bound">increasing the number of training steps</a><br>or changing the learning rate schedule.</li></ul></li><li>Has performance on the training and validation sets saturated long before<br>the final training step?<ul><li>If so, this indicates that we are in the<br><a href="#determining-the-number-of-steps-for-each-training-run">“not compute-bound”</a><br>regime and that we may be able to<br><a href="#deciding-how-long-to-train-when-training-is-not-compute-bound">decrease the number of training steps</a>.</li></ul></li><li>Although we cannot enumerate them all, there are many other additional<br>behaviors that can become evident from examining the training curves (e.g.<br>training loss <em>increasing</em> during training usually indicates a bug in the<br>training pipeline).</li></ul></details><h4 id="Detecting-whether-a-change-is-useful-with-isolation-plots"><a href="#Detecting-whether-a-change-is-useful-with-isolation-plots" class="headerlink" title="Detecting whether a change is useful with isolation plots"></a>Detecting whether a change is useful with isolation plots</h4><details><summary><em>[Click to expand]</em></summary><br><p align="center" id="figure-2"><img src="assets/isolation_plot.png" width="49%" alt="Isolation plot that investigates the best value of weight decay for ResNet-50trained on ImageNet."></p><p align="center"><b>Figure 2:</b> Isolation plot that investigates the best value of weight decay for ResNet-50 trained on ImageNet.</p><ul><li>Often, the goal of a set of experiments is to compare different values of a<br>scientific hyperparameter.<ul><li>For example, we may want to determine the value of weight decay that<br>results in the best validation error.</li></ul></li><li>An <strong>isolation plot</strong> is a special case of the basic hyper-parameter axis<br>plot. Each point on an isolation plot corresponds to the performance of the<br><em>best</em> trial across some (or all) of the nuisance hyperparameters.<ul><li>In other words, we plot the model performance after “optimizing away”<br>the nuisance hyperparameters.</li></ul></li><li>An isolation plot makes it easier to perform an apples-to-apples comparison<br>between different values of the scientific hyperparameter.</li><li>For example, <a href="#figure-2">Figure 2</a> reveals the value of weight decay that<br>produces the best validation performance for a particular configuration of<br>ResNet-50 trained on ImageNet.<ul><li>If our goal is to determine whether to include weight decay at all, then<br>we would compare the best point from this plot against the baseline of<br>no weight decay. For a fair comparison, the baseline should also have<br>its learning rate equally well tuned.</li></ul></li><li>When we have data generated by (quasi)random search and are considering a<br>continuous hyperparameter for an isolation plot, we can approximate the<br>isolation plot by bucketing the x-axis values of the basic hyperparameter<br>axis plot and taking the best trial in each vertical slice defined by the<br>buckets.</li></ul></details><h4 id="Automate-generically-useful-plots"><a href="#Automate-generically-useful-plots" class="headerlink" title="Automate generically useful plots"></a>Automate generically useful plots</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>The more effort it is to generate plots, the less likely we are to look at<br>them as much as we should, so it behooves us to set up our infrastructure to<br>automatically produce as many of them as possible.</li><li>At a minimum, we automatically generate basic hyperparameter axis plots for<br>all hyperparameters that we vary in an experiment.</li><li>Additionally, we automatically produce training curves for all trials and<br>make it as easy as possible to find the best few trials of each study and<br>examine their training curves.</li><li>There are many other potential plots and visualizations we can add that can<br>be useful. Although the ones described above are a good starting point, to<br>paraphrase Geoffrey Hinton, “Every time you plot something new, you learn<br>something new.”</li></ul></details><h3 id="Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration"><a href="#Determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration" class="headerlink" title="Determining whether to adopt a training pipeline change or hyperparameter configuration"></a>Determining whether to adopt a training pipeline change or hyperparameter configuration</h3><p><em><strong>Summary:</strong></em> <em>When deciding whether to make a change to our model or training<br>procedure or adopt a new hyperparameter configuration going forward, we need to<br>be aware of the different sources of variation in our results.</em></p><ul><li>When we are trying to improve our model, we might observe that a particular<br>candidate change initially achieves a better validation error compared to<br>our incumbent configuration, but find that after repeating the experiment<br>there is no consistent advantage. Informally, we can group the most<br>important sources of variation that might cause such an inconsistent result<br>into the following broad categories:<ul><li><strong>Training procedure variance</strong>, <strong>retrain variance</strong>, or <strong>trial<br>variance</strong>: the variation we see between training runs that use the same<br>hyperparameters, but different random seeds.<ul><li>For example, different random initializations, training data<br>shuffles, dropout masks, patterns of data augmentation operations,<br>and orderings of parallel arithmetic operations, are all potential<br>sources of trial variance.</li></ul></li><li><strong>Hyperparameter search variance</strong>, or <strong>study variance</strong>: the variation<br>in results caused by our procedure to select the hyperparameters.<ul><li>For example, we might run the same experiment with a particular<br>search space, but with two different seeds for quasi-random search<br>and end up selecting different hyperparameter values.</li></ul></li><li><strong>Data collection and sampling variance</strong>: the variance from any sort of<br>random split into training, validation, and test data or variance due to<br>the training data generation process more generally.</li></ul></li><li>It is all well and good to make comparisons of validation error rates<br>estimated on a finite validation set using fastidious statistical tests, but<br>often the trial variance alone can produce statistically significant<br>differences between two different trained models that use the same<br>hyperparameter settings.</li><li>We are most concerned about study variance when trying to make conclusions<br>that go beyond the level of an individual point in hyperparameters space.<ul><li>The study variance depends on the number of trials and the search space<br>and we have seen cases where it is larger than the trial variance as<br>well as cases where it is much smaller.</li></ul></li><li>Therefore, before adopting a candidate change, consider running the best<br>trial N times to characterize the run-to-run trial variance.<ul><li>Usually, we can get away with only recharacterizing the trial variance<br>after major changes to the pipeline, but in some applications we might<br>need fresher estimates.</li><li>In other applications, characterizing the trial variance is too costly<br>to be worth it.</li></ul></li><li>At the end of the day, although we only want to adopt changes (including new<br>hyperparameter configurations) that produce real improvements, demanding<br>complete certainty that something helps isn’t the right answer either.</li><li>Therefore, if a new hyperparameter point (or other change) gets a better<br>result than the baseline (taking into account the retrain variance of both<br>the new point and the baseline as best we can), then we probably should<br>adopt it as the new baseline for future comparisons.<ul><li>However, we should only adopt changes that produce improvements that<br>outweigh any complexity they add.</li></ul></li></ul><h3 id="After-exploration-concludes"><a href="#After-exploration-concludes" class="headerlink" title="After exploration concludes"></a>After exploration concludes</h3><p><em><strong>Summary:</strong></em> <em>Bayesian optimization tools are a compelling option once we’re<br>done exploring for good search spaces and have decided what hyperparameters even<br>should be tuned at all.</em></p><ul><li>At some point, our priorities will shift from learning more about the tuning<br>problem to producing a single best configuration to launch or otherwise use.</li><li>At this point, there should be a refined search space that comfortably<br>contains the local region around the best observed trial and has been<br>adequately sampled.</li><li>Our exploration work should have revealed the most essential hyperparameters<br>to tune (as well as sensible ranges for them) that we can use to construct a<br>search space for a final automated tuning study using as large a tuning<br>budget as possible.</li><li>Since we no longer care about maximizing our insight into the tuning<br>problem, many of<br><a href="#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning">the advantages of quasi-random search</a><br>no longer apply and Bayesian optimization tools should be used to<br>automatically find the best hyperparameter configuration.<ul><li>If the search space contains a non-trivial volume of divergent points<br>(points that get NaN training loss or even training loss many standard<br>deviations worse than the mean), it is important to use black box<br>optimization tools that properly handle trials that diverge (see<br><a href="https://arxiv.org/abs/1403.5607">Bayesian Optimization with Unknown Constraints</a><br>for an excellent way to deal with this issue).</li></ul></li><li>At this point, we should also consider checking the performance on the test<br>set.<ul><li>In principle, we could even fold the validation set into the training<br>set and retraining the best configuration found with Bayesian<br>optimization. However, this is only appropriate if there won’t be future<br>launches with this specific workload (e.g. a one-time Kaggle<br>competition).</li></ul></li></ul><h2 id="Determining-the-number-of-steps-for-each-training-run"><a href="#Determining-the-number-of-steps-for-each-training-run" class="headerlink" title="Determining the number of steps for each training run"></a>Determining the number of steps for each training run</h2><ul><li>There are two types of workloads: those that are compute-bound and those<br>that are not.</li><li>When training is <strong>compute-bound</strong>, training is limited by how long we are<br>willing to wait and not by how much training data we have or some other<br>factor.<ul><li>In this case, if we can somehow train longer or more efficiently, we<br>should see a lower training loss and, with proper tuning, an improved<br>validation loss.</li><li>In other words, <em>speeding up</em> training is equivalent to <em>improving</em><br>training and the “optimal” training time is always “as long as we can<br>afford.”</li><li>That said, just because a workload is compute-limited doesn’t mean<br>training longer&#x2F;faster is the only way to improve results.</li></ul></li><li>When training is <strong>not compute-bound</strong>, we can afford to train as long as we<br>would like to, and, at some point, training longer doesn’t help much (or<br>even causes problematic overfitting).<ul><li>In this case, we should expect to be able to train to very low training<br>loss, to the point where training longer might slightly reduce the<br>training loss, but will not meaningfully reduce the validation loss.</li><li>Particularly when training is not compute-bound, a more generous<br>training time budget can make tuning easier, especially when tuning<br>learning rate decay schedules, since they have a particularly strong<br>interaction with the training budget.<ul><li>In other words, very stingy training time budgets might require a<br>learning rate decay schedule tuned to perfection in order to achieve<br>a good error rate.</li></ul></li></ul></li><li>Regardless of whether a given workload is compute-bound or not, methods that<br>increase the variance of the gradients (across batches) will usually result<br>in slower training progress, and thus may increase the number of training<br>steps required to reach a particular validation loss. High gradient variance<br>can be caused by:<ul><li>Using a smaller batch size</li><li>Adding data augmentation</li><li>Adding some types of regularization (e.g. dropout)</li></ul></li></ul><h3 id="Deciding-how-long-to-train-when-training-is-not-compute-bound"><a href="#Deciding-how-long-to-train-when-training-is-not-compute-bound" class="headerlink" title="Deciding how long to train when training is not compute-bound"></a>Deciding how long to train when training is <em>not</em> compute-bound</h3><ul><li>Our main goal is to ensure we are training long enough for the model to<br>reach the best possible result, while avoiding being overly wasteful in the<br>number of training steps.</li><li>When in doubt, err on the side of training longer. Performance should never<br>degrade when training longer, assuming retrospective (optimal) checkpoint<br>selection is used properly and checkpoints are frequent enough.</li><li>Never tune the <code>max_train_steps</code> number in a study. Pick a value and use it<br>for all trials. From these trials, plot the training step that retrospective<br>checkpoint selection finds in order to refine the choice of<br><code>max_train_steps</code>.<ul><li>For example, if the best step is always during the first 10% of<br>training, then the maximum number of steps is way too high.</li><li>Alternatively, if the best step is consistently in the last 25% of<br>training we might benefit from training longer and re-tuning the decay<br>schedule.</li></ul></li><li>The ideal number of training steps can change when the architecture or data<br>changes (e.g. adding data augmentation).</li><li>Below we describe how to pick an initial candidate value for<br><code>max_train_steps</code> based on the number of steps necessary to “perfectly fit”<br>the training set using a constant learning rate.<ul><li>Note, we are not using the phrase “perfectly fit the training set” in a<br>precise or mathematically well-defined way. It is merely meant as an<br>informal descriptor to indicate a very low training loss.<ul><li>For example, when training with the log loss, absent regularization<br>terms, we might see the training loss keep slowly improving until we<br>reach floating point limits as the network weights grow without<br>bound and the predictions of the model on the training set become<br>increasingly confident. In this case, we might say the model<br>“perfectly fit” the training set around the time the<br>misclassification error reached zero on the training set.</li></ul></li><li>The starting value for <code>max_train_steps</code> we find may need to be<br>increased if the amount of gradient noise in the training procedure<br>increases.<ul><li>For example, if data augmentation or regularizers like dropout are<br>introduced to the model.</li></ul></li><li>It may be possible to decrease <code>max_train_steps</code> if the training process<br>improves somehow.<ul><li>For example, with a better tuned optimizer or a better tuned<br>learning rate schedule.</li></ul></li></ul></li></ul><h4 id="Algorithm-for-picking-an-initial-candidate-for-max-train-steps-using-a-learning-rate-sweep"><a href="#Algorithm-for-picking-an-initial-candidate-for-max-train-steps-using-a-learning-rate-sweep" class="headerlink" title="Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep"></a>Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>This procedure assumes it is possible to not only “perfectly” fit the<br>training set, but to do so using a constant learning rate schedule.</li><li>If it is possible to perfectly fit the entire training set, then there must<br>exist a configuration (with some value of <code>max_train_steps</code>) that perfectly<br>fits the training set; find any such configuration and use its value of<br><code>max_train_steps</code> as a starting point <code>N</code>.</li><li>Run a constant learning rate sweep (i.e. grid search the learning rate)<br>without data augmentation and without regularization where each trial trains<br>for <code>N</code> steps.</li><li>The number of steps required for the fastest trial in the sweep to reach<br>perfect training performance is our initial guess for <code>max_train_steps</code>.</li><li><strong>NOTE:</strong> Bad search spaces can make it possible to engage in<br>self-deception.<ul><li>For example, if all the learning rates in a study are too small, we<br>might incorrectly conclude that a very large value of <code>max_train_steps</code><br>is necessary.</li><li>At a minimum, we should check that the optimal learning rate in the<br>study is not at the boundary of the search space.</li></ul></li></ul></details><h3 id="Deciding-how-long-to-train-when-training-is-compute-bound"><a href="#Deciding-how-long-to-train-when-training-is-compute-bound" class="headerlink" title="Deciding how long to train when training is compute-bound"></a>Deciding how long to train when training is compute-bound</h3><ul><li>In some cases, training loss keeps improving indefinitely and our patience<br>and computational resources become the limiting factors.</li><li>If training loss (or even validation loss) keeps improving indefinitely,<br>should we always train as long as we can afford? Not necessarily.<ul><li>We might be able to tune more effectively by running a larger number of<br>shorter experiments and reserving the longest “production length” runs<br>for the models we hope to launch.</li><li>As the training time for trials approaches our patience limit, tuning<br>experiments become more relevant for our potential launch candidates,<br>but we can complete fewer of them.</li><li>There are probably many questions we can answer while only training for<br>~10% of the production length, but there is always a risk that our<br>conclusions at this time limit will not apply to experiments at 20% of<br>the production length, let alone 100%.</li></ul></li><li>Tuning in multiple rounds with increasing, per-trial training step limits is<br>a sensible approach.<ul><li>We can do as many rounds as we want, but usually 1-3 are the most<br>practical.</li><li>Essentially, try to obtain as much understanding of the problem as<br>possible using trials with a very quick turnaround time, trading off<br>tuning thoroughness with relevance to the final, longest runs.</li><li>Once a given per-trial time limit has generated useful insights, we can<br>increase the training time and continue tuning, double-checking our<br>conclusions from the shorter runs as needed.</li></ul></li><li>As a starting point, we recommend two rounds of tuning:<ul><li>Round 1: Shorter runs to find good model and optimizer hyperparameters.</li><li>Round 2: Very few long runs on good hyperparameter points to get the<br>final model.</li></ul></li><li>The biggest question going from <code>Round i</code> &rarr; <code>Round i+1</code> is how to<br>adjust learning rate decay schedules.<ul><li>One common pitfall when adjusting learning rate schedules between rounds<br>is using all the extra training steps with too small of a learning rate.</li></ul></li></ul><h4 id="Round-1"><a href="#Round-1" class="headerlink" title="Round 1"></a>Round 1</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>Unfortunately, there is no guarantee that good hyperparameters found in<br>short, incomplete training are still good choices when training length is<br>significantly increased. However, for some kinds of hyperparameters, they<br>are often correlated enough for Round 1 to be useful.</li><li>What hyperparameter values found in shorter runs do we expect to transfer to<br>longer training runs? For all of this, we need more research. But based on<br>what we know so far, here are the authors’ suspicions in order of decreasing<br>probability of transferring:<ul><li>Very likely to transfer<ul><li>Early training instability can be resolved in the first round of<br>tuning using a smaller number of training steps. Perhaps these<br>hyperparameters are the closest thing to a sure bet for transfer<br>that we have.<ul><li>Warmup length</li><li>Initialization</li></ul></li></ul></li><li>Likely to transfer<ul><li>Model architecture - A dramatic win in the model architecture will<br>usually transfer, but there are probably many counterexamples.</li></ul></li><li>Might transfer<ul><li>Optimization algorithm&#x2F;optimizer hyperparameters - We think this<br>would “loosely” transfer. It’s definitely weaker than the things<br>above it.</li><li>Data augmentation</li><li>Regularization<ul><li>If it isn’t possible to perfectly fit the training set, the<br>model might be in a regime where regularization is unlikely to<br>help very much.</li></ul></li></ul></li><li>Unlikely to transfer<ul><li>Learning rate schedule: unlikely to transfer perfectly.<ul><li><a href="https://arxiv.org/abs/2203.15556">This paper</a> suggests that<br>even decay schedule transfers, but we don’t believe this is true<br>in general. Example: Tuning sqrt decay on small # of training<br>steps then extending to large # will result in the majority of<br>training occurring at overly small steps.<ul><li>One can likely do “good enough” with most schedules in the<br>limit of extreme training budget, but noticeable performance<br>improvements can likely be seen if it is tuned.</li></ul></li><li><a href="https://arxiv.org/abs/1803.02021">Understanding Short-Horizon Bias in Stochastic<br>Meta-Optimization</a> describes<br>the dangers of trying to pick learning rates myopically.</li></ul></li></ul></li></ul></li></ul></details><h4 id="Round-2"><a href="#Round-2" class="headerlink" title="Round 2"></a>Round 2</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>Run the best hyperparameter configuration from Round 1.</li><li><strong>(Speculation)</strong> 🤖 Use the extra steps to extend the period of training at<br>a high learning rate.<ul><li>E.g. if linear schedule then keep the length of the decay fixed from<br>Round 1 and extend the period of constant lr in the beginning.</li><li>For cosine decay, just keep the base lr from Round 1 and extend<br><code>max_train_steps</code> as in<br><a href="https://arxiv.org/abs/2203.15556">Chinchilla paper</a>.</li></ul></li><li>More rounds might make sense for teams with very mature modeling and tuning<br>pipelines and very long and expensive production training runs, but they<br>will often be overkill.<ul><li>We’ve described how to transfer from Step 1 &rarr; Step 2. If we didn’t care<br>about analysis time and if making efficient use of compute was the<br>overriding concern, then the ideal would be to exponentially increase<br>the length of training runs (and thus the end-to-end time to complete a<br>study) over many different rounds of tuning.<ul><li>At each round we systematically ensure our choices continue to hold<br>up.</li><li>New ideas go through a pipeline that progressively derisks them<br>using increasingly long-running experiments from Step i to Step i+1.</li></ul></li></ul></li></ul></details><h2 id="Additional-guidance-for-the-training-pipeline"><a href="#Additional-guidance-for-the-training-pipeline" class="headerlink" title="Additional guidance for the training pipeline"></a>Additional guidance for the training pipeline</h2><h3 id="Optimizing-the-input-pipeline"><a href="#Optimizing-the-input-pipeline" class="headerlink" title="Optimizing the input pipeline"></a>Optimizing the input pipeline</h3><p><em><strong>Summary:</strong></em> <em>The causes and interventions of input-bound pipelines are highly<br>task-dependent; use a profiler and look out for common issues.</em></p><ul><li>Use an appropriate profiler to diagnose input-bound pipelines. For example,<br><a href="https://jax.readthedocs.io/en/latest/profiling.html">Perfetto</a> for JAX or<br><a href="https://www.tensorflow.org/guide/profiler">TensorFlow profiler</a> for<br>TensorFlow.</li><li>Ultimately, the specific causes and interventions will be highly<br>task-dependent. Broader engineering considerations (e.g. minimizing disk<br>footprint) may warrant worse input pipeline performance.</li><li>Common causes:<ul><li>Data are not colocated with the training process, causing I&#x2F;O latency<br>(this might happen when reading training data over a network).</li><li>Expensive online data preprocessing (consider doing this once offline<br>and saving).</li><li>Unintentional synchronization barriers that interfere with data pipeline<br>prefetching. For example, when synchronizing metrics between the device<br>and host in CommonLoopUtils<br>(<a href="https://github.com/google/CommonLoopUtils/blob/fea2518ada8814a78e1492023fd9f00edb0b0568/clu/metrics.py#L291">link</a>).</li></ul></li><li>Common tips:<ul><li>Instrument input pipeline to prefetch examples (e.g.<br><a href="https://www.tensorflow.org/guide/data_performance#prefetching">tf.data.Dataset.prefetch</a>)</li><li>Remove unused features&#x2F;metadata from each as early in the pipeline as<br>possible.</li><li>Increase the replication of the number of jobs generating examples for<br>the input pipeline. For example, by using the<br><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/service">tf.data service</a>.</li></ul></li></ul><h3 id="Evaluating-model-performance"><a href="#Evaluating-model-performance" class="headerlink" title="Evaluating model performance"></a>Evaluating model performance</h3><p><em><strong>Summary:</strong></em> <em>Run evaluation at larger batch sizes than training. Run<br>evaluations at regular step intervals, not regular time intervals.</em></p><h4 id="Evaluation-settings"><a href="#Evaluation-settings" class="headerlink" title="Evaluation settings"></a>Evaluation settings</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>There are several settings in which we can evaluate the performance of our<br>models.<ul><li><strong>Online evaluation</strong> - metrics are collected when the model is serving<br>predictions in a production environment.</li><li><strong>Offline evaluation</strong> - metrics are collected when the model is run on<br>offline train&#x2F;validation&#x2F;test sets that are representative of the<br>production environment.</li><li><strong>Periodic evaluations</strong> - metrics are collected during model training<br>that might either be a proxy for the offline evaluation, and&#x2F;or on a<br>subset of the data used in offline evaluation.</li></ul></li><li>Online evaluation is the gold standard, but is often impractical during the<br>model development phase.</li><li>Depending on the problem, offline evaluation can be fairly involved and<br>computationally expensive.</li><li>Periodic evaluations are the most practical and economical choice, but may<br>not fully represent the production environment.<ul><li>Our goal during periodic evaluation is to use an expedient proxy of the<br>offline evaluation, without sacrificing the reliability of the signal we<br>get during training.</li></ul></li></ul></details><h4 id="Setting-up-periodic-evaluations"><a href="#Setting-up-periodic-evaluations" class="headerlink" title="Setting up periodic evaluations"></a>Setting up periodic evaluations</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>We run periodic evaluations during training to monitor its progress in real<br>time, to<br><a href="#saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint">facilitate retrospective model checkpoint selection</a>,<br>and so that we can<br><a href="#examining-the-training-curves">examine the training curves at the end of training</a>.</li><li>The simplest configuration is to perform both training and periodic<br>evaluations within the same compute instance, periodically alternating<br>between training and evaluation.<ul><li>In this case, the batch size used to perform evaluations should be <em>at<br>least</em> as large as the batch size used for training because model<br>activations don’t need to be maintained during evaluation, lowering the<br>computational requirements per example.</li></ul></li><li>Periodic evaluations should be done at regular step intervals, not time<br>intervals.<ul><li>Evaluating based on time intervals can make it harder to interpret the<br>training curves, especially when training may suffer from preemptions of<br>the training jobs, network latency issues, etc.</li></ul></li><li>Periodicity in valid&#x2F;test metrics (when using a shuffled<br>train&#x2F;validation&#x2F;test split) can indicate implementation bugs such as test<br>data having overlap with training data, or training data not being properly<br>shuffled. Evaluating at regular step intervals can make these issues easier<br>to catch.</li><li>Partial batches can occur when the evaluation sets are not divisible by the<br>batch size. Ensure that the padded examples are correctly weighed to prevent<br>the loss function from being biased by them. Often, these padded examples<br>can be given a weight of zero.</li><li>Save sufficient information per evaluation to support offline analysis.<br>Ideally, we would save predictions on a selection of individual examples<br>since they can be invaluable for debugging.<ul><li>Generating artifacts like<br><a href="https://www.tensorflow.org/guide/saved_model">SavedModels</a> make it easy<br>to do ad-hoc model inspection after evaluation jobs finish.</li></ul></li></ul></details><h4 id="Choosing-a-sample-for-periodic-evaluation"><a href="#Choosing-a-sample-for-periodic-evaluation" class="headerlink" title="Choosing a sample for periodic evaluation"></a>Choosing a sample for periodic evaluation</h4><details><summary><em>[Click to expand]</em></summary><br><ul><li>The periodic evaluation job might not run fast enough to compute metrics on<br>the full offline evaluation set in a reasonable amount of time. This often<br>necessitates sampling data for periodic evaluation.</li><li>We consider the following factors when constructing a sampled dataset:<ul><li><ins>Sample size</ins><ul><li>Check that the performance computed on the sampled dataset used by<br>the periodic job matches the performance on the whole offline<br>evaluation set, i.e. there is no skew between the sampled set and<br>the full dataset.</li><li>The dataset used for periodic evaluation should be small enough that<br>it’s easy to generate model predictions over its entirety, but large<br>enough that improvements to the model can be accurately measured<br>(i.e. not overwhelmed by label noise).</li><li>It should be large enough to accommodate multiple such evaluations<br>across trials in sequence, and still produce accurate estimates.<br>That is, to avoid adaptively “fitting” to the validation set over<br>time, in a way that doesn’t generalize to a held-out test set.<br>However, this consideration is rarely a practical concern.</li></ul></li><li><ins>Imbalanced datasets</ins><ul><li>For imbalanced datasets, performance on rare classes of examples<br>will often be noisy.</li><li>For datasets with a small number of examples in a class label, log<br>the number of examples predicted correctly to get more insight into<br>accuracy improvements (.05 sensitivity improvement sounds exciting,<br>but was it just one more example correct?).</li></ul></li></ul></li></ul></details><h3 id="Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint"><a href="#Saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint" class="headerlink" title="Saving checkpoints and retrospectively selecting the best checkpoint"></a>Saving checkpoints and retrospectively selecting the best checkpoint</h3><p><em><strong>Summary:</strong></em> <em>Run training for a fixed number of steps and retrospectively<br>choose the best checkpoint from the run.</em></p><ul><li>Most deep learning frameworks support<br><a href="https://flax.readthedocs.io/en/latest/api_reference/flax.training.html">model checkpointing</a>.<br>That is, the current state of the model is periodically preserved on disk.<br>This allows the training job to be resilient to compute instance<br>interruptions.</li><li>The best checkpoint is often not the last checkpoint, particularly when the<br>validation set performance does not continue to increase over time but<br>rather fluctuates about a particular value.</li><li>Set up the pipeline to keep track of the N best checkpoints seen so far<br>during training. At the end of training, model selection is then a matter of<br>choosing the best checkpoint seen during training. We call this<br><strong>retrospective optimal checkpoint selection</strong>.</li><li>Supporting prospective early stopping is usually not necessary, since we’re<br>pre-specifying a trial budget and are preserving the N best checkpoints seen<br>so far.</li></ul><h3 id="Setting-up-experiment-tracking"><a href="#Setting-up-experiment-tracking" class="headerlink" title="Setting up experiment tracking"></a>Setting up experiment tracking</h3><p><em><strong>Summary:</strong></em> <em>When tracking different experiments, make sure to note a number<br>of essentials like the best performance of a checkpoint in the study, and a<br>short description of the study.</em></p><ul><li>We’ve found that keeping track of experiment results in a spreadsheet has<br>been helpful for the sorts of modeling problems we’ve worked on. It often<br>has the following columns:<ul><li>Study name</li><li>A link to wherever the config for the study is stored.</li><li>Notes or a short description of the study.</li><li>Number of trials run</li><li>Performance on the validation set of the best checkpoint in the study.</li><li>Specific reproduction commands or notes on what unsubmitted changes were<br>necessary to launch training.</li></ul></li><li>Find a tracking system that captures at least the information listed above<br>and is convenient for the people doing it. Untracked experiments might as<br>well not exist.</li></ul><h3 id="Batch-normalization-implementation-details"><a href="#Batch-normalization-implementation-details" class="headerlink" title="Batch normalization implementation details"></a>Batch normalization implementation details</h3><p><em><strong>Summary:</strong></em> <em>Nowadays batch norm can often be replaced with LayerNorm, but in<br>cases where it cannot, there are tricky details when changing the batch size or<br>number of hosts.</em></p><ul><li>Batch norm normalizes activations using their mean and variance over the<br>current batch, but in the multi-device setting these statistics are<br>different on each device unless explicitly synchronized.</li><li>Anecdotal reports (mostly on ImageNet) say calculating these normalizing<br>statistics using only ~64 examples actually works better in practice (see<br>Ghost Batch Norm from <a href="https://arxiv.org/abs/1705.08741">this paper</a>).</li><li>Decoupling the total batch size and the number of examples used to calculate<br>batch norm statistics is particularly useful for batch size comparisons.</li><li>Ghost batch norm implementations do not always correctly handle the case<br>where the per-device batch size &gt; virtual batch size. In this case we’d<br>actually need to subsample the batch on each device in order to get the<br>proper number of batch norm statistic examples.</li><li>Exponential moving averages used in test mode batch norm are just a linear<br>combination of training statistics, so these EMAs only need to be<br>synchronized before saving them in checkpoints. However, some common<br>implementations of batch norm do not synchronize these EMAs and only save<br>the EMA from the first device.</li></ul><h3 id="Considerations-for-multi-host-pipelines"><a href="#Considerations-for-multi-host-pipelines" class="headerlink" title="Considerations for multi-host pipelines"></a>Considerations for multi-host pipelines</h3><p><em><strong>Summary:</strong></em> <em>for logging, evals, RNGs, checkpointing, and data sharding,<br>multi-host training can make it very easy to introduce bugs!</em></p><ul><li>Ensure the pipeline is only logging and checkpointing on one host.</li><li>Make sure before evaluation or checkpointing is run, the batch norm<br>statistics are synchronized across hosts.</li><li>It is critical to have RNG seeds that are the same across hosts (for model<br>initialization), and seeds that are different across hosts (for data<br>shuffling&#x2F;preprocessing), so make sure to mark them appropriately.</li><li>Sharding data files across hosts is usually recommended for improved<br>performance.</li></ul><h2 id="FAQs"><a href="#FAQs" class="headerlink" title="FAQs"></a>FAQs</h2><h3 id="What-is-the-best-learning-rate-decay-schedule-family"><a href="#What-is-the-best-learning-rate-decay-schedule-family" class="headerlink" title="What is the best learning rate decay schedule family?"></a>What is the best learning rate decay schedule family?</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>It’s an open problem. It’s not clear how to construct a set of rigorous<br>experiments to confidently answer what the “best” LR decay schedule is.</li><li>Although we don’t know the best schedule family, we’re confident that it’s<br>important to have some (non-constant) schedule and that tuning it matters.</li><li>Different learning rates work best at different times during the<br>optimization process. Having some sort of schedule makes it more likely for<br>the model to hit a good learning rate.</li></ul></details><h3 id="Which-learning-rate-decay-should-I-use-as-a-default"><a href="#Which-learning-rate-decay-should-I-use-as-a-default" class="headerlink" title="Which learning rate decay should I use as a default?"></a>Which learning rate decay should I use as a default?</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>Our preference is either linear decay or cosine decay, and a bunch of other<br>schedule families are probably good too.</li></ul></details><h3 id="Why-do-some-papers-have-complicated-learning-rate-schedules"><a href="#Why-do-some-papers-have-complicated-learning-rate-schedules" class="headerlink" title="Why do some papers have complicated learning rate schedules?"></a>Why do some papers have complicated learning rate schedules?</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>It’s not uncommon to see papers with complicated piecewise learning rate<br>(LR) decay schedules.</li><li>Readers often wonder how the authors arrived at such a complicated study.</li><li>Many complicated LR decay schedules are the result of tuning the schedule as<br>a function of the validation set performance in an ad hoc way:<ol><li>Start a single training run with some simple LR decay (or a constant<br>learning rate).</li><li>Keep training running until the performance seems to stagnate. If this<br>happens, pause training. Resume it with a perhaps steeper LR decay<br>schedule (or smaller constant learning rate) from this point. Repeat<br>this process until the conference&#x2F;launch deadline.</li></ol></li><li>Blithely copying the resulting <em>schedule</em> is generally not a good idea since<br>the best particular schedule will be sensitive to a host of other<br>hyperparameter choices.<ul><li>Better to copy the <em>algorithm</em> that produced the schedule, although this<br>is rarely possible when arbitrary human judgment produced the schedule.</li></ul></li><li>This type of validation-error-sensitive schedule is fine to use if it can be<br>fully automated, but human-in-the-loop schedules that are a function of<br>validation error are brittle and not easily reproducible, so we recommend<br>avoiding them.<ul><li>Before publishing results that used such a schedule, please try to make<br>it fully reproducible.</li></ul></li></ul></details><h3 id="How-should-Adam’s-hyperparameters-be-tuned"><a href="#How-should-Adam’s-hyperparameters-be-tuned" class="headerlink" title="How should Adam’s hyperparameters be tuned?"></a>How should Adam’s hyperparameters be tuned?</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>As discussed above, making general statements about search spaces and how<br>many points one should sample from the search space is very difficult. Note<br>that not all the hyperparameters in Adam are equally important. The<br>following rules of thumb correspond to different “budgets” for the number of<br>trials in a study.<ul><li>If &lt; 10 trials in a study, only tune the (base) learning rate.</li><li>If 10-25 trials, tune learning rate and $\beta_1$.</li><li>If 25+ trials, tune the learning rate, $\beta_1$ and $\epsilon$.</li><li>If one can run substantially more than 25 trials, additionally tune<br>$\beta_2$.</li></ul></li></ul></details><h3 id="Why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning"><a href="#Why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning" class="headerlink" title="Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?"></a>Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?</h3><details><summary><em>[Click to expand]</em></summary><ul><li>Quasi-random search (based on<br><a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence">low-discrepancy sequences</a>)<br>is our preference over fancier black box optimization tools when used as<br>part of an iterative tuning process intended to maximize insight into the<br>tuning problem (what we refer to as the “exploration phase”). Bayesian<br>optimization and similar tools are more appropriate for the exploitation<br>phase.</li><li>Quasi-random search based on randomly shifted low-discrepancy sequences can<br>be thought of as “jittered, shuffled grid search”, since it uniformly, but<br>randomly, explores a given search space and spreads out the search points<br>more than random search.</li><li>The advantages of quasi-random search over more sophisticated black box<br>optimization tools (e.g. Bayesian optimization, evolutionary algorithms)<br>include:<ol><li>Sampling the search space non-adaptively makes it possible to change the<br>tuning objective in post hoc analysis without rerunning experiments.<ul><li>For example, we usually want to find the best trial in terms of<br>validation error achieved at any point in training. But the<br>non-adaptive nature of quasi-random search makes it possible to find<br>the best trial based on final validation error, training error, or<br>some alternative evaluation metric without rerunning any<br>experiments.</li></ul></li><li>Quasi-random search behaves in a consistent and statistically<br>reproducible way.<ul><li>It should be possible to reproduce a study from six months ago even<br>if the implementation of the search algorithm changes, as long as it<br>maintains the same uniformity properties. If using sophisticated<br>Bayesian optimization software, the implementation might change in<br>an important way between versions, making it much harder to<br>reproduce an old search. It isn’t always possible to roll back to an<br>old implementation (e.g. if the optimization tool is run as a<br>service).</li></ul></li><li>Its uniform exploration of the search space makes it easier to reason<br>about the results and what they might suggest about the search space.<ul><li>For example, if the best point in the traversal of quasi-random<br>search is at the boundary of the search space, this is a good (but<br>not foolproof) signal that the search space bounds should be<br>changed. <a href="#identifying-bad-search-space-boundaries">This section</a><br>goes into more depth. However, an adaptive black box optimization<br>algorithm might have neglected the middle of the search space<br>because of some unlucky early trials even if it happens to contain<br>equally good points, since it is this exact sort of non-uniformity<br>that a good optimization algorithm needs to employ to speed up the<br>search.</li></ul></li><li>Running different numbers of trials in parallel versus sequentially will<br>not produce statistically different results when using quasi-random<br>search (or other non-adaptive search algorithms), unlike with adaptive<br>algorithms.</li><li>More sophisticated search algorithms may not always handle infeasible<br>points correctly, especially if they aren’t designed with neural network<br>hyperparameter tuning in mind.</li><li>Quasi-random search is simple and works especially well when many tuning<br>trials will be running in parallel.<ul><li>Anecdotally<sup id="fnref:3" class="footnote-ref"><a href="#fn:3" rel="footnote"><span class="hint--top hint--rounded" aria-label="Ben Recht and Kevin Jamieson[pointed out](http://www.argmin.net/2016/06/20/hypertuning/) how strong2X-budget random search is as a baseline (the[Hyperband paper](https://jmlr.org/papers/volume18/16-558/16-558.pdf)makes similar arguments), but it is certainly possible to find searchspaces and problems where state-of-the-art Bayesian optimizationtechniques crush random search that has 2X the budget. However, in ourexperience beating 2X-budget random search gets much harder in thehigh-parallelism regime since Bayesian optimization has no opportunity toobserve the results of previous trials.">[3]</span></a></sup>, it is very hard for an adaptive algorithm to beat a<br>quasi-random search that has 2X its budget, especially when many<br>trials need to be run in parallel (and thus there are very few<br>chances to make use of previous trial results when launching new<br>trials).</li><li>Without expertise in Bayesian optimization and other advanced black<br>box optimization methods, we might not achieve the benefits they<br>are, in principle, capable of providing. It is hard to benchmark<br>advanced black box optimization algorithms in realistic deep<br>learning tuning conditions. They are a very active area of current<br>research, and the more sophisticated algorithms come with their own<br>pitfalls for inexperienced users. Experts in these methods are able<br>to get good results, but in high-parallelism conditions the search<br>space and budget tend to matter a lot more.</li></ul></li></ol></li><li>That said, if our computational resources only allow a small number of<br>trials to run in parallel and we can afford to run many trials in sequence,<br>Bayesian optimization becomes much more attractive despite making our tuning<br>results harder to interpret.</li></ul></details><h3 id="Where-can-I-find-an-implementation-of-quasi-random-search"><a href="#Where-can-I-find-an-implementation-of-quasi-random-search" class="headerlink" title="Where can I find an implementation of quasi-random search?"></a>Where can I find an implementation of quasi-random search?</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>We use<br><a href="https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py">this implementation</a><br>that generates a Halton sequence for a given search space (intended to<br>implement a shifted, scrambled Halton sequence as recommended in<br><a href="https://arxiv.org/abs/1706.03200">https://arxiv.org/abs/1706.03200</a>).</li><li>If a quasi-random search algorithm based on a low-discrepancy sequence is<br>not available, it is possible to substitute pseudo random uniform search<br>instead, although this is likely to be slightly less efficient.<ul><li>In 1-2 dimensions, grid search is also acceptable, although not in<br>higher dimensions (see<br><a href="https://www.jmlr.org/papers/v13/bergstra12a.html">Bergstra &amp; Bengio, 2012</a>).</li></ul></li></ul></details><h3 id="How-many-trials-are-needed-to-get-good-results-with-quasi-random-search"><a href="#How-many-trials-are-needed-to-get-good-results-with-quasi-random-search" class="headerlink" title="How many trials are needed to get good results with quasi-random search?"></a>How many trials are needed to get good results with quasi-random search?</h3><details><summary><em>[Click to expand]</em></summary><br><p align="center"><img src="assets/have_we_sampled_enough.png" width="49%" alt="A box plot showing the importance of sampling enough"></p><p align="center"><b>Figure 3:</b> A ResNet-50 was tuned on ImageNet with 100trials. Via bootstrapping, different amounts of tuning budget were simulated.Box plots of the best performances for each trial budget are plotted above.<ul><li>There is no way to answer this question in general, but we can look at<br>specific examples.</li><li>As the Figure 3 shows, the number of trials in a study can have a<br>substantial impact on the results.<ul><li>Notice how large the interquartile ranges are when 6 trials were<br>sampled, versus when 20 trials were sampled.</li><li>Even with 20 trials, it is likely that the difference between especially<br>lucky and unlucky studies will be larger than the typical variation<br>between re-trains of this model on different random seeds, with fixed<br>hyperparameters, which for this workload might be around +&#x2F;- 0.1% on a<br>validation error rate of ~23%.</li></ul></li></ul></details><h3 id="How-can-optimization-failures-be-debugged-and-mitigated"><a href="#How-can-optimization-failures-be-debugged-and-mitigated" class="headerlink" title="How can optimization failures be debugged and mitigated?"></a>How can optimization failures be debugged and mitigated?</h3><details><summary><em>[Click to expand]</em></summary><br><p><em><strong>Summary:</strong></em> <em>If the model is experiencing optimization difficulties, it’s<br>important to fix them before trying other things. Diagnosing and correcting<br>training failures is an active area of research.</em></p><p align="center"><img src="assets/stride_instability.png" width="80%" alt="Changing the strides in a single residual block in a WideResnet results in training instability."></p><p align="center"><b>Figure 4:</b> Changing the strides in a single residual block (2x2 -> 1x1) in a WideResnet results in training instability. This does not degrade performance at low learning rates, but high learning rates no longer train well due to the instability. Applying 1000 steps of learning rate warmup resolves this particular instance of instability, allowing stable training at max learning rate of .1.</p><h4 id="Identifying-unstable-workloads"><a href="#Identifying-unstable-workloads" class="headerlink" title="Identifying unstable workloads"></a>Identifying unstable workloads</h4><ul><li>Any workload will become unstable if the learning rate is too large.<br>Instability is only an issue when it forces us to use a learning rate that’s<br>too small.</li><li>There are at least two types of training instability worth distinguishing:<ol><li>Instability at initialization&#x2F;early in training.</li><li>Sudden instability in the middle of training.</li></ol></li><li>We can take a systematic approach to identifying stability issues in our<br>workload.<ol><li>Do a learning rate sweep and find the best learning rate lr*.</li><li>Plot training loss curves for learning rates just above lr*.</li><li>If the learning rates &gt; lr* show loss instability (loss goes up not down<br>during periods of training), then it is likely that fixing the<br>instability will result in better training.</li></ol></li><li>Log the L2 norm of the full loss gradient during training, outlier values<br>can result in spurious instability in the middle of training. This can<br>inform how to pick gradient&#x2F;update clipping.</li></ul><p><strong>NOTE:</strong> Some models show very early instability followed by a recovery that<br>results in slow but stable training. <strong>Common evaluation schedules can miss<br>these issues by not evaluating frequently enough!</strong></p><p>To check for this, we can train for an abbreviated run of just ~500 steps using<br><code>lr = 2 * current best</code>, but evaluate every step.</p><p align="center"><img src="assets/more_frequent_evals.png" width="80%" alt="Illustration of the value of more frequent evaluations at the start oftraining."></p><p align="center"><b>Figure 5:</b> Illustration of the value of more frequent evaluations at the start of training. Useful if there’s a suspicion that the model suffers from early training instability.</p><h4 id="Potential-fixes-for-common-instability-patterns"><a href="#Potential-fixes-for-common-instability-patterns" class="headerlink" title="Potential fixes for common instability patterns"></a>Potential fixes for common instability patterns</h4><ul><li>Apply learning rate warmup<ul><li>Best for early training instability.</li></ul></li><li>Apply gradient clipping<ul><li>Good for both early and mid training instability, may fix some bad inits<br>that warmup cannot.</li></ul></li><li>Try a new optimizer<ul><li>Sometimes Adam can handle instabilities that Momentum can’t. This is an<br>active area of research.</li></ul></li><li>We can ensure that we’re using best practices&#x2F;initializations for our model<br>architecture (examples below).<ul><li>Add residual connections and normalization if the model doesn’t contain<br>it already.</li></ul></li><li>Normalization should be the last operation before the residual. E.g. x +<br>Norm(f(x)).</li><li>Norm(x + f(x)) known to cause issues.</li><li>Try initializing residual branches to 0 (e.g.<br><a href="https://arxiv.org/abs/2003.04887">ReZero init</a>).</li><li>Lower the learning rate<ul><li>This is a last resort.</li></ul></li></ul><h4 id="Learning-rate-warmup"><a href="#Learning-rate-warmup" class="headerlink" title="Learning rate warmup"></a>Learning rate warmup</h4><p align="center"><img src="assets/instability_during_warmup.png" width="80%" alt="An example of instability during a warmup period (note the horizontal axis logscale)."></p><p align="center"><b>Figure 6:</b> An example of instability during a warmup period (note the horizontal axis log scale). 40k steps of warmup was needed for successful training in this case.</p><h5 id="When-to-apply-learning-rate-warmup"><a href="#When-to-apply-learning-rate-warmup" class="headerlink" title="When to apply learning rate warmup"></a>When to apply learning rate warmup</h5><p align="center"><img src="assets/axis_model_with_instability.png" width="49%" alt="Axis plot for model with instability"></p><p align="center"><b>Figure 7a:</b> An example of a hyperparameter axis plot for a model exhibiting training instability. The best learning rate is at the edge of what is feasible. An "infeasible" trial is defined as one that either produces NaNs or uncharacteristically high values of the loss.</p><p align="center"><img src="assets/loss_model_with_instability.png" width="49%" alt="Loss curve for model with instability"></p><p align="center"><b>Figure 7b:</b> The training loss of a model trained with a learning rate where we see instability.</p><ul><li>Figure 7a shows a hyperparameter axis plot that indicates a model<br>experiencing optimization instabilities, because the best learning rate is<br>right at the edge of instability.</li><li>Figure 7b shows how this can be double-checked by examining the training<br>loss of a model trained with a learning rate either 5x or 10x larger than<br>this peak. If that plot shows a sudden rise in the loss after a steady<br>decline (e.g. at step ~10k in the figure above), then the model likely<br>suffers from optimization instability.</li></ul><h5 id="How-to-apply-learning-rate-warmup"><a href="#How-to-apply-learning-rate-warmup" class="headerlink" title="How to apply learning rate warmup"></a>How to apply learning rate warmup</h5><p align="center"><img src="assets/beneficial_effect_warmup.png" width="80%" alt="Beneficial effect of warmup on training instabilities"></p><p align="center"><b>Figure 8:</b> Beneficial effect of learning rate warmup on addressing training instabilities.</p><ul><li>Using the section immediately above, we assume that the practitioner has<br>already identified the learning rate at which the model becomes unstable.<br>This is the <code>unstable_base_learning_rate</code>.</li><li>Warmup involves prepending a learning rate schedule that ramps up the<br>learning rate from 0 to some stable <code>base_learning_rate</code>, that is at least<br>one order of magnitude larger than <code>unstable_base_learning_rate</code>. The<br>default would be to try a <code>base_learning_rate</code> that’s 10x<br><code>unstable_base_learning_rate</code>. Although note that it’d be possible to run<br>this entire procedure again for something like 100x<br><code>unstable_base_learning_rate</code>. The specific schedule is:<ul><li>Ramp up from 0 to <code>base_learning_rate</code> over <code>warmup_steps</code>.</li><li>Train at a constant rate for <code>post_warmup_steps</code>.</li></ul></li><li>Our goal is to find the shortest number of <code>warmup_steps</code> that allows us to<br>access peak learning rates that are much higher than<br><code>unstable_base_learning_rate</code>.</li><li>So for each <code>base_learning_rate</code>, we need to tune <code>warmup_steps</code> and<br><code>post_warmup_steps</code>. It’s usually fine to set <code>post_warmup_steps</code> to be<br><code>2*warmup_steps</code>.</li><li>Warmup can be tuned independently of an existing decay schedule.<br><code>warmup_steps</code> should be swept at a few different orders of magnitude. For<br>example, an example study could try [10, 10<sup>3</sup>, 10<sup>4</sup>,<br>10<sup>5</sup>]. The largest feasible point shouldn’t be more than 10% of<br><code>max_train_steps</code>.</li><li>Once a <code>warmup_steps</code> that doesn’t blow up training at <code>base_learning_rate</code><br>has been established, it should be applied to the baseline model.<br>Essentially, we prepend this schedule onto the existing schedule, and use<br>the optimal checkpoint selection discussed above to compare this experiment<br>to the baseline. For example, if we originally had 10,000 <code>max_train_steps</code><br>and did <code>warmup_steps</code> for 1000 steps, the new training procedure should run<br>for 11,000 steps total.</li><li>If long <code>warmup_steps</code> are required for stable training (&gt;5% of<br><code>max_train_steps</code>), <code>max_train_steps</code> may need to be increased to account<br>for this.</li><li>There isn’t really a “typical” value across the full range of workloads.<br>Some models only need 100 steps, while others (particularly transformers)<br>may need 40k+.</li></ul><h4 id="Gradient-clipping"><a href="#Gradient-clipping" class="headerlink" title="Gradient clipping"></a>Gradient clipping</h4><p align="center"><img src="assets/gradient_clipping.png" width="80%" alt="Gradient clipping on early training instabilities"></p><p align="center"><b>Figure 9:</b> Illustration of gradient clipping correcting early training instability.</p><ul><li>Gradient clipping is most useful when large or outlier gradient issues<br>occur.</li><li>Clipping can fix either early training instability (large gradient norm<br>early), or mid training instabilities (sudden gradient spikes mid training).</li><li>Sometimes longer warmup periods can correct instabilities that clipping does<br>not: see <a href="#How-to-apply-learning-rate-warmup">this section above</a>.<ul><li>🤖 What about clipping during warmup?</li></ul></li><li>The ideal clip thresholds are just above the “typical” gradient norm.</li><li>Here’s an example of how gradient clipping could be done:<ul><li>If the norm of the gradient $\left | g \right |$ is greater than the<br>gradient clipping threshold $\lambda$, then do ${g}’&#x3D; \lambda \times \frac{g}{\left | g \right |}$ where ${g}’$ is the new gradient.</li></ul></li><li>Log the unclipped gradient norm during training. By default, generate:<ul><li>A plot of gradient norm vs step</li><li>A histogram of gradient norms aggregated over all steps</li></ul></li><li>Choose a gradient clipping threshold based on the 90th percentile of<br>gradient norms.<ul><li>The threshold will be workload dependent, but 90% is a good starting<br>point. If it doesn’t work, this threshold can be tuned.</li><li>🤖 What about some sort of adaptive strategy?</li></ul></li><li>If we try gradient clipping and the instability issues remain, we can try it<br>harder (i.e. make the threshold smaller).</li><li>Extremely aggressive gradient clipping is in essence a strange way of<br>reducing the learning rate. If we find ourselves using extremely aggressive<br>clipping, we probably should just cut the learning rate instead.</li><li>We would usually consider having &gt;50% of the updates getting clipped somehow<br>as “extremely aggressive”.</li><li>If we need to do extremely aggressive gradient clipping to deal with our<br>instability issues, then we might as well reduce the learning rate.</li></ul></details><h3 id="Why-do-you-call-the-learning-rate-and-other-optimization-parameters-hyperparameters-They-are-not-parameters-of-any-prior-distribution"><a href="#Why-do-you-call-the-learning-rate-and-other-optimization-parameters-hyperparameters-They-are-not-parameters-of-any-prior-distribution" class="headerlink" title="Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution."></a>Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution.</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>It is true that the term “hyperparameter” has a precise<br><a href="https://en.wikipedia.org/wiki/Hyperparameter">meaning</a> in Bayesian machine<br>learning and referring to the learning rate and most of the other parameters<br>we tune in deep learning as “hyperparameters” is an abuse of terminology.</li><li>We would prefer to use the term “metaparameter” for learning rates,<br>architectural parameters, and all the other things we tune in deep learning,<br>since it avoids the potential for confusion that comes from misusing the<br>word “hyperparameter” (confusion that is especially likely when discussing<br>Bayesian optimization where the probabilistic response surface models have<br>their own true hyperparameters).</li><li>Unfortunately, although potentially confusing, the term hyperparameter has become<br>extremely common in the deep learning community.</li><li>Therefore, for a document, such as this one, intended for a wide audience<br>that includes many people who are unlikely to be aware of this technicality,<br>we made the choice to contribute to one source of confusion in the<br>field in hopes of avoiding another.</li><li>That said, we might make a different choice when publishing a research<br>paper, and we would encourage others to use “metaparameter” instead in most<br>contexts.</li></ul></details><h3 id="Why-shouldn’t-the-batch-size-be-tuned-to-directly-improve-validation-set-performance"><a href="#Why-shouldn’t-the-batch-size-be-tuned-to-directly-improve-validation-set-performance" class="headerlink" title="Why shouldn’t the batch size be tuned to directly improve validation set performance?"></a>Why shouldn’t the batch size be tuned to directly improve validation set performance?</h3><details><summary><em>[Click to expand]</em></summary><br><ul><li>Changing the batch size <em>without changing any other details of the training pipeline</em> will often affect the validation set performance.</li><li>However, the difference in validation set performance between two batch sizes typically goes away if the training pipeline is optimized independently for each batch size.</li><li>The hyperparameters that interact most strongly with the batch size, and therefore are most important to tune separately for each batch size, are the optimizer hyperparameters (e.g. learning rate, momentum) and the regularization hyperparameters.<ul><li>Smaller batch sizes introduce more noise into the training algorithm due to sample variance, and this noise can have a regularizing effect. Thus, larger batch sizes can be more prone to overfitting and may require stronger regularization and&#x2F;or additional regularization techniques.</li></ul></li><li>In addition, <a href="#choosing-the-batch-size-to-minimize-training-time">the number of training steps may need to be adjusted</a> when changing the batch size.</li><li>Once all these effects are taken into account, there is currently no convincing evidence that the batch size affects the maximum achievable validation performance (see <a href="https://arxiv.org/abs/1811.03600">Shallue et al. 2018</a>).</li></ul></details><h3 id="What-are-the-update-rules-for-all-the-popular-optimization-algorithms"><a href="#What-are-the-update-rules-for-all-the-popular-optimization-algorithms" class="headerlink" title="What are the update rules for all the popular optimization algorithms?"></a>What are the update rules for all the popular optimization algorithms?</h3><details><summary><em>[Click to expand]</em></summary><br><h4 id="Stochastic-gradient-descent-SGD"><a href="#Stochastic-gradient-descent-SGD" class="headerlink" title="Stochastic gradient descent (SGD)"></a>Stochastic gradient descent (SGD)</h4><p>$$\theta_{t+1} &#x3D; \theta_{t} - \eta_t \nabla \mathcal{l}(\theta_t)$$</p><h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h4><p>$$v_0 &#x3D; 0$$</p><p>$$v_{t+1} &#x3D; \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$</p><p>$$\theta_{t+1} &#x3D; \theta_{t} - \eta_t v_{t+1}$$</p><h4 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h4><p>$$v_0 &#x3D; 0$$</p><p>$$v_{t+1} &#x3D; \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$</p><p>$$\theta_{t+1} &#x3D; \theta_{t} - \eta_t( \gamma v_{t+1} + \nabla \mathcal{l}(\theta_{t})$$</p><h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p>$$v_0 &#x3D; 1 \text{,} m_0 &#x3D; 0$$</p><p>$$v_{t+1} &#x3D; \rho v_{t} + (1 - \rho) \nabla \mathcal{l}(\theta_t)^2$$</p><p>$$m_{t+1} &#x3D; \gamma m_{t} + \frac{\eta_t}{\sqrt{v_{t+1} + \epsilon}}\nabla \mathcal{l}(\theta_t)$$</p><p>$$\theta_{t+1} &#x3D; \theta_{t} - m_{t+1}$$</p><h4 id="ADAM"><a href="#ADAM" class="headerlink" title="ADAM"></a>ADAM</h4><p>$$m_0 &#x3D; 0 \text{,} v_0 &#x3D; 0$$</p><p>$$m_{t+1} &#x3D; \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$</p><p>$$v_{t+1} &#x3D; \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l}(\theta_t)^2$$</p><p>$$b_{t+1} &#x3D; \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$</p><p>$$\theta_{t+1} &#x3D; \theta_{t} - \alpha_t \frac{m_{t+1}}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$</p><h4 id="NADAM"><a href="#NADAM" class="headerlink" title="NADAM"></a>NADAM</h4><p>$$m_0 &#x3D; 0 \text{,} v_0 &#x3D; 0$$</p><p>$$m_{t+1} &#x3D; \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$</p><p>$$v_{t+1} &#x3D; \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l} (\theta_t)^2$$</p><p>$$b_{t+1} &#x3D; \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$</p><p>$$\theta_{t+1} &#x3D; \theta_{t} - \alpha_t \frac{\beta_1 m_{t+1} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$</p></details><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><ul><li>我们要感谢Max Bileschi，Roy Frostig，Zelda Mariet，Stan Bileschi，Mohammad Norouzi，Chris Dubois和Charles Sutton阅读了手稿并提供了宝贵的反馈。</li><li>我们使用了了一些最初由Naman Agarwal生产的其他联合研究机构的实验数据。</li><li>我们要感谢Will Chen在文档的介绍中的宝贵建议。</li><li>We would also like to thank Rohan Anil for useful discussions.</li></ul><h2 id="Citing"><a href="#Citing" class="headerlink" title="Citing"></a>Citing</h2><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs armasm"><span class="hljs-comment">@misc&#123;tuningplaybookgithub,</span><br>  author = &#123;Varun Godbole <span class="hljs-keyword">and</span> George E. Dahl <span class="hljs-keyword">and</span> Justin Gilmer <span class="hljs-keyword">and</span> Christopher J. Shallue <span class="hljs-keyword">and</span> Zachary Nado&#125;,<br>  title = &#123;Deep Learning Tuning Playbook&#125;,<br>  url = &#123;http:<span class="hljs-comment">//github.com/google/tuning_playbook&#125;,</span><br>  year = &#123;<span class="hljs-number">2023</span>&#125;,<br>  note = &#123;Version <span class="hljs-number">1</span>.<span class="hljs-number">0</span>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="Contributing"><a href="#Contributing" class="headerlink" title="Contributing"></a>Contributing</h2><ul><li><p>This is not an officially supported Google product.</p></li><li><p>We’d love to hear your feedback!</p><ul><li>If you like the playbook, please <a href="https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository">leave a star</a>! Or email<br>deep-learning-tuning-playbook [at] googlegroups.com. Testimonials help<br>us justify creating more resources like this.</li><li>If anything seems incorrect, please file an issue to start a discussion.<br>For questions or other messages where an issue isn’t appropriate, please<br>open a new discussion topic on GitHub.</li></ul></li><li><p>As discussed in the preamble, this is a living document. We anticipate<br>making periodic improvements, both small and large. If you’d like to be<br>notified, please watch our repository (see <a href="https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository">instructions</a>).</p></li><li><p>Please don’t file a pull request without first coordinating with the authors<br>via the issue tracking system.</p></li></ul><h3 id="Contributor-License-Agreement"><a href="#Contributor-License-Agreement" class="headerlink" title="Contributor License Agreement"></a>Contributor License Agreement</h3><p>Contributions to this project must be accompanied by a Contributor License<br>Agreement (CLA). You (or your employer) retain the copyright to your<br>contribution; this simply gives us permission to use and redistribute your<br>contributions as part of the project. Head over to<br><a href="https://cla.developers.google.com/">https://cla.developers.google.com/</a> to see your current agreements on file or<br>to sign a new one.</p><p>You generally only need to submit a CLA once, so if you’ve already submitted one<br>(even if it was for a different project), you probably don’t need to do it<br>again.</p><h3 id="Code-Reviews"><a href="#Code-Reviews" class="headerlink" title="Code Reviews"></a>Code Reviews</h3><p>All submissions, including submissions by project members, require review. We<br>use GitHub pull requests for this purpose. Consult<br><a href="https://help.github.com/articles/about-pull-requests/">GitHub Help</a> for more<br>information on using pull requests.</p><h3 id="Community-Guidelines"><a href="#Community-Guidelines" class="headerlink" title="Community Guidelines"></a>Community Guidelines</h3><p>This project follows<br><a href="https://opensource.google/conduct/">Google’s Open Source Community Guidelines</a>.</p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:3" class="footnote-text"><span>Ben Recht and Kevin Jamieson    <a href="http://www.argmin.net/2016/06/20/hypertuning/">pointed out</a> how strong    2X-budget random search is as a baseline (the    <a href="https://jmlr.org/papers/volume18/16-558/16-558.pdf">Hyperband paper</a>    makes similar arguments), but it is certainly possible to find search    spaces and problems where state-of-the-art Bayesian optimization    techniques crush random search that has 2X the budget. However, in our    experience beating 2X-budget random search gets much harder in the    high-parallelism regime since Bayesian optimization has no opportunity to    observe the results of previous trials.<a href="#fnref:3" rev="footnote" class="footnote-backref"> ↩</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>重复的眼泪 就是你自欺的安慰奖</title>
    <link href="/2023/20230120/"/>
    <url>/2023/20230120/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>转载自 :港樂講樂公众号</p><hr><p>歌有歌命。其实每首歌从旋律创作到录音到正式出街面世，无论是多么资深的音乐从业者都没法预料其受欢迎程度，有些歌被选中做主打歌却鲜有人记起，有些原本以为是sidetrack却能成为大热之作。</p><p>词人黄伟文曾说”陈奕迅冇sidetrack”。虽有些微浮夸之嫌，但因为Eason的演出多，他的粉丝基数也十分庞大。在某一个演唱会选一首sidetrack来重绎，加上如今快如闪电的网络传播速度，那么这首sidetrack确实很大机会一夜间便能成为热单。</p><p>按照近几年乐坛的势头，其实这句话可以套用在另一位歌手身上。</p><p>“张敬轩都冇sidetrack”</p><span id="more"></span><p>最近轩公举行了自己的sidetrack演唱会，带领乐迷Revisit了自己较少被人熟知的作品。虽说是sidetrack，但对于歌迷人数逐年递增的张敬轩而言。无论是多么冷门的作品，都必然有其捧场客，必然有人奉为心中挚爱。</p><p>今晚这首《悲剧人物》相信亦是不少澱粉的心头好，但多年来一直鲜有机会听到轩公现场演绎。幸好终于在去年的《The Next 20》，皇上终于将这首被冷落的佳作搬上红馆舞台。</p><p>歌曲出自2007年神专《酷爱》。这是张敬轩继《笑忘书》之后推出的第二张粤语大碟。除了几首为人熟知的热门派台歌外。深受死忠歌迷喜爱的《感情用事》《男孩最痛》《遥吻》《悲剧人物》都是出自该碟。</p><p>林夕和林若宁师徒出马包办了全碟十首歌的九首歌词，其中林老爷写了四首半。题材风格各异。其中便包括当年一砲双响夺下叱吒至尊歌以及叱吒我最喜爱歌曲大奖的专辑点题作《酷爱》。</p><p>《悲剧人物》林夕以第三身作为叙事视角的作品。词中负责表达观点的那个”我”并不是故事里头的主角，而是一位类似朋友的旁观者。这种写法在夕爷的词作中也颇为常见。同样以这种手法写给王菲的《给自己的情书》可算是其代表之作。</p><p>两首歌皆是夕爷为深陷红尘情劫中的世人写下的寄语。不同的是，《给自己的情书》是轻轻抚摸著头发在耳边的柔声细语。而《悲剧人物》则是从发端淋至脚尖的一盆刺骨冰水。</p><p>给听者一个响亮的耳光。</p><p>清醒吧。</p><p>你总以为卑微才是爱，你以为如林黛玉般哭得梨花带雨便是爱的凭证。你以为受伤害便是伟大的表现，你不介意甚至你很乐意让世人看见自己有多痛。你身边的朋友，要时刻迁就你。你的家人，要时刻担心你。只要你到场，黑气便随之散播在每一个人之间。但你却丝毫不自知。</p><p>就如尘世间不喜欢开心。喜欢痛心。</p><blockquote><p>要为错的人伤过恨过 方算是勇敢</p><p>尘世间不喜欢开心喜欢痛心</p><p>喜欢你让我下沉 喜欢你让我哭</p><p>能持续获得糟蹋亦满足</p><p>——《痛爱》-容祖儿</p></blockquote><p>你以为自己还沉浸于他伤害你的伤痛中不能自拔。事实上，你是在享受这份伤痛，伤口早已结痂，你却一遍又一遍将其翻开。你不停用各式各样的方法让自己停留在黑暗之中。你害怕有一天哭不出眼泪。你认为哭够一百零一次才体现出这份爱情的壮烈。</p><blockquote><p>感情受创 要哭够一百零一次</p><p>先有气力 再度脱胎来开始</p><p>——《到底发生过什么事》-dear jane</p></blockquote><p>是的，爱哭的孩子有糖吃。但你是否明白，当他已经不爱你，已经决意离开你。你的眼泪，能感动的，只是你自己。</p><p><strong>「談情是為歡喜<br>無須啞忍受氣<br>只得你 還沉迷淒美<br>你沒法 笑著去別離」</strong></p><p>开篇便是全篇最佳金句。</p><p>谈情，是为了欢喜。不是为了给自己的伤痛史添上浓墨重彩的一笔。假若你在情爱中感受到被伤害，感受到情人不爱你，实则无须继续哑忍下去。当你泪流满面时，请你谨记，永远都有名为“放手”这个选项。</p><p>可惜你却依旧沉迷于这份凄美哀婉，何故无法潇洒笑著说别离。</p><p><strong>「情人若愛不起<br>離去是種福氣<br>得不到 當做看不到<br>卻恨你 享受被拋棄」</strong></p><p>若认清他是你爱不起的那人，今日的离去对未来的自己而言绝对是个天大的福气。早日结束，亦是早日开始。既然得不到的，又何必长念于心。即管随地抛下便逃走。但我却恨你，明明他以离开你，你却十分享受这份被遗弃的感觉。</p><p><strong>「凡是沒法得到<br>才抱著不捨棄<br>不爭氣 有誰憐惜你<br>眼淚會 逼人避開你」</strong></p><p>不是因为他有多好你才多爱他，而是他的离开。让你无法得到他，才让你对其无法捨弃。你一直沉醉于暗无天日的漩涡中。你不愿握紧那双救命的手。试问又叫人如何怜惜你。持续不断的眼泪，只会逼迫身边一个又一个爱你，远离你。</p><p><strong>「你固執 或放任<br>不管是否試過合襯<br>常為了剎那的吸引<br>得不到應得的教訓」</strong></p><p>说你固执又好，放任也罢。你总是为了刹那间的吸引，便交出自己的所有。你从未先细想过对方是否真心待你。仿佛此前情路还未让你记得那应得的教训。</p><p><strong>「纏綿後便慶幸<br>為對方哭過了亦興奮<br>大家都替你氣憤<br>要珍惜你自尊心」</strong></p><p>和他缠绵游戏过后你便感到庆幸，甚至一定要为对方哭过痛过方才可让你感到圆满与兴奋。旁人为你心痛，为你担心，替你气氛，都在劝说你要珍惜自己。好好保护你的自尊心。</p><p><strong>「重複的眼淚<br>就是你自欺的安慰獎<br>你擅長 創造背叛你的失意賣相<br>是你堅拒平凡對象<br>習慣活在遐想」</strong></p><p>一遍又一遍重複著的眼泪就是你每次自欺的安慰奖品，要双眼哭至红肿方可觉得实在。你十分擅长于塑造爱人背叛自己之后那你失魂落魄的样子，是你一直活在遐想之内，却坚拒相信其实自己亦可拥有平淡且快乐的爱情。</p><p><strong>「為膚淺漂亮<br>長期舔著失戀的創傷<br>你擅長 製造你無悔的偉大形象<br>凡是歌裡有悲哀這個字<br>你才願唱」</strong></p><p>明明是爱得肤浅，却被你装饰得漂亮。长期舔着失恋带来的伤口。试问它有如何结痂重生。你只擅长于制造一个无悔于心，一切悲痛独自承受的伟大形象。你这悲剧人物。看来以后唯有悲歌，才可获得你的青睐。</p><p>爱情中收到伤害。固然值得悲哀。也理所当然要痛哭一场。所以词人才会写出如“越美丽的东西我越不可碰”。“被世界遗弃不可怕 喜欢你有时还可怕”这样如此伤心欲绝撕心裂肺的情词。但这是让你在失恋最初的伤痛期，听不进旁人任何劝慰话语之时，陪你边听边落泪的歌词。但痛哭永远是为了重头开始，不是为了沉醉其中。</p><p>林夕曾说过。他是个很乐观的人。很多人看他的词总以为他是个爱得很卑微，十分悲观的人。他在《四面楚歌》中写道“怨恨太多 或快乐声线换到太少耳朵”。大多数能流行的热门情歌。基本上都是失恋惨情歌。似乎快乐的题材永远不及伤痛来得动人。</p><p>但经历过的人应该深有体会，泪水实则并不很凄美，欢乐才应是爱情中最值得铭记的动人时光。</p><blockquote><p>最动人时光 未必地老天荒</p><p>难忘的 因你太念念 才难忘</p><p>容易抱住谁十年 最难是放</p><p>——《罗生门》-麦浚龙</p></blockquote><p><strong>別放大創傷</strong></p><p><strong>別以為用淚水</strong></p><p><strong>為失戀結賬</strong></p><p><strong>才是正常</strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>各式各样的self-attention</title>
    <link href="/2023/20230118/"/>
    <url>/2023/20230118/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>参考：李宏毅老师的<a href="https://www.bilibili.com/video/BV1FG411K7n5">各式各样的Attention</a></p><hr><p>self-attention只是模型的一部分。只有当输入向量维度过大时候，加速、优化self-attention才起作用。</p><span id="more"></span><h2 id="简要介绍"><a href="#简要介绍" class="headerlink" title="简要介绍"></a>简要介绍</h2><p>一个长度为N的句子，会得到N个key和N个query。</p><p>然后两两做dot-product，得到attention matrix。</p><p>然后再用attention matrix对value做weight sum。</p><p>痛点在于计算这个N×N的attention matrix计算量可能会非常惊人。</p><p>所以有一系列方法加速这个计算过程。</p><p><img src="/2023/20230118/sat-1.jpg"></p><h2 id="人为设定优化"><a href="#人为设定优化" class="headerlink" title="人为设定优化"></a>人为设定优化</h2><p>由于大矩阵计算占用大量资源，于是如何矩阵也是一种优化self-attention的方法。</p><h3 id="local-attention-truncated-attention"><a href="#local-attention-truncated-attention" class="headerlink" title="local attention&#x2F;truncated attention"></a>local attention&#x2F;truncated attention</h3><p>在self attention如果我们只看邻居，那我们就能把很远的地方的值设为0。</p><p>缺点：只能看到小范围的数值，跟CNN很像。</p><p><img src="/2023/20230118/local-attention.jpg"></p><h3 id="stride-attention"><a href="#stride-attention" class="headerlink" title="stride attention"></a>stride attention</h3><p>类似于local-attention，但是间隔更大。</p><p><img src="/2023/20230118/stride-attention.jpg"></p><h3 id="global-attention"><a href="#global-attention" class="headerlink" title="global attention"></a>global attention</h3><p>以上都是以某个位置为中心看左右的事情，如果我们关心整个sequence，那么我们可以用global attention。我们可以加入一个特殊token到原始的sequence里面。在这里，global attention会做两件事情：</p><p>（1）每个特殊的token都加入每一个token，收集全局信息。<br>（2）每个特殊的token都被其他所有的token加入，以用来获取全局信息。</p><p><img src="/2023/20230118/global-attention.jpg"></p><h3 id="混合"><a href="#混合" class="headerlink" title="混合"></a>混合</h3><p>上面提到了三种选择，我们选择哪一种呢？小孩子才做选择，我们都选择。</p><p><img src="/2023/20230118/mix.jpg"></p><h2 id=""><a href="#" class="headerlink" title=""></a></h2><h2 id="data-driven方式"><a href="#data-driven方式" class="headerlink" title="data-driven方式"></a>data-driven方式</h2><h3 id="将较小值取0"><a href="#将较小值取0" class="headerlink" title="将较小值取0"></a>将较小值取0</h3><p><img src="/2023/20230118/zero.jpg"></p><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>Reformer和routing transformer用了类似的方法。</p><p>第一步先把query和key聚类。</p><p>聚类的计算可以用近似（approximate）但快速（fast）的方式，从而加快计算。Reformer和routing transformer就采用了不同的clustering方式来加速计算。</p><p>聚类完后，我们只计算归为同一个类别的query和key之间的attention score，其他位置的attention score直接设置为0。</p><p><img src="/2023/20230118/cluster.jpg"></p><p><img src="/2023/20230118/cluster2.jpg"></p><h2 id="自动设定优化"><a href="#自动设定优化" class="headerlink" title="自动设定优化"></a>自动设定优化</h2><p>我们能不能学习attention matrix哪块需要，哪块不需要吗？或者说能不能学习attention weight呢？</p><h3 id="Sinkhorn-Sorting-Network"><a href="#Sinkhorn-Sorting-Network" class="headerlink" title="Sinkhorn Sorting Network"></a>Sinkhorn Sorting Network</h3><p><img src="/2023/20230118/learnable.jpg"></p><p>Sinkhorn Sorting Network做法是learn另外一个network来决定这个1-0矩阵。</p><p>新的network会input一个sequence，sequence中每个位置的vector都输出一个跟sequence长度一致的vector。Sequence中每个位置都产生一个vector，拼起来就产生一个N×N的矩阵。然后将这个矩阵通过某种计算变成一个1-0矩阵，并且保证这个计算过程可以微分。</p><p>产生1-0矩阵的计算和整个self-attention是jointly learned的。</p><p>当然也可以选择top-k方法，而不是top-1。</p><p>疑问：用了一个NN，真的比不用NN直接计算所有矩阵运算量小吗？</p><p>答：不一定的。所以Sinkhorn Sorting Network会将输入的向量划分为几个部分，各个部分共用一个经过NN产生的向量。</p><h3 id="linformer"><a href="#linformer" class="headerlink" title="linformer"></a>linformer</h3><p>attention matrix有很多redundant columns ，研究者计算attention matrix的rank发现是低rank的（低秩的）。</p><p>那我们能不能去掉重复，产生小的attention matrix，加快attention的速度呢？</p><p><img src="/2023/20230118/linformer1.jpg"></p><h4 id="具体做法"><a href="#具体做法" class="headerlink" title="具体做法"></a>具体做法</h4><p>挑出K个有代表性的key（黄色vectors），然后计算出N×K的matrix，然后再挑出K给有代表性的value（蓝色vectors）。然后把K个key（黄色vectors）对第一个query算出来的attention weight（红色框）对这个K个value（蓝色vectors）做weighted sum得到self-attention的output（绿色vector）。</p><p>疑问：但是，为什么我们选择有代表性的keys而不选有代表性的queries？</p><p>答：因为如果query减少了，output sequence length也会变小。这种做法对于sequence中每个位置都需要output的情况是不适用的。</p><h4 id="如何挑选代表性的keys"><a href="#如何挑选代表性的keys" class="headerlink" title="如何挑选代表性的keys"></a>如何挑选代表性的keys</h4><p><img src="/2023/20230118/reduce-key.jpg"></p><p>Compressed Attention是用CNN扫过整个句子，得到较短的output当作有代表性的keys。</p><p>Linformer是直接乘上N×K矩阵，做线性变化。</p><h2 id="加速self-attention计算"><a href="#加速self-attention计算" class="headerlink" title="加速self-attention计算"></a>加速self-attention计算</h2><h3 id="self-attention-运算过程"><a href="#self-attention-运算过程" class="headerlink" title="self-attention 运算过程"></a>self-attention 运算过程</h3><p><img src="/2023/20230118/acc_sat_cal_0.jpg"></p><p>我们暂时忽略softmax后，输出O可以当成是三个矩阵相乘。</p><h3 id="简单的加速"><a href="#简单的加速" class="headerlink" title="简单的加速"></a>简单的加速</h3><p><img src="/2023/20230118/acc_sat_cal_2.jpg"></p><p>将上面的运算过程变成下面的运算过程。</p><p>上面的运算过程乘法次数为$（d+d’）N^2$</p><p>下面的运算过程乘法次数为$2d’dN$</p><p>我们再把softmax放回。</p><p><img src="/2023/20230118/acc_sat_cal_3.jpg"></p><p>上图为原先的。</p><p> <img src="/2023/20230118/acc_sat_cal_4.jpg"></p><p>上图为变换顺序后的。</p><p>其中$exp(\mathbf{q}·\mathbf{k})\approx\phi(\mathbf{q})·\phi(\mathbf{k})$</p><p>具体数学推导见李宏毅原视频。</p><h4 id="phi-的确定"><a href="#phi-的确定" class="headerlink" title="$\phi$的确定"></a>$\phi$的确定</h4><p> <img src="/2023/20230118/phi.jpg"></p><h2 id="Synthesizer"><a href="#Synthesizer" class="headerlink" title="Synthesizer"></a>Synthesizer</h2><p>不用q和v，直接学attention matrix</p><p><img src="/2023/20230118/synthesizer.jpg"></p><p>把它当成网络里的参数</p><h2 id="丢掉self-attention"><a href="#丢掉self-attention" class="headerlink" title="丢掉self-attention"></a>丢掉self-attention</h2><p><img src="/2023/20230118/att_free.jpg"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LRA score表示方法越好，圈圈的大小代表需要用的memory大小</p><p><img src="/2023/20230118/summary.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大型语言模型可以成为MNIST的few-shot分类器吗</title>
    <link href="/2023/20230116/"/>
    <url>/2023/20230116/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>具体代码见：<a href="https://colab.research.google.com/drive/1I4RCsJ6N1pONdKHvx6KD-whJAz_z6Voj?usp=sharing#scrollTo=qdAMTEAwR3en">colab</a></p><p>众所周知，像GPT-3等大学语言模型善于few-shot learning。</p><p>那能不能在图像上发挥作用呢？</p><p>研究者将图像翻译成ASCII来让语言模型理解图像。</p><span id="more"></span><p><img src="/2023/20230116/fungpt.jpg"></p><p>prompt模板：</p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs avrasm"><span class="hljs-symbol">Input:</span> [flattened_ascii_image]  <br><span class="hljs-symbol">Output:</span> [class_label]  <br><span class="hljs-meta">###</span><br><span class="hljs-symbol">Input:</span> [flattened_ascii_image]  <br><span class="hljs-symbol">Output:</span> [class_label]  <br><span class="hljs-meta">###</span><br>...<br><span class="hljs-symbol">Input:</span> [flattened_ascii_image]  <br><span class="hljs-symbol">Output:</span><br></code></pre></td></tr></table></figure><p>MNIST的总体准确率达到了20&#x2F;30≈66%，勉勉强强，而且样本也较少。</p><p>研究者还在CIFAR-10进行了测试，准确率只有50%，比瞎猜还是好一点的。</p><hr><p>这个想法是挺有趣的，就是缺乏大量和广泛的样本测试。</p><p>openai是用了大量的语言数据进行测试，但是我并不觉得ASCII字谜在openai的训练数据内。</p><p>分类效果尽管偏低，但是还是让人眼前一亮的。这种few-shot learning让我想起了最近流行的chatgpt的扫雷prompt。</p><p>CV和NLP是否能大一统？</p><p>但是，也正如研究者所说：<strong>But the world is not just all natural language!</strong> </p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算神经科学能否成为未来人工智能的发展方向？</title>
    <link href="/2023/20230101/"/>
    <url>/2023/20230101/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我个人认为计算神经科学对AI的帮助是微乎其微的，过去没有，未来十年也难以有，就算现在AI可能正面对瓶颈期，我也并不觉得计算神经科学是有效的破瓶器。大二上学院开设有一门神经科学，在相对悲观的态度下，我也没有选这门课。**以下来自知乎用户周鹏程的<a href="https://www.zhihu.com/question/304574109/answer/2728729351">回答</a>，**我觉得值得一看。</p><hr><p>刚刚过去的一个周末，在twitter上的神经科学圈发酵了一起不大不小的争论，引得领域内好几个著名学者，包括Yann Lecun的参与。 最初争论的是神经科学是否推动了人工智能，后来就更多变成了未来的人工智能是否需要神经科学。</p><p>吃完瓜后，感觉里边还是有一些很不错的观点，在此小记复盘一下。其中的翻译并非按照原文逐词翻译，只是换成我个人语言理解而已。抛砖引玉，欢迎大家有更多的观点。中国在类脑智能领域的投入也在增加，“<strong>该不该类脑</strong>”以及“<strong>如何类脑</strong>”这样的问题都值得在广泛范围内讨论。<span id="more"></span></p><h2 id="争论的起点"><a href="#争论的起点" class="headerlink" title="争论的起点"></a>争论的起点</h2><p>10月15号时候，神经科学领域和人工智能领域一群大佬，如Terry Sejnowski, Yoshua Bengio， Yann LeCun，, Eero Simoncelli, James DiCarlo, Alex Pouget 以及今天争论的主角Konrad Kording， 在arXiv上发表了一篇白皮书文章</p><p><a href="https://arxiv.org/abs/2210.08340">Toward Next-Generation Artificial Intelligence: Catalyzing the NeuroAI Revolutionarxiv.org&#x2F;abs&#x2F;2210.08340</a></p><p>文章的观点非常简单，摘要只有两句话：<strong>Neuroscience has long been an important driver of progress in artificial intelligence (AI). We propose that to accelerate progress in AI, we must invest in fundamental research in NeuroAI.</strong> </p><p>概括起来就是：神经科学+人工智能非常有前途，政府请打钱。</p><p>中文的公众号文章可以参考 @AI科技评论的<a href="https://zhuanlan.zhihu.com/p/575911093">这篇</a></p><p>。一般这种事情都是大佬们利用自己的声音，对领域的发展提出一个方向，向政府建言争取更多资源支持，对自己江湖地位和声望有好处，热度炒起来了，相关从业的小虾米也开心。</p><p>所以文章发表之后，作者之一，宾夕法尼亚大学教授的Konrad Kording就开开心心的发了条tweet推广这篇文章，并呼吁了 一些东西。前两天还相安无事儿，大家其乐融融。</p><p>没想到两天后，可能是周末比较清闲，来自DeepMind的**<a href="https://link.zhihu.com/?target=http://davidpfau.com/">David Pfau</a>**对着Kording的这篇tweet开喷了：</p><blockquote><p>神经科学从来都没推动过人工智能，你们白皮书中还说continue to drive AI progress </p><p>你们真的认为发明Transformers&#x2F; ADAM的人看过一篇神经科学论文吗？你们就假装在为人工智能做贡献吧。要点脸吧 “<strong>it’s embarrasing</strong>“(原文）</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-5d124cad31733700d744a8ec07fcb1d1_720w.webp?source=1940ef5c" alt="img"></p><p>这样的回复立马就炸雷了，引起了后面很多人的“参战”。</p><p>这里简单提一下这位Pfau，他其实是正儿八经的神经科学博士，毕业于<strong>哥伦比亚大学的神经生物学专业</strong>，附属于<a href="https://link.zhihu.com/?target=https://ctn.zuckermaninstitute.columbia.edu/">Center for Theoretical Neuroscience</a> （CTN）。跟我（博后期间）同一个导师，只不过我们时间上没有交集，并没有任何交流。</p><p>说这个背景，主要是让大家不要觉得Pfau是不懂任何神经科学，盲目自大，他是受过完整神经科学训练的。并且在CTN里边有Larry Abbott和Ken Miller等计算神经科学大佬，毕业生中走出了很多在人工智能领域的佼佼者，如David Sussillo (下文会出现）。Pfau对于这神经科学和人工智能两个领域都不陌生。  </p><h2 id="诸神参战"><a href="#诸神参战" class="headerlink" title="诸神参战"></a>诸神参战</h2><h3 id="David-Sussillo"><a href="#David-Sussillo" class="headerlink" title="David Sussillo"></a>David Sussillo</h3><p>Pfau的评论一处，上文我们所提到的David Sussillo就出来说话了</p><blockquote><p>过去几年，我在Google Brain跟Transformer的主要贡献人交往很多。我虽然不能冒昧地推定到底是什么启发了他发明transformer，但是他对神经科学是发自内心的感兴趣，问了很多神经科学的问题。  </p><p>格局打开，深度学习&#x2F;人工智能的科学原理早晚都需要被解决，我押宝计算&#x2F;理论&#x2F;神经科学领域的人。</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-b380205c6474ec2e695955ca80dbf1b5_720w.webp?source=1940ef5c" alt="img"></p><p>对此， Pfau也直接回复</p><blockquote><p>人工智能的人对神经科学感兴趣没问题，这和神经科学推动AI进展是两码子事儿。</p><p>Sussillo你自己在运动控制方面的神经科学工作非常有影响力，你能举出来任何这些工作在机器人领域的应用吗？</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-3ec0d00c45cccf4bfed872cbbf7d51e9_720w.webp?source=1940ef5c" alt="img"></p><p>对此，Sussillo倒是也坦诚</p><blockquote><p>没，我在运动控制领域的工作在AI中没啥影响。我曾经试图将动力学系统方面的思路引入到深度学习的理解中，但是没啥大成果。这个领域不太鼓励进展缓慢的增量式工作。</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-efab7a04f010371497c09e0a73c406bf_720w.webp?source=1940ef5c" alt="img"></p><p>Kording一看Sussillo这么坦诚，被Pfau将了一军，赶紧出来发言助攻。</p><blockquote><p><a href="https://link.zhihu.com/?target=https://homes.cs.washington.edu/~todorov/index.php">Emo Todorov</a>在运动控制方面的工作在人工智能领域的应用比比皆是。</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-34f1dff85e438037b689375bf9ad2cf0_720w.webp?source=1940ef5c" alt="img"></p><p>Pfau对此也毫不客气</p><blockquote><p>得了吧，Todorov的主要身份就不是神经科学家。他是做过一些神经科学的工作，但是我认为人工智能领域所认可他的工作主要来自控制理论。</p></blockquote><p>Kording也立马回复</p><blockquote><p>他的控制理论一开始就是为了解释神经科学的运动行为的。</p></blockquote><p><strong>至此，我们可以看出来分歧已经出现在：到底什么才能被定义为神经科学？为了解释神经系统功能所用到的控制理论、数学、物理、统计、计算机手段是否还能被认为是神经科学</strong>？我的个人观点在后边说，大家继续看戏。</p><p>Sussillo这时候估计也是觉得这样辩论就没意思了，赶紧总结了自己的观点，给双方台阶下，准备退出</p><blockquote><p>算了，如果你就是不爽Kording所说的“神经科学继续推动人工智能”的话，那就算是吧。</p><p>过去的十年，深度学习领域到处都在跑马圈地，偶尔有一系列真正令人惊叹的想法出现。我认为神经科学在这个过程中没有贡献。 我承认这一点，并且同意目前神经科学对人工智能的贡献远不如后者对前者的大。  </p><p>但我不爽的是，你说“神经科学家宣称对人工智能有贡献”就是embarrassing的。你说的太过了。</p><p> 人工智能的历史吸纳了很多不同领域的人到一起。就拿NIPS为例，神经科学也依然在这个会议中有一席之地。 </p></blockquote><p><img src="https://picx.zhimg.com/80/v2-6b47718e9098753778458ba4a229a5c2_720w.webp?source=1940ef5c" alt="img"></p><p>这里我简单插入一点内容：近些年在中文互联网上， NIPS的文章讨论已经几乎全部是计算机领域的了，但实际上它是有一部分留给神经科学的。有趣的是，我在wiki上看NIPS的介绍，说的还是NIPS is a <strong>machine learning and computational neuroscience</strong> conference, 以及说NIPS was designed as a meeting for researchers <strong>exploring biological and artificial neural networks</strong>，但是NIPS的官网的mission statement说的是foster the exchange of research advances in <strong>Artificial Intelligence and Machine Learning。</strong></p><p>这可真是从“小甜甜”到“牛夫人”的转变啊。NIPS都如此，这也难怪现在人工智能领域的新生代不屑于了解神经科学。而在上个世纪，最早那一批做人工智能的对脑智能还是非常感兴趣的。</p><p><img src="https://pica.zhimg.com/80/v2-bda2c9a4f93f250fa667bb4ff51f4dd5_720w.webp?source=1940ef5c" alt="img"></p><p>最后Sussillo给Pfau推荐了几篇做人工智能可以看看的神经科学文章，然后退出了争论。</p><blockquote><p><a href="https://link.zhihu.com/?target=https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1.abstract">https://www.biorxiv.org/content/10.1101/2022.08.15.503870v1.abstract</a> </p><p><a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2004.08013">How recurrent networks implement contextual processing in sentiment analysis</a> </p><p><a href="https://link.zhihu.com/?target=https://proceedings.neurips.cc/paper/2021/hash/a57ecd54d4df7d999bd9c5e3b973ec75-Abstract.html">Reverse engineering learned optimizers reveals known and novel mechanisms</a></p></blockquote><p><img src="https://picx.zhimg.com/80/v2-ab68c5e85b8a531b4e46f75cdda3fa8b_720w.webp?source=1940ef5c" alt="img"></p><h3 id="Ken-Miller"><a href="#Ken-Miller" class="headerlink" title="Ken Miller"></a>Ken Miller</h3><p><a href="https://link.zhihu.com/?target=http://www.columbia.edu/cu/neurotheory/Ken/">Ken Miller</a>是Pfau所毕业的哥大理论神经科学中心的大佬，他肯定是认识Pfau。Ken是一个非常受人尊敬的理论神经科学家，致力于研究大脑皮层环路的计算规则，是研究脑智能的一座山，现在突然听到Pfau这番言论，肯定是很不爽的。于是连发了三条评论就离开了，而Pfau也没有直接回复Ken，不知道是否是出于一种尊重，或者是他觉得Ken这种说法太泛了。 </p><blockquote><p>Pfau你小子，真觉得一个从没听说过神经元和突触相关概念的人会想到使用人工神经网络来实现人工智能吗？</p><p>你忘了1990-2012年的神经网络寒冬了吗？  一帮受脑智能启发的头铁的牛人给神经网络续了命，虽然那段时间整个人工智能&#x2F;机器学习领域都抛弃了他们。</p><p>人类擅长抽象推理模式，但这是不会带领我们想到通过神经网络&#x2F;卷积网络来实现智能的。而那帮头铁的老家伙们就是坚信“大脑可以做到，那人工神经网络也行”，靠着这个信念坚持走过了人工神经网络的黑暗时期。  </p><p>现在你居然在问“神经科学为人工智能做了啥”？真拿神经科学是牛夫人了？ </p><p>上一次神经科学对人工智能的贡献在于认识到大脑是通过层级间的神经元和突触连接实现信息处理这一抽象的概念，<strong>我认为下一波重大贡献将来自于我们真正理解大脑如何完成计算的</strong>。目前我们还远没到这个地步。</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-d0ffdaf3d10960424331b05dc12519b9_720w.webp?source=1940ef5c" alt="img"></p><h3 id="Yann-Lecun"><a href="#Yann-Lecun" class="headerlink" title="Yann Lecun"></a>Yann Lecun</h3><p>大佬出马了，直接就一句”You are wrong”甩到Pfau老兄脸上了</p><blockquote><p>你错了 。神经科学极大并且直接启发了我和Hinton </p><p>另外神经网络通过调节突触权重来实现学习这一整套想法确定无疑来自神经科学。</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-792b95924eda07437d9b3fc379a3b1e9_720w.webp?source=1940ef5c" alt="img"></p><p>当然了，遇到和大佬对线的机会，Pfau也不会轻易放弃，赶紧说</p><blockquote><p>从一些经典工作获得一些概念上的启发和直接从最近研究中获得启发是不同的。你有从Neuron或者Cell杂志上的最新文章受到任何启发，并作出新的工作吗？如果有的话，我没注意到你文章中有这些体现。</p></blockquote><p>这里我觉得Pfau有点太偏执了，神经科学的第一任务是理解大脑的工作原理，而不是为了整天为人工智能提供新的启发。你这对比就是拿classic和latest research对比。LeCun估计也是觉得这样说就没意思了，就直接不回复了。</p><p>此外，Pfau又补了一句</p><blockquote><p>Mike Jordan说当年的PDP group都非常讨厌backprop，因为它在生物学上不现实。</p><p>直到你们这波人不管生物学意义后，才变得流行了。大佬，你当年因为一些神经学家过分追求生物合理性而被刁难的日子，你都忘了吗？  </p></blockquote><p><img src="https://picx.zhimg.com/80/v2-339baf63e596dfae634c4a58d755eb60_720w.webp?source=1940ef5c" alt="img"></p><h3 id="Tony-Zador"><a href="#Tony-Zador" class="headerlink" title="Tony Zador"></a>Tony Zador</h3><p><a href="https://link.zhihu.com/?target=https://zadorlab.labsites.cshl.edu/">Tony Zador</a> 是冷泉港实验室的一个大佬，他也是前文提到的白皮书作者之一。他看到Pfau对LeCun的回复后，就开喷了</p><blockquote><p>你丫这么说就类似于问一个物理学家是否看过哪些最近发表的数学论文，并启发自己完成一些新的工作？  </p><p>“嘿，费曼，你最近看过Acta Mathematica上的论文吗，有没有直接推动你工作的？如果没有的话，那物理学家不需要学数学了”</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-3890ac0de209560bbcd624815bdb1ea4_720w.webp?source=1940ef5c" alt="img"></p><p>Pfau也挺善战，立马就说</p><blockquote><p>你要说费曼，我就不困了。在《别逗了，费曼先生》一书中，他说他觉得纯数太繁琐并且不相关。他有问题就直接去找数学家问了</p></blockquote><p>不过我觉得这回答的是个啥？Zador也作出了回应（如下，我就不翻译了）。</p><p><img src="https://picx.zhimg.com/80/v2-3f148b8bb71b54e08709294f30de56d2_720w.webp?source=1940ef5c" alt="img"></p><p>在另外一条线中，Zador也参与了一点争论。起因是有人说Hinton在报告中提到Dropout的发现是因为他注意到神经元的活动是随机的。我查了Hinton的那篇文章，没有提任何neuroscience的事情，也没有引用。对文章撰写来说，而也是很正常的，文章最后的组织和表述不用完全反映真实的心路历程。</p><p><img src="https://picx.zhimg.com/80/v2-c20eb44c4590575022bf0471bc94e4bb_720w.webp?source=1940ef5c" alt="img"></p><p>然后Pfau估计那天也是杀疯了，上来就说</p><blockquote><p>我怀疑Hinton从来没有看过任何一篇神经科学的论文。被一些high-level的直觉启发，跟被实际的神经科学研究启发是两回事儿。</p></blockquote><p>于是Zador就说</p><blockquote><p>你是不是指望神经科学文章中的某个图片结论直接被应用到某些 人工智能算法中？如果这是你期待的话，那肯定不是这样的。  </p><p>事实上，我们从神经科学获得一些启发，然后把那些重要的想法移植到AI中并进而改善它。</p></blockquote><p>Pfau也直接回复</p><blockquote><p>实际上你说的这种形式也不常见。有些人从神经科学中寻找启发，其他人完全无视神经科学也没啥，依然可以作出领域内的重大贡献。</p></blockquote><p><img src="https://pic1.zhimg.com/80/v2-14d255df216629499353f836ddc8fd75_720w.webp?source=1940ef5c" alt="img"></p><p>这里我是不认同Pfau关于Hinton不看神经科学论文这一说法的。</p><p>**<a href="https://link.zhihu.com/?target=https://en.wikipedia.org/wiki/Terry_Sejnowski">Terry Sejnowski</a>*<em>在</em>The Deep Learning Revolution*一书中花了很多篇幅写自己和Hinton的关系。人工智能领域的新生代可能不了解Terry，他是美国的National Academy of Sciences， American Academy of Arts and Sciences， American Association Advancement of Science, National Academy of Engineering, National Academy of Medicine, National Academy of Inventors 多院的院士（学术称号buff加满的存在），计算神经科学领域的先驱，高山仰止那种角色。</p><p>他和Hinton早期有很多关于神经网络方面的合作。后来两个人的研究方向分别走向了两个领域：一个注重利用神经网络实现人工智能，另外一个专注于理解大脑工作原理。我相信两位大佬是希望这两个领域是可以互相促进的。在Terry的书中，他写了这么一段话</p><blockquote><p>Every few years, I get a call from Geoffrey that begins with <strong>“I figured out how the brain works.”</strong> Each  time, he tells me about a clever new scheme for improving neural network models. It has taken many such schemes and refinements for deep learning in multilayered neural networks to achieve a  level of performance comparable to humans in recognizing speech on cell phones and objects in photos. </p></blockquote><p>从这里我们看出，Hinton每次对他网络的修改，总是想和大脑对应上，寻求二者原理上的共性。所以我觉得这里Pfau是太武断了。 </p><h3 id="Gary-Marcus"><a href="#Gary-Marcus" class="headerlink" title="Gary Marcus"></a>Gary Marcus</h3><p><a href="https://link.zhihu.com/?target=http://garymarcus.com/">Gary Marcus</a> 曾经是NYU心理系最年轻的荣誉退休教授。他对Pfau所说的神经学家过度关心生物合理性也回应了一下</p><blockquote><p>是的，对神经网络生物合理性的要求过度了。但我同意LeCun，Pfau这小子太口无遮拦了，缺乏对历史背景的了解。</p></blockquote><p><img src="https://pica.zhimg.com/80/v2-cdb4451d16cbb0661cbebaca996bdad5_720w.webp?source=1940ef5c" alt="img"></p><p>Pfau老哥怎能服气，立马就杠上了</p><blockquote><p>你瞎说，我当然知道历史背景。我只是认为几个教科书中关于抽象的启发完全不等于是主要推动力。 </p></blockquote><p>Marcus也很老道，不跟他硬杠</p><blockquote><p>好吧，你这么说也没毛病。那咱们就说“60年代有一些是有实际贡献，1970年以后就没啥大的推动了”</p></blockquote><p><img src="https://picx.zhimg.com/80/v2-6ff265c037fec9efa88a066e74dffe53_720w.webp?source=1940ef5c" alt="img"></p><p>最后这个Markus还是非常善意的提供了一些神经科学中可供AI领域参考的概念，然后消失了，深藏功与名。</p><p><img src="https://picx.zhimg.com/80/v2-4ac975d4f03559cf36d51737ca9ef3e1_720w.webp?source=1940ef5c" alt="img"></p><h3 id="Eran-Mukamel"><a href="#Eran-Mukamel" class="headerlink" title="Eran Mukamel"></a>Eran Mukamel</h3><p>**<a href="https://link.zhihu.com/?target=https://brainome.ucsd.edu/">Eran Mukamel</a>**是UCSD认知神经科学系的一个教授，湿实验比较多。我知道他是因为我之前一些工作经常会拿他的一个自动分析算法对比。Mukamel和Pfau俩家简单斗了一下嘴，没啥特别有趣的争论。大家自己看截图吧。</p><p><img src="https://picx.zhimg.com/80/v2-cff2f66e789cda3d7bd0c1ad3c3a05ae_720w.webp?source=1940ef5c" alt="img"></p><h2 id="鸣金收兵"><a href="#鸣金收兵" class="headerlink" title="鸣金收兵"></a>鸣金收兵</h2><p>后来双方都很累了，Pfau也没想到自己直接点燃了一个火药桶，一个周末就这么荒废了，全程与各位对战，也承认自己处境很尴尬，不过他是坚决不认输。</p><p><img src="https://picx.zhimg.com/80/v2-fbeed4639e784b970109a74710cfa2bb_720w.webp?source=1940ef5c" alt="img"></p><p>争论中的另一个主角也不开心地发了个佛系的tweet。 </p><p><img src="https://picx.zhimg.com/80/v2-1cb9093659ab33dddb8c969dcd558cba_720w.webp?source=1940ef5c" alt="img"></p><h2 id="个人观点"><a href="#个人观点" class="headerlink" title="个人观点"></a>个人观点</h2><p>我是战火结束了才跑去吃瓜的，吃得还是挺开心。我很欣赏David Pfau这样的人抛出来这个话题，并且面对各个大佬都从容应对，虽然我不是很认同他的某些说法。但是我觉得Nicole Rust下边的这条评论基本表达了我的感受。</p><p><img src="https://pica.zhimg.com/80/v2-787de9dc27420dc7cad1e96da447a4a5_720w.webp?source=1940ef5c" alt="img"></p><p>这条twitter下的争论还有很多，我原本想着一会儿工作就整理完了，结果我严重低估了工作量。剩下的我就暂时先不整理了。如果感兴趣的话，可以去围观另外一条主线，来自Harvard的教授Sam Gershman也开辟了一个战场，下面依然有Tony Zador大神的火力输出。<a href="https://link.zhihu.com/?target=https://twitter.com/gershbrain/status/1583785652516098049">https://twitter.com/gershbrain/status/1583785652516098049</a></p><p><img src="https://picx.zhimg.com/80/v2-2399422a37eabfa343871d2a90948d8c_720w.webp?source=1940ef5c" alt="img"></p><p>另外大家不要觉得都是喷Pfau的，其实也有很多人是支持Pfau，并且认为“根本不用关心neuroscience进展的”，以及“neuroscience在这一波deep learning浪潮中啥也没干”，对此我觉得也没啥不妥。</p><p>早在2015年时候，就有一个知乎讨论，有个回答也是说“不觉得做人工智能一定要先学点生物”，对此我是认同的。神经科学不需要所有人都学习。</p><p><a href="https://www.zhihu.com/question/27716888/answer/37809407">a722 赞同 · 126 评论回答</a></p><p>最后发表一下我在吃瓜过程中的一些看法，欢迎讨论。 </p><h3 id="人工智能有很多途径，NeuroAI只是一个选择"><a href="#人工智能有很多途径，NeuroAI只是一个选择" class="headerlink" title="人工智能有很多途径，NeuroAI只是一个选择"></a>人工智能有很多途径，NeuroAI只是一个选择</h3><p>我觉得这里没必要厚此薄彼。有人觉得神经科学 -&gt;人工智能是一条不错的道路，有人觉得完全不需要神经科学也一样可以做的很好。那没问题，各走各的道，谁也别想着干涉谁。神经科学对于过去十年的所谓的“人工智能”发展没啥大的贡献，这是事实；如果有，那也是早年的先驱概念性启发，跟当代的神经科学没啥大的关系，没必要去邀功。 </p><p>但是你要说未来是否有贡献，那我觉得是会的。有人觉得不会，那也没关系，就像1990-2012年间那样，大部分人觉得人工神经网络不值得研究一样。 </p><p>另外Pfau这种“神经科学对人工智能没有贡献的说法”，我觉得有一个很大的问题：**谁来定义人工智能？**有人从工程角度研究，有人从脑智能启发。后者目前慢，但是就直接否定了其存在，从人工智能领域踢出去是否合适？我们是否换个角度看，研究脑科学就是研究人工智能？</p><p>总结起来就是，做神经科学的不要随意邀功，免得一种“我祖上阔过”的印象；做非神经相关的AI研究者，也不要随意贬低其它领域的研究。</p><h3 id="属于脑科学的“空气动力学”还未到来"><a href="#属于脑科学的“空气动力学”还未到来" class="headerlink" title="属于脑科学的“空气动力学”还未到来"></a>属于脑科学的“空气动力学”还未到来</h3><p>大家经常会拿鸟和飞机来类比脑科学和人工智能。白皮书中也对此有一个回应，主要是强调这个类比不合理。造飞机的目的不是为了像鸟，但是人工智能的一个目的是为了像人（此处有争议，有些觉得人工智能不需要像人，但是要超越人类的水平）。</p><p>我个人觉得除了白皮书中所说的，还有一条就是属于脑科学的空气动力学还未到来。人们从研究鸟发现了空气动力学，然后就可以抛开鸟类，专注空气动力学就可以造出来更大更快的飞机。但是对于脑智能来说，我们还没有发现可以解释其计算原理的理论，在类脑智能领域尚没有可以脱离大脑可以独立发展的“空气动力学”。所以我觉得Ken Miller的评论是中肯的。 </p><p><img src="https://pic1.zhimg.com/80/v2-749423efe5ca3ec71696fb2f82a6996d_720w.webp?source=1940ef5c" alt="img"></p><p>译：即使是一些意识到神经科学在塑造这一领域的历史重要性的研究人员，也常常认为它已经失去了相关性。“工程师们研究鸟类不是为了制造更好的飞机，这是人们经常说的话。但这种类比并不成立，部分原因是航空先驱确实研究过鸟类(Lilienthal 1911;Culick 2001)，有些人仍然这样做(Shyy et al. 2008;Akos et al. 2010)。此外，这种类比在更基本的层面上也不成立:现代航空工程的目标不是实现“鸟级”飞行，而人工智能的一个主要目标确实是达到(或超过)“人类级”智能。正如计算机在许多方面超过了人类，比如计算质数的能力，飞机在速度、航程和载货量等特性上也超过了鸟类。但如果航空工程师的目标确实是制造一种机器，这种机器具有“鸟的水平能力，可以飞越茂密的森林树叶，轻轻地降落在树枝上”。他们最好密切关注鸟类是如何做到这一点的。同样，如果人工智能的目标是实现动物水平的常识感觉运动智能，研究人员最好向动物学习，以及它们为在不可预测的世界中表现而进化出的解决方案。</p><h3 id="神经科学如何界定？"><a href="#神经科学如何界定？" class="headerlink" title="神经科学如何界定？"></a>神经科学如何界定？</h3><p>Pfau在争论中有个观点其实是具有迷惑性的。他只承认实验得到的神经科学结论，而拒绝认为来自控制理论、数学、认知科学的研究属于神经科学。</p><p>如果持有这个观点，那么他很难在辩论中失败。因为所有被人工智能所吸收的理论都会被他说成是另外一个学科，而非神经学科。</p><p>但实际上神经科学的定义非常模糊，我迄今都没有严格区分neuroscience, neurology, neurobiology, coginitive neuroscience等细分方向。我觉得凡是<strong>以神经系统为研究对象，为此开发的理论、方法以及实验发现都属于神经科学。</strong> </p><p>毕竟现代神经网络的先驱们很多都是数学、物理方向，大多都是怀着对人类智能的兴趣而进行相关的探索。他们有的走向人工神经网络，有的走向生物脑的原理解析。 </p><p>对于<strong>历史上的大师而言，他们眼中没有细分的专业，而只有人类未解之谜。如果需要，他们可以自己创造一个专业</strong>。</p><h3 id="目前神经科学还处于初级阶段，但已经是最好的时代"><a href="#目前神经科学还处于初级阶段，但已经是最好的时代" class="headerlink" title="目前神经科学还处于初级阶段，但已经是最好的时代"></a>目前神经科学还处于初级阶段，但已经是最好的时代</h3><p>神经科学是一个非常吸引人的学科，里边的未解之谜实在太多。有很多人在年轻时候都试图去研究它，进了一些神经科学的博士项目。</p><p>但真正进入了之后，才发现我们的研究手段如此初级，离自己想象差距好大，感叹在有生之年估计是无法解答自己的疑惑。</p><p>比如Jeff Hawkins进入了UC Berkeley的神经科学PhD项目，读了一段时间觉得有点希望渺茫，于是就辍学去了硅谷，创建了Palm Computing公司，赚了大钱后，在UC Berkeley赞助了一个Redwood Center for Theoretical Neuroscience，继续赞助一帮大佬进行神经科学研究。</p><p><img src="https://picx.zhimg.com/80/v2-e85e4b1d8600261cbf975827ffdba448_720w.webp?source=1940ef5c" alt="img"></p><p>图片致谢深圳先进院的徐放博士</p><p>我之所以认为现在是神经科学发展最好的时代，是因为随着更多领域的人加入，神经科学研究的工具越来越多，手段越来越多样化，很多过去受技术限制而不能回答的问题正变得可能。正如上图所说，“科学的进步取决于<strong>新技术、新发现和新思想</strong>，可能是按照这个顺序”，所以我对神经科学的发展是乐观的。</p><h3 id="对人工智能的启发可以有很多种"><a href="#对人工智能的启发可以有很多种" class="headerlink" title="对人工智能的启发可以有很多种"></a>对人工智能的启发可以有很多种</h3><p>神经元和突触连接这些概念对于最早期的人工神经网络来说，我认为是肯定很关键的。<strong>这是从其它实体中很难获得的一种启发，其难度应该可以类比从我们日常生活想象<a href="https://www.zhihu.com/search?q=%E9%87%8F%E5%AD%90%E5%8A%9B%E5%AD%A6&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22:%22answer%22,%22sourceId%22:2728729351%7D">量子力学</a>一样</strong>。这要得益与人类历史上一些伟大的实验科学家和一群对智能原理孜孜不倦思索的理论家。</p><p>但是具体到某些算法，可获得的启发就可以来自方方面面了。Hinton在<a href="https://link.zhihu.com/?target=https://www.reddit.com/r/MachineLearning/comments/4w6tsv/comment/d6dgyse/?utm_source=share&utm_medium=web2x&context=3">reddit上提了关于发明dropout的一些历程</a>。其中提到了2004年时候，他觉得大脑执行一个任务只用部分神经元激活即可，因此有了dropout的想法。后来他又从银行工作人员轮流上班而不影响客户办理业务。这两种事情都促使他有了最初的 dropout想法。</p><p><img src="https://pic1.zhimg.com/80/v2-ba798bd6ae2df4f4f23c6c37d3260367_720w.webp?source=1940ef5c" alt="img"></p><h3 id="未来的NeuronAI政府资助"><a href="#未来的NeuronAI政府资助" class="headerlink" title="未来的NeuronAI政府资助"></a>未来的NeuronAI政府资助</h3><p>与此次争论无关的几条回复来自美国的<a href="https://link.zhihu.com/?target=https://www.iarpa.gov/">IARPA</a>的一个负责人David Markowitz，他是我之前参与的一个MICrONS项目的政府方面的管理者。MICrONS算是美国政府投入非常大的一个NeuroAI项目。David从经费管理者角度上回应了这篇文章，总体上是支持这一领域，但是科学家们需要拿出更多成果，给经费分配者以信心。</p><p><img src="https://pic1.zhimg.com/80/v2-ddc82982b30bbeccc37b00fb973b327d_720w.webp?source=1940ef5c" alt="img"></p><p><img src="https://picx.zhimg.com/80/v2-3f574efb7e199e3c6116c0f6c3835b62_720w.webp?source=1940ef5c" alt="img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能辅助的超分辨率宇宙学模拟</title>
    <link href="/2022/20221128/"/>
    <url>/2022/20221128/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>传统“星系形成”的宇宙学模拟受到了有限计算资源的限制，而这篇论文试图借助深度学习来解决这个问题。具体来说，通过生成 512 倍数以上的粒子并预测它们从初始位置的位移来提高模拟的分辨率。该方法的结果可以被视为是模拟实现的本身，而不是对其密度场的投影。由于生成过程是随机的，使得我们能够对大尺度环境下的小尺度模式进行采样。值得一提的是，该模型仅从16 对低分辨率 - 高分辨率的模拟中学习，就能够生成超分辨率的模拟。将模型进一步部署在比训练模拟框大 1000 倍的模拟中，该实验同样表明该模型可以快速生成高分辨率的模拟。由此，我们可以得出结论，人工智能辅助有可能彻底改变大型宇宙学体积中小规模星系形成物理学的建模。</p><p>参考：</p><p>论文：《AI-assisted superresolution cosmological simulations》</p><p>视频：<a href="https://www.bilibili.com/video/BV1bg411i7Nv/">【沈向洋带你读论文】人工智能辅助的超分辨率宇宙学模拟【StyleGAN2】【天文宇宙学】【超分辨率】</a></p><span id="more"></span><h2 id="AI-天文在研究什么"><a href="#AI-天文在研究什么" class="headerlink" title="AI 天文在研究什么"></a>AI 天文在研究什么</h2><p>有大量的天文观测数据，成指数增长，他也有自己的摩尔定律，已经增长到必须使用深度学习方法来处理的程度了。</p><p>从分论上来看，大概有：</p><p>（1）天文学图像</p><p>（2）光变曲线，就是光强随时间变化的这么一个函数。</p><p>可以用时间序列分析的各种，比如说 Neural ODE 和 Neural SDE 的方法来解决</p><p>（3）天文学光谱，光强随波长这么一个分布</p><p>可以用PCA 或者是非负矩阵分解，然后再用神经网络的方法来处理。</p><p>这些海量的数据使 AI for Astronomy 变得非常的 promising。有可能在将来从这些数据中发现现在未知的一些物理规律以及天文现象。</p><h2 id="论文中的方法"><a href="#论文中的方法" class="headerlink" title="论文中的方法"></a>论文中的方法</h2><p>利用超分辨率的方法来超分辨率化数字模拟。</p><h3 id="需要考虑的几个点："><a href="#需要考虑的几个点：" class="headerlink" title="需要考虑的几个点："></a>需要考虑的几个点：</h3><h4 id="（1）建模"><a href="#（1）建模" class="headerlink" title="（1）建模"></a><strong>（1）建模</strong></h4><p>由于我们宇宙一开始是非常均匀的，所以我们可以设置初始条件，把它放在一个格点上。然后从这个每个格点出发，指向他们现在所在的位置，然后把这些位移排在格点上,它就是一个矢量场。</p><p><img src="/2022/20221128/1.jpg"></p><p>如果我们分开来看，每一个分量的话，就可以把它们分别列为，三维图像的三个channel，就像RGB一样。然后这样做有一个好处就是，它对于每个粒子这样做之后，我们确保了宇宙中的物质的质量是守恒的，因为我们的粒子不会变多，也不会变少。</p><p><img src="/2022/20221128/2.jpg"></p><p>其次我们对于结果，可以同样把它处理为一个simulation的结果，就是说它与正常的 simulation 的结果看上去没有任何的不同。在此基础上，我们还可以加入速度的信息，比如说除了位移之外，我们还可以预测每个粒子的速度，从而得到整个相空间的物质通道信息。</p><h4 id="（2）对称性原理"><a href="#（2）对称性原理" class="headerlink" title="（2）对称性原理"></a>（2）对称性原理</h4><p>另外，宇宙在大尺度上是平移不变和旋转不变的，也就是对称性原理。</p><p>基于可以对立方体和正八面体可以做48种操作而不改变它本身的特性，在数据增强模块上，对input &#x2F;output data 同时做48种操作之一。</p><p>其次还有这种平移不变性，这种传统的 CNN 其实已经本身是平移不变的，但是传统的 padding 的办法，比如说 zero padding 会破坏这种平移不变性。为了保护这种平移不变性引入这种周期性的 padding 的办法。</p><h4 id="（3）一对多映射"><a href="#（3）一对多映射" class="headerlink" title="（3）一对多映射"></a>（3）一对多映射</h4><p>超分辨率是一个映射，而我们需要的映射不是个一对一的映射，需要它是一个一对多的映射。因为我们的高分辨率模拟中它的初始条件包含了低分辨率模拟中不存在的一些信息。这些高频的信息其实是随机的，而它有无穷多种可能性，所以这种映射其实是从一到无穷多的一个映射。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="/2022/20221128/model.jpg" alt="model"></p><p>具体模型结合了stylegan2的架构。</p><p><img src="/2022/20221128/stru.jpg"></p><p>StyleGAN2本身是用来做图像合成的,不是用来做超分辨率的，作者利用其中的一部分，然后把低分辨率输入作为 StyleGaN2 Generator 的输入，经过层层的上采样，然后就形成了超分辨率的这么一个结果，其中最重要的就是左边这个要不停的加入 noise，来形成更精细的结构。而 discriminator 的架构就是比较经典的ResNet。</p><p>训练数据采用了 16 对小体积的数值模拟，其中我们的低分辨率和高分辨率模拟之间，它的分辨率差了 512 倍。</p><h3 id="实验效果"><a href="#实验效果" class="headerlink" title="实验效果"></a>实验效果</h3><h4 id="一些专有定义"><a href="#一些专有定义" class="headerlink" title="一些专有定义"></a>一些专有定义</h4><p>展现实验结果前先介绍文中经常出现的z的定义。</p><p>李寅老师的回答：z通常表示红移（redshift），过去天体所发射光子的波长随着宇宙的膨胀而变长，因在可见光波段表现为变红而被称为红移。因此红移越大表明当时的宇宙越小，在越久远的过去。宇宙的大小相对今天大小的比率称为scale factor，用符号a表示，也就是a &#x3D; 宇宙某时刻大小 &#x2F; 宇宙现在的大小。a与z的关系是a &#x3D; 1 &#x2F; (1+z)，也就是z&#x3D;2时的宇宙是今天的1&#x2F;3大小。</p><h4 id="猜猜猜"><a href="#猜猜猜" class="headerlink" title="猜猜猜"></a>猜猜猜</h4><p>下图中有三张图片是生成的，只有一张是真的，猜猜哪个是真的？</p><p><img src="/2022/20221128/guess.jpg"></p><p>（答案是A）</p><h4 id="功率谱比较"><a href="#功率谱比较" class="headerlink" title="功率谱比较"></a>功率谱比较</h4><p><img src="/2022/20221128/power.jpg"></p><p><img src="/2022/20221128/power1.jpg"></p><p>功率谱其实是傅里叶空间的两点关联函数，它刻画了不同尺度上物质的密度分布的一个涨落大小。我们可以看到在小尺度上面,超分别率和高分别率混合得非常好。</p><h4 id="有趣的发现"><a href="#有趣的发现" class="headerlink" title="有趣的发现"></a>有趣的发现</h4><p>将超分别率模型应用到更大的体积中，体积是训练模拟体积的1000倍，结果出现了训练数据中不存在的暗物质晕。</p><h2 id="宇宙线模拟综述推荐"><a href="#宇宙线模拟综述推荐" class="headerlink" title="宇宙线模拟综述推荐"></a>宇宙线模拟综述推荐</h2><p><a href="https://readpaper.com/paper/4568278127084576769?channel=harryreadpaper">Large-scale dark matter simulations</a></p><h2 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h2><p>未来方向有：<br>a) 尝试新的生成模型如diffusion models；<br>b) 更小尺度的超分辨率，需要考虑星系尺度的物理过程；<br>c) 时间上的连续性问题；<br>d) 取长补短地与数值模拟模拟（例如作者刚刚开源的<a href="https://github.com/eelregit/pmwd">基于JAX的可微模拟pmwd</a>）结合，让快速有效的数值算法来处理长程的引力相互作用，用神经网络来补充短程相互作用。混合模型使用伴随法（adjoint method）训练，以探索星系形成过程的隐变量空间（latent space）。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>天文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Chain of Thought</title>
    <link href="/2022/20221126/"/>
    <url>/2022/20221126/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Ai需要鼓励吗？</p><p>参考：<a href="https://www.bilibili.com/video/BV1t8411e7Ug">Chain of Thought论文、代码和资源【论文精读】</a></p><hr><h3 id="原始版本"><a href="#原始版本" class="headerlink" title="原始版本"></a>原始版本</h3><p>《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》——思维链概念的开山之作</p><p>这篇文章是现任谷歌大脑研究员的Jason Wei在22年1月放到arxiv上面的文章，在上文所说的大背景下提出了思维链这个概念。<span id="more"></span>简单来说，思维链是一种离散式提示学习，更具体地，大模型下的上下文学习（即不进行训练，将例子添加到当前样本输入的前面，让模型一次输入这些文本进行输出完成任务），相比于之前传统的上下文学习，即通过x1,y1,x2,y2,….x_test作为输入来让大模型补全输出y_test，思维链多了中间的一些闲言碎语絮絮叨叨，以下面这张图为例子：</p><p><img src="https://pic3.zhimg.com/80/v2-e1b5adff46170f633e8ed635e7a57646_720w.webp" alt="img"></p><p>思维链的絮絮叨叨即不直接预测y，而是将y的“思维过程”r（学术上有很多学者将这种过程统称为relationale）也要预测出来。当然最后我们不需要这些“思维过程”，这些只是用来提示获得更好的答案，只选择最后的答案即可。作者对不同的数据集的原本用于上下文学习的提示标注了这些思维链然后跑了实验，发现这么做能够显著的提升性能。</p><p>COT一经提出就引发了社区对它的热烈讨论，类似 AI 是不是也需要鼓励来获得更好的表现之类的问题</p><h3 id="具体讲解"><a href="#具体讲解" class="headerlink" title="具体讲解"></a>具体讲解</h3><h4 id="zero-shot"><a href="#zero-shot" class="headerlink" title="zero-shot"></a>zero-shot</h4><ul><li><p>输入问题，等待输出结果。</p></li><li><p>文献：Large Language Models are Zero-Shot Reasoners(<a href="https://arxiv.org/abs/2205.11916">https://arxiv.org/abs/2205.11916</a>)</p></li><li><p>语言模型的输入是一道数学题连接一个字符串“The answer is”，然后让语言模型进行续写</p></li></ul><p><img src="/2022/20221126/zeroshot.png" alt="zeroshot"></p><h4 id="CoT"><a href="#CoT" class="headerlink" title="CoT"></a>CoT</h4><ul><li><p>比如输入问题并提示Let’s think step by step</p></li><li><p>语言模型的输入还是一道数学题连接一个字符串“Let’s think step by step”，然后让语言模型进行续写</p></li><li><p>这种情况下，语言模型会续写出中间推理步骤，并最终生成答案</p></li></ul><p><img src="/2022/20221126/zeroshot.png" alt="zeroshot"></p><h4 id="Manual-CoT"><a href="#Manual-CoT" class="headerlink" title="Manual-CoT"></a>Manual-CoT</h4><p> 一种few shot方法，所以构造了一些模板Q&amp;A（模板A中也有Let’s think step by step），然后再给出问题并提示Let’s think step by step。</p><ul><li>文献：Chain of Thought Prompting Elicits Reasoning in Large Language Models（<a href="https://arxiv.org/abs/2201.11903%EF%BC%89">https://arxiv.org/abs/2201.11903）</a></li><li>这种情况下使用到了少样本学习，在输入问题之前，手动设计一些问题和答案的样例（样例的答案给出中间推理步骤），这些问题和答案都需要手动构造，所以叫 Manual-CoT</li><li>语言模型的输入是一些手动设计的问题和答案的参考样例连接一个真正需要求解的问题，然后让语言模型进行续写</li><li>这里少样本训练中的问题和答案的样例都需要人为构造并手动设计，因此为了和第四种自动 CoT 做区分，这里称为 Manual-CoT</li><li>Manual-CoT 比 Zero-Shot-CoT 的性能要好，因为它采用的是 few shot ，在输入中提供了一些问题、中间推理步骤以及答案的样例给语言模型进行参考。但是，提供这些样例需要进行人工设计，这就需要一定的人工成本</li></ul><p><img src="/2022/20221126/zeroshot.png" alt="zeroshot"></p><h4 id="Auto-CoT"><a href="#Auto-CoT" class="headerlink" title="Auto-CoT"></a>Auto-CoT</h4><p>采样多个问题，每个问题提示Let’s think step by step，让模型给出答案。然后拼接所有生成的Q&amp;A并给出最终问题，并提示Let’s think step by step。</p><ul><li>文献：Automatic Chain of thought Prompting in Large Language Models（<a href="https://arxiv.org/abs/2210.03493">https://arxiv.org/abs/2210.03493</a> ）</li></ul><p>Auto-CoT 其实也是受到了 Manual-CoT 的启发，既然Manual-CoT 比 Zero-Shot-CoT 的性能要好，而且性能好的关键就在于人工设计的问题、中间推理步骤和答案的样例，那么就可以考虑将这部分进行自动化，从而节省人工成本</p><p>实时发现是可行的，做法主要分为两步</p><ol><li>通过多样性选取有代表性的问题</li><li>对于每一个采样的问题拼接上“Let’s think step by step”（类似于 Zero-Shot-CoT ）输入到语言模型，让语言模型生成中间推理步骤和答案，然后把这些所有采样的问题以及语言模型生成的中间推理步骤和答案全部拼接在一起，构成少样本学习的样例，最后再拼接上需要求解的问题一起输入到语言模型中进行续写</li></ol><ul><li><p>最终模型续写出了中间的推理步骤以及答案，并且质量非常高</p></li><li><p>值得一提的是，在十个数据集上 Auto-CoT 是可以匹配甚至超越 Manual-CoT 的性能，也就说明自动构造的 CoT 的问题、中间推理步骤和答案样例比人工设计的还要好，而且还节省了人工成本</p></li><li><p>在 Auto-CoT 中，其实也是用到了很多个“Let’s think step by step”对每个采样的问题分别触发中间推理步骤和答案，这也是为什么叫它“Let’s think not just step by step but also one by one”，也就是AI需要多鼓励几次</p></li></ul><p><img src="/2022/20221126/autocot.png" alt="autocot"></p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>自然语言处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习预测世界杯胜负</title>
    <link href="/2022/20221122/"/>
    <url>/2022/20221122/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://www.kaggle.com/code/sslp23/predicting-fifa-2022-world-cup-with-ml/notebook">Predicting FIFA 2022 World Cup with ML</a>。</p><p>预测出来的结果是巴西夺冠。</p><p>今年阿根廷夺冠！！！！</p><p>当然事在人为，最终谁夺的冠军还是让我们来期待一下。</p><p>根据历年数据计算的准确率只有不到70的准确率，AUC为0.75。</p><p>预测准确率25&#x2F;48≈52%</p><span id="more"></span><hr><h3 id="数据处理与特征选取"><a href="#数据处理与特征选取" class="headerlink" title="数据处理与特征选取"></a>数据处理与特征选取</h3><p>数据集使用1872年至2022年的国际足球成绩和1992-2022年的国际足联世界排名。</p><p>该模型预测了主队的胜率和客场的平局&#x2F;胜率。由于世界杯没有主场和客场之分，为了去除客场球队的优势，分别预测了客场和主场球队的变化结果，并使用两个预测的平均值作为概率。</p><p>一些特征：</p><p>（1）过去的比赛积分<br>（2）过去的进球和失球<br>（3）比赛的重要性(友谊赛或非友谊赛)<br>（4）球队排名<br>（5）球队的排名增加<br>（6）取决于排名的进球和失球</p><p>初始选取特征：<br>（1）球队在世界杯周期的平均进球数。<br>（2）球队最近5场比赛的平均进球数。<br>（3）在世界杯周期中，球队的平均失球数。<br>（4）球队最近5场比赛的平均失球数。<br>（5）球队在世界杯周期中所面对的国际足联平均排名。<br>（6）球队在过去5场比赛所面对的国际足联平均排名。<br>（7）在世界杯周期中中赢得的FIFA积分。<br>（8）在最近5场比赛中获得的FIFA积分。<br>（9）在周期中平均比赛点数。<br>（10）最近5场比赛的平均比赛点数。<br>（11）在周期中，按排名的平均比赛点数。<br>（12）最近5场比赛按排名的平均比赛点数。</p><p>作者通过小提琴和箱线图来分析以上特征。</p><p>作者分析得到有几个特征较好：</p><p>（1）排名差异（两个国家FIFA排名相减）</p><p>（2）平均进球数差异（还是相减）</p><p>（3）近五场平均进球数差异（还是相减）</p><p>（4）平均失球数差异</p><p>（5）近五场平均失球数差距</p><p>（6）主客场FIFA排名差距</p><p>（7）近五场主客场FIFA排名差距</p><p>（8）主客场每积分进球差距</p><p>（9）近五场主客场每积分进球差距</p><p>（10）主客场按排名的平均比赛点数差距</p><p>（11）近五场主客场按排名的平均比赛点数差距</p><p>（12）是否友谊赛</p><p>作者尝试了随机森林模型和Gradient Boosting模型，使用recall来判断模型好坏。根据AUC，随机森林效果比Gradient Boosting稍好，但是有过拟合的嫌疑，故作者选取了Gradient Boosting作为最终的模型。</p><h3 id="模拟结果"><a href="#模拟结果" class="headerlink" title="模拟结果"></a>模拟结果</h3><p>AI并没有给出胜或负的肯定的结果，只有胜负概率，下面的取最高赢球概率的为胜方。</p><h4 id="小组赛"><a href="#小组赛" class="headerlink" title="小组赛"></a>小组赛</h4><h5 id="A组："><a href="#A组：" class="headerlink" title="A组："></a>A组：</h5><p>卡塔尔 败 厄瓜多尔  【准】</p><p>塞内加尔 败 荷兰  【准】</p><p>卡塔尔 败 塞内加尔【准】</p><p>荷兰 胜 厄瓜多尔 【准】</p><p>厄瓜多尔 胜 塞内加尔  【不准】</p><p>荷兰 胜 卡塔尔 【准】</p><h5 id="B组"><a href="#B组" class="headerlink" title="B组"></a>B组</h5><p>英格兰 胜 伊朗 【准】</p><p>美国 平 威尔士 【准】</p><p>威尔士 胜 伊朗  【不准】</p><p>英格兰 胜 美国 【不准】</p><p>威尔士 败 英格兰 【准】</p><p>伊朗 败 美国 【准】</p><h5 id="C组"><a href="#C组" class="headerlink" title="C组"></a>C组</h5><p>阿根廷 胜 沙特阿拉伯 【不准】</p><p>墨西哥 平 波兰 【准】</p><p>波兰 胜 沙特阿拉伯 【准】</p><p>阿根廷 胜 墨西哥 【准】</p><p>波兰 败 阿根廷 【准】</p><p>沙特阿拉伯 败 墨西哥 【准】</p><h5 id="D组"><a href="#D组" class="headerlink" title="D组"></a>D组</h5><p>丹麦 胜 突尼斯 【不准】</p><p>法国 胜 澳大利亚 【准】</p><p>突尼斯 平 澳大利亚 【不准】</p><p>法国 平 丹麦 【不准】</p><p>澳大利亚 败 丹麦 【不准】</p><p>突尼斯 败 法国  【不准】</p><h5 id="E组"><a href="#E组" class="headerlink" title="E组"></a>E组</h5><p>德国 胜 日本 【不准】</p><p>西班牙 胜 哥斯达黎加 【准】</p><p>日本 平 哥斯达黎加 【不准】</p><p>西班牙 平 德国 【准】</p><p>日本 败 西班牙 【不准】</p><p>哥斯达黎加 败 德国 【准】</p><h5 id="F组"><a href="#F组" class="headerlink" title="F组"></a>F组</h5><p>摩洛哥 败 克罗地亚  【不准】</p><p>比利时 胜 加拿大 【准】</p><p>比利时 胜 摩洛哥 【不准】</p><p>克罗地亚 胜 加拿大 【准】</p><p>克罗地亚 败 比利时 【不准】</p><p>加拿大 平 摩洛哥: 【不准】</p><h5 id="G组"><a href="#G组" class="headerlink" title="G组"></a>G组</h5><p>瑞士 胜 喀麦隆 【准】</p><p>巴西 胜 塞尔维亚 【准】</p><p>喀麦隆 败 塞尔维亚 【不准】</p><p>巴西 平 瑞士 【不准】</p><p>塞尔维亚 败 瑞士 【准】</p><p>喀麦隆 败 巴西 【不准】</p><h5 id="H组"><a href="#H组" class="headerlink" title="H组"></a>H组</h5><p>乌拉圭 胜韩国 【不准】</p><p>葡萄牙 胜 加纳: 【准】</p><p>韩国 胜 加纳 【不准】</p><p>葡萄牙 平 乌拉圭 【不准】</p><p>加纳 败 乌拉圭 【准】</p><p>韩国 败 葡萄牙  【不准】</p><h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p>机器学习计算出来的胜率最高也知道70%+，一般都在50%<del>60</del>左右徘徊，差距是不大的，正所谓足球是圆的，发生什么都有可能，比如2014年格策的绝杀等都是机器无法预测的。现有的机器也不过是统计学习罢了，事在人为。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AI教育报告</title>
    <link href="/2022/20221013/"/>
    <url>/2022/20221013/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该报告来自2022斯坦福AI指数报告。</p><span id="more"></span><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0001.jpg"></p><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0002.jpg"></p><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0003.jpg"></p><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0004.jpg"></p><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0005.jpg"></p><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0006.jpg"></p><p><img src="/images/AI%E6%95%99%E8%82%B2_page-0007.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>State of AI 报告之AI发展预测</title>
    <link href="/2022/20221012/"/>
    <url>/2022/20221012/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该预测出自State of AI Report 2022，由AI投资者Nathan Benaich 和 Ian Hogarth 撰写。今年是第五年。每次报告都会进行预测，去年预测了8个，结果只对了一半。</p><span id="more"></span><h5 id="去年的预测"><a href="#去年的预测" class="headerlink" title="去年的预测"></a>去年的预测</h5><p>1.transformer将取代rnn来学习世界模型，强化学习agent在大型丰富的游戏中将超越人类的表现。</p><blockquote><p>准</p></blockquote><p>2.ASML（一家光刻机公司）的市值将达到500B美元</p><blockquote><p>不准。2022年10月3号只有165B美元</p></blockquote><p>3.Anthropic公司在GPT、Dota、AlphaGo的水平上发表文章，将自己确立为AGI研究的第三极。</p><blockquote><p>不准。</p></blockquote><p>4.AI半导体行业的整合浪潮，Graphcore、Cerebras、SambaNova、Groq或Mythic中至少有一家被大型技术公司或主要半导体公司收购。</p><blockquote><p>不准。目前还没有新宣布的人工智能半导体整合。</p></blockquote><p>5.小型transformer+ CNN混合模型与当前SOTA在ImageNet top-1精度(CoAtNet-7, 90.88%， 2.44B params)匹配，参数少10倍。</p><blockquote><p>准。谷歌提出的带有475M参数的MaxViT几乎与CoAtNet-7的性能(89.53%)匹配90.88%)。</p></blockquote><p>6.DeepMind在物理科学由重大突破</p><blockquote><p>准。比如Deepmind提出的《通过深度强化学习对托卡马克等离子体进行磁控》（Magnetic control of tokamak plasmas through deep reinforcement learning）</p></blockquote><p>7.根据Papers With Code的测量，JAX框架的每月仓库数的比例从1%增长到5%。</p><blockquote><p>不准。JAX的使用在paper With Code的月仓库数中仍然占不到1%。</p><p>说句题外话，华为提出的AI框架Mindspore在近几个月有大幅增长的趋势。2022年9月甚至超过了tensorflow仓库数。</p></blockquote><p>8.一个新的专注于人工智能的研究公司成立了，有重要的支持和一个专注于行业垂直方向的路线图(例如，开发人员工具，生命科学)。</p><blockquote><p>准。Adept.ai 是由Transformer的作者共同创立的，专注于通过软件工具使用自动化实现AGI</p></blockquote><h5 id="今年的预测"><a href="#今年的预测" class="headerlink" title="今年的预测"></a>今年的预测</h5><ol><li>DeepMind将提出训练参数为10B的多模态RL模型比，且比Gato大一个数量级。A 10B parameter multimodal RL model is trained by DeepMind, an order of magnitude larger than Gato.</li></ol><blockquote><p>GATO 是DeepMind提出的单一的「通才」智能体 Gato，它具有多模态、多任务、多具身（embodiment）特点，可以玩雅达利游戏、给图片输出字幕、和别人聊天、用机械臂堆叠积木等等。</p></blockquote><ol start="2"><li>NVIDIA将与一个专注于AGI的组织建立战略关系。NVIDIA announces a strategic relationship with an AGI focused organisation.</li></ol><blockquote><p>AGI：Artificial general intelligence，通用人工智能。</p></blockquote><ol start="3"><li>SOTA LM在比Chinchilla多10倍的数据点上进行训练，探究了数据集规模改变和参数规模改变的规律。A SOTA LM is trained on 10x more data points than Chinchilla, proving data-set scaling vs. parameter scaling</li></ol><blockquote><p>LM:language model</p><p> Gopher ：DeepMind 提出的大型语言模型。最近展示的 Gopher 模型已经拥有 2800 亿参数，在语言建模、阅读理解和问答等任务中展现了领先的性能.基于该结果。</p><p>研究团队发现，目前包括 DeepMind 发布的 Gopher 模型在内的许多大型语言模型，在大幅增加模型参数量的同时，并没有相应成比例地增加训练数据量，这是对于计算成本的巨大浪费。对于 Gopher 所花费的成本来说，训练的数据量应当是其 4 倍才可真正实现计算预算的最大价值。也可以说，要想达到 Gopher 的性能，实际上只需要 Gopher 模型参数量的四分之一的模型已经足够。在这一发现的基础上，研究人员进一步对名为“Chinchilla”的模型进行了训练，其训练数据量是 Gopher 的 4 倍，但参数数量仅是 Gopher 模型四分之一（700 亿个）</p></blockquote><ol start="4"><li><p>到2023年9月，生成式音频工具将吸引超过10万名开发者。Generative audio tools emerge that attract over 100,000 developers by September 2023. </p></li><li><p>GAFAM向AGI或开源AI公司(如OpenAI)投资超过10亿美元。GAFAM invests &gt;$1B into an AGI or open source AI company (e.g. OpenAI).</p></li></ol><blockquote><p>GAFAM:谷歌(Google)、苹果(Apple)、Facebook(Meta)、亚马逊(Amazon)、微软(microsoft)</p></blockquote><ol start="6"><li><p>面对NVIDIA的主导地位，半导体初创企业面临严峻的现实，一家备受瞩目的初创企业被关闭或以低于其最新估值50%的价格被收购。Reality bites for semiconductor startups in the face of NVIDIA’s dominance and a high profile start-up is shut down or acquired for &lt;50% of its most recent valuation. </p></li><li><p>一项像生物安全实验室一样监管AGI实验室的提议得到了当选的英国、美国或欧盟政治家的支持。A proposal to regulate AGI Labs like Biosafety Labs gets backing from an elected UK, US or EU politician. </p></li><li><p>随着越来越多的人意识到让人工智能能力先于安全发展所面临的风险，明年将向专门的人工智能联盟组织投资超过1亿美元。$&gt;$100M is invested in dedicated AI Alignment organisations in the next year as more people become aware of the risk we are facing by letting AI capabilities run ahead of safety. </p></li><li><p>主要的用户生成内容方(如Reddit)与一家生产AI模型的初创公司(如OpenAI)协商商业协议，以对其用户生成内容的语料库进行培训。A major user generated content side (e.g. Reddit) negotiates a commercial settlement with a start-up producing AI models (e.g. OpenAI) for training on their corpus of user generated content.</p></li></ol><p>参考资料:</p><p>[1]<a href="https://stateof.ai/">https://stateof.ai</a></p><h5 id=""><a href="#" class="headerlink" title=""></a></h5><hr><p>预测还是从比较宏观的方面入手的，比较偏向于AGI。该报告也不如斯坦福AI报告更学术，主要是从投资者的视角开展。</p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>利用金星凌日、火星冲、小行星冲等天象测定太阳赤道地平视差的方法</title>
    <link href="/2022/20221009/"/>
    <url>/2022/20221009/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>如何利用金星凌日、火星冲、小行星冲等天象测定太阳赤道地平视差的方法?</p><p>（来自《天文学基础》作业四）</p><hr><p>太阳赤道地平视差也被称为<a href="https://zh.m.wikipedia.org/zh-sg/%E8%A7%86%E5%B7%AE">太阳视差</a>。爱德蒙·哈雷在1716年提出来了利用进行凌日来测量日地距离。尽管使用金星凌日会因为黑滴现象产生一定的误差，但其方法仍是一个比较不错的方法。</p><span id="more"></span><p>我们可以参考其方法来利用金星凌日、火星冲、小行星冲等天象测定太阳赤道地平视差。</p><p>设地球上有两位观测者P、P’（不同维度），P点的观测者看到金星从A点穿过太阳到B点。</p><p><img src="/2022/20221009/Venus1a.gif" alt="Track of Venus across the Sun"></p><p>金星的轨道平面和地球的轨道平面非常接近，通常被称为黄道平面(很接近，但不完全相同——如果是，每次金星超过地球都会发生凌日)。<br>因此AB线几乎与天球上标记黄道的线平行，也就是黄道平面与我们看到的天空相交的那条线。</p><p>如果我们测量AB和A’ B ‘之间的距离D，原则上我们可以应用一些简单的三角函数，利用金地距离或其他相关数据等来推导出日地平均距离（AU)，进一步得到太阳视差。</p><p><img src="/2022/20221009/Venus1b.gif" alt="Geometry of the Transit"></p><p>开始我们的推导过程，符号由图所示，其中A、B两点为金星内切太阳平面时的中心，所以到太阳中心的距离为R-r。</p><p><img src="/2022/20221009/Venus4.gif" alt="Variables used in the calculation"></p><p>$h&#x3D;(R-r)cos\theta$</p><p>由于过境时间T正比于距离AB，故$T&#x3D;k·AB&#x3D;2k(R-r)cos\theta$</p><p>我们设A’B’的角为$\theta+\delta$，则：</p><p>$h’&#x3D;(R-r)cos(\theta+\delta)&#x3D;(R-r)(cos\theta cos\delta-sin\theta sin\delta)$</p><p>由于$\delta$是一个比较小的角，所以我们可以把$cos\delta$近似为1。</p><p>进一步的，$cos\delta&#x3D;(1-sin^2\delta)^{\frac{1}{2}} \sim 1-\frac{1}{2}sin^2\delta $，我们把cos近似化，而不对sin近似化，其影响也是比较小的。</p><p>故$h’&#x3D; (R-r)(cos\theta -sin\theta sin\delta)$</p><p>$D&#x3D;h-h’&#x3D;(R-r)sin\theta sin\delta$</p><p>类似的：</p><p>$T’&#x3D;2k(R-r)sin(\theta+\delta)\ &#x3D;2k(R-r)(sin\theta+cos\theta sin\delta)\ &#x3D;T+2k(R-r)cos\theta sin\delta$</p><p>故$\Delta T&#x2F;T&#x3D;\frac{T’-T}{T}&#x3D;\frac{cos\theta sin\delta}{sin\theta}$</p><p>$sin\delta &#x3D;\frac{T’sin\theta}{Tcos\theta}$</p><p>故$D&#x3D;\frac{(R-r)\Delta Tsin^2\theta}{Tcos\theta}$</p><p>我们可以根据观测数据来获得$T、θ$等数据。</p><p>参考资料:</p><p>[1]<a href="https://pwg.gsfc.nasa.gov/stargaze/Svenus1.htm">https://pwg.gsfc.nasa.gov/stargaze/Svenus1.htm</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>天文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>自恋与爱情</title>
    <link href="/2022/20220818/"/>
    <url>/2022/20220818/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于B站《<a href="https://www.bilibili.com/video/BV1sv4y1c7iD?vd_source=2bbaa5483ec5f3b54f583580af154a76">EP149 你不是在爱 而是自恋！心理学 霍妮&amp;弗洛姆</a>》</p><p>卡伦·霍妮和弗洛姆都是我比较喜欢的心理学家，像霍妮的《我们时代的神经症人格》、《我们内心的冲突》，弗洛姆的《逃避自由》、《爱的艺术》、《为自己的人》等都是很有价值、值得一读的心理学著作。</p><hr><p> 恋爱当中的人该有怎样的内在品质， 才能开启段合格的爱情 ，这里先给出精神分析学派的答案： 那就是克服自恋。</p><span id="more"></span><p>自人出生以来， 时刻就处在个体化与孤独感的矛盾之中 ，仍迫切需要和外界建立联系以便克服自己的孤独。</p><p>心理学家弗洛姆说道：（人 为了解决孤独这一焦虑)，一种是在人际交往中通过真正爱的方式来应对孤独 而另一种都是通过消极的自恋方式来应对孤独。</p><p>简单来说，很多在亲密关系中的人，你以为自己在爱对方，其实你只是在自恋而已。没有对对方人格上的共情力和积极关心的能力 他只是为了自私总想着加油对方、支配对方 。极端的情况下还会引发pua和施虐。 他会因为害怕失去你而变成顺从， 表面上看上去他对你百依百顺 ，很无私。但这也是自恋而已。</p><p>其实我们可以分为显性自恋和隐性自恋两种，不管是显性自恋， 还是那种看似无私的隐性自恋，精神分析学家霍尼都一针见血的指出：“自恋的人他都有一种神经质，他在生活中会表现出一种执念，一种对权力、美貌、名望和财富等等的病态追求和执念，而且他对自己身上任何一点软弱无能都十分的敏感，比如说一个男人总觉得自己必须要有很多钱才能配得上自己心爱的人。一个女人总觉得自己必须要很漂亮才能获得相应的爱， 一旦自己没钱、不漂亮的时候 ，她们就会极其的焦虑和高度的敏感。以及他们在生活中遇到一点点的问题 ，比如工作 、学习创业 、人际交往中遇到了挫折，一次失败，别人的一个抱怨、一个批评， 他相比于其他人来说 ，也更加的敏感 ，他总觉得自己受到了屈辱 ，受到了侮辱。”</p><p> 首先 ，我们说这种显性自恋的人 ，他们在亲密关系中也是如此。既希望控制自己， 也希望控制对方。 但凡只要不是他同意的 、他发起的， 他都不希望发生，同时也希望自己永远是正确的 。一旦被证明自己出了错 ，他们就会变得恼怒。 </p><p>他们这种人对控制的追求也可以用一种淡化的方式，他有意识的允许恋人享有充分的自由 ，但他却要坚持知道对方所做的一切。一旦对方有什么对他隐瞒的 他都会柏然大陆以及他总是设法逃避接受猎人的指导劝告和帮助，他在遇到问题 遇到双方争论时都是充耳不闻 他会理解为这是屈辱 还想反过来控制对方。</p><p>霍尼老师解释道， 他其实是觉得自己不应该有软弱无能的表现， 他其实也不是诚心想反对对方， 想控制对方， 而只是对自己软弱无能的反抗罢了。他如果越是感到自己的软弱无能 ，他就越焦虑 ，越想逃避一切看起来和软弱相关的东西。所以 他会继续投入到权利、名望、 财富、 美貌等等东西 ，而不想让对方看到自己这一焦虑。</p><p>他觉得自己能够驾驭一切 ，无论是多么困难的处境， 他自己都可以对付他 ，而在遇到感情出现危机时 ，他也总是继续单纯幻想着。只要我继续更加努力赚钱 ，我哪天升职加薪了， 只要这个项目我成了 ，或者我只要更漂亮 、更会打扮在整个脸 ，再整个容 ，你就会回到我身边。</p><p>他根本意识不到自己的自恋问题 ，他只是害怕自己的软弱无能而已， 一种病态的害怕、一种高度敏感的害怕。</p><p>你只是在care你那个理想化的自我， 还非要认为是别人嫌你没钱 、嫌你不漂亮才分手、才离婚的。要分手后， 你只会骂前任是个拜金女、是个渣男 ，而根本意识不到其实是自己在自恋而已。</p><p>另外就是说， 如果自恋类型的人，他压抑了他想控制对方的欲望 ，他会感到不开心 、感到不爽、 感到敌意 ，但他会在无意识里将这些敌意压抑了起来。但是这些压抑的心理能量不会凭空消失 ，而会以其他方式释放出来， 就接近我们所说的隐性自恋了。</p><p>这表面上， 生活中他也看似和和气气的 ，是个老好人， 遇到分歧没有愤怒， 也没跟你吵， 也没跟你闹。这只是因为他潜意识里就把敌意和不爽压抑了起来， 但他也会在之后的日子里爆发出来。比方说 ，霍尼说道 ，在之后的约会时他会迟到 ，或者会出现胃痛不舒服了症状等等， 而且连他自己都搞不懂为何自己会不舒服，他或许会归因于饮食不好 、天气不好等无关的原因 ，但是实际上他意识不到的事，其实都是因为自己想要控制对方的这股隐喻欲望导致的。</p><p>当自恋类型的人他的控制欲被过分压抑起来时， 就会以躯体化的方式躯体化的症状出现。这之前还是开开心心的，还是好好的， 下一刻就突然不开心了， 就突然emo了， 这儿不舒服那儿不舒服。而且他的身体反应也是真的 ，不能说他是装的原因呢 ，双方都答不上来。 所以这种占有欲强的自恋者，不管他是表现出来了，还是让他压抑起来了 ，他可以是显性的 也可以是隐性的。</p><p>隐性的自恋不代表他没有影响 ，而是会继续在之后作用在他身上 ，他会是别扭的。</p><p>霍尼还提到些例子， 比方说这种自恋占有欲的女性可能感受到某个男人对她的吸引力。一旦这个男人真的爱上了她， 她也会转而轻视他 ，看不起他 。因为这意味着他失去了他想要的控制欲，以及可能会让自己卷入未知的领域中去 。以及如果丈夫或者恋人不能符合自己的期望 ，如果他迟到了，忘了打电话 忘了纪念日等等。女性神经质的人就会觉得他并不爱她，她的这种感觉也是因为对方未能顺应自己而产生了一种愤怒反应。</p><p>霍尼提到， 众人往往会产生一种奇怪的矛盾现象， 这种矛盾几乎可以使一切恋爱关系都归于失败。一个神经质的姑娘， 由于瞧不起任何的软弱无能 所以他不可能爱上一个”软弱”的男人。但是呢 ,他又因为自己总是希望自己的伴侣能够顺从自己, 能满足自己的占有欲。就想要对方顺从， 又使他不可能和一个同样坚强自恋的男人相处 。他渴望对方是一个硬汉 同时让对方屈服于自己的占有欲。这种矛盾是她注定走入失败的恋爱，经历一次次感情的失败 ，自恋类型的人，他们都害怕屈服于别人 ，只想别人顺从自己，也害怕改变自己。</p><p>但是我们也要知道 ，在真正的亲密关系中 ，就意味着双方肯定会有屈服和让步的。无论是男人和女人，无论是显性自恋还是隐性自恋， 这种占有欲神经质的人如果越是不能做出这种让步， 他恋爱关系就会出现问题。<br>自恋的人无论男女， 他都有一种迫切的需要 ，想要吸引对方的注意力 ，受到他人的尊敬和崇拜。他会产生以美貌与聪明材质 ，以出色的成就、挥金如土等等来实现。他如果得不到崇拜和目光 ，他就会感到一蹶不振， 因为他们极其敏感。</p><p>显性自恋的人， 他会有一种明显愤怒来应对。而隐性自恋的人看似没怎么愤怒。 因为他将这些屈辱压抑了起来， 把敌意压抑了起来。他会在之后爆发 产生新的焦虑。 而为了应对焦虑 他又会继续捣鼓， 继续吸引对方的注意。他们不断的沉溺于自我扩张 ，维护起自己的那个理想的虚幻的自我， 不接受时然的自我。</p><p>其实只是为了保护自己，你对抗屈辱感会软弱无力感 。换句话说 ，他也是为了恢复被压碎了的自尊心，来企图今后继续占有他人， 防止失去爱的保护性手段而已 。</p><p>这不是爱，这是自恋。 在社会当中， 显性自恋是机制的人很容易发现 但是隐性自恋的人发现起来稍微有点困难。</p><p>霍尼提到， 因为他本人也意识不到这当中包含的敌意， 他也认为自己是一个性情温和的人，他也认为人要在感情生活中要懂得无私 ，要懂得奉献， 懂得去让步， 其大道理他都懂 ，只是因为他的道德超我不允许。而在潜意识里， 他把敌意压抑了起来 ，将占有欲压抑了起来 。霍尼比喻道，就隐性自恋的人有一种“安全阀”，经过这一“安全阀”， 一定量的敌意可以一种非破坏性的方式释放出来。用刚才我们提到的例子，那些莫名其妙出现的躯体化症状， 一会不舒服、 那不舒服的各种别扭扭捏等等的小报复。隐性自恋者 他们这种态度本身不会像显性自恋者那种破坏性， 而是一种淡化了的表现。他阻止了纯粹破坏性的冲动 ，不会怎么愤怒， 但他们有种问题， 他们依旧会受到压抑的敌意， 还是会产生新的焦虑和新的问题。</p><p>霍尼比喻道， 他还是会用他的那种“软弱无能” 像鞭子一样去抽打对方， 以便驱使对方对自己的意志服务。他们向对方索取无止境的关怀和帮助， 而事实上，这种人从未感到别人为他做的努力中感到好处。而只会不断的抱怨和不断的要求 ，而更坏的情况就是说别人忽视了他 ，亏待了他。</p><p>另外个重点就是那些隐藏极深的隐性自恋， 他也觉得自恋不好， 自私发脾气的这种品格不好。在恋爱和婚姻关系中也总是对对方百依百顺， 外人看上去性格好好， 老好人一个。但是表现出过分的顺从不见得是个好现象， 看似很无我、 但也搞不好会是自恋人格的表现。这些人往往在头年时期经历过种种屈辱， 或许是学校 ，或许是家庭 ，或者其他原因。受到歧视 ，被人瞧不起 ，别人冷落 ，这种经验往往因为具有痛苦的性质而被他遗忘。 然而一旦自己长大以后 一些生活中的遭遇明显涉及屈辱时。 这时 ，这些经验就会在他的意识中再度出现。想支配他人的敌意就有了， 有了种敌意 ，但他受到自己过去经历的影响 ，又不能下这个敌意的决心 ，不能表达自己真切的确切的感受，其结果就是他会过分的顺从， 这就会造成恶性循环 ，即霍妮提到，他经历了屈辱感。于是他就产生了想侮辱 、想占有别人的欲望， 但是由于自己害怕受到报复 而对屈辱感高度的敏感了。于是他更加希望占有欲和更加想侮辱别人 从而他就陷入了一种自我强化的此循环，即不断强化才是最可怕的 。当强化到情绪安全阀都不管用时 ，就会是一种可怕的爆发 ，他就会走向暴力伤害对方。</p><p>所以霍尼希望自恋者正视自己的心理问题 ，而将自己无意识的那些东西给意识化，这里说的无论是显性还是隐性的自恋， 他都想要占有欲 ，都想对方顺从自己。</p><p>霍尼说道 ，他们都不是真的相信自己一贯正确 ，而只是不断的、不顾一切的需要显得正当合理 。换句话说， 这只是他们的一种心理防御机制。他们并不是每时每刻都在想着占有，他们也会这么照顾你、关心你，但不是一起床就开始支配你。哪有这种人， 有这种人， 你也不会和他确立关系。 </p><p>而只是说他们在遭遇到自己觉得是侮辱性事件时， 比方说遇到生活的挫折， 遇到不顺， 遇到争论， 遇到矛盾 ，当这股压力来的时候 ，他们才会表露出自己的占有欲。这个时候他的支配欲和因支配欲得不到满足 ，才会产生怨恨的合理化。他会把这种占有欲、 支配欲当做防御机制来应对压力， 即自恋者们要明白 ：当遇到压力时，你为什么这么偏执的想要支配别人 ，想要占有别人 想要追求权利和美貌来解决这个问题 都是因为自己无法面对自己的软弱无能。</p><p>你需要进一步知道自己为什么对软弱无能这么敏感 ，为什么发生一点点的争论、 建议、批评 、挫折就理解为是自己身上的软弱无能 ，就觉得自己自尊受辱了。要学会去回溯自己过去的经历与现在情节的因果关系 ，在感情生活中面对有压力的事件时，去识别到自己的支配欲。当压力发生的时候 ，要相信对方的建议和能力 ，去适当采纳恋人的建议，去放下自己的愤怒。</p><p>自恋的人 ，他越感到焦虑， 就越应该知道此时此刻正是自己的无意识里的防御机制在捣鬼。这种时候 ，他就越应该调用自己的理性 ，越应该去相信恋人 ，相信对方， 和对方协商。而不只是只想着支配局面或者释放自己的怨恨而隐性自恋的人， 虽然他们之前不会愤怒 ，但是要意识到自己的战友欲在作祟， 意识到自己压抑的那些育而不得的敌意 ，去正视他，去意识化他。 在自己的感情生活中 ，要懂得去表达自己的敌意， 而不是一味的压抑后的顺从。</p><p>这种人要懂得去表达自己的不满和表达自己内心的真实想法 ，而不是在潜意识里就直接调用压抑这机制来处理他 ，如此无论是显性还是隐性的 ，才能开启段真正合格的爱情。</p><p>弗洛姆也同意说道， 要培养爱的能力 ，首先就需要克服自恋， 克服想要利用他人的愿望，而那些与爱相关的什么关照 、负责 、尊重 、相知等等的态度 ，都是在自己去除自恋以后才能真正拥有的品格。克服不了自恋的人， 什么关照、负责 、尊重、相知等等都是形同虚设 。只有成熟的人才会拥有这些态度</p><p>自恋不是简单的以自我为中心，背后涉及了占有、支配、 顺从， 防御机制以及无意识压抑等等复杂的问题，涉及到他的过去经历， 涉及他的情节，</p><p>弗洛姆补充道 ，自恋是种欲望，强度在许多人身上毫不亚于性欲和求生欲 ，不要认为为自恋的人是少数 ，他在我们生活当中太常见了。弗洛摩提到自恋的反面是客观， 客观就是能够如实的看待人和物 ，所谓客观就是能把客观景象与处于愿望和恐惧的景象区分开来。</p><p>这和霍尼老师说的一样， 自恋的反面不是我们以为的无私登德的美德， 因为那些无我奉献的人，百般顺从的人， 他也大有可能是自恋类型， 即他们也是看不清客观的人， 看不清死循环、 看不清心理防御、看不清为何这么追求控制和支配欲。其自恋的真正本质在于，他限制了我们的理性 ，阻止了一个人在存在当中看到现实。</p><p>弗洛姆提到 ，对于神经质的人来说，只有一个现实， 那就是他自己内心有的东西， 就是他的愿望和恐惧 。外部世界只是他内心世界的符号和投射而已。</p><p>所以我们需要提高客观， 加强理性 ，一旦能够客观能够理性 ，我们学习爱的路程就走了一半。只有先克服了自恋 ，才会走向之后真正的爱， 真正的双方平等的爱 ，一种主体坚信的爱。</p><p>弗洛姆强调，爱是指的一种在保持自我完整性和独立性的前提下与他人结为一体，同时， 爱也是对生命以及我们所爱之物生长的积极关心， 如果我对一个人说 ：“我爱你。”那我也可以这么说 ：“我在你身上爱所有的人 、爱世界、 也爱我自己。”</p><p>而自恋的人， 他的爱情最终只会走向自我毁灭，就像古希腊传说中的纳尔克索斯， 一个俊美而自负的少年， 拒绝了仙女恶科的爱，后者心碎而死 ，复仇女神就惩罚了他 让他爱上了湖水中自己的倒影，之后在自我欣赏中 ，他就掉进了湖中 ，淹死了 。</p><p>自恋是一种诅咒， 在极端形势下 ，他就会自我毁灭而告终。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>心理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《一小时漫画基金实战法》笔记</title>
    <link href="/2022/20220817/"/>
    <url>/2022/20220817/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于管鹏的《一小时漫画基金实战法》书籍</p><p>比较入门的一本书，比较浅显，定投部分没什么价值。</p><hr><h3 id="一些基金知识"><a href="#一些基金知识" class="headerlink" title="一些基金知识"></a>一些基金知识</h3><h4 id="如何选择基金经理（笼统方法）"><a href="#如何选择基金经理（笼统方法）" class="headerlink" title="如何选择基金经理（笼统方法）"></a>如何选择基金经理（笼统方法）</h4><p>（1）经验筛选，选择5年以上从业经验的</p><p>（2）收益筛选，过去五年能在基金经理排行榜排前20%，近两年内能排进前30%</p><p>（3）另外，还可以看经历了几轮牛熊、师承何处、风格如何、获奖如何。</p><span id="more"></span><p>注意，基金经理也只有一两个特别擅长的行业与领域，往往不能做到基金收益百花齐放，而是一枝独秀。</p><h4 id="基金指标"><a href="#基金指标" class="headerlink" title="基金指标"></a>基金指标</h4><p><strong>（1）收益类指标</strong></p><p>净值增长率</p><p>持有胜率（它是指购买基金并持有一定时期后盈利的概率，即基金取得正收益的周期占比。）</p><p><strong>（2）风险类指标</strong></p><p>波动率</p><p>最大回撤率（一段时间内基金从净值最高点到最低点的跌幅，用于衡量投资后可能出现的最坏情况。）</p><p><strong>（3）综合指标</strong></p><p><strong>夏普比率</strong></p><p>夏普比率越高，说明在风险一定时，收益较高；或者在收益一定时，风险较低。总之，这个比率是越高越好。</p><p><strong>贝塔系数</strong></p><p>反映基金和基金评价基准关系的指标，大多数基金都喜欢对标沪深300指数，所以贝塔系数也可以理解为基金相对于整个大盘的波动情况。</p><p>虽然贝塔系数越小，说明基金的稳定性越好，但在牛市中，人们更喜欢贝塔系数高的基金，因为它能超过市场平均收益，而在熊市中，贝塔系数则越小越好，负数更佳。</p><p><strong>阿尔法系数</strong></p><p>阿尔法系数是指基金的绝对回报与按照贝塔系数计算的预期风险回报之间的差额。</p><p>阿尔法系数&#x3D;绝对回报-预期回报</p><p>其中绝对回报也叫超额回报，可以衡量基金经理的投资技术。无风险投资收益在我国就是1年期银行定期存款利率或者十年期国债利率。</p><h4 id="别踩排名的坑"><a href="#别踩排名的坑" class="headerlink" title="别踩排名的坑"></a>别踩排名的坑</h4><p> 如果一只基金只追求排名，那他肯定会无限放大风险，无异于将投资者的资金置于悬崖之上</p><h3 id="基金分类"><a href="#基金分类" class="headerlink" title="基金分类"></a>基金分类</h3><h4 id="按投资对象分类"><a href="#按投资对象分类" class="headerlink" title="按投资对象分类"></a>按投资对象分类</h4><p><strong>（1）货币型基金</strong></p><p>看名字也知道，货币型基金就是只投资货币市场的基金，比如现金、银行定期存款、大额存单、短期国债、债券逆回购、央行票据等，差不多可以理解为投资于“钱”。</p><p>比如余额宝。</p><p>可以作为生活备用金。不要选择小规模基金。</p><p><strong>（2）债券型基金</strong></p><p>尽量选择国家发行或者有政府背书的利率债，谨慎选择风险更大的信用债——即使信用债往往会给出更高的利率作为风险补偿。</p><p><strong>（3） 股票型基金</strong></p><p><strong>（4）混合型基金</strong></p><p>混合型基金一般混合持有股票和债券，有时候也混点货币，并且根据配置不同，分为偏股型基金、偏债型基金、平衡型基金和配置型基金。</p><p>可转债基金在一定程度上也属于混合基金。这就要从可转债的性质说起。可转债是指持有者可以在一定时期内按一定比例或价格将之转换成另一种证券的债券，说白了就是可以把债券换成股票，从当债主变成当股东。</p><p><strong>（5)特种基金</strong></p><p>特种基金就是募集资金，买入房地产、黄金等资产，可以将流行性较低的、非证券形式的资产，直接转化为资本市场上的证券资产，以方便交易。</p><h4 id="开放与封闭"><a href="#开放与封闭" class="headerlink" title="开放与封闭"></a>开放与封闭</h4><p>基金根据设立方式的不同，分为开放式基金与封闭式基金。</p><p>开放式基金，投资者可以随时申购和赎回。</p><p>封闭式基金则在发行之前已经确定份额，并且在发行完毕后的规定期限内处于封闭状态，不能再进行申购和赎回。虽然在封闭期不能赎回，但是封闭式基金经常会有大比例的分红。</p><h4 id="私募与公募"><a href="#私募与公募" class="headerlink" title="私募与公募"></a>私募与公募</h4><p>公募基金，也就是以公开方式向社会公众投资者募集资金的基金。</p><p>私募基金，是以非公开方式向特定投资者募集资金。</p><p>私募基金的投资基本不受限制，可以投资股票、债券，也可以投资非标债券、股权和经营项目等，是以收益为最大追求，所以风险和波动比较大。私募基金的基金经理一般不收取管理费，只收取业绩报酬，一切全靠业绩说话。因为风险较大，私募基金对投资的要求也比较高。高门槛，比如100万元起购。</p><h4 id="A与B与C"><a href="#A与B与C" class="headerlink" title="A与B与C"></a>A与B与C</h4><p>很多基金分为A类、B类和C类，基金代码不同，但实际上却是由同一个基金经理管理的同一只基金。字母表示的只是它们收费方式的不同，A类表示前端收费，也就是申购的时候一次性收取申购费，赎回的时候根据持有的时间收费，甚至可能不收费。C类表示后端收费，不收取申购费，但根据持有的期限收取赎回费，以及服务费。</p><p>B常见于货币型基金，一般需要500万元起投，收益也比A类货币型基金要高。</p><p>从收取费用上来看，对于打算短期持有的基金，我们适合申购C类，不然钱都花在买入和卖出手续费上了；对于打算长期持有的基金，我们可以申购A类，只交一次申购和赎回费就行，不用一直交服务费。</p><h3 id="基金定投"><a href="#基金定投" class="headerlink" title="基金定投"></a>基金定投</h3><p>智能定投</p><p>所谓智能定投，也是一笔一笔地投入，但是不定期也不定额，讲究低位时多投、高位时少投，以实现利润最大化。</p><p>比如设定规则：画一条蓝线，价格在蓝线上方时，正常投入；价格在蓝线下方时，翻倍投入。</p><h4 id="指数基金定投"><a href="#指数基金定投" class="headerlink" title="指数基金定投"></a>指数基金定投</h4><h5 id="一、选指数"><a href="#一、选指数" class="headerlink" title="一、选指数"></a>一、选指数</h5><p><strong>（1）排除法</strong></p><p>所以，跟踪不能生长的指数的基金，最好不要买，可以先排除掉。我们看一下上证指数，它是上证所有公司的加权平均，不管一家公司好不好，都要计算进去，只是权重不同而已。</p><p><strong>（2）比较法</strong></p><p>因为指数实在太多，我们挑3个大众点儿的指数来比较一下，重点是讲方法。先说明一下，中证全指和上证综指一样，应该在上一轮使用排除法时就被删除了，能出现在这一轮，完全是作为参考数据。</p><p>综合来看，中证500是一个非常不错的指数，懒得动脑的朋友可以直接选它。如果想保守一点，可以将中证500和沪深300搭配投资，能冲、能抗、效果更佳。</p><p><strong>（3）特例分析法</strong></p><h5 id="二、选基金"><a href="#二、选基金" class="headerlink" title="二、选基金"></a>二、选基金</h5><p>为了简单快捷，建议直接选ETF联接基金。</p><p>购买指数型基金的本意就是购买指数。</p><h3 id="什么时候卖"><a href="#什么时候卖" class="headerlink" title="什么时候卖"></a>什么时候卖</h3><p>答案是永远都不用卖，当你觉得钱太多了，每年的被动收入都花不完的时候，倒是可以停止定投了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>理财</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kaggle竞赛小技巧之指数加权ensemble</title>
    <link href="/2022/20220815/"/>
    <url>/2022/20220815/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自<a href="https://www.kaggle.com/code/jbomitchell/exponentially-weighted-ensemble-spaceship-titanic">JOHN MITCHELL</a></p><p>权重&#x3D;$ e^{b*(x-s)}$</p><p>x是每个模型的LB分数。得分越大越好，因此权重随着x的增加而变大，随着x的减少而变小（模型越好，权重越大）。<br>b是模型的一个可调参数，b越大，权重衰减越快，分数越低。S是一个校准参数，其定义如下：如果S设置为最佳单模型分数，则最高非标准化权重exp（b*（x-S））为1.0，这是方便的，但不是必需的。</p><p>最后还要归一化。</p>]]></content>
    
    
    
    <tags>
      
      <tag>kaggle</tag>
      
      <tag>学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高斯混合模型和EM算法</title>
    <link href="/2022/20220801/"/>
    <url>/2022/20220801/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Tabular Playground Series - Jul 2022是一场无监督聚类比赛，<a href="https://www.kaggle.com/competitions/tabular-playground-series-jul-2022/discussion/341023">第一名采用了不到一百行代码</a>，主要核心内容是使用GMM和EM。</p><span id="more"></span><h2 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h2><p>假设观测数据集为 ${x_1, x_2, \dots, x_N}$，每个 $x_i\in\mathbb{R}^D$。GMM 假设数据的生成过程为：</p><ol><li>从 $K$ 个高斯分布中，以概率 $\pi_k$ 选取第 $k$ 个分量，其中<br>$$<br>  \sum_{k&#x3D;1}^K \pi_k &#x3D; 1,\quad \pi_k\ge0.<br>$$</li><li>给定选中的组件 $k$，从第 $k$ 个高斯分布中采样：<br>$$<br>  x_i \sim \mathcal{N}(x\mid\mu_k,\Sigma_k).<br>$$</li></ol><p>因此，对任意一个观测 $x$ 的似然为：<br>$$<br>p(x\mid {\pi_k,\mu_k,\Sigma_k}_ {k&#x3D;1}^K)<br>  &#x3D; \sum_ {k&#x3D;1}^K \pi_k,\mathcal{N}(x\mid\mu_k,\Sigma_k).<br>$$</p><h2 id="目标：最大化对数似然"><a href="#目标：最大化对数似然" class="headerlink" title="目标：最大化对数似然"></a>目标：最大化对数似然</h2><p>我们希望找到参数集合 $\Theta&#x3D;{\pi_k,\mu_k,\Sigma_k}_ {k&#x3D;1}^K$，使得观测数据的对数似然最大：<br>$$<br>\mathcal{L}(\Theta)&#x3D;ln\prod_{i&#x3D;1}^N \sum _ {k&#x3D;1}^K \pi _ k,\mathcal{N}(x _ i \mid \mu _ k,\Sigma _ k) &#x3D; \sum _ {i&#x3D;1}^N ln(\sum _ {k&#x3D;1}^K \pi _ k,\mathcal{N}(x_i \mid \mu_k,\Sigma _ k))<br>$$<br>直接对上式求导并解出闭式解困难，因此引入 EM 算法来迭代优化。</p><h2 id="EM-算法概览"><a href="#EM-算法概览" class="headerlink" title="EM 算法概览"></a>EM 算法概览</h2><p>EM 算法将“数据点属于哪个组件”视作隐变量 $z_i$，其中 $z_i&#x3D;k$ 表示第 $i$ 个点来自第 $k$ 个高斯分量。算法在每次迭代中分两步：</p><ol><li><p><strong>E 步（Expectation）</strong><br>计算当前参数下，各数据点属于各分量的“后验概率”——责任度（responsibility）<br>$$<br>\gamma_{ik}<br>  &#x3D; p(z_i &#x3D; k \mid x_i,,\Theta^{\text{old}})<br>  &#x3D; \frac{\pi_k^{\text{old}};\mathcal{N}(x_i\mid\mu_k^{\text{old}},\Sigma_k^{\text{old}})}<br>     {\sum_{j&#x3D;1}^K \pi_j^{\text{old}};\mathcal{N}(x_i\mid\mu_j^{\text{old}},\Sigma_j^{\text{old}})}.<br>$$</p></li><li><p><strong>M 步（Maximization）</strong><br>利用上面算得的 $\gamma_{ik}$ 来更新参数，使得期望完全数据对数似然最大：  </p><ul><li>有效样本数：<br>$$<br>  N_k &#x3D; \sum_{i&#x3D;1}^N \gamma_{ik}.<br>$$</li><li>更新混合权重：<br>$$<br>  \pi_k^{\text{new}} &#x3D; \frac{N_k}{N}.<br>$$</li><li>更新均值：<br>$$<br>  \mu_k^{\text{new}}<br>  &#x3D; \frac{1}{N_k} \sum_{i&#x3D;1}^N \gamma_{ik},x_i.<br>$$</li><li>更新协方差：<br>$$<br>\Sigma_k^{\text{new}}<br>  &#x3D; \frac{1}{N_k} \sum_{i&#x3D;1}^N \gamma_{ik},(x_i - \mu_k^{\text{new}})(x_i - \mu_k^{\text{new}})^\top.<br>$$</li></ul></li></ol><p>不断迭代 E 步和 M 步，直到参数收敛（对数似然增益低于阈值或达到最大迭代次数）。</p><p><img src="/2022/20220801/gaussian-mixture-models-em-method-math.gif"></p><h2 id="EM-算法伪代码"><a href="#EM-算法伪代码" class="headerlink" title="EM 算法伪代码"></a>EM 算法伪代码</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs text">输入：数据 X=&#123;x₁,…,xₙ&#125;，组件数 K，收敛阈值 ε，最大迭代 T<br>初始化：随机或用 K-Means 给出 &#123;πₖ, μₖ, Σₖ&#125;<br><br>for t = 1 to T:<br>  — E 步 —<br>  对 i=1…N, k=1…K 计算责任度 γᵢₖ<br><br>  — M 步 —<br>  对 k=1…K 计算 Nₖ = ∑ᵢ γᵢₖ<br>  更新：<br>    πₖ ← Nₖ / N<br>    μₖ ← (1/Nₖ) ∑ᵢ γᵢₖ xᵢ<br>    Σₖ ← (1/Nₖ) ∑ᵢ γᵢₖ (xᵢ − μₖ)(xᵢ − μₖ)ᵀ<br><br>  计算对数似然 Lᵗ<br>  如果 |Lᵗ − Lᵗ⁻¹| &lt; ε: 退出<br><br>返回：&#123;πₖ, μₖ, Σₖ&#125;ₖ₌₁…K<br></code></pre></td></tr></table></figure><h2 id="和kmeans关系"><a href="#和kmeans关系" class="headerlink" title="和kmeans关系"></a>和kmeans关系</h2><p>考虑一个GMM，$\epsilon$是被所有分量共享的方差参数。<br>$$<br>P(x|\mu_k,\Sigma_k)&#x3D;\frac{1}{(2\pi \epsilon)^{D&#x2F;2}}exp(-\frac{1}{2\epsilon}||x-\mu_K||)<br>$$<br>我们把$\epsilon$看作一个固定的常数，则责任度为：<br>$$<br>  \frac{\pi_k exp(-\frac{||x_n-\mu_k||^2}{2\epsilon})}<br>         {\sum_{j&#x3D;1}^K \pi_j exp(-\frac{||x_n-\mu_j||^2}{2\epsilon})}.<br>$$<br>当我们考虑$\epsilon \to0$，则项j责任度趋向于趋向于1，其他趋向于0。相当于对数据点聚类的硬分配。</p><p>进一步的$\epsilon \to0$时，对数似然函数为：</p><p>$-\frac{1}{2}\sum_{n&#x3D;1}^N\sum_{k&#x3D;1}^K r_{nk}||x_n-\mu_k||^2$</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>PRML</p><p><a href="https://www.kaggle.com/competitions/tabular-playground-series-jul-2022/discussion/334887">https://www.kaggle.com/competitions/tabular-playground-series-jul-2022/discussion/334887</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数学建模课堂</title>
    <link href="/2022/20220730/"/>
    <url>/2022/20220730/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>笔记笔记</p><span id="more"></span><hr><h1 id="数模大课堂"><a href="#数模大课堂" class="headerlink" title="数模大课堂"></a>数模大课堂</h1><h2 id="第一次"><a href="#第一次" class="headerlink" title="第一次"></a>第一次</h2><h3 id="论文写作"><a href="#论文写作" class="headerlink" title="论文写作"></a>论文写作</h3><p>一句话说明题干要求</p><p>题目数据画表更清晰</p><p>问题提出部分介绍各小问要求</p><p>问题分为两部分：模型建立与求解</p><p>建立部分重数学原理、公式推导、原因分析</p><p>求解部分将据图算法思路呈现清楚，数据实验结果可视化</p><h4 id="可视化："><a href="#可视化：" class="headerlink" title="可视化："></a>可视化：</h4><p>（1）流程图，Latex,Visio,PPT</p><p>（2）实验结果图，层次清晰，图例注明</p><h4 id="收尾工作-：评价，引用与附录"><a href="#收尾工作-：评价，引用与附录" class="headerlink" title="收尾工作 ：评价，引用与附录"></a>收尾工作 ：评价，引用与附录</h4><p>部分模型需要灵敏性分析</p><p>模型评价尽量中肯、全面、有理有据：</p><p>（1）强调模型实验结果的有效性、求解性能、建模考虑设定等等</p><p>（2）不忌讳缺点，给出做不到&#x2F;做不好的原因</p><h4 id="最后一个下午：abstract-time"><a href="#最后一个下午：abstract-time" class="headerlink" title="最后一个下午：abstract time"></a>最后一个下午：abstract time</h4><p>一页页面</p><p>问题核心-&gt;模型构造-&gt;算法求解-&gt;实验结果</p><p>忌讳过多数据，选取最核心结果</p><p>适当加粗关键字</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p><strong>模型&#x2F;软件方法学习</strong></p><p>模型脑图，LaTex、SPSS、可视化。</p><p><strong>真题研究讨论</strong></p><p>对比多篇解答优缺点</p><p><strong>模拟开题</strong></p><p>限时现场读题（1h）-&gt;讨论选题（1-2h）-&gt;搜集资料-&gt;各小问建模大纲</p><p><img src="/images/mmodel1.jpg"></p><h1 id="校内讲座"><a href="#校内讲座" class="headerlink" title="校内讲座"></a>校内讲座</h1><h2 id="冯国灿——如何准备数学建模竞赛及案例分析"><a href="#冯国灿——如何准备数学建模竞赛及案例分析" class="headerlink" title="冯国灿——如何准备数学建模竞赛及案例分析"></a>冯国灿——如何准备数学建模竞赛及案例分析</h2><p><strong>评判准则：</strong></p><p>假设的合理性</p><p>建模的创造性</p><p>结果的正确性</p><p>表述的清晰程度</p><p><strong>参赛准备：</strong></p><p><strong>写作原则：</strong></p><p>1.结构要清晰、有条理</p><p>2.表达要简洁，运用图标、动画、视频</p><p>3.使用的符号要说明</p><p>4.别人的工作一定要注明，否则是抄袭</p><p>5.尽量用已有的背景模型，要有分析，仿真，要有结果。</p><p><strong>忌：</strong> 有表达式，没有结果；或结果是胡乱凑的</p><p>6.创新的体现（亮点）</p><p><strong>假设</strong></p><p>假设的合理性，不宜太多，关键的假设</p><p>如果与常识相悖，应做详细解释</p><p><strong>文献引用</strong></p><p>书，期刊论文，网页，硕博论文</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>中国房地产行业的压力</title>
    <link href="/2022/20220720/"/>
    <url>/2022/20220720/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>中国房地产行业的压力（Stresses in China’s Real Estate Sector）——来自美联储报告第五章Near-Term Risks to the Financial System。</p><p>以下为翻译：</p><span id="more"></span><hr><h3 id="简要"><a href="#简要" class="headerlink" title="简要"></a>简要</h3><p><strong>中国的压力，包括房地产行业的压力，可能蔓延到美国。</strong></p><p>在中国，房地产行业的债务水平很高，去年房地产行业的活动和价格大幅下降（见“中国房地产行业的压力”方框）。如果这场衰退加剧，其对中国市场和金融机构的影响可能会因2019冠状病毒疾病疫情的进一步爆发、新的监管限制（包括遏制科技行业的进一步行动）或因地缘政治动机或风险担忧而导致的其他国家贸易或投资的任何缩减而扩大。鉴于中国经济和金融体系的规模，以及中国与世界其他地区广泛的贸易联系，中国的金融压力可能会通过风险情绪恶化和经济活动中断给全球金融市场带来压力，这可能会影响美国。</p><h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>在过去的几十年里，中国的扩张性政策帮助维持了经济的快速增长，这已经超过了大多数其他国家。中国在世界GDP中的份额已达到约17%（图A）。对中国企业的信贷增长更快，支持了GDP增长，但由此产生的企业部门杠杆效应使其越来越容易受到冲击。<br>中国的非金融企业信贷已达到GDP的160%左右，这一水平远高于大多数其他新兴市场经济体（图B）。中国房地产行业的企业负债特别高，房地产行业一直是中国快速增长的关键引擎，房地产开发和相关活动的贷款增长迅速。</p><p><img src="/images/2.jpg"></p><p>在过去几年中，中国政府加强了对房地产市场的监管，包括对购房、银行房地产行业风险敞口以及一些市场的抵押贷款施加新的限制。 2020年8月，监管机构宣布了直接针对房地产开发商的进一步措施：根据杠杆和流动性的具体审慎限制（通常称为“三条红线”），逐步收紧对借款的限制。从长远来看，这些约束应该有助于控制杠杆，提高房地产部门和金融体系的弹性。<br>这些举措实施后不久，房地产销售急剧放缓（图C）。房价和建筑活动也有所下降。客户通常在项目竣工前向建筑公司付款，如果买家对开发商完成住房单元的能力失去信心，不利的动态可能会加剧。几家房地产开发商已经出现了付款违约，其他房地产开发商在国内和国外都出现了严重的流动性紧缩以及海外融资。在经历了多年的强劲增长后，国内银行对房地产开发商的贷款正在下降，一些大型中国房地产开发商在离岸美元市场发行的债券今年的交易价格正处于日益低迷的水平（图D）。</p><p><img src="/images/3.jpg"></p><p>尽管到目前为止，中国政府设法控制住了其影响，但房地产市场低迷的严重恶化可能会影响中国的金融体系。中资银行对开发商的直接敞口占其一级资本的一半以上，并通过向其他公司发放以房地产为抵押的贷款，对房地产市场有大量间接敞口。通过向散户投资者销售银行赞助的理财产品，中资银行还间接接触了房地产开发商。地方政府还受到中国房地产市场的影响，因为它们的财政收入有很大一部分来自卖地，而且它们的杠杆率也很高。去年，包括表外融资工具在内的地方政府债务的广义估计超过了GDP的70%。12月，国家政府宣布在2022年第一季度放松对地方政府债券融资的限制，这将部分缓解短期压力，并为基础设施投资提供资金。今年第一季度，地方政府的债券发行似乎表现强劲，年初中国固定资产投资增速加快，反映出今年财政刺激的前期沉重负担。</p><p>到目前为止，对美国的溢出效应在范围上是有限的，部分原因是美国的直接出口。中国大陆的风险敞口相对较小。银行的风险敞口不到一级资本的10%。投资者的风险敞口也很有限:现有数据显示，持有的中国证券(包括通过离岸关联公司发行的证券)仅占美国国债的1%左右。此外，最近的研究估计，对中国的销售占美国公司收入的不到5%。</p><p>但是，如果房地产市场的影响加剧，并导致中国银行承受巨大压力，从而降低银行贷款和GDP增长，那么压力将通过实体和金融渠道——尤其是贸易和全球风险情绪——向美国传递。<br>鉴于中国在全球经济中扮演的重要角色，贸易渠道是重要的，美联储工作人员的研究发现，负面的中国GDP惊喜往往会降低全球大宗商品价格和其他国家的贸易量。风险情绪也可以是一个重要的国际溢出渠道，中国过去的严重压力时期扰乱了全球市场，如2015年。<br>中国政府汇率管理机制的变化加剧了人们对中国经济增长的担忧(图E)。随之而来的资本外流加速和中国股市大幅回调，伴随着全球和美国市场的波动，以及美元的大幅升值。</p><p><img src="/images/4.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>学习</tag>
      
      <tag>经济</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>latex简短速成教程</title>
    <link href="/2022/20220719/"/>
    <url>/2022/20220719/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>给国赛组员写的。</p><span id="more"></span><h2 id="Latex教程"><a href="#Latex教程" class="headerlink" title="Latex教程"></a>Latex教程</h2><h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p><strong>另起一页:</strong></p><p>\newpage</p><p>空格</p><p>\quad  空一格<br>\qquad 空两格</p><p><strong>章节：</strong></p><p>\section{标题}</p><p>\subsection{标题}</p><p>\subsubsection{标题}</p><p>逐次类推</p><p><strong>字体：</strong></p><p>\textbf{}  粗体  (快捷键：CTRL+B)</p><p>\textsl{}   斜体</p><h3 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h3><p><strong>输入公式方式：</strong></p><p>（1）<strong>行内公式：</strong> 直接在正文中加“$ $”即可，公式输在两个$中即可</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs latex">我是公式<span class="hljs-built_in">$</span>x<span class="hljs-built_in">^</span>2<span class="hljs-built_in">$</span><br></code></pre></td></tr></table></figure><p>（ 2）行间公式：需要用<code>$$..$$</code>来输入</p><p><strong>常见的符号：</strong></p><p><img src="/2022/20220719/latex.jpg"></p><table><thead><tr><th>名称</th><th>latex代码</th><th>效果</th><th>注释</th></tr></thead><tbody><tr><td>上标</td><td>a^{b}</td><td>$a^b$</td><td>不引起歧义的情况下，{}可省略</td></tr><tr><td>下标</td><td>a_{b}</td><td>$a_b$</td><td>不引起歧义的情况下，{}可省略</td></tr><tr><td>符号正上下标</td><td>\sum\limits_{i&#x3D;1}^n</td><td>$\sum\limits_{i&#x3D;1}^n$</td><td>只能用于符号，比如”1\limits_{i&#x3D;1}^n”就是错的。</td></tr><tr><td>分式</td><td>\frac{a}{b}</td><td>$\frac{a}{b}$</td><td></td></tr><tr><td>根式</td><td>\sqrt{a}</td><td>$\sqrt{a}$</td><td></td></tr><tr><td>乘法</td><td>\times或\cdot</td><td>$\times或\cdot$</td><td></td></tr><tr><td>除法</td><td>\div</td><td>$\div$</td><td></td></tr><tr><td>积分、二重积分、三重积分</td><td>\int、\iint、\iiint</td><td>$\int、\iint、\iiint$</td><td></td></tr><tr><td>偏导</td><td>\partial</td><td>$\partial$</td><td></td></tr><tr><td>求和</td><td>\Sigma 或\sum</td><td>$\Sigma$ 或$\sum$</td><td>建议用\sum。另外latex符号是区分大小写的（因为希腊字母是有大小写的），\sigma就是$\sigma$</td></tr><tr><td>小于等于</td><td>a\leq b</td><td>$a\leq b$</td><td></td></tr><tr><td>大于等于</td><td>a\geq b</td><td>$a\geq b$</td><td></td></tr><tr><td>远小于</td><td>a\ll b</td><td>$a\ll b$</td><td></td></tr><tr><td>远大于</td><td>a\gg b</td><td>$a\gg b$</td><td></td></tr><tr><td>交、并、属于、子集</td><td>\cap、\cup、\in、\subset</td><td>$\cap、\cup、\in、\subset$</td><td></td></tr><tr><td>各种戴帽子的符号</td><td>\hat{x}、\bar{x}、\tilde{x}、\vec{x}、\dot{x}</td><td>$\hat{x}、\bar{x}、\tilde{x}、\vec{x}、\dot{x}$</td><td></td></tr><tr><td>空心符号</td><td>\mathbb{R}</td><td>$\mathbb{R}$</td><td></td></tr><tr><td>花体</td><td>\mathcal{R、L}</td><td>$\mathcal{R、L}$</td><td></td></tr></tbody></table><p>其他复杂的符号可在线输入</p><p><a href="https://www.latexlive.com/">https://www.latexlive.com/</a></p><h3 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h3><p><strong>最基本的</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;figure&#125;[htbp]  <br>    <span class="hljs-keyword">\centering</span>  <br>    <span class="hljs-keyword">\includegraphics</span>[width=8cm]&#123;图片.jpg&#125;  <br>    <span class="hljs-keyword">\caption</span>&#123;图片标题&#125;  <br><span class="hljs-keyword">\end</span>&#123;figure&#125;<br></code></pre></td></tr></table></figure><p>『h』当前位置。将图形放置在正文文本中给出该图形环境的地方。如果本页所剩的页面不够，这一参数将不起作用。<br>『t』顶部。将图形放置在页面的顶部。<br>『b』底部。将图形放置在页面的底部。<br>『p』浮动页。将图形放置在一只允许有浮动对象的页面上。</p><p>按h→t→b→p的顺序满足排版。</p><p> <strong>多图</strong></p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs latex"><span class="hljs-keyword">\begin</span>&#123;figure&#125;[htbp]<br>    <span class="hljs-keyword">\centering</span><br>    <span class="hljs-keyword">\begin</span>&#123;minipage&#125;&#123;0.32<span class="hljs-keyword">\linewidth</span>&#125;<br>        <span class="hljs-keyword">\centering</span><br>        <span class="hljs-keyword">\includegraphics</span>[width=0.9<span class="hljs-keyword">\linewidth</span>]&#123;Figure/chutian.JPG&#125;<br>        <span class="hljs-keyword">\caption</span>&#123;图片1&#125;<br>    <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    <span class="hljs-keyword">\begin</span>&#123;minipage&#125;&#123;0.32<span class="hljs-keyword">\linewidth</span>&#125;<br>        <span class="hljs-keyword">\centering</span><br>        <span class="hljs-keyword">\includegraphics</span>[width=0.9<span class="hljs-keyword">\linewidth</span>]&#123;Figure/chutian.JPG&#125;<br>        <span class="hljs-keyword">\caption</span>&#123;chutian2&#125;<br>        <span class="hljs-keyword">\label</span>&#123;chutian2&#125;<span class="hljs-comment">%文中引用该图片代号</span><br>    <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    <span class="hljs-keyword">\begin</span>&#123;minipage&#125;&#123;0.32<span class="hljs-keyword">\linewidth</span>&#125;<br>        <span class="hljs-keyword">\centering</span><br>        <span class="hljs-keyword">\includegraphics</span>[width=0.9<span class="hljs-keyword">\linewidth</span>]&#123;Figure/chutian.JPG&#125;<br>        <span class="hljs-keyword">\caption</span>&#123;chutian2&#125;<br>        <span class="hljs-keyword">\label</span>&#123;chutian2&#125;<span class="hljs-comment">%文中引用该图片代号</span><br>    <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    <span class="hljs-keyword">\begin</span>&#123;minipage&#125;&#123;0.32<span class="hljs-keyword">\linewidth</span>&#125;<br>        <span class="hljs-keyword">\centering</span><br>        <span class="hljs-keyword">\includegraphics</span>[width=0.9<span class="hljs-keyword">\linewidth</span>]&#123;Figure/chutian.JPG&#125;<br>        <span class="hljs-keyword">\caption</span>&#123;chutian3&#125;<br>        <span class="hljs-keyword">\label</span>&#123;chutian3&#125;<span class="hljs-comment">%文中引用该图片代号</span><br>    <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br>    <span class="hljs-keyword">\begin</span>&#123;minipage&#125;&#123;0.32<span class="hljs-keyword">\linewidth</span>&#125;<br>        <span class="hljs-keyword">\centering</span><br>        <span class="hljs-keyword">\includegraphics</span>[width=0.9<span class="hljs-keyword">\linewidth</span>]&#123;Figure/chutian.JPG&#125;<br>        <span class="hljs-keyword">\caption</span>&#123;chutian4&#125;<br>        <span class="hljs-keyword">\label</span>&#123;chutian4&#125;<span class="hljs-comment">%文中引用该图片代号</span><br>    <span class="hljs-keyword">\end</span>&#123;minipage&#125;<br><span class="hljs-keyword">\end</span>&#123;figure&#125;<br></code></pre></td></tr></table></figure><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><p>建议直接使用在线表格转latex代码</p><p><a href="https://www.tablesgenerator.com/#">https://www.tablesgenerator.com/#</a></p><h3 id="论文中的引用"><a href="#论文中的引用" class="headerlink" title="论文中的引用"></a>论文中的引用</h3><p>引用图&#x2F;表<code>:\ref&#123;标题&#125; </code> 标题此处填图&#x2F;表中\label中的内容</p><p>引用参考文献：\cite{}</p><h3 id="导入引用文献"><a href="#导入引用文献" class="headerlink" title="导入引用文献"></a>导入引用文献</h3><p>直接在.bib文件中输入论文的对应bibtex代码</p><p><strong>如何获取论文的bibtex</strong></p><p>使用谷歌学术，搜索后点击“引用”，点击“bibtex”</p><h3 id="国赛模板"><a href="#国赛模板" class="headerlink" title="国赛模板"></a>国赛模板</h3><p><a href="https://www.overleaf.com/xxxx">https://www.overleaf.com/xxxx</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>学习</tag>
      
      <tag>latex</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>研究的艺术</title>
    <link href="/2022/20220713/"/>
    <url>/2022/20220713/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>李沐的<a href="https://www.bilibili.com/video/BV1hY411T7vy?share_source=copy_web">《研究的艺术》</a>的笔记。</p><span id="more"></span><hr><h3 id="一些介绍"><a href="#一些介绍" class="headerlink" title="一些介绍"></a>一些介绍</h3><p><strong>两件事情：</strong></p><p>1.语文或英语成绩好不代表你的论文写作就很好</p><ul><li><p>如果是30岁左右在国内接受教育的话（跟老师一样背景的话）很有可能你在国内教育的时候，没有接受过系统的专业论文的写作；</p></li><li><p>如果你在20岁的话，可能在语文和英语课里面写的作文跟 论文所要求的那一种格式不一样；</p></li></ul><p>1.研究做了一些年，甚至发表过一些顶级的会议和期刊的情况下，很有可能 论文写作没有你想象的那么好（可能是<br>2.能够符合发表的标准；写东西比较快）要写得很好 很优美 很清晰 是需要大量的练习的，这还有很多的提升空间的 </p><p><strong>好的写作能带来的好处：</strong></p><p>1.让你的思考更加的深入，更加的有条理；<br>2.让更多的人愿意读你的论文，之后论文的影响力；</p><p>参考书名：《The Craft of Research, Fourth Edition (Chicago Guides to Writing, Editing, and Publishing) 》</p><ul><li>这本书不仅对论文写作有用，对商业的写作也是有用的（总结报告、项目计划等）；</li><li>如果是英语写作的话，强烈建议读它的英语版本的；</li><li>当然中文版也是可以去读的（大部分东西是跨语言的）；</li><li>过一遍可能是不够的，需要再去读几遍</li></ul><p><strong>The Craft of Research 的介绍：</strong></p><ul><li><p>这本书是整个芝加哥 关于写作系列丛书之一；</p></li><li><p>这些丛书中，这几本也是非常有名的：</p><ul><li>《The Chicago Guide to Grammar, Usage, and Punctuation》：关于语法使用和标点符号的写作指导，是非常基本的 也是权威的一本关于英语写作的书；</li><li>《A Manual for Writers of Research Papers, Theses, and Dissertations》：关于写研究论文的比较基础的书；</li></ul></li><li><p>这本书是教怎么样去讲这个故事，并将这个故事写下来；</p></li><li><p>书的版本不同可能是在找文献这一块不同，可能看前面的版本也问题不大；</p></li><li><p>在作者上面均是大学英语系的，在写作上面应该是比较权威的（用词与写作方法上），但是在举的例子上，比较偏文学和自然科学（写作的想法是相通的，适用范围广）；</p></li></ul><p><strong>介绍目录：</strong></p><p>第一部分，提出了一个核心的观点：在写作的时候，永远要知道你的读者是谁，他们想要知道什么；<br>第二部分，关于是 怎么样去问问题，怎么样去找答案；<br>第三部分，是讲 怎么样讲一个故事（怎么样提一个论点，怎么样安排论据来支撑你的论点）；<br>第四部分，是讲 怎么样把这个故事给你写下来（关于写作主要在这个部分）；</p><h3 id="第一部分——研究、研究者和读者"><a href="#第一部分——研究、研究者和读者" class="headerlink" title="第一部分——研究、研究者和读者"></a>第一部分——研究、研究者和读者</h3><p>本书的受众很广：可以是 正准备做第一个研究项目的本科生，也可以是比较年轻的研究者，本课的受众就缩小了范围（可以是研究项目已经做到一半了或者快要做完了，开始准备想怎么写论文；或已经做过了几个研究写过了几篇论文但想提升论文写作的），所以跳过了前言部分（当然也是可以去读一下的）</p><h4 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h4><p>比较偏背景的知识,其中 介绍了 研究是什么东西（研究就是要去收集信息来回答一个疑问，这个回答完之后呢能解决某一个问题（比较偏社会和自然学科这一块））</p><ul><li>对于技术类来说，收集信息可能是提出一个新的解决方案，然后去做一些实验来证实他的有效性；方法是解决某一个问题（不会把question 和 problem 在两个东西分开），但是这里为了使得我们去想这件事；</li></ul><h4 id="1-2-为什么要去写一篇文章"><a href="#1-2-为什么要去写一篇文章" class="headerlink" title="1.2 为什么要去写一篇文章"></a>1.2 为什么要去写一篇文章</h4><ul><li>写文章自己会记得；</li><li>写的时候会帮助你理解事情；</li><li>写作可以用来测试我们的想法；</li></ul><p>所以说，写作使我们，记得更加精确，理解更好以及评估我们的想法是不是客观的；</p><h4 id="1-3-为什么要用一个正式的论文格式："><a href="#1-3-为什么要用一个正式的论文格式：" class="headerlink" title="1.3 为什么要用一个正式的论文格式："></a>1.3 为什么要用一个正式的论文格式：</h4><p>研究就是跟 在同一个研究领域的同行之间进行交流，而论文的格式可认为是通讯中的协议，为了方便个体之间相互快速的理解研究的内容</p><h4 id="1-4-写作也是一种思考"><a href="#1-4-写作也是一种思考" class="headerlink" title="1.4 写作也是一种思考"></a>1.4 写作也是一种思考</h4><ul><li><p>需要为读者思考：去想自己的受众要去怎么样去理解这些东西，他们会怎么去想；</p></li><li><p>写作的核心在于，要找到一个 topic（自己真正关心的问题，并且自己真的想去回答的问题，就是说去找到自己的兴趣点在哪）</p></li><li><p>作者的补充：<br>举例子；<br>作者希望能够告诉我们怎么样去权衡 自己对自己项目的价值的信念 和 老师和同行之间的一个需求；</p></li><li><p>老师的想法：</p><ul><li><p>对一个人的羡慕，不在乎说你有多少钱、你有多少地位、你有多少什么什么东西，更羡慕的是说能找到自己人生的意义，能做一件事情真的是觉得能够实现人生价值的</p></li><li><p>为什么呢？是因为人的时间都是一样的，每个人就有那么多的时间，你把你的时间花在你认为值得东西上，可能别人觉得值不值。其实真的不重要，只是说你用你的时间去实现你的人生目标的话，那就是最值得钦佩的。</p></li></ul></li></ul><h3 id="第二部分-跟读者建立联系"><a href="#第二部分-跟读者建立联系" class="headerlink" title="第二部分 跟读者建立联系"></a>第二部分 跟读者建立联系</h3><p>摘要：如果一个研究没有人读的话，可能也算不了什么。就算是很有经验的研究员，在有时候也会忘记掉把他们的读者放在自己的脑海里面。在这一节，我们将告诉你怎么样去为你的读者考虑，甚至是在你开始你的项目之前。</p><ul><li>大部分重要的事情，通常是要跟别人一起来合作完成的，作者反驳了大众对研究人员的刻板印象之后，提出了：读书或读论文时其实是跟作者的无声交流，写作也一样，是将自己的“声音” 写进文字里</li></ul><h4 id="2-1-文章是跟读者的对话"><a href="#2-1-文章是跟读者的对话" class="headerlink" title="2.1 文章是跟读者的对话"></a>2.1 文章是跟读者的对话</h4><p>在日常生活中，我们离不开沟通，在交流的过程可以根据具体情况（别人的反馈），改变交流的方式（改语速、加背景知识等）。<br>但是，写作是一个想象中的对话，在开始写的时候，就决定了作者是什么样的角色，以及受众是什么样的角色。比如，</p><ul><li>作者是作为一个老师，来教学生一些东西，就是教科书的写法；</li><li>作者给大家分享一些有意思的东西，blog 是怎么写的；</li></ul><p>而这些角色（读者与作者扮演的角色）从文章的开头到结尾是固定的，</p><ul><li>一旦改变的话，会让人觉得这个上下文不一致，文不对头</li><li>同时，文本是静态的东西，不可能根据读者现在的情况，来改变下面的文字的输出</li></ul><p>在关于读者读a的文章 的例子中，应该在开始的时候，就带入读者的身份，思考读者可能会困惑的地方，然后适当的去调整一些论文。</p><h4 id="2-2-理解作者的角色"><a href="#2-2-理解作者的角色" class="headerlink" title="2.2 理解作者的角色"></a>2.2 理解作者的角色</h4><p>作为一个作者的角色，通常有三种</p><ul><li>发现了有意思的事，让大伙儿也乐乐（视频作者）</li><li>找到了实际问题的解决方案（有点像写博客）</li><li>找到了一个重要问题的答案，陈述问题与答案是什么（写论文）</li></ul><h4 id="2-3-想像读者的角色"><a href="#2-3-想像读者的角色" class="headerlink" title="2.3 想像读者的角色"></a>2.3 想像读者的角色</h4><p>当作者扮演不同的角色的时候，读者也会得到对应的角色</p><ul><li><p>追求娱乐的人</p></li><li><p>需要解决实际问题的人</p></li><li><p>想要更好去理解东西的人(大部分论文要求·的读者)</p></li></ul><p>作者给出了问题清单，方便我们思考：</p><p>1.我们的读者是谁？谁会读我们的文章<br>a.对这一块相当了解的人（评审人）<br>b.有一定知识的一般大众<br>c.根本不知道这一块的大众<br>d.在背景知识的介绍上，根据不同读者，用词是不一样的</p><p>2.他们希望我来干什么事情<br>a.来娱乐他们吗？<br>b.帮他们了解新的东西？<br>c.帮他们更好的去了解东西？<br>d.帮他们解决实际问题？</p><p>3.理解一下你的读者的知识的储备<br>a.读者是不是理解你这个研究的话题<br>b.他其实他们有这个问题，但是他们还没有意识到<br>c.这个问题他们根本没有，只是我的问题<br>d.他们是不是理解这个问题的严重性</p><p>4.要预测读者对你的回答以及方法做的反馈<br>a.如果观点跟所有人的观点是对立的，那么要非常小心来改变别人的观点；<br>b.他们会不会做一些标准的问句来反对我们提出的解决方案；<br>c.他们是不是特别关心我是怎么去解决这个问题的；</p><p><strong>总结：</strong></p><p>整个论文的写作，就是跟你的读者的一个无声的交流。<br>跟正常的交流不一样的是说，他不是互动的，而说你要一开始就想好，整个交流应该是一个什么的过程，要想清楚读者是谁，他们需要什么。<br>所以你在写文章时，而写的能满足他们的需求，使得他们能信服你写的东西。</p><h3 id="第三部分-从话题到问题（topics-to-questions）"><a href="#第三部分-从话题到问题（topics-to-questions）" class="headerlink" title="第三部分 从话题到问题（topics to questions）"></a>第三部分 从话题到问题（topics to questions）</h3><p>摘要：在自己的兴趣之中，找到一个话题然后适度的调整它的大小，使得我们能在这上面做研究。对于这个话题，我们需要提出一些问题，来指导我们做研究。</p><p>正文：</p><p>在研究的时候，可以自由的选择话题去做（学术界 相对高；工业界 相对低）<br>自由也有危害：定不下来具体要做什么事情（进入一个新的行业 或者 刚开始做研究）<br>一些定义：</p><ul><li>subject：学术领域（气候变换、人工智能、计算机视觉等）</li><li>topic：话题，即领域内特别兴趣点（气候变换对鸟的迁移带来的影响、怎么样高效的设计卷积神经网络使得图片的分类精度更高）</li></ul><p>为什么要选择话题？【因为很难有一个研究是针对整个研究领域来做的，】所以一定要在自己的领域里面找到一个话题，这样才可以往深挖。</p><p>话题是一个使得你可以在里面问一些问题的途径，如果回答这个问题回答得比较好的话呢，整个领域的研究人员都会对它感兴趣，而且好的答案能够在一定程度上推进整个领域往前发展。</p><p>话题本身，也可能比较大也可能比较小。</p><ul><li>大就是说，在整个研究领域中很常见的一个话题（心理学中害羞或愿意冒险是人类学来的还是天生的）</li><li>小众一点的也可以，但只有某些特定的研究人员感兴趣（把咖啡放在桌上之后留下的印子为什么一定是个圆形）</li><li>大家都感兴趣的话题，肯定有更多人参与研究，所以竞争更加激烈一点；</li><li>比较小众的话题在里面做出比别人更好的工作的概率更高一点，但是会导致你就算做出来受众没那么广；</li></ul><p>最重要还是对这个话题问一些问题，然后你问题的答案，别人可能会觉得重要，甚至这个答案在一定程度上能改变整个研究领域。 </p><p><strong>区分什么是question什么是problem？</strong></p><p>question就是疑问，就是说问一个问题，然后需要来做答案的；</p><p>problem，可认为是一个困难，就是存在一个问题，如果不去解决他的话，可能会带来一些危害；</p><p>往细的说，一个疑问不一定会带来一个问题，一些问题回答与否，并不都会带来什么后果；但有些问题的回答可以解决很大的问题</p><ul><li>a problem或者一个困难或者一个问题，就是研究界觉得是值得去解决的一件事情</li><li>a question或者说一个疑问呢，是找到一个问题的途径，也就是说，针对话题问一些问题使得我们可以找到一些方法来解决某个problem</li></ul><p>这两个是可以区分开的，也就是针对一个话题，要问很多这样子的问题，然后找出里面到底谁值得去回答</p><h4 id="3-1-怎么样从兴趣到一个话题-兴趣到话题再到疑问"><a href="#3-1-怎么样从兴趣到一个话题-兴趣到话题再到疑问" class="headerlink" title="3.1 怎么样从兴趣到一个话题(兴趣到话题再到疑问)"></a>3.1 怎么样从兴趣到一个话题(兴趣到话题再到疑问)</h4><p>在通过读一些文章之后去发现自己没有发现的兴趣，然后从中找到一个话题<br>﻿话题主要取决于自己的兴趣</p><p>对于技术领域来说，能考虑的就那么几件事情：</p><ul><li>怎么样把一个东西的效果做出来，就是针对一个问题我们提出个算法，使得之前的算法都不是很行，用了这个算法之后行了；</li><li>怎么样把它做大，数据更大，整个规模更大；</li><li>怎么把它做便宜一点；</li><li>怎么样把它做安全一些；</li></ul><p>看到一个技术点或者一个话题可以带上以上四点来考虑；</p><p>在这之后看看自己对什么东西感兴趣，然后针对这些兴趣去找到对应的话题。</p><p>这本书介绍了很多东西可以去看一看，但主要还是偏文科的，跟我们理工科还是有不一样的地方</p><h4 id="3-2-如何将话题变小，并使其能开始做研究？"><a href="#3-2-如何将话题变小，并使其能开始做研究？" class="headerlink" title="3.2 如何将话题变小，并使其能开始做研究？"></a>3.2 如何将话题变小，并使其能开始做研究？</h4><p>怎么使话题促成一个比较好的研究工作？<br>能不能把这个话题换算成一个论点？这个论点看上去是有价值去讨论的。<br>﻿﻿<br>当找到合适的话题的时候，就要问一些问题了：<br>常见的错误：当找到一个合适的话题的时候，就会迫不及待的把这个所有话题相关的文章、资料都找起来读一读。这样子的效率是比较低的。因为你去读的时候，如果没有带着问题出发的话，你可能读起来就不那么的专注了。﻿</p><p><strong>老师对于上面例子的观点：</strong><br>﻿1.与作者不同的是老师觉得，如果你是真的新进入一个领域或者一个话题的时候，不一定能问出什么样的问题出来，可能干的事情就是把那些文章都找出来读一读然后总结一下。<br>2.因为很多第1年第2年的硕士生或者博士生他干的事情，就是找到一个话题之后，就把整个话题的文章拿出来写一个综述文章，这样子能够对整个话题有一个比较全面的认识，当比较熟悉之后就知道哪一些东西是可以问合适的问题。<br>3.反过来讲，当你打算去写综述的时候呢你其实也已经问了一个问题：这个话题上有没有很好的综述文章，如果答案是有的话，那么就不要干这个事情了，就读一下综述的文章，在里面有一个比较全面认识，就可以开始想自己的问题；如果没有的话，就变成了一个实际的problem，可以给大家写一个综述的文章，给大家带来价值。在这个时候很有可能就可以问了一个很好的问题。<br>4.当开始真正的写综述的时候，再去读文章就会去想这篇文章提出的东西怎么样放进我的综述文章里面，也就是综述里面需要对整个话题、整个子领域给画一个比较大的图，然后把所有的工作放进图里面把他们之间联系起来。<br>5.更好的是说，在读论文的时候，能带着一些具体的一些问题（能够真的成为最后研究的一些问题）来去读的话，专注度会更好一点。</p><h4 id="3-4-怎么样评估你的问题"><a href="#3-4-怎么样评估你的问题" class="headerlink" title="3.4 怎么样评估你的问题"></a>3.4 怎么样评估你的问题</h4><p>当对某一个问题感兴趣的时候，应该问自己个更难的问题，叫做”so what“。</p><ul><li>就是说我如果解决了这个问题，又怎么样呢，别人会不会关心，他能给别人带来什么样的好处，是不是能够推动整个领域的发展，或者推动领域在一个小地方的发展，能不能启发别人做更多的工作；</li><li>也可以反过来问，假设我不去回答这样个问题，会不会有什么样损失，别人会不会觉得错过了一个亿，是不是整个领域可能会停止向前发展很多年。</li><li>如果你的答案是说没有，就是说是因为兴趣而做的也没有关系，这可以使得我们继续往深里面走，但是还得不断的去问SO WHAT这个问题，去想我现在的东西so what，别人care不care。</li><li>对自己狠一点，这样子才能比较客观一点。在生活中，时不时很tougher的问一问自己so what 这个问题，能够使得生活变得更加简单，发现很多事情其实没必要做的，会发现一些更有意义的事情</li></ul><p><strong>怎么样找到我们的问题以及怎么样去问so what这个问题：</strong><br>先把话题给列出来（把topic的名字你找出来），句式：我想去 学习&#x2F;做&#x2F;研究 _______<br>有了话题之后再其后加间接的问题，就是加个w词（6w1h）<br>去想清楚问题的重要性，如果问题是一个开放性的问题的话，那么问题的重要性很有可能就是为了帮助读者更好的了解 怎么样 为什么 或者 是不是 的问题啊；<br>﻿﻿<br><strong>在so what 这个问题上继续展开：</strong><br>在一开始找问题的时候，很有可能找到的问题就是这个领域关心的一些问题。那么这时候就不那么需要去关心这个东西的意义在什么地方；<br>在技术领域，通常我们去找一个方法来解决某一个痛点，只要你的方法真的能够比较好的解决这个痛点的情况下，一般都是有意义的，最后的意义的大小取决你这个痛点到底是有多痛以及是多少人觉得痛。<br>但是，很多时候其实是并不知道我们的这个东西为什么重要，特别是偏理论点的研究或者是很开放性的问题。在这个情况下一开始是OK的，就是说一开始并不知道为什么要做这个事情，随着研究慢慢深入，我们不断的去问自己说，找出来的这个东西（发现这个东西）到底有没有用，别人到底在不在意。<br>所以不断的去想这个事情，也会指导你的研究的方向。当我们有没有想清楚这个事情的时候，很有可能这个研究是不能停的。只有当我们大概是知道为什么别人觉得这东西有用的时候，我们是可以把研究停下来，开始写论文了。<br>总结一下：这里面是有3步的：我们的话题是什么；在这个话题里面的问题是什么；为什么别人会在意这个事情</p><p>﻿<strong>怎么去找话题：</strong></p><p>当我们去参加一个比如说科研实践这样子的课的话，在你的兴趣点里面找到一些话题来研究；<br>如果已经有了一个研究领域的话要去怎么样找：<br>去读一些教科书啊或者去上节课（书和课要比现在学的要再往上一层，因为了解一下自己已经懂得东西，可能也没什么新的想法）；<br>参加一些这个领域的学术报告，参加一下研讨会，跟大家讨论一下、问一些问题，看看大家在讨论什么东西；<br>问问老师（如果你的老师是一线的研究员的话，他肯定在心中是有一些想法的）；<br>去网上找一找谈论；<br>﻿<br>﻿<br>老师的看法：<br>除非是在一个比较大的团队里面，有很多人一起做一个事情，以及说有大的数据或者有比较多的计算资源的话，一般来说尽量不要去碰那种大家都在谈的特别大的领域。因为这里面的人特别多，比如说啊图片分类问题，目标检测问题，以及现在大家都在做的那些特别大的transformer模型。因为就算有一个很好的想法，但是要把文章写出来的话，需要大量的实验以及大量的精力去调这种东西，要做出效果的话，不是一个人很短时间就能做出来的。<br>反过来讲，可以去关注一些稍微小一点的问题，从小的地方开始，也许可以往大的方向靠。那么怎么样找小的问题，可以在网上找一找（reddit上面很多这样子讨论，大家会分享一下自己的使用的一些经验）<br>可能比较好的一个办法是说：如果有时间的话，可以去公司里实习，去看一下整个工业界中在应用人工智能的时候，遇到的一些痛点。很多时候工业界的人，他自己可能没这个想法，反正就这么痛，用过去就已经习惯了。但是我们作为一个刚进去的实习生，是个外来者，可以去仔细的观察大家是怎么样用的，遇到了什么问题，去把这些小的事情抽象出来，总结出来就变成了一个研究的问题了。然后只要观察仔细的话，其实是会发现很多这样子的问题，这里面的小问题很有可能是能支撑我们几年的研究的。</p><h5 id="本节的核心思想："><a href="#本节的核心思想：" class="headerlink" title="本节的核心思想："></a>本节的核心思想：</h5><p>怎么样从兴趣点开始去找到一个话题，然后把这个话题缩小到足够小的范围，使得我们能够去驾驭它，也使得它足够大到支撑一个比较好的研究；<br>要在这个话题里面去问很多问题，因为我们的研究都是用来解决问题的，所以一定要有问题。<br>有了问题之后，你要去问，最重要的 so what的问题，就是说解决这个问题要怎么样。在真的动手去解决问题之前，真的就要去想一想，假设半年之后，1年之后我解决了这个问题，那么别人关不关心这个事情。<br>在做任何项目之前，都应该去想想这个东西做出来它的意义在于什么，不要去想说能不能做出来。就假设说有足够的资源，运气足够好，能把它做出来最好的结果以及能做出来情况下，去想一想对别人的影响有多大（这个事情的意义的天花板在什么地方）。</p><h3 id="第4章-从疑问到问题"><a href="#第4章-从疑问到问题" class="headerlink" title="第4章 从疑问到问题"></a>第4章 从疑问到问题</h3><p>前言:</p><p>在这一节将解释怎么样去把一个question变成一个problem，而且是一个读者认为值得解决的问题；<br>作者把问的问题question和值得解决问题problem给区分开来，帮助我们去了解这样子的一个思考的过程；<br>这个思考的过程，不仅仅是对新手有用，在以后如果要指导别人做研究的话，也需要了解这一个过程来帮助别人来更好的思考。</p><p>正文：</p><p>回顾一下在上一章所提出了的3步 公式来解决问题，首先提出话题：在研究什么，然后问一个问题，然后需要去找出回答问题之后的意义在哪里。这样就找到了一个读者认为是值得解决的问题，也就能创建一个与读者的强联系。﻿</p><h4 id="4-1节-理解研究问题"><a href="#4-1节-理解研究问题" class="headerlink" title="4.1节 理解研究问题"></a>4.1节 理解研究问题</h4><p>研究的问题有两类：<br>实际问题（practical problem）：比如说一个算法他跑的比较慢，解决他的话会让它跑得更快了；一个任务在某个数据集上的精度不是很高，可以提出一个新的算法解决它；这个领域没有一个很好的数据集能够让大家更好的去评估这个问题，那我提出个数据集；这都是解决一些实际的问题<br>概念问题（conceptual problem）：这些问题不是要去解决某一个实际的痛点，而是说如果我们回答他的话，<br>会对某一个事情或者这个世界有一个更好的认识。在科学领域，绝大部分问题是这种概念上的一些问题，帮我们更好的理解某一个东西。<br>但是在技术领域，更偏向实际的问题。当然也有很多偏理论的问题，比如说了解batch normalization为什么能够工作，以及说深度学习为什么能工作，甚至机器学习为什么能工作。这里面有很多的理论工作，但是我们只是目前为止，没有太多的去涉及这些的论文，所以导致大家可能觉得这些东西不多。但实际上在整个人工智能领域 特别是机器学习，可能是在深度学习之前啊，可能至少有1&#x2F;3的论文是关于这样子的一些概念性的问题。</p><p>﻿这个问题是怎么样来的：</p><p>首先是有一些实际的问题（在日常生活中碰到一些东西）；<br>然后就会有动机（motivates）去找到一些研究的问题（值得research的问题，通常是一些开放性的你不可能立马就知道答案的或者别人觉得没有很容易就解决的，其中就一些有价值的问题）；<br>有价值的问题就定义了一些啊研究的实际的或者是概念上的问题；<br>这样子的问题导致说，我们要去研究出他的答案，就是整个研究工作在干的事情；<br>这些研究答案能够去帮助解决这些实际的问题；﻿</p><p><strong>为什么要有问题？</strong></p><p>如果你没有问题的话，可能你整个论文写出来就是很多数据在里面，很多证据摆在里面，但是别人看上去就没有能找到你的观点（point）在哪里；<br>很多新手写作就是没有把东西聚焦在某一个问题上，使得读者跟不上你的节奏，也不知道到底在问什么问题，所以还是需要有一个问题；</p><h4 id="4-2-节-一个问题的结构是什么样子的："><a href="#4-2-节-一个问题的结构是什么样子的：" class="headerlink" title="4.2 节 一个问题的结构是什么样子的："></a>4.2 节 一个问题的结构是什么样子的：</h4><p>不管是实际的问题还是概念上的问题都有一个下面的结构：<br>situation condition就是说你的状况<br>你不想要的一个结果（如果不解决它的话，可能需要去付某一些不想付的代价）<br>因为你的状况和你的代价的不同，导致的两类问题是不一样的<br>举例子（实际的问题是长什么样子）<br>﻿<br>﻿<br>需要注意的是所谓的代价，其实是读者关心的，不是我们自己要付的代价而是他们要付的代价</p><p>从读者的角度去找出这样子的代价﻿</p><p>怎么样从读者的角度去找出这样子的代价：从自己的状况开始不断的问so what﻿</p><h5 id="概念性的问题会有什么样的区别："><a href="#概念性的问题会有什么样的区别：" class="headerlink" title="概念性的问题会有什么样的区别："></a><strong>概念性的问题会有什么样的区别：</strong></h5><p>在实际问题上，我们的状况通常是一些状态，导致会付出我们不想要付的代价；<br>但对于这种概念上的问题来讲我们的状态通常是说不知道某或者不了解某一个东西，这就是condition；一般来说不理解某一个东西，不会给我们带来立即的一个代价（除了考试），所以一般不会去谈他的代价。<br>那它的后果是什么：如果不能了解这个问题的话，那么就会无法去理解一个更重要的问题，而这个重要的问题，大家都觉得我们应该去了解的；</p><p>﻿作者画了幅图来说明：我们去了解我们的第一个问题（研究要去回答的问题）能帮助我们了解一个更大的一个问题，就是从Q1 一直到Q2，只要Q2足够大而且确实是能过去的话，那么这还是有价值的。</p><h5 id="如何更好的去找到这样子的一个后果："><a href="#如何更好的去找到这样子的一个后果：" class="headerlink" title="如何更好的去找到这样子的一个后果："></a><strong>如何更好的去找到这样子的一个后果：</strong></h5><p>可以反问大家，如果我不能回答这个问题的话，会导致我不能回答什么事情；</p><p>概念性的问题，比实际的问题在问上去觉得更加虚一点，因为看不到一些事实上可见的代价。很多时候这些问题最后也只是满足了人类的一个好奇心。</p><h5 id="区别纯研究和应用的研究"><a href="#区别纯研究和应用的研究" class="headerlink" title="区别纯研究和应用的研究"></a>区别纯研究和应用的研究</h5><p>怎么样去区别纯研究和应用的研究？</p><p>纯研究就是去回答一个概念上的一个问题，应用的研究就是去解决一个实际上的问题；<br>纯研究通常会出现在比如说自然学科领域，在学术界会多一点。在工业界的话，大部分的研究是应用性的研究，当然在学校也是一样，偏技术类的研究通常都是应用性的研究，就是去回答一些实际的问题；<br>但具体的区别就是说你最后你的重要性也就说 so what 的问题，这个问题是要去回答一个问题，就是满足大家的一个好奇心，就是纯研究，还是说去解决一个实际的问题，降低某一个代价，就是应用的一个研究；<br>同样主题的东西，最后的significance不一样，导致了在纯研究和应用的研究的差异。应用性的研究，大家可能会觉得问题不大，反正因为能看到实际的一些结果。但是对纯研究的话，很多说大家对上面不是那么的理解。如果是做纯研究领域的话，想让大家能够更好的理解你研究价值，也可以往实际的那些应用去靠；﻿</p><h4 id="4-3节"><a href="#4-3节" class="headerlink" title="4.3节"></a>4.3节</h4><h5 id="怎么样找到一个好的研究问题："><a href="#怎么样找到一个好的研究问题：" class="headerlink" title="怎么样找到一个好的研究问题："></a>怎么样找到一个好的研究问题：</h5><p>1.找人帮忙：去跟别人讨论然后不断的去问so what的问题，有人帮你讨论其实会更好一点；<br>2.在阅读中寻找问题；<br>3.从自己的总结中选择：在写的时候通常会想的比较全面，所以写的比较客观，然后把它写下的时候也是帮助我们将整个思维想的更全面。<br>4.找到一个好的问题，是一个研究者可能需要花很多年的一个事情，需要跟可能比我们更善于去找好问题的研究者去合作，从他那里学习不断的去学习怎么样找到这个领域的好的的问题。</p><h4 id="4-4节："><a href="#4-4节：" class="headerlink" title="4.4节："></a>4.4节：</h4><p>在真的去解决一个问题之前，不要去想说能不能解决它。能不能解决它其实是后一步的事情，先要问的问题是说读者认不认为这个东西值得解决。<br>作者直接把后果说了出来，假设我们不这么做的话，那么很有可能读者会不在意（care）我们的工作，也就是为什么在做一个问题之前，一定要想说读者会不会觉得这个问题值得解决；</p><h5 id="怎么样摆脱进入一个新的领域时不知道问题重不重要的焦虑："><a href="#怎么样摆脱进入一个新的领域时不知道问题重不重要的焦虑：" class="headerlink" title="怎么样摆脱进入一个新的领域时不知道问题重不重要的焦虑："></a>怎么样摆脱进入一个新的领域时不知道问题重不重要的焦虑：</h5><p>首先要承认这个问题，不可能在一开始就知道问题是不是重要的，很多时候需要写一篇论文然后看一下数据怎么样，所以不是一两天的事情；</p><p>﻿<strong>老师的办法：</strong></p><p>不要等到真的把研究做完论文写完，再去给别人看说这个东西值不值得做。要在一开始的时候，尽量的在有想法的时候，跟别人谈，跟你的同学谈也行，同事谈也行，老师谈也行，甚至是可能针对的用户谈也行，就看看他们关不关心这个事情。<br>不要等到你的研究真的做完，可以是做了中间一半比如说1&#x2F;3或者1&#x2F;2的时候，准备一个10分钟、20分钟的小报告，然后在本地的一些研究小组里面给大家讲一讲，看一看大家的一个反馈。（可以在workshop上面投稿，看看大家的想法）<br>在工作中，在公司的时候，想做任何研究的时候，在做它之前一定要想清楚说哪个产品团队或哪个客户可能会需要这个东西，他们会不会把这个研究工作做进产品里面。在做之前先别急着动手，先想清楚你要做什么样的问题，可能用什么方法，然后你去跟这些未来可能存在的这些啊合作方去聊，告诉他说：我准备做这个问题，你感不感兴趣，你感不感兴趣把这个东西放进你的产品里面。<br>核心是说要在做研究的过程中，要不断的跟潜在的读者（用户）沟通，使得可以根据这个交互来帮助我们改变整个研究的一个方向。也是确保在研究做完之后，最后是有人关心有人买账。</p><h3 id="第五部分"><a href="#第五部分" class="headerlink" title="第五部分"></a>第五部分</h3><p><strong>在有了问题的时候，怎么样去找资源（文中偏文科）：</strong></p><p>在技术领域，要去解决某个问题，先去找到跟你这个问题最相关的一两篇论文，然后去看他引用了谁，就是往前走，看看他之前的工作长什么样子。<br>不要去看一篇论文对前面工作总结，要去把那些他真的引用的那些东西真的拿去看一下，完成一个自己的一个总结，然后再看一下谁引用这一篇文章；<br>把这个论文往前走往后走，基本上能找到要的那些资源了；</p><h3 id="第六部分"><a href="#第六部分" class="headerlink" title="第六部分"></a>第六部分</h3><h4 id="怎么样评估一篇论文的好坏："><a href="#怎么样评估一篇论文的好坏：" class="headerlink" title="怎么样评估一篇论文的好坏："></a>怎么样评估一篇论文的好坏：</h4><p>看引用，大概就知道这篇文章有多少人读它，多少人关心它，这个方法主要是针对已经发表了几年的文章；<br>对于新的文章是说，至少在计算机领域，不管是杂志也好会议好都是有排名的，虽然排名也不那么可信。假设一篇文章引用的那几篇主文章 就是说我们的方法、做对比的或者基于的那些文章，发表在某一个会议的时候，我们的文章也通常要发表过去；<br>研究也是有圈子的，假设想发一个一流会议的文章的话，那么我们去读的和引用的，和基于的工作最好也是一流会议甚至是更好。</p><p>总结：</p><p>第2部分的标题叫做问问题然后找到答案<br>核心是说先找到大小合适的话题，然后问一些问题，然后把一个读者认为值得去了解答案的问题抽出来，做成一个研究问题，我们的研究问题可能是实际的，也可能是概念上的。<br>不管是哪类问题都要想清楚：<br>他的状况是什么；<br>不解决他的话他的后果是什么；<br>后面两节是说给一个问题怎么样找到资源，就找到前面的工作，然后怎么样读懂别人的工作，以及把自己的工作放在别人的工作之上。<br>核心是说要学懂前面两章讲的东西，就是话题、问题以及他的后果。</p>]]></content>
    
    
    
    <tags>
      
      <tag>学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>十年之约</title>
    <link href="/2022/20220710/"/>
    <url>/2022/20220710/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="我也不知道密码。" data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="2e608f38c8dd4a55bb9f9337056cf5ae9b3d04359395cc7d0d7ae5ceeb702938">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">请输入密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>黄伟文x麦浚龙爱情七部曲</title>
    <link href="/2022/20220708/"/>
    <url>/2022/20220708/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>跨越十年的七首歌贯穿了一段十年的故事。</p><p>《耿耿于怀》、《念念不忘》、《罗生门》这三部曲是系列的主线。</p><span id="more"></span><hr><p>2004年《耿耿于怀》</p><p>2014年《念念不忘》</p><p>2015年《罗生门》</p><p>《雷克雅未克》</p><p>《睡前服》</p><p>《瑕疵》</p><p>《单鱼座》</p><p><img src="https://upload-images.jianshu.io/upload_images/8274446-5e983970fa71bfcd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/690/format/webp" alt="img"></p><hr><h5 id="《耿耿于怀》"><a href="#《耿耿于怀》" class="headerlink" title="《耿耿于怀》"></a>《耿耿于怀》</h5><p>你　最近还好吗　尚爱看少女漫画吗<br>最近　近乎没露面　你有新对象吗<br>真想带你见见　我刚识到的她<br>我想听你意见　这算是病吧<br>为何无论我　愿意怎样试　怎样也<br>不可一样爱慕她<br>难道没练习太耐　感觉都追不回来<br>试图再　努力爱　也显得不自在<br>不懂得如何谈恋爱<br>还是我太爱你　对过去太放不开<br>难道是寂寞太耐　生秀的锁不能开<br>钥匙也拆断了　留在旧患所在怀内<br>放满对你的爱<br>难怪跟谁也　再没法恋爱<br>我　有时仍很怕　路过你那从前的家<br>往事若然未落幕　再揭起有害吗<br>真想带你见见　我刚识到的她<br>我想听你意见　这算是病吧<br>为何无论我　愿意怎样试　怎样也<br>(没令自己恋上她)<br>难道没练习太耐　感觉都追不回来<br>试图再　努力爱　也显得不自在<br>不懂得如何谈恋爱<br>还是我太爱你　对过去太放不开<br>难道是寂寞太耐　生秀的锁不能开<br>钥匙也拆断了　留在旧患所在怀内<br>放满对你的爱<br>难怪跟谁也　再没法恋爱<br>难道没练习太耐　感觉都追不回来<br>试图再　努力爱　也显得不自在<br>耿耿于怀从前的爱<br>从没有　振作过　痛了再痛也应该<br>难道是寂寞太耐　生秀的锁不能开<br>往事却　似断箭　还剩下在体内<br>若怀内　放满对你的爱<br>害怕一直也　再没法恋爱</p><hr><h5 id="《念念不忘》"><a href="#《念念不忘》" class="headerlink" title="《念念不忘》"></a>《念念不忘》</h5><p>十年又過去 舉止仍像少女<br>你跟我每夜仍聚聚 到夢裡追<br>贈你哈囉吉蒂那玩具 這天早變作茉莉香水<br>你的笑 卻是照舊和煦</p><p>留在你漫畫書裡 當初那美麗神仙伴侶<br>就像那青春洪水 現在已經不可能追</p><p>那故事倉猝結束不到氣絕便已安葬<br>教兩人心裡有道不解的咒沒法釋放<br>讓我們打聽對方今天過得一切平安<br>縱使相見已是路人茫茫 臉書等愛侶入睡卻偷看</p><p>自離別剎那 今生停頓了嗎<br>縱使我最後曾認命 邂逅了她<br>但信一天總會再遇吧 我想聽你說別喜歡她<br>你跟我 以後抱著回家</p><p>其實你是一幅畫 狠狠往這舊人心上掛<br>現實過得不順嗎 定定望向這畫中曇花</p><p>那故事倉猝結束不到氣絕便已安葬<br>教兩人心裡有道不解的咒沒法釋放<br>讓我們打聽對方今天過得一切平安<br>縱使相見已是路人茫茫 記憶中你仍像初戀好看</p><p>吻過二十年還未寒 離去六十年仍熱燙<br>共你就似被舊情下了降 像下了降<br>每晚都想起對方</p><p>誰亦會講 假使那樣懷念必會再次有迴光<br>其實只要 讓我耿耿某人思憶早閃閃發光<br>個個也探問<br>愛戀不老的秘方 唯獨壯烈離座可百世流芳<br>你未忘 我未忘 猶勝伴在旁</p><p>那故事倉猝結束不到氣絕便已安葬<br>才成就心裡那道不解的咒沒法釋放<br>讓我們打聽對方天天過得一切平安<br>縱使相見已是路人茫茫 這生恐怕會念念你不放<br>流連著不想過對岸</p><hr><h5 id="《睡前服》"><a href="#《睡前服》" class="headerlink" title="《睡前服》"></a>《睡前服》</h5><p>承著這份寧靜有話對你講<br>還是趁著難眠夜去衝個浪<br>還是對著頭上朦朧月光寫一首詩叫你明天看</p><p>其實我夢遊在你萬呎套房<br>還是我在沈沈睡到出了汗<br>還是我在床上離奇睡醒 輾轉一刻你正在觀看</p><p>或者我 正在 魂飄蕩<br>前面是你嗎 不可能吧<br>明明還很清楚坐在我家</p><p>但目下光影是狂想嗎<br>模糊輪廓後是特別美好嗎</p><p>撐著 暈著 但尚是我吧<br>怎麼醫好無眠者半夜時的感性<br>長夜有點灰</p><p>斟一杯水拿來睡前服那顆寶貝<br>不高興 再續杯<br>朦朦鬆鬆剎那特別想潛入你心跟你說<br>有件憾事讓我終身有悔<br>未敢約的約會<br>仍在半夢與半醒之時被化灰</p><p>其實睡了嗎 不可能吧<br>明明還很清醒算著創疤<br>但現在思想 沒邏輯嗎</p><p>遺忘常理後是特別勇敢嗎<br>虛弱 無力 但沒亂說話<br>怎麼醫好無眠者半夜時的感性</p><p>長夜有點灰<br>斟一杯水拿來睡前服那顆寶貝</p><p>明日到 醒起再後悔<br>想跟你說為什麼人在痛心 失去你<br>卻未盡力 大概擔心我未配<br>未敢約的約會<br>仍在半夢與半醒之時未化灰 未化灰<br>讓幻象暫時陪我 關起燈 開開舞會<br>待藥力運行全身驅走一片灰<br>我只可這樣 享受你 然後徹底崩潰<br>共你 甜蜜過先崩潰 都算不悔<br>明白你只在彌留的一剎<br>願意待我好 卻不會 再夢迴</p><hr><h5 id="《瑕疵》"><a href="#《瑕疵》" class="headerlink" title="《瑕疵》"></a>《瑕疵》</h5><p>男：差些想放弃吧　为何未放手<br>差一些不要我吧　为何又再忍<br>并没什么亏欠我吧<br>为何犯罪作恶都找到借口　作罢<br>曾怀疑与后悔吗<br>差一些失去你吧　为何没法改<br>明明瑕疵多似乱麻　不去掩盖一下<br>其实你也很多景仰者爱你吧<br>很感激依然留下<br>情人间　能容许小秘密吗<br>应该揭穿但揭穿担心更卑下<br>情人间　能容许小错处吗<br>女：真的想过没送花也没情话<br>也算是爱情吧（男：但我不只有这点错）<br>严重多几倍吧<br>忍耐到底　光阴都不算白花<br>（男：为何人性会这么可怕）<br>先变笨吧　换我忍忍得到吗<br>男：其实曾内心挣扎吗（男：就当赠你的礼物吧）<br>女：罪案不去调查　男:亦想听你讲一下<br>女：差些想放弃吧　为何未放手<br>差一些不要你吧　为何又再忍<br>是未服输的个性吧<br>男：要爱我有阵时（女：不舍得不宽恕）不易<br>女：早知道原理吧　因此我也没惊讶<br>找快乐先修课程是要接受最差<br>登天国拿胜利也一秒吧<br>不过谪仙之路遥远又可怕<br>我共你关系如等大石开花<br>美好一剎　可杀死我吧<br>因此不舍这满场　<br>颓垣败瓦</p><hr><h5 id="《罗生门》"><a href="#《罗生门》" class="headerlink" title="《罗生门》"></a>《罗生门》</h5><p>男：若果你　未覺荒謬<br>被傳聞談論的瘋子挽著手</p><p>女：很感激　喜歡我十年仍不休<br>近日舊同學說我已　耿耿於你心六百週<br>很可惜　這一世未能長廝守<br>但事實如若告訴你或更內疚<br>我愛過哈囉吉蒂嗎似乎沒有</p><p>狄更斯是漫畫嗎　仍然少女誤會了嗎<br>迷戀蔽眼才給美化　但其實真懂得我嗎</p><p>那動人時光　不用常回看<br>能提取溫暖以後渡嚴寒　就關起那間房<br>最動人時光　未必地老天荒<br>難忘的因你太念念才難忘<br>容易抱住誰十年　最難是放</p><p>真心講　想起那段情仍不枉<br>若路上重遇會笑笑問你近況<br>你每晚更新的臉書卻無謂看</p><p>離別了若想心安　先不要每夜重翻舊案<br>望著更好的地方　為下段愛戀吸收陽光</p><p>那動人時光　不用常回看<br>能提取溫暖以後渡嚴寒　就關起那間房<br>最動人時光　未必地老天荒<br>難忘的因你太念念才難忘<br>容易眼淚流十年　難在擦乾</p><p>男：還在嘆息那愛戀不到氣絕便已安葬<br>(女：怕掛念太猖狂)<br>男：敎兩人心裡有道不解的咒沒法釋放</p><p>　合：個個也探問愛戀不老的秘方<br>　男：唯獨壯烈離座 可百世流芳<br>(女：難道抱著殘像　可百世流芳)<br>　男：你未忘　我未忘　猶勝伴在旁<br>(女：你未忘　我未忘　情信亦泛黃)</p><p>男：不心安　清早與夜來亦望望<br>收不到信號過兩秒又再看看<br>你說過常聯繫對方</p><p>男：從前號碼　等於老地方　不敢拆掉再裝<br>(女：那動人時光　不用常回看)<br>(能提取溫暖以後渡嚴寒　就關起那間房)<br>男：猶如絕症　天天有預感　幸福即將再降<br>(女：最動人時光　未必地老天荒)<br>(最纏綿那黑影　即使每夜遊蕩　其實一早已給安葬)</p><p>男：情人若你也未忘<br>約定誰過十年暗渡陳倉　再續夜航</p><p>女：別瘋狂　男：下個一月<br>男：願如期團聚於　冰島某地方<br>(女：讓前塵沉澱於　福島某地方)</p><hr><h5 id="《单鱼座》"><a href="#《单鱼座》" class="headerlink" title="《单鱼座》"></a>《单鱼座》</h5><p>无故地眼湿 突然悲痛莫名<br>再切入有关你的情景<br>还有话要讲 没有声 这大概乃双鱼宿命<br>半世任传闻话我痴 却没有辩驳那本领<br>灭了音的情歌 却依然动听 </p><p>何时火 何时冰 仍无声 仍无反应<br>天生过份冷静<br>举重也可若轻<br>就算心 多汹涌 不泄漏半升 </p><p>无暇又辩证<br>谁人好 谁人差 早心领<br>世界图腾怎拼 只得我能够 抽丝剥茧一一看清<br>只爱悬崖绝岭<br>由人去 诬蔑我 抑郁症<br>最怕逢场助庆<br>当天涯无风景 重投心境 </p><p>如流失 就会死 的感性<br>已在血管之中 别矫正<br>跟你吻的场景<br>就算它 是我的 故事入面 凭空设定 并无实证<br>仍留低 能重返 的小径 </p><p>个个严辞指正<br>怎么我能够 一声不响修改圣经<br>真爱才能获胜<br>旁人会 疑问我 极多情<br>如何赚取尊敬<br>很多人能心倾 然而都 非因即兴<br>像恶梦仍然爱 但爱极仍无声<br>多得宿命感性<br>得一刹美好 却杀掉一生的感性<br>忍不了时时 也想念你<br>唯有 驯服我 坏心情<br>维持 安静 端正<br>当天涯无风景 重投心境<br>重投心境 抹去 风声</p><hr><p>《雷克雅未克》</p><p>还请你兑现约定 飞到为我破冰</p><p>置身冰岛名胜 杀不死依存症</p><p>前尘埋没 那段情</p><p>约誓残念 却没有清</p><p>而你有讲过</p><p>来年再续旧场景</p><p>冒雪独自踏故城</p><p>北欧式配乐 灵动而冷静</p><p>潜入我 随身听 蓝鲸在 旋律间低唱 遗孤的心声</p><p>相比这市面 缓慢而洁净</p><p>徬徨路人像我 携同不必的感性</p><p>北极里有一束光</p><p>你话留来伴我望</p><p>我若觉得绝望</p><p>想到亦会全释放</p><p>这 绿里透紫的光</p><p>从未天际怒放</p><p>好比原来 一起能到达 的远方 而未同往</p><p>冰川未访 火山未闯</p><p>还是 我自困 空想之中</p><p>未熄的热情 用十年来放送</p><p>彷佛一眼热泉 涌进 寒冷冰窖中</p><p>炎凉大战中 谁够耐性便胜利</p><p>冰封旧情再解冻</p><p>或者失约 一早已在你 预备中</p><p>坚守冰岛只是我 未望通</p><p>根本不是 天下情人都 求重逢 重温美梦</p><p>情人都 求重逢 重修破洞</p><p>如情人都 能重逢</p><p>情歌少很多 精彩内容</p><p>来 踏步空 早些心死 早上进</p><p>念念的 失约以后 我便我便放松</p><p>让挂念 葬在最深 白雪中</p><p>北极里有一束光</p><p>你话留来伴我望</p><p>我若觉得绝望</p><p>想到亦会全释放</p><p>这 绿里透紫的光</p><p>从未天际怒放</p><p>好比原来 一起能到达 的远方 而未同往</p><p>冰川未访 火山未闯</p><p>或我自负又气盛</p><p>冰岛的约会 纯属场错认</p><p>从没有留心听</p><p>情人话 重返的都市 无端改了姓</p><p>终不可碰面 无谓来怨命</p><p>从来并无洞察 旁人心底的本领</p><p>请你兑现约定 飞到伴我看星</p><p>往昔福岛名胜 破损中显神圣</p><p>谁人能为 昨日情</p><p>过十年后 再度远征</p><p>时间废墟里</p><p>寻那 不经污染 相恋的铁证</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>回顾二十年</title>
    <link href="/2022/20220627/"/>
    <url>/2022/20220627/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>emo后突然想写篇文章记录自己的二十年。</p><p>不管是快乐的经历也好还是不快乐的经历也好。</p><p>我一直觉得剖析自己是一件极痛苦的事情，我向来是不喜欢写日记的，虽然我现在断断续续地在写。</p><p>在这里立个flag。</p><hr><p>不想写了，搞得像写自传一样，谁年轻的时候写自传啊（笑</p><p>又不是弥留之际</p>]]></content>
    
    
    
    <tags>
      
      <tag>回忆</tag>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>爱欲之死</title>
    <link href="/2022/20220618/"/>
    <url>/2022/20220618/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《爱欲之死》是韩炳哲的一部哲学作品。看到一段较好的书评，故摘抄于此。</p><hr><p>我渴望找回爱人的能力，而不是把我全部的爱倾注在自然、四季、音乐、文学，在后者中投射太多自己的影子会大病一场。<span id="more"></span>我相信爱人才是唯一的解药，我愿跪倒在她的脚边，深深地吻它。让我们再次在心里默念吧，人类啊，相爱吧，我们都是长行的旅客，向着同一的归宿。 </p><blockquote><p>爱欲会激发一种自愿的忘我和自我牺牲。一种衰弱的感觉向坠入爱河的人的心头袭来，但同时一种变强的感觉接踵而至。这种双重的感觉不是“自我”营造的，而是他者的馈赠</p></blockquote><p>，</p><blockquote><p>当今的爱情不仅是被“女性化”那么简单。随着所有生活领域出现的一种积极化趋势，爱情也被驯化成一种消费模式，不存在风险，不考量胆识，杜绝疯癫和狂迷，避免产生任何消极和被否定的感觉。舒适的感觉和无须承担任何不良后果的刺激取代了痛苦和激情。在快餐式性交、邂逅后上床和舒压式做爱已经司空见惯的当代，性生活已经不存在任何消极面。消极面的缺失导致了当今爱情的枯萎，成了可消费、可计算的享乐主义的对象。人们满足于追求同好者的那份舒适，放弃了对他者的渴望。被追求的是一种舒服的、最终缓慢沉淀在意识之内的熟悉感。超验性在当今的爱情中不复存在。</p></blockquote><p>在当今这个世界里，我的同辈人不敬畏神、不相信爱情、更无视理想。他们简单的脑袋里盘桓的除了谋生便是享乐，对于任何美的感知都少的可怜。为此他们呼朋喝友，或在宿醉中度过青春年华、或在粗浅的短视频中消费光阴。</p><p>他们将世界想象成一幅平白的画卷，只要他们一天挨着一天过去，那些辗转难眠的忧虑将会随着时间消逝。</p><p>可是我们当下生存的世界，不管是虚拟的网络世界还是拥挤的现实世界都充满着种种陷阱，多少人为着原始积累设下层层机关等待着年轻人踏入。</p><p>那些吃人不吐骨头的996企业把年轻人个性揉碎、凹断他们刚萌芽的理想主义。意气奋发少年变成面颊凹陷的996工作机器。即使他们自作聪明的在工作时间浑水摸鱼拿到基本薪资，对于他们的整个生命来说简直输的一塌涂地。</p><p>因为即使再不求上进的员工也将自己的时间放在格子间，他没有机会和心力再去缠绵其他。只有披星戴月的身影出现在交通站台。</p><p>在他正当少年时那颗沸腾炙热的心早就被磨，在最该做爱的年纪他们蜷居在格子间，任凭电脑辐射着生殖器。而他们却像待宰的羔羊被打了麻醉剂，面对这样艰难糟糕的人生，竟也不做一丝反抗。</p><p>这才是这个时代最大的弊病，多少年轻人毕业后随着一身西装革履转眼变成大人。就连那颗跳动的心也枯萎死去。在这中途没有一丝反抗、没有一丝犹豫，仿佛他们生来就是如此。</p><p>于是，在我同辈人中少有爱情。莫说爱情，就连青春荷尔蒙催动下的性欲都无处释放。如果你走进他们细细拨开他们的生活，那彻底而重复的苍白会让你懊丧。很难想象一个二十多岁的生命会是这样粗鄙乏味。</p><p>这代看着金庸武侠长大的年轻人向往着佩剑于江湖却没能学会真正的英雄主义。他们无视于张无忌的勇敢，只是浅显的觉得张无忌是一个花心的渣男。他们不清楚也不懂得能够爱人本身就是一种真正的英雄主义</p><p>一个幼年触目双亲惨死的少年，一个幼年就得重病的小小少年，却在长大后能够不恨这个世界。没有成为一个杀人如麻的魔头，却能温柔的对待身边人，还能认真的活着，情意绵绵的爱着身边的姑娘。张无忌不似郭靖救济苍生却总能在我心中泛起阵阵涟漪</p><p>如果说上世纪有着纳粹主义和法西斯残害人的生命，那这个世纪就有低级娱乐腐蚀着人的心灵。对于人类来说每个时代都有着看似主流的毁灭性的打击。</p><p>从前是战争，现在是低级娱乐。前者是大规模的屠杀人的生命，后者则是普遍范围的弱化人的智力、撕裂人对一切人类尊严的基本诉求。</p><p>对于我的同辈中的许多人，没有人直接的告诉他们潦草一生，却都在潜移默化的将他们平庸化。因为他们现在的喜好都被某些精英预先的既定了。</p><p>假设他们不愿主动的寻找生命的其他可能性，那么对于他们来说一切生存的动机都将是别人赋予的思想枷锁。于是我同辈中的所有人都朝着镣铐的方向奔跑，满心以为这样便可以获得自由。</p><p>结果就是他们年轻的时候连躁动的荷尔蒙和那种圣洁而崇高的情感无法统一。我敢在这里保证，即使我同辈中的许多人连爱情的滋味都没有尝过就已经匆匆老去。同样他们连荷尔蒙触角缠绕躯体的眩晕感都来不及把握。</p><p>对于我同辈中的许多男性，他们的情感状况正如我前文中所述。而女性更是潜移默化成为一种社会资源。性别对立的矛盾被空前的放大，男女之间的信任感随着一段段失败的恋情逐渐耗损。</p><p>在这样一个紧张的时候“单身经济”应运而生，于是我们的同辈中许多人出现脱单意愿低迷，不相信爱情，随即就是颓唐潦草度过此生。</p><p>这是一场双方皆输的赌局，当年轻人不再相信爱情时，那么美好崇高的自然远离。留下的只有疾驰昂扬的消费型社会。</p><p>我不认为一个鼓吹女性权利、鼓吹单身的时代是一个好的时代。那些女王、女神为噱头的宣言背后则是资本狡黠的面容。</p><p>一个真正好的时代该是自由的，没有人去提女性权利问题，更没有将婚姻选择看成：“虽千万人吾往矣”的英雄主义。它该是一种人本真的自然需求。</p><p>是对于疲惫生活最英雄的个人主义、是充满人道情怀的互相怜惜、是一个人立于世对抗上位者制定的种种规则限制的最有力途径。</p><p>没有人告诉我们怎么活，可我们该知道怎么去爱，怎么将自己的生命贯穿于另外一个生命中。它不是依附、不是索求，该是一个生命对一个生命的尊重。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>误差传播定律</title>
    <link href="/2022/20220531/"/>
    <url>/2022/20220531/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在统计学中，误差传播是指变量的不确定性（或误差，更具体地说是随机误差）对基于它们的函数的不确定性的影响。</p><span id="more"></span><p>设$\vec{Y}&#x3D;f(\vec{X})$。</p><p>而$\sigma_y&#x3D;\frac{d(f(x))}{dx}\sigma_x$</p><p>我们通过一阶泰勒公式进行线性化：<br>$$<br>\vec{Y}&#x3D;f(\vec{X})\mathop\approx\limits^{\vec{X}&#x3D;\vec{\mu_X}} f(\mu_X)+f_x(\vec{X})(X-\vec{\mu_X})<br>$$<br>计算协方差有：<br>$$<br>cov(\vec{Y})&#x3D;cov(f(\vec{X}))&#x3D;cov(h(\mu_X))+cov(f_x(\vec{X})(X-\vec{\mu_X}))<br>\&#x3D;0+cov(f_x(\vec{X})(\vec{X}-\vec{\mu_X}))<br>\&#x3D;f_x(\vec{X})Cov(\vec{X})f_x(\vec{X})^T<br>$$</p><p>而$f_x(\vec{X})$其实就是雅克比矩阵J。</p><p>故进一步简化为:$\Sigma^\vec{Y}&#x3D;J\Sigma^\vec{X}J^T$</p><p>以上就是误差传播定律。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>20220522保研考研经验分享会</title>
    <link href="/2022/20220522/"/>
    <url>/2022/20220522/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>20220522保研考研经验分享会笔记。</p><p>演讲者：</p><p>胡月海 保研直博到同济大学城市交通学院<br>陈启瀚 保研到智能工程学院控制科学与工程专业<br>刘洁如 保研到上海交通大学医学院临床医学专业<br>吴冠达 考研至智能工程学院控制科学与工程<br>王秋璇 考研至智能工程学院交通运输。<span id="more"></span></p><h1 id="保研篇"><a href="#保研篇" class="headerlink" title="保研篇"></a>保研篇</h1><h2 id="演讲者：胡月海"><a href="#演讲者：胡月海" class="headerlink" title="演讲者：胡月海"></a>演讲者：胡月海</h2><h3 id="保研到底意味着什么"><a href="#保研到底意味着什么" class="headerlink" title="保研到底意味着什么"></a><strong>保研到底意味着什么</strong></h3><p>可以避免漫长而艰苦的考研生活与失败风险</p><p>可以拥有更多选择与机会，不用孤注一掷</p><p>可以拥有比较高压的第六学期和轻松的大四学年：</p><p>可以慢慢做毕设</p><p>提前适应研究生工作生活</p><h3 id="保研流程"><a href="#保研流程" class="headerlink" title="保研流程"></a><strong>保研流程</strong></h3><h4 id="前期准备："><a href="#前期准备：" class="headerlink" title="前期准备："></a><strong>前期准备：</strong></h4><p>简历、推荐信、提前联系导师</p><h4 id="夏令营："><a href="#夏令营：" class="headerlink" title="夏令营："></a><strong>夏令营：</strong></h4><p>多在5-7月，留意官网（学院官网、公众号）通知</p><p>疫情期间多线上面试，也有笔试、机试，不同专业的考核内容不同</p><h4 id="预推免："><a href="#预推免：" class="headerlink" title="预推免："></a><strong>预推免</strong>：</h4><p>难度比夏令营小一些，多在7-8月开展，有些学校可能不开展预推免，</p><h4 id="推免："><a href="#推免：" class="headerlink" title="推免："></a><strong>推免：</strong></h4><p>与预推免情况类似，名额所剩不多，建议以本院的推免阶段为保底</p><p>大多与大四上学期初开展</p><h4 id="考研复试"><a href="#考研复试" class="headerlink" title="考研复试:"></a><strong>考研复试:</strong></h4><p>未成功保研，夏令营成绩将转为考研复试成绩（同分优先录取）</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="资料准备："><a href="#资料准备：" class="headerlink" title="资料准备："></a>资料准备：</h4><p>时刻关注想去的学校学院官网，通过去年的夏令营公告了解去年的考核内容与资料要求。</p><p>准备相应的资料，如个人简历、<strong>老师推荐信</strong>、<strong>获奖证明材料</strong>等等。</p><p>设法润色自己的材料（大三下多参加点竞赛，努力考高六级成绩）</p><h4 id="夏令营考核准备"><a href="#夏令营考核准备" class="headerlink" title="夏令营考核准备"></a>夏令营考核准备</h4><p>多问问对应的师兄师姐，了解考核具体形式与内容（是否要面试、笔试或者机考）</p><p>针对夏令营考核内容制定合理的<strong>复习计划</strong></p><p>合理规划期末复习时间与夏令营综合复习时间（<strong>以免最后一学期翻车</strong>）</p><p>提前发邮件给自己心仪的导师，或找本院老师进行推荐、争取提前跟导师接触或者<strong>预面试</strong></p><h3 id="一些对比"><a href="#一些对比" class="headerlink" title="一些对比"></a>一些对比</h3><p><strong>好学习VS好专业好导师</strong></p><p>提供更多科研课题与资源，科研道路会更顺利</p><p><strong>读研VS直博</strong></p><p>多付出2年的时间成本，学业难度大</p><p>未来找到对口工作可能性更大，科研高度更高</p><p>但是找到好的导师非常重要</p><h2 id="演讲者-刘洁茹"><a href="#演讲者-刘洁茹" class="headerlink" title="演讲者 刘洁茹"></a>演讲者 刘洁茹</h2><h3 id="未来规划"><a href="#未来规划" class="headerlink" title="未来规划"></a>未来规划</h3><p>工作</p><p>升学：保研、考研、出国</p><h3 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h3><h4 id="收集信息"><a href="#收集信息" class="headerlink" title="收集信息"></a>收集信息</h4><p>官网</p><p>公众号（保研人、保研岛）</p><p>公众论坛</p><h4 id="申请材料"><a href="#申请材料" class="headerlink" title="申请材料"></a>申请材料</h4><p>突出个人优势（成绩、竞赛、科研）</p><p>注意时间节点、提交方式（邮寄、是否要老师前面）</p><h4 id="准备笔试"><a href="#准备笔试" class="headerlink" title="准备笔试"></a>准备笔试</h4><p>看看专业知识</p><h4 id="准备面试"><a href="#准备面试" class="headerlink" title="准备面试"></a>准备面试</h4><p>备稿，看看往届的经验</p><p>最好可以模拟一下</p><h3 id="心态调整"><a href="#心态调整" class="headerlink" title="心态调整"></a>心态调整</h3><p>积累经验</p><p>与老师同学朋友沟通</p><h3 id="绩点、科研、比赛"><a href="#绩点、科研、比赛" class="headerlink" title="绩点、科研、比赛"></a>绩点、科研、比赛</h3><h2 id="演讲者：陈启瀚"><a href="#演讲者：陈启瀚" class="headerlink" title="演讲者：陈启瀚"></a>演讲者：陈启瀚</h2><h3 id="学习篇：当个不挑食的吃货"><a href="#学习篇：当个不挑食的吃货" class="headerlink" title="学习篇：当个不挑食的吃货"></a>学习篇：当个不挑食的吃货</h3><ul><li>上课不挑食</li></ul><p>上课不挑食，公选也好，思政也好，除了极少数老师没备课，都要认真听讲</p><ul><li>互联网是个好东西</li></ul><p>明确需要解决什么问题，优先上网查找</p><ul><li>拉胯的某些考试</li></ul><p>任何需要死记硬背的科目（军理文化考试，思政考试，and需要背的专业课）</p><h3 id="科研竞赛篇"><a href="#科研竞赛篇" class="headerlink" title="科研竞赛篇"></a>科研竞赛篇</h3><h4 id="比赛队友带我飞"><a href="#比赛队友带我飞" class="headerlink" title="比赛队友带我飞"></a>比赛队友带我飞</h4><p>积极参与，主动交流解决问题。</p><blockquote><p>好队友很重要</p></blockquote><h4 id="科研老师指点迷津"><a href="#科研老师指点迷津" class="headerlink" title="科研老师指点迷津"></a>科研老师指点迷津</h4><p>要多尝试</p><blockquote><p>老师很重要，一定要抓住大创的机会好好搞。</p><p>要多主动。</p></blockquote><h4 id="分享使我快乐"><a href="#分享使我快乐" class="headerlink" title="分享使我快乐"></a>分享使我快乐</h4><p>CSDN，Github，B站</p><h3 id="保研奖学金篇"><a href="#保研奖学金篇" class="headerlink" title="保研奖学金篇"></a>保研奖学金篇</h3><h4 id="知道自己想要什么"><a href="#知道自己想要什么" class="headerlink" title="知道自己想要什么"></a>知道自己想要什么</h4><p>不断尝试发现自己的路</p><h4 id="知道自己能要什么"><a href="#知道自己能要什么" class="headerlink" title="知道自己能要什么"></a>知道自己能要什么</h4><p>清华北大本部去不了，深研院估计也难，所以演讲者选择了本校</p><h4 id="求不得的不会酸很久"><a href="#求不得的不会酸很久" class="headerlink" title="求不得的不会酸很久"></a>求不得的不会酸很久</h4><p>享受自己拥有的一切</p><h1 id="考研篇"><a href="#考研篇" class="headerlink" title="考研篇"></a>考研篇</h1><h2 id="演讲者：吴冠达"><a href="#演讲者：吴冠达" class="headerlink" title="演讲者：吴冠达"></a>演讲者：吴冠达</h2><h3 id="考研总流程"><a href="#考研总流程" class="headerlink" title="考研总流程"></a>考研总流程</h3><p>预报名：九月末（等同于正式报名）</p><p>正式报名（预报名过的不需再报名）</p><p>网上确认（提交证件照、身份证照片、手持身份证照片）</p><p>准考证下载（注意留好）</p><h4 id="初试"><a href="#初试" class="headerlink" title="初试"></a>初试</h4><p>第一天：政治、数学</p><p>第二天：英语、专业课</p><h4 id="初试后流程"><a href="#初试后流程" class="headerlink" title="初试后流程"></a>初试后流程</h4><p>公布成绩</p><p>复试材料</p><p>复试</p><h3 id="初试分享"><a href="#初试分享" class="headerlink" title="初试分享"></a>初试分享</h3><h4 id="政治"><a href="#政治" class="headerlink" title="政治"></a>政治</h4><p>8月-11月初：看视频（徐涛强化班），刷题（肖秀荣1000题），建议二刷</p><p>11月初-考前：做卷子，肖八选择题建议全刷（建议全对），大题可以不做</p><p>12月：背肖四论述题</p><h4 id="英语"><a href="#英语" class="headerlink" title="英语"></a>英语</h4><p>三月-考前：每天背单词</p><p>7月-11月：刷题，重点做阅读和新题型，有需要可以看一下阅读、语法长难句、作文的网课</p><p>11月到考前：刷套卷，写作文，被一些关键句型即可，无需全篇背诵</p><h4 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h4><p>3月-11月：看视频、刷题（张宇36讲）</p><p>10月-考前：刷套卷（真题、张宇8+4、李林6+4），一到两天一套，张宇过关版和考研难度差不多，高分版较难</p><h4 id="专业课"><a href="#专业课" class="headerlink" title="专业课"></a>专业课</h4><p>3月-考前：看自控视频（B站卢京潮），刷课后习题</p><p>9月-考前：数据结构看视频（王道）、刷题（概念、代码分析）</p><p>注：由于22考研本院的专业课改革，所以数据结构的复习才从9月开始</p><h3 id="复试分享"><a href="#复试分享" class="headerlink" title="复试分享"></a>复试分享</h3><p><strong>自我介绍：</strong></p><p>2~3分钟</p><p><strong>英语：</strong></p><p>翻译，建议提前背单词找感觉</p><p><strong>专业课：</strong></p><p>建议提前复习专业课，注重概念</p><p><strong>自由问答：</strong></p><p>毕设、自我介绍内容</p><h2 id="演讲者-：王秋璇"><a href="#演讲者-：王秋璇" class="headerlink" title="演讲者 ：王秋璇"></a>演讲者 ：王秋璇</h2><h3 id="信息收集"><a href="#信息收集" class="headerlink" title="信息收集"></a>信息收集</h3><p>学硕和专硕最好现在确定下来（学硕、专硕英语数学考的难度不一样）</p><h3 id="计划安排"><a href="#计划安排" class="headerlink" title="计划安排"></a>计划安排</h3><h4 id="英语-1"><a href="#英语-1" class="headerlink" title="英语"></a>英语</h4><p>只做真题</p><p><strong>单词</strong></p><p>重复3遍以上</p><p><strong>阅读</strong></p><p>每天1-2篇，限时15min</p><p>要多尝试,完型10min，翻译15-20min</p><p>**作文：**暑假后开始</p><p>先听课看范文</p><p>后尝试写作文，多练习做到可以不修改</p><h4 id="数学-1"><a href="#数学-1" class="headerlink" title="数学"></a>数学</h4><p>练习册推荐660和李林880，一般要做完1-2本练习册，以及二刷错题，不推荐汤家凤1800</p><p>高数课推荐武忠祥</p><p>线代课推荐李永乐</p><p>真题从8-9月开始做，限时训练，3h小时内完成，11月二刷</p><p>模拟题选用：</p><p>李林6+4、合工大超越</p><p>追求140以上可以做李艳芳</p><h4 id="时间安排"><a href="#时间安排" class="headerlink" title="时间安排"></a>时间安排</h4><p><strong>暑假前：</strong></p><p>平衡好课程与复习的关系</p><p><strong>暑假-实习：</strong></p><p>抓紧每一天备考</p><p><strong>实习后-11月</strong></p><p>经历保持7-9小时的学习时间</p><p><strong>12月：</strong></p><p>冲刺阶段，进入模拟考试阶段</p><h3 id="状态调整"><a href="#状态调整" class="headerlink" title="状态调整"></a>状态调整</h3><p>考研是一场持久战，“适当摆烂有助于更好的学习”</p><p>找到使自己能长时间学习的方法</p><p>找一个志同道合的研友很重要，可以共享经验、资料。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>考研</tag>
      
      <tag>学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>“励志”的高山低谷</title>
    <link href="/2022/20220515/"/>
    <url>/2022/20220515/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《高山低谷》由陈咏谦作词，林奕匡作曲并演唱<span id="more"></span>，是我的粤语歌Top1(目前暂时是)。</p><p>《声生不息》中由楊千嬅 、周笔畅 、单依纯三人演唱了这首歌曲，其实个人不太喜欢。没有节目组所说的味道“展现了渐行渐远的人之间的羁绊，不同的人生状态，讲述她们自己追求的那个高山，交织出逆境顺境中的无畏与勇敢”。感觉是“贵族游戏，街角游记”版的《高山低谷》，全无意味。</p><p>最后接一段古巨基的《欢乐今宵》，更是显得狗尾续貂。</p><p>事实上这并不是一首励志而是一首极绝望的歌曲。</p><blockquote><p>你快樂過生活 我拼命去生存</p><p>…</p><p>你界定了生活 我侮辱了生存</p><p>…</p><p>未見終點 也未見恩典 我與你極遠</p><p>…</p><p>我卻尚要生存 偷偷存活於山之谷等到某天魂斷</p><p>——《高山低谷》</p></blockquote><p>插句题外话，某天发现我喜欢的纯音乐作者Martin Czerny和陈咏谦不约而同,有一首《You live in luxury ,but I am starving》。我觉得这是“你快樂過生活 我拼命去生存”的很好的翻译。所以什么时候能有英文版呢。</p><p>当初林奕匡没有多大起色，外界反响平平。陈咏谦也收到了公司的明示：这可能是林奕匡的最后一支歌了。尽管陈咏谦和林奕匡不是第一次合作，但是谁也不能保证就能引起反响。陈咏谦当时其实也是有种当做遗书的味道来写，站在当时的林奕匡的角度来写。</p><blockquote><p>唱片公司有講過這或許是林奕匡的last shot，但你問我是否特別俾心機寫呢？其實又無，亦不會特別擺個靚陣出來，只不過，有如此夕陽的氣氛，就自然有如此感覺去寫一份歌詞，結果這種夕陽氣氛又剛好切合香港人共有的情緒，所以就成功了。</p><p>——陈咏谦</p></blockquote><p>虽然歌曲很绝望，但是歌曲创作者的经历显得很励志。</p><p>尽管林奕匡在一出道时获取了不菲的成绩，但是由于版权等种种原因又迅速陷入了低迷期。作为当初最后的一支歌，林奕匡唱不好就只能滚蛋，回到加拿大，从此将远离乐坛。很幸运，《高山低谷》反响极好，拯救了林奕匡。《高山低谷》为林奕匡获得了叱吒十大第十位及劲歌金曲奖，叱咤乐坛唱作人银奖。就近而说，在最近一年的叱咤乐坛最佳唱作人，前三甲林奕匡仍占有一席，诠释了从低谷到高山的逆袭。</p><p>陈咏谦虽然没有林奕匡这么低谷，前一年还打败了他的师傅——黄伟文获得了叱咤，但是当时也处于事业的平静期。《高山低谷》也给了陈咏谦挣得了一些奖项，让香港乐坛认识了一位出色的词人。</p><p>其实我们也能见到倘若林奕匡没有在当初步上高山，现在会怎样——同为新秀歌唱大赛温哥华选拔赛获奖者的骆振伟一直远离乐坛，直到出道13年才有第一首自己的歌。</p><p>林奕匡和陈咏谦后续也有一系列合作，如《查无此字》《难得一遇》。这就是后话了。</p><p>一颗乐坛唱作人新星就此冉冉升起。</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如果我听歌可眼红</title>
    <link href="/2022/20220514/"/>
    <url>/2022/20220514/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>H3M是陈奕迅09年的一张大碟，由当时和陈奕迅合作的乐队集体每人写一首歌所形成的唱片，有很多比较优秀的比如《今天只做一件事》《七百年后》《沙龙》《于心有愧》《不来也不去》。</p><span id="more"></span><p>十首歌中，林夕贡献了《不来也不去》《太阳照常升起》《于心有愧》。《于心有愧》，作为渣男三部曲之一，《于心有愧》可以看成是《富士山下》的续集。</p><blockquote><p>渣男三部曲：《富士山下》《再见二丁目》《于心有愧》</p><p>林夕不像Wyman一样喜欢写一系列的n部曲，以上都是坊间好事者凑的</p><p>同一张专辑中由黄伟文写就的《沙龙》倒是真实的n部曲——男玩四部曲之一</p></blockquote><p>从《富士山下》里的决绝决绝来往的男主，到《再见二丁目》释怀的女主，最终到《于心有愧》男主吐露饱受精神煎熬的心声。</p><blockquote><p>如若你非我不嫁 彼此终必火化——《富士山下》</p></blockquote><p>在我听课眼红的时候，我总是想起了你。我在当初怎么能狠心说出“如若你非我不嫁 彼此终必火化”。我当初宽慰你，但是没想到放不下的是我，我的心现在充斥着当初待你不好的愧疚感。</p><blockquote><p>忘掉我跟你恩怨 </p><p>樱花开了几转</p><p> 东京之旅一早比一世遥远</p><p>——《富士山下》</p></blockquote><blockquote><p>如果我听歌可眼红</p><p>何以待你好偏不懂</p><p>…</p><p>追悔无用</p><p>转眼发现 你失踪</p><p>——《于心有愧》</p></blockquote><p>我像个救世主一样，想拯救世界，可笑的是，其实需要被拯救的是自己。</p><p>像个刺猬一样，刺伤接近我的人。</p><blockquote><p>志助世人脱贫以为</p><p>便伟大到像多么有为</p><p>这种刺猬 连谁曾待我好</p><p>都可带来伤势</p><p>被我害过来接受我跪</p><p>是我在制造眼泪居然想救世</p><p>——《于心有愧》</p></blockquote><p>我知道我们不会再碰面，“全城来撞你，但最后处处由险阻”。我只希望有一天能得到你的原谅。只有当我某年某月安息于葬礼，你全家到齐是我的最大愿望。</p><blockquote><p>大概当初我未懂得顾忌</p><p>年少率性害惨你</p><p>令人受伤滋味 难保更可悲</p><p>这心地 再善良终生</p><p>怎去向你说对不起</p><p>…</p><p>内疚内疚内疚没作为</p><p>直到在某年某日</p><p>我能安息于葬礼</p><p>仍想你一家可到齐</p><p>——《于心有愧》</p></blockquote><hr><p>好吧其实上面都是瞎写的，其实是林夕写给母亲的。</p><hr><p>其实看MV又是另一番解读。</p><hr><p>歌词：</p><p>如果我听歌可眼红</p><p>何以待你好偏不懂</p><p>自细做过多少美梦</p><p>慈悲的伟论</p><p>连乞丐喊穷心也痛</p><p>竟怕放怀拥抱你</p><p>让你露欢容</p><p>追悔无用</p><p>转眼发现 你失踪</p><p>曾听说过 你某夜结婚</p><p>未曾露笑容</p><p>实在不敢知道我是元凶</p><p>大概当初我未懂得顾忌</p><p>年少率性害惨你</p><p>令人受伤滋味 难保更可悲</p><p>这心地 再善良终生</p><p>怎去向你说对不起</p><p>良心有愧 原来随便错手</p><p>可毁了人一世</p><p>立志助世人脱贫以为</p><p>便伟大到像多么有为</p><p>这种刺猬 连谁曾待我好</p><p>都可带来伤势</p><p>被我害过来接受我跪</p><p>是我在制造眼泪居然想救世</p><p>就算积蓄献尽饥荒赤地</p><p>而太多债没处理</p><p>累人累己滋味</p><p>余生也记起</p><p>数一数</p><p>我实情不只得你要说句对不起</p><p>良心有愧 原来随便错手</p><p>可毁了人一世</p><p>立志助世人脱贫以为</p><p>便伟大到像多么有为</p><p>这种刺猬 连谁曾待我好</p><p>都可带来伤势</p><p>被我害过来接受我跪</p><p>是我在制造眼泪居然想救世</p><p>于心有愧 原来随便错手</p><p>可毁了人一世</p><p>立志助世人脱贫以为</p><p>便伟大到像多么有为</p><p>这种刺猬 连谁曾待我好</p><p>都可带来伤势</p><p>内疚内疚内疚没作为</p><p>直到在某年某日</p><p>我能安息于葬礼</p><p>仍想你一家可到齐</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>实践论</title>
    <link href="/2022/20220513/"/>
    <url>/2022/20220513/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p align="center">实践论</p><p align="center">论认识和实践的关系——知和行的关系</p><p align="center">（一九三七年七月）</p><span id="more"></span><blockquote><p>$\ \ $ *在中国共产党内，曾经有一部分教条主义的同志长期拒绝中国革命的经验，否认“马克思主义不是教条而是行动的指南”这个真理，而只生吞活剥马克思主义书籍中的只言片语，去吓唬人们。还有另一部分经验主义的同志长期拘守于自身的片断经验，不了解理论对于革命实践的重要性，看不见革命的全局，虽然也是辛苦地——但却是盲目地在工作。这两类同志的错误思想，特别是教条主义思想，曾经在一九三一年至一九三四年使得中国革命受了极大的损失，而教条主义者却是披着马克思主义的外衣迷惑了广大的同志。毛泽东的《实践论》，是为着用马克思主义的认识论观点去揭露党内的教条主义和经验主义——特别是教条主义这些主观主义的错误而写的。因为重点是揭露看轻实践的教条主义这种主观主义，故题为《实践论》。毛泽东曾以这篇论文的观点在延安的抗日军事政治大学作过讲演。</p></blockquote><p>　　马克思以前的唯物论，离开人的社会性，离开人的历史发展，去观察认识问题，因此不能了解认识对社会实践的依赖关系，即认识对生产和阶级斗争的依赖关系。<br>　　首先，马克思主义者认为人类的生产活动是最基本的实践活动，是决定其它一切活动的东西。人的认识，主要地依赖于物质的生产活动，逐渐地了解自然的现象、自然的性质、自然的规律性、人和自然的关系；而且经过生产活动，也在各种不同程度上逐渐地认识了人和人的一定的相互关系。一切这些知识，离开生产活动是不能得到的。在没有阶级的社会中，每个人以社会一员的资格，同其它社会成员协力，结成一定的生产关系，从事生产活动，以解决人类物质生活问题。在各种阶级的社会中，各阶级的社会成员，则又以各种不同的方式，结成一定的生产关系，从事生产活动，以解决人类物质生活问题。这是人的认识发展的基本来源。<br>　　人的社会实践，不限于生产活动一种形式，还有多种其它的形式，阶级斗争，政治生活，科学和艺术的活动，总之社会实际生活的一切领域都是社会的人所参加的。因此，人的认识，在物质生活以外，还从政治生活文化生活中（与物质生活密切联系），在各种不同程度上，知道人和人的各种关系。其中，尤以各种形式的阶级斗争，给予人的认识发展以深刻的影响。在阶级社会中，每一个人都在一定的阶级地位中生活，各种思想无不打上阶级的烙印。<br>　　马克思主义者认为人类社会的生产活动，是一步又一步地由低级向高级发展，因此，人们的认识，不论对于自然界方面，对于社会方面，也都是一步又一步地由低级向高级发展，即由浅入深，由片面到更多的方面。在很长的历史时期内，大家对于社会的历史只能限于片面的了解，这一方面是由于剥削阶级的偏见经常歪曲社会的历史，另方面，则由于生产规模的狭小，限制了人们的眼界。人们能够对于社会历史的发展作全面的历史的了解，把对于社会的认识变成了科学，这只是到了伴随巨大生产力——大工业而出现近代无产阶级的时候，这就是马克思主义的科学。<br>　　马克思主义者认为，只有人们的社会实践，才是人们对于外界认识的真理性的标准。实际的情形是这样的，只有在社会实践过程中（物质生产过程中，阶级斗争过程中，科学实验过程中），人们达到了思想中所预想的结果时，人们的认识才被证实了。人们要想得到工作的胜利即得到预想的结果，一定要使自己的思想合于客观外界的规律性，如果不合，就会在实践中失败。人们经过失败之后，也就从失败取得教训，改正自己的思想使之适合于外界的规律性，人们就能变失败为胜利，所谓“失败者成功之母”，“吃一堑长一智”，就是这个道理。辩证唯物论的认识论把实践提到第一的地位，认为人的认识一点也不能离开实践，排斥一切否认实践重要性、使认识离开实践的错误理论。列宁这样说过：“实践高于（理论的）认识，因为它不但有普遍性的品格，而且还有直接现实性的品格。”[1]马克思主义的哲学辩证唯物论有两个最显着的特点：一个是它的阶级性，公然申明辩证唯物论是为无产阶级服务的；再一个是它的实践性，强调理论对于实践的依赖关系，理论的基础是实践，又转过来为实践服务。判定认识或理论之是否真理，不是依主观上觉得如何而定，而是依客观上社会实践的结果如何而定。真理的标准只能是社会的实践。实践的观点是辩证唯物论的认识论之第一的和基本的观点[2]。<br>　　然而人的认识究竟怎样从实践发生，而又服务于实践呢？这只要看一看认识的发展过程就会明了的。<br>　　原来人在实践过程中，开始只是看到过程中各个事物的现象方面，看到各个事物的片面，看到各个事物之间的外部联系。例如有些外面的人们到延安来考察，头一二天，他们看到了延安的地形、街道、屋宇，接触了许多的人，参加了宴会、晚会和群众大会，听到了各种说话，看到了各种文件，这些就是事物的现象，事物的各个片面以及这些事物的外部联系。这叫做认识的感性阶段，就是感觉和印象的阶段。也就是延安这些各别的事物作用于考察团先生们的感官，引起了他们的感觉，在他们的脑子中生起了许多的印象，以及这些印象间的大概的外部的联系，这是认识的第一个阶段。在这个阶段中，人们还不能造成深刻的概念，作出合乎论理（即合乎逻辑）的结论。<br>　　社会实践的继续，使人们在实践中引起感觉和印象的东西反复了多次，于是在人们的脑子里生起了一个认识过程中的突变（即飞跃），产生了概念。概念这种东西已经不是事物的现象，不是事物的各个片面，不是它们的外部联系，而是抓着了事物的本质，事物的全体，事物的内部联系了。概念同感觉，不但是数量上的差别，而且有了性质上的差别。循此继进，使用判断和推理的方法，就可产生出合乎论理的结论来。《三国演义》上所谓“眉头一皱计上心来”，我们普通说话所谓“让我想一想”，就是人在脑子中运用概念以作判断和推理的工夫。这是认识的第二个阶段。外来的考察团先生们在他们集合了各种材料，加上他们“想了一想”之后，他们就能够作出“共产党的抗日民族统一战线的政策是彻底的、诚恳的和真实的”这样一个判断了。在他们作出这个判断之后，如果他们对于团结救国也是真实的的话，那末他们就能够进一步作出这样的结论：“抗日民族统一战线是能够成功的。”这个概念、判断和推理的阶段，在人们对于一个事物的整个认识过程中是更重要的阶段，也就是理性认识的阶段。认识的真正任务在于经过感觉而到达于思维，到达于逐步了解客观事物的内部矛盾，了解它的规律性，了解这一过程和那一过程间的内部联系，即到达于论理的认识。重复地说，论理的认识所以和感性的认识不同，是因为感性的认识是属于事物之片面的、现象的、外部联系的东西，论理的认识则推进了一大步，到达了事物的全体的、本质的、内部联系的东西，到达了暴露周围世界的内在的矛盾，因而能在周围世界的总体上，在周围世界一切方面的内部联系上去把握周围世界的发展。<br>　　这种基于实践的由浅入深的辩证唯物论的关于认识发展过程的理论，在马克思主义以前，是没有一个人这样解决过的。马克思主义的唯物论，第一次正确地解决了这个问题，唯物地而且辩证地指出了认识的深化的运动，指出了社会的人在他们的生产和阶级斗争的复杂的、经常反复的实践中，由感性认识到论理认识的推移的运动。列宁说过：“物质的抽象，自然规律的抽象，价值的抽象以及其它等等，一句话，一切科学的（正确的、郑重的、非瞎说的）抽象，都更深刻、更正确、更完全地反映着自然。”[3]马克思列宁主义认为：认识过程中两个阶段的特性，在低级阶段，认识表现为感性的，在高级阶段，认识表现为论理的，但任何阶段，都是统一的认识过程中的阶段。感性和理性二者的性质不同，但又不是互相分离的，它们在实践的基础上统一起来了。我们的实践证明：感觉到了的东西，我们不能立刻理解它，只有理解了的东西才更深刻地感觉它。感觉只解决现象问题，理论才解决本质问题。这些问题的解决，一点也不能离开实践。无论何人要认识什么事物，除了同那个事物接触，即生活于（实践于）那个事物的环境中，是没有法子解决的。不能在封建社会就预先认识资本主义社会的规律，因为资本主义还未出现，还无这种实践。马克思主义只能是资本主义社会的产物。马克思不能在自由资本主义时代就预先具体地认识帝国主义时代的某些特异的规律，因为帝国主义这个资本主义最后阶段还未到来，还无这种实践，只有列宁和斯大林才能担当此项任务。马克思、恩格斯、列宁、斯大林之所以能够作出他们的理论，除了他们的天才条件之外，主要地是他们亲自参加了当时的阶级斗争和科学实验的实践，没有这后一个条件，任何天才也是不能成功的。“秀才不出门，全知天下事”，在技术不发达的古代只是一句空话，在技术发达的现代虽然可以实现这句话，然而真正亲知的是天下实践着的人，那些人在他们的实践中间取得了“知”，经过文字和技术的传达而到达于“秀才”之手，秀才乃能间接地“知天下事”。如果要直接地认识某种或某些事物，便只有亲身参加于变革现实、变革某种或某些事物的实践的斗争中，才能触到那种或那些事物的现象，也只有在亲身参加变革现实的实践的斗争中，才能暴露那种或那些事物的本质而理解它们。这是任何人实际上走着的认识路程，不过有些人故意歪曲地说些反对的话罢了。世上最可笑的是那些“知识里手”[4]，有了道听途说的一知半解，便自封为“天下第一”，适足见其不自量而已。知识的问题是一个科学问题，来不得半点的虚伪和骄傲，决定地需要的倒是其反面——诚实和谦逊的态度。你要有知识，你就得参加变革现实的实践。你要知道梨子的滋味，你就得变革梨子，亲口吃一吃。你要知道原子的组织同性质，你就得实行物理学和化学的实验，变革原子的情况。你要知道革命的理论和方法，你就得参加革命。一切真知都是从直接经验发源的。但人不能事事直接经验，事实上多数的知识都是间接经验的东西，这就是一切古代的和外域的知识。这些知识在古人在外人是直接经验的东西，如果在古人外人直接经验时是符合于列宁所说的条件“科学的抽象”，是科学地反映了客观的事物，那末这些知识是可靠的，否则就是不可靠的。所以，一个人的知识，不外直接经验的和间接经验的两部分。而且在我为间接经验者，在人则仍为直接经验。因此，就知识的总体说来，无论何种知识都是不能离开直接经验的。任何知识的来源，在于人的肉体感官对客观外界的感觉，否认了这个感觉，否认了直接经验，否认亲自参加变革现实的实践，他就不是唯物论者。“知识里手”之所以可笑，原因就是在这个地方。中国人有一句老话：“不入虎穴，焉得虎子。”这句话对于人们的实践是真理，对于认识论也是真理。离开实践的认识是不可能的。<br>　　为了明了基于变革现实的实践而产生的辩证唯物论的认识运动——认识的逐渐深化的运动，下面再举出几个具体的例子。<br>　　无产阶级对于资本主义社会的认识，在其实践的初期——破坏机器和自发斗争时期，他们还只在感性认识的阶段，只认识资本主义各个现象的片面及其外部的联系。这时，他们还是一个所谓“自在的阶级”。但是到了他们实践的第二个时期——有意识有组织的经济斗争和政治斗争的时期，由于实践，由于长期斗争的经验，经过马克思、恩格斯用科学的方法把这种种经验总结起来，产生了马克思主义的理论，用以教育无产阶级，这样就使无产阶级理解了资本主义社会的本质，理解了社会阶级的剥削关系，理解了无产阶级的历史任务，这时他们就变成了一个“自为的阶级”。<br>　　中国人民对于帝国主义的认识也是这样。第一阶段是表面的感性的认识阶段，表现在太平天国运动和义和团运动等笼统的排外主义的斗争上[5]。第二阶段才进到理性的认识阶段，看出了帝国主义内部和外部的各种矛盾，并看出了帝国主义联合中国买办阶级和封建阶级以压榨中国人民大众的实质，这种认识是从一九一九年五四运动[6]前后才开始的。<br>　　我们再来看战争。战争的领导者，如果他们是一些没有战争经验的人，对于一个具体的战争（例如我们过去十年的土地革命战争）的深刻的指导规律，在开始阶段是不了解的。他们在开始阶段只是身历了许多作战的经验，而且败仗是打得很多的。然而由于这些经验（胜仗，特别是败仗的经验），使他们能够理解贯串整个战争的内部的东西，即那个具体战争的规律性，懂得了战略和战术，因而能够有把握地去指导战争。此时，如果改换一个无经验的人去指导，又会要在吃了一些败仗之后（有了经验之后）才能理会战争的正确的规律。<br>　　常常听到一些同志在不能勇敢接受工作任务时说出来的一句话：没有把握。为什么没有把握呢？因为他对于这项工作的内容和环境没有规律性的了解，或者他从来就没有接触过这类工作，或者接触得不多，因而无从谈到这类工作的规律性。及至把工作的情况和环境给以详细分析之后，他就觉得比较地有了把握，愿意去做这项工作。如果这个人在这项工作中经过了一个时期，他有了这项工作的经验了，而他又是一个肯虚心体察情况的人，不是一个主观地、片面地、表面地看问题的人，他就能够自己做出应该怎样进行工作的结论，他的工作勇气也就可以大大地提高了。只有那些主观地、片面地和表面地看问题的人，跑到一个地方，不问环境的情况，不看事情的全体（事情的历史和全部现状），也不触到事情的本质（事情的性质及此一事情和其它事情的内部联系），就自以为是地发号施令起来，这样的人是没有不跌交子的。<br>　　由此看来，认识的过程，第一步，是开始接触外界事情，属于感觉的阶段。第二步，是综合感觉的材料加以整理和改造，属于概念、判断和推理的阶段。只有感觉的材料十分丰富（不是零碎不全）和合于实际（不是错觉），才能根据这样的材料造出正确的概念和论理来。<br>　　这里有两个要点必须着重指明。第一个，在前面已经说过的，这里再重复说一说，就是理性认识依赖于感性认识的问题。如果以为理性认识可以不从感性认识得来，他就是一个唯心论者。哲学史上有所谓“唯理论”一派，就是只承认理性的实在性，不承认经验的实在性，以为只有理性靠得住，而感觉的经验是靠不住的，这一派的错误在于颠倒了事实。理性的东西所以靠得住，正是由于它来源于感性，否则理性的东西就成了无源之水，无本之木，而只是主观自生的靠不住的东西了。从认识过程的秩序说来，感觉经验是第一的东西，我们强调社会实践在认识过程中的意义，就在于只有社会实践才能使人的认识开始发生，开始从客观外界得到感觉经验。一个闭目塞听、同客观外界根本绝缘的人，是无所谓认识的。认识开始于经验——这就是认识论的唯物论。<br>　　第二是认识有待于深化，认识的感性阶段有待于发展到理性阶段——这就是认识论的辩证法[7]。如果以为认识可以停顿在低级的感性阶段，以为只有感性认识可靠，而理性认识是靠不住的，这便是重复了历史上的“经验论”的错误。这种理论的错误，在于不知道感觉材料固然是客观外界某些真实性的反映（我这里不来说经验只是所谓内省体验的那种唯心的经验论），但它们仅是片面的和表面的东西，这种反映是不完全的，是没有反映事物本质的。要完全地反映整个的事物，反映事物的本质，反映事物的内部规律性，就必须经过思考作用，将丰富的感觉材料加以去粗取精、去伪存真、由此及彼、由表及里的改造制作工夫，造成概念和理论的系统，就必须从感性认识跃进到理性认识。这种改造过的认识，不是更空虚了更不可靠了的认识，相反，只要是在认识过程中根据于实践基础而科学地改造过的东西，正如列宁所说乃是更深刻、更正确、更完全地反映客观事物的东西。庸俗的事务主义家不是这样，他们尊重经验而看轻理论，因而不能通观客观过程的全体，缺乏明确的方针，没有远大的前途，沾沾自喜于一得之功和一孔之见。这种人如果指导革命，就会引导革命走上碰壁的地步。<br>　　理性认识依赖于感性认识，感性认识有待于发展到理性认识，这就是辩证唯物论的认识论。哲学上的“唯理论”和“经验论”都不懂得认识的历史性或辩证性，虽然各有片面的真理（对于唯物的唯理论和经验论而言，非指唯心的唯理论和经验论），但在认识论的全体上则都是错误的。由感性到理性之辩证唯物论的认识运动，对于一个小的认识过程（例如对于一个事物或一件工作的认识）是如此，对于一个大的认识过程（例如对于一个社会或一个革命的认识）也是如此。<br>　　然而认识运动至此还没有完结。辩证唯物论的认识运动，如果只到理性认识为止，那末还只说到问题的一半。而且对于马克思主义的哲学说来，还只说到非十分重要的那一半。马克思主义的哲学认为十分重要的问题，不在于懂得了客观世界的规律性，因而能够解释世界，而在于拿了这种对于客观规律性的认识去能动地改造世界。在马克思主义看来，理论是重要的，它的重要性充分地表现在列宁说过的一句话：“没有革命的理论，就不会有革命的运动。”[8]然而马克思主义看重理论，正是，也仅仅是，因为它能够指导行动。如果有了正确的理论，只是把它空谈一阵，束之高阁，并不实行，那末，这种理论再好也是没有意义的。认识从实践始，经过实践得到了理论的认识，还须再回到实践去。认识的能动作用，不但表现于从感性的认识到理性的认识之能动的飞跃，更重要的还须表现于从理性的认识到革命的实践这一个飞跃。抓着了世界的规律性的认识，必须把它再回到改造世界的实践中去，再用到生产的实践、革命的阶级斗争和民族斗争的实践以及科学实验的实践中去。这就是检验理论和发展理论的过程，是整个认识过程的继续。理论的东西之是否符合于客观真理性这个问题，在前面说的由感性到理性之认识运动中是没有完全解决的，也不能完全解决的。要完全地解决这个问题，只有把理性的认识再回到社会实践中去，应用理论于实践，看它是否能够达到预想的目的。许多自然科学理论之所以被称为真理，不但在于自然科学家们创立这些学说的时候，而且在于为尔后的科学实践所证实的时候。马克思列宁主义之所以被称为真理，也不但在于马克思、恩格斯、列宁、斯大林等人科学地构成这些学说的时候，而且在于为尔后革命的阶级斗争和民族斗争的实践所证实的时候。辩证唯物论之所以为普遍真理，在于经过无论什么人的实践都不能逃出它的范围。人类认识的历史告诉我们，许多理论的真理性是不完全的，经过实践的检验而纠正了它们的不完全性。许多理论是错误的，经过实践的检验而纠正其错误。所谓实践是真理的标准，所谓“生活、实践底观点，应该是认识论底首先的和基本的观点”[9]，理由就在这个地方。斯大林说得好：“理论若不和革命实践联系起来，就会变成无对象的理论，同样，实践若不以革命理论为指南，就会变成盲目的实践。”[10]<br>　　说到这里，认识运动就算完成了吗？我们的答复是完成了，又没有完成。社会的人们投身于变革在某一发展阶段内的某一客观过程的实践中（不论是关于变革某一自然过程的实践，或变革某一社会过程的实践），由于客观过程的反映和主观能动性的作用，使得人们的认识由感性的推移到了理性的，造成了大体上相应于该客观过程的法则性的思想、理论、计划或方案，然后再应用这种思想、理论、计划或方案于该同一客观过程的实践，如果能够实现预想的目的，即将预定的思想、理论、计划、方案在该同一过程的实践中变为事实，或者大体上变为事实，那末，对于这一具体过程的认识运动算是完成了。例如，在变革自然的过程中，某一工程计划的实现，某一科学假想的证实，某一器物的制成，某一农产的收获，在变革社会过程中某一罢工的胜利，某一战争的胜利，某一教育计划的实现，都算实现了预想的目的。然而一般地说来，不论在变革自然或变革社会的实践中，人们原定的思想、理论、计划、方案，毫无改变地实现出来的事，是很少的。这是因为从事变革现实的人们，常常受着许多的限制，不但常常受着科学条件和技术条件的限制，而且也受着客观过程的发展及其表现程度的限制（客观过程的方面及本质尚未充分暴露）。在这种情形之下，由于实践中发现前所未料的情况，因而部分地改变思想、理论、计划、方案的事是常有的，全部地改变的事也是有的。即是说，原定的思想、理论、计划、方案，部分地或全部地不合于实际，部分错了或全部错了的事，都是有的。许多时候须反复失败过多次，才能纠正错误的认识，才能到达于和客观过程的规律性相符合，因而才能够变主观的东西为客观的东西，即在实践中得到预想的结果。但是不管怎样，到了这种时候，人们对于在某一发展阶段内的某一客观过程的认识运动，算是完成了。<br>　　然而对于过程的推移而言，人们的认识运动是没有完成的。任何过程，不论是属于自然界的和属于社会的，由于内部的矛盾和斗争，都是向前推移向前发展的，人们的认识运动也应跟着推移和发展。依社会运动来说，真正的革命的指导者，不但在于当自己的思想、理论、计划、方案有错误时须得善于改正，如同上面已经说到的，而且在于当某一客观过程已经从某一发展阶段向另一发展阶段推移转变的时候，须得善于使自己和参加革命的一切人员在主观认识上也跟着推移转变，即是要使新的革命任务和新的工作方案的提出，适合于新的情况的变化。革命时期情况的变化是很急速的，如果革命党人的认识不能随之而急速变化，就不能引导革命走向胜利。<br>　　然而思想落后于实际的事是常有的，这是因为人的认识受了许多社会条件的限制的缘故。我们反对革命队伍中的顽固派，他们的思想不能随变化了的客观情况而前进，在历史上表现为右倾机会主义。这些人看不出矛盾的斗争已将客观过程推向前进了，而他们的认识仍然停止在旧阶段。一切顽固党的思想都有这样的特征。他们的思想离开了社会的实践，他们不能站在社会车轮的前头充任向导的工作，他们只知跟在车子后面怨恨车子走得太快了，企图把它向后拉，开倒车。<br>　　我们也反对“左”翼空谈主义。他们的思想超过客观过程的一定发展阶段，有些把幻想看作真理，有些则把仅在将来有现实可能性的理想，勉强地放在现时来做，离开了当前大多数人的实践，离开了当前的现实性，在行动上表现为冒险主义。<br>　　唯心论和机械唯物论，机会主义和冒险主义，都是以主观和客观相分裂，以认识和实践相脱离为特征的。以科学的社会实践为特征的马克思列宁主义的认识论，不能不坚决反对这些错误思想。马克思主义者承认，在绝对的总的宇宙发展过程中，各个具体过程的发展都是相对的，因而在绝对真理的长河中，人们对于在各个一定发展阶段上的具体过程的认识只具有相对的真理性。无数相对的真理之总和，就是绝对的真理[11]。客观过程的发展是充满着矛盾和斗争的发展，人的认识运动的发展也是充满着矛盾和斗争的发展。一切客观世界的辩证法的运动，都或先或后地能够反映到人的认识中来。社会实践中的发生、发展和消灭的过程是无穷的，人的认识的发生、发展和消灭的过程也是无穷的。根据于一定的思想、理论、计划、方案以从事于变革客观现实的实践，一次又一次地向前，人们对于客观现实的认识也就一次又一次地深化。客观现实世界的变化运动永远没有完结，人们在实践中对于真理的认识也就永远没有完结。马克思列宁主义并没有结束真理，而是在实践中不断地开辟认识真理的道路。我们的结论是主观和客观、理论和实践、知和行的具体的历史的统一，反对一切离开具体历史的“左”的或右的错误思想。<br>　　社会的发展到了今天的时代，正确地认识世界和改造世界的责任，已经历史地落在无产阶级及其政党的肩上。这种根据科学认识而定下来的改造世界的实践过程，在世界、在中国均已到达了一个历史的时节——自有历史以来未曾有过的重大时节，这就是整个儿地推翻世界和中国的黑暗面，把它们转变过来成为前所未有的光明世界。无产阶级和革命人民改造世界的斗争，包括实现下述的任务：改造客观世界，也改造自己的主观世界——改造自己的认识能力，改造主观世界同客观世界的关系。地球上已经有一部分实行了这种改造，这就是苏联。他们还正在促进这种改造过程。中国人民和世界人民也都正在或将要通过这样的改造过程。所谓被改造的客观世界，其中包括了一切反对改造的人们，他们的被改造，须要通过强迫的阶段，然后才能进入自觉的阶段。世界到了全人类都自觉地改造自己和改造世界的时候，那就是世界的共产主义时代。<br>　　通过实践而发现真理，又通过实践而证实真理和发展真理。从感性认识而能动地发展到理性认识，又从理性认识而能动地指导革命实践，改造主观世界和客观世界。实践、认识、再实践、再认识，这种形式，循环往复以至无穷，而实践和认识之每一循环的内容，都比较地进到了高一级的程度。这就是辩证唯物论的全部认识论，这就是辩证唯物论的知行统一观。</p><hr><p><strong>注释</strong></p><p>[1]见列宁《黑格尔〈逻辑学〉一书摘要》。新的译文是：“实践高于（理论的）认识，因为它不仅具有普遍性的品格，而且还具有直接现实性的品格。”（《列宁全集》第55卷，人民出版社1990年版，第183页）</p><p>[2] 参见马克思《关于费尔巴哈的提纲》（《马克思恩格斯选集》第1卷，人民出版社1972年版，第16—19页）和列宁《唯物主义和经验批判主义》第二章第六节（《列宁全集》第18卷，人民出版社1988年版，第144页）。</p><p>[3]见列宁《黑格尔〈逻辑学〉一书摘要》（《列宁全集》第55卷，人民出版社1990年版，第142页）。</p><p>[4]里手，湖南方言，内行的意思。</p><p>[5]说：“《实践论》中将太平天国放在排外主义一起说不妥，出选集时拟加修改，此处暂仍照原。”</p><p>[6]五四运动是一九一九年五月四日发生的反帝反封建的爱国运动。当时，第一次世界大战刚刚结束，英、美、法、日、意等战胜国在巴黎召开对德和会，决定由日本继承德国在中国山东的特权。中国是参加对德宣战的战胜国之一，但北洋军阀政府却准备接受这个决定。五月四日，北京学生游行示威，反对帝国主义的这一无理决定和北洋军阀政府的妥协。这次运动迅速地获得了全国人民的响应，到六月三日以后，发展成为有工人阶级、城市小资产阶级和民族资产阶级参加的广大群众性的反帝反封建的爱国运动。五四运动也是反对封建文化的新文化运动。以一九一五年《青年杂志》（后改名《新青年》）创刊为起点的新文化运动，竖起“民主”和“科学”的旗帜，反对旧道德，提倡新道德，反对旧文学，提倡新文学。五四运动中的先进分子接受了马克思主义，使新文化运动发展成为马克思主义思想运动，他们致力于马克思主义同中国工人运动相结合，在思想上和干部上准备了中国共产党的成立。</p><p>[7]参见列宁《黑格尔〈逻辑学〉一书摘要》：“要理解，就必须从经验开始理解、研究，从经验上升到一般。”（《列宁全集》第55卷，人民出版社1990年版，第175页）</p><p>[8]见列宁《俄国社会民主党人的任务》（《列宁全集》第2卷，人民出版社1984年版，第443页）；并见列宁《怎么办？》第一章第四节（《列宁全集》第6卷，人民出版社1986年版，第23页）。</p><p>[9]见列宁《唯物主义和经验批判主义》第二章第六节（《列宁全集》第18卷，人民出版社1988年版，第144页）。</p><p>[10]见斯大林《论列宁主义基础》第三部分《理论》。新的译文是：“离开革命实践的理论是空洞的理论，而不以革命理论为指南的实践是盲目的实践。”（《斯大林选集》上卷，人民出版社1979年版，第199—200页）</p><p>[11]参见列宁《唯物主义和经验批判主义》第二章第五节。原文是：“人类思维按其本性是能够给我们提供并且正在提供由相对真理的总和所构成的绝对真理的。”（《列宁全集》第18卷，人民出版社1988年版，第135页）</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
      <tag>哲学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>银河修理员</title>
    <link href="/2022/20220509/"/>
    <url>/2022/20220509/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>​                                </p><div id="aplayer-YXFwbBsF" class="aplayer aplayer-tag-marker" style="margin-bottom: 20px;"><pre class="aplayer-lrc-content">[00:00.000] 作词 : 黄伟文[00:01.000] 作曲 : Howie@Dear Jane[00:02.14]编曲 : Dear Jane[00:15.10]除了会痛一切都美好[00:18.06]除了挫折面前仍有路[00:21.45]除了厌世总有某些 修补可以做[00:28.60]残破世界令人学成 悲观中找鼓舞[00:33.88]来纾减身边恐怖[00:37.61]能照料你日子都不算糟[00:42.84]尽量去弥补 难逃那烦恼[00:48.78]修修补补乱世中 一起苍老[00:53.85]沿途在 修理着熄了的曙光[00:59.02]祝你在乱流下平安[01:02.58]真爱是任何形状[01:05.41]对付百孔千疮[01:08.08]谁能望穿我[01:11.89]这种坚壮非坚壮[01:15.71]形势坏透只好对抗[01:19.05]由我硬撑着 使你心安[01:30.37]谁也破了等某位去补[01:33.65]而你有我保养和爱慕[01:37.16]缝了再破穿了再补[01:39.71]这乱世未必可修理好[01:44.66]绝望里乐观 亦是个情操[01:50.55]东歪西倒但至少 牵手偕老[01:55.83]沿途在 修理着熄了的曙光[02:00.78]祝你在乱流下平安[02:04.23]真爱是任何形状[02:07.12]对付百孔千疮[02:09.85]谁能望穿我[02:13.69]这种坚壮非坚壮[02:17.35]形势坏透只好对抗[02:20.68]由我硬撑着 使你心安[02:32.70]漂亮的天真鲁莽[02:35.95]若被推倒可再装[02:38.96]巨匠的手不怕肮脏[02:46.34]贴着胶纸都俊朗[02:49.68]尽是补钉都发光[02:52.66]结局再破烂同奔往[02:57.40]银河上 边跌宕边看紧对方[03:02.56]跨宇宙又横越洪荒[03:06.12]不怕在尽头无岸[03:09.01]远近我都护航[03:12.00]还能互安慰[03:15.49]不必天气多清朗[03:18.99]狂雨暴雪一起对抗[03:22.49]任岁月再坏 不致心慌</pre></div><script>var ap = new APlayer({element: document.getElementById("aplayer-YXFwbBsF"),narrow: false,autoplay: false,showlrc: 2,music: {title: "银河修理员",author: "Dear Jane",url: "/music/银河修理员.mp3",pic: "/images/Galactic_Repairman.png",}});window.aplayers || (window.aplayers = []);window.aplayers.push(ap);</script><span id="more"></span><hr><p>歌词：</p><p>除了会痛一切都美好<br>除了挫折面前仍有路<br>除了厌世总有某些 修补可以做</p><p>残破世界令人学成 悲观中找鼓舞<br>来纾减身边恐怖<br>能照料你日子都不算糟</p><p>尽量去弥补 难逃那烦恼<br>修修补补乱世中 一起苍老</p><p>沿途在 修理著熄了的曙光<br>祝你在乱流下平安 真爱是任何形状<br>对付百孔千疮 谁能望穿我<br>这种坚壮非坚壮 形势坏透只好对抗<br>由我硬撑著 使你心安</p><p>谁也破了等某位去补 而你有我保养和爱慕<br>缝了再破穿了再补 这乱世未必可修理好</p><p>绝望里乐观 亦是个情操<br>东歪西倒但至少 牵手偕老</p><p>沿途在 修理著熄了的曙光<br>祝你在乱流下平安 真爱是任何形状<br>对付百孔千疮 谁能望穿我<br>这种坚壮非坚壮 形势坏透只好对抗<br>由我硬撑著 使你心安</p><p>漂亮的天真鲁莽 若被推倒可再装<br>巨匠的手不怕肮脏 贴著胶纸都俊朗<br>尽是补钉都发光 结局再破烂同奔往</p><p>银河上 边跌宕边看紧对方<br>跨宇宙又横越洪荒 不怕在尽头无岸<br>远近我都护航 还能互安慰<br>不必天气多清朗 狂雨暴雪一起对抗<br>任岁月再坏 不致心慌</p><hr><p>《银河修理员》由我最爱的Wyman作词。</p><p>但由于有心人将其引入了政治意味，被内地下架。</p><p>为了不想每次想听只能去youtube听，所以备份一份在博客上。</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>尘埃堕地的距离有几远？</title>
    <link href="/2022/20220508/"/>
    <url>/2022/20220508/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>林夕+泽日生&#x3D;yyds</p><p>林夕自然不必说了，撑起了大半个粤语词坛。泽日生当初十五分钟写就的《富士山下》也是脍炙人口。林夕加泽日生就是长句长句长句，一气呵成，连绵不断。</p><span id="more"></span><p>林夕+泽日生的组合大概有以下几个作品</p><blockquote><p>张敬轩：断气三部曲<br>陈奕迅：《富士山下》《一丝不挂》《任我行》<br>容祖儿：《搜神记》《不好意思我爱你》<br>谢安琪：《钟无艳》《你们的幸福》<br>麦家瑜《好得很》《不方便的真相》<br>林峯：《顽石》<br>关楚耀：《旧好》<br>汪晨蕊：《快高长大》<br>林家谦：《下一位前度》</p></blockquote><p>断气三部曲之一的《尘埃落定》由林夕作词、Christopher Chak（泽日生）作曲，也是我最喜欢的张敬轩的歌曲之一。如果要强行给我的心头之好排个序的话，《尘埃落定》可以排第四（前面是《高山低谷》、《无条件》、《春秋》）</p><p>看到网上没有多少对歌词的解析，于是想谈谈自己的理解。</p><hr><p>如愛要老實為何自欺<br>如愛夠偉大為何自卑<br>如自問未能容許抑鬱不理<br>不要再說 喜歡你</p><p><strong>（爱让我学会自欺，也让我变得自卑。什么是爱？我对你的是爱吗？如果自己都想不明白，那还是放下告白的念头。</strong></p><p>如你已應驗甜蜜夢境<br>塵埃飄渺間早已落定<br>從出生當天角色早已禮成<br>只可合照 縮影</p><p><strong>（我曾梦见我和你手挽着手亲密地走着，但是现在甜蜜的梦已经转换了主角，我也不过是路人。。我看不到走在一起的未来。我们的命运是上天注定的吗？是注定我只能看着我们少得可怜的合照偷偷想念吗？）</strong></p><p>維持著熟悉表情陌生關係不要變<br>只等到紅白儀式一場偶遇才會面<br>現實前被逼安分才戒掉了閃縮掛念<br>總算立地頂天</p><p><strong>（我们的关系维持着熟悉，但像两条平行线永远无法靠近，默契地渐行渐远，默契地成为了熟悉的陌生人。我们的距离实在太远了，我们下次见面是什么时候呢？大概只有你结婚或者离开这个世界，我才能再见到你吧。我一直想见你想见你想见你，大概只有我看到你和登对的他手挽着手时，我才会戒掉了对你的思念与爱）</strong></p><p>無愛可失 得不到相戀別說失戀<br>只感到天國近了相聚遠<br>只知道比你更愛你這種愛沒分寸<br>太肉麻累物累人原應了斷</p><p><strong>(回头想想，我们其实也没有经历过什么，都没有相恋，又何必一副失恋的样子。岁月在流逝，我们距离在变远。“我没有为你伤春悲秋不配有憾事，你没有共我踏过万里不够剧情延续故事“。还是说我要靠着扮弱者玩失意来换取你的一点关注？还是说要靠着自我感动来延续故事？我这种爱使大家都很累，不应该再继续了，应该了断了。）</strong></p><p>祝福你半天一生都得一句那麼短<br>無名義給你快樂不必兜轉<br>無權去把驚擾你的心捧起贈給你<br>即使有話想講已經將識過的字用完</p><p><strong>（想给你写祝福语，写了半天，有很多话想说，但只写了短短几句。不想兜兜转转地说一堆客套的祝福，说一句“祝你幸福”就好了。我还没有身份去给你快乐，给你承诺，不想以非伴侣的身份打扰你的人生。</strong></p><img src="/images/butilikeUjpg"  style="zoom:50%;" /><p>曾過敏了便麻木自己<br>塵埃於暗室總會墮地<br>如遭蚊叮蚤咬的酥癢過程<br>朝生已可暮死</p><p><strong>（我爱你就像得了霍乱，迅速扩散，持久蔓延，不能治愈。你的冷漠让我明白我们是不可能的。我应该会渐渐忘了你吧，渐渐忘记那些快乐时光，那些曾珍而重之的细节。好像什么都无所谓了）</strong></p><blockquote><p>在夜晚 说早晨</p><p>闲谈后 你更像别人</p><p>字幕里 说冬日灰暗</p><p>回答你 这边的气氛</p><p>就像你 已记不起了</p><p>连怀旧 也格外寂寥</p><p>杂物里 遗物和旧照</p><p>谁变卖 谁弃掉 谁看到破晓</p><p>渐渐我什么都不想知道</p><p>我觉得迷失竟比醒觉好</p><p>渐渐我离开都不想宣布</p><p>怕记忆 最后变话题 太俗套</p><p>——《渐渐》-陈奕迅</p></blockquote><p>想講句一切算了你聽不到又怎算<br>想失也無可失這刻我也曾賺了溫暖</p><p><strong>（你又听不到，我还有什么可说的呢。写了好久的信，也尘封在抽屉里。我从未拥有你，连失去你的机会都没有呢。但是想起我们之间的微不足道的点点滴滴，我也很快乐。）</strong></p><hr><p>张敬轩的另一首歌曲《春秋》和《尘埃落定》很配，一起听非常有感觉。</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>暗通道先验去雾</title>
    <link href="/2022/20220507/"/>
    <url>/2022/20220507/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>暗通道去雾由何恺明提出，其基于一个观察：在绝大多数户外无雾图像的<strong>非天空区域</strong>，<strong>至少有一个颜色通道的强度值非常低，趋近于零</strong>。这个“非常低的强度值”就是所谓的“暗像素”，而由这些暗像素构成的图像就是“暗通道”。</p><span id="more"></span><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>暗通道去雾算法主要包含以下几个关键步骤：</p><ol><li><p><strong>计算暗通道图像 (Compute Dark Channel):</strong></p><ul><li>对于输入图像的每一个像素，在一个局部窗口内（例如15x15），分别找出其R、G、B三个通道的最小值。</li><li>然后，再在这些最小值中找出最小的那个值，作为该像素在暗通道图像中的值。</li><li>用公式表达就是：<br>  $J^{dark}(x) &#x3D; \min_{y \in \Omega(x)} (\min_{c \in {r,g,b}} J^c(y))$<br>  其中，$J^c$ 是彩色图像的一个颜色通道，$Ω(x)$ 是以像素 $x$ 为中心的局部块。</li></ul></li><li><p><strong>估计大气光照 (Estimate Atmospheric Light):</strong></p><ul><li>大气光 $A$ 通常是图像中最亮的部分，也即雾最浓的区域。</li><li>在暗通道图像中，选取最亮的一定比例（例如0.1%）的像素。</li><li>在原始有雾图像中，找到这些对应位置的像素，并取这些像素的平均值或最大值作为大气光 $A$ 的估计值。</li></ul></li><li><p><strong>估计透射率 (Estimate Transmission Map):</strong></p><ul><li>大气散射模型可以简化为： $I(x) &#x3D; J(x)t(x) + A(1-t(x))$<ul><li>$I(x)$ 是我们观察到的有雾图像。</li><li>$J(x)$ 是我们想要恢复的无雾图像。</li><li>$A$ 是全局大气光。</li><li>$t(x)$ 是透射率，表示光线从场景点到达相机过程中没有被散射的比例。$t(x)$ 的值越小，表示雾越浓。</li></ul></li><li>根据暗通道先验，$J^{dark}(x) \approx 0$。将这个先验代入到归一化后的大气散射模型中（即各项除以 $A$），可以得到透射率的估计。更准确地，原文中是对 $I(x)&#x2F;A$ 求暗通道来估计透射率：<br>  $t(x) \approx 1 - \omega \cdot\frac{J^{dark}(x)}{A^c}$<br>  其中，$A^c$ 是大气光 $A$ 对应通道的强度值。$\omega$ 是一个修正系数（通常取0.95），用于保留少量远景的雾，使图像看起来更自然。</li><li>这个初步得到的透射率图通常是块状的，需要进行<strong>精细化处理</strong>。</li></ul></li><li><p><strong>精细化透射率图 (Refine Transmission Map):</strong></p><p>为了得到更精确的透射率图，通常会使用<strong>软抠图 (Soft Matting)</strong> 或者<strong>导向滤波 (Guided Filter)</strong> 等方法对初始透射率图进行平滑和边缘保持处理。导向滤波因其高效性和良好的效果而被广泛使用。</p><p>导向滤波，即参考另一张图像（称之为“引导图像”）的结构信息对一张图像进行平滑处理。引导图像“告诉”滤波器哪里是边缘，哪里是平坦区域。当滤波器处理输入图像时，如果引导图像在某个位置显示有边缘，滤波器就会努力保持输入图像在该位置的边缘；如果引导图像显示为平坦区域，滤波器就会对输入图像的相应区域进行更强的平滑。</p><p><img src="/2022/20220507/guided_filter.jpg"></p></li><li><p><strong>恢复无雾图像 (Recover Scene Radiance):</strong></p><ul><li>有了大气光 $A$ 和精细化的透射率图 $t(x)$，就可以根据大气散射模型反解出无雾图像 $J(x)$：<br>  $J(x) &#x3D; \frac{I(x) - A}{t(x)} + A$</li><li>为了防止当 $t(x)$ 值过小时除数过小导致结果不稳定，通常会设定一个透射率的下限 $t_0$ (例如0.1)：<br>  $J(x) &#x3D; \frac{I(x) - A}{\max(t(x), t_0)} + A$</li></ul></li></ol><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><strong>未去雾：</strong> （摄于深圳铁仔山）</p><p><img src="/2022/20220507/1.jpg"></p><p><strong>去雾后：</strong></p><p><img src="/2022/20220507/%E6%9A%97%E5%8E%9F%E8%89%B2%E5%85%88%E9%AA%8C.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>研究工作的价值</title>
    <link href="/2022/20220505/"/>
    <url>/2022/20220505/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自：</p><p><a href="https://www.bilibili.com/video/BV1oL411c7Us">李沐-如何判断（你自己的）研究工作的价值【论文精读】</a>、<a href="https://www.bilibili.com/video/BV1ea41127Bq?spm_id_from=333.999.0.0">你（被）吐槽过论文不够 novel 吗？【论文精读】</a></p><span id="more"></span><h4 id="价值公式"><a href="#价值公式" class="headerlink" title="价值公式"></a>价值公式</h4><p>用有<strong>新意</strong>的方法<strong>有效</strong>地解决一个<strong>研究</strong>问题。</p><p>有效：是一个相对的说法，是相对于之前的问题。</p><p>新意：1.不代表该领域之前从没人提过，只是近期很少人用 2.可以借鉴其他领域的方法</p><p>研究问题：注意区别于工程问题，很多情况是现有方法是行不通的，更需要探索</p><p>粗糙公式：价值&#x3D;新意度×有效性X问题大小</p><p>举例：</p><p><strong>问题大小：</strong></p><p>1：前面的工作某个点做的不好的地方进行改进</p><p>10：CV里面某一个视觉的子任务</p><p>100：提升机器对图片的理解</p><p><strong>有效性：</strong></p><p>1：模型精度比前人好一点</p><p>10：数据集精度提升一个点</p><p>100：数据集精度提升十个点</p><p>三个方向：效果（比如精度）、规模、安全</p><p><strong>新意度：</strong></p><p>1：不意外</p><p>10：有一定新意度</p><p>100：大家之前不熟的，打开了新世界的大门。相对而言，任何一个技术事实上不可能之前从来没出现过</p><h4 id="新意度"><a href="#新意度" class="headerlink" title="新意度"></a>新意度</h4><p>来自一篇有意思的文章：<a href="https://perceiving-systems.blog/en/post/novelty-in-science">https://perceiving-systems.blog/en/post/novelty-in-science</a></p><p>（作者michael J.Black）</p><p>建议新意度（novelty）换成优美度（beauty）。优美度从技术和复杂中脱离出来。</p><p>误区：</p><p><strong>1.复杂度衡量新意度</strong></p><p>如果一个非常小的改动它能产生大的结果他就是有新意的，无关复杂度。【比如一行代码上顶会的flooding方法？】</p><p><strong>2.用困难衡量新意度</strong></p><p>并不是说模型必须复杂才能发文章，才是新意的。</p><p><strong>3.用惊讶衡量新意度</strong></p><p>比如mae，这么小的想法效果却那么好</p><p>4.<strong>用技术上的新意性来衡量工作的新意度</strong></p><p>比如机器学习解数学题就是新意的</p><p><strong>5.用有效性和价值衡量新意度</strong></p><p>不是新的想法都是有用的。现在没有，以后可能会有实用性</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>灰度图像彩色化</title>
    <link href="/2022/20220430/"/>
    <url>/2022/20220430/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>基于中山大学国赛校内选拔赛试题。很幸运在solo的情况下获得了三等奖，成为了公费参赛的十支队伍之一。</p><span id="more"></span><h2 id="Lab彩色空间模型"><a href="#Lab彩色空间模型" class="headerlink" title="Lab彩色空间模型"></a>Lab彩色空间模型</h2><p>L 通道分量用于表示像素的亮度信息，取值范围为[0，100]，表示从纯黑到纯白的明暗变化；a 通道分量用于表示 像素从深绿色到灰色再到亮粉红色的颜色信息变化，取值范围为[-128，127]；b 通道分量用于表示像素从亮蓝色到灰色再到黄色的颜色信息变化，取值范围也是 [-128，127]。</p><p>L相当于灰度图像，故我们训练一个模型，用L来预测ab，最后再把L和ab合并。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p><img src="/2022/20220430/1.jpg"></p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="/2022/20220430/1.png"></p><p><img src="/2022/20220430/2.png"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>基于卷积神经网络的灰度图像彩色化 方法研究</p>]]></content>
    
    
    
    <tags>
      
      <tag>深度学习</tag>
      
      <tag>人工智能</tag>
      
      <tag>计算机视觉</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>粤语歌的高频词</title>
    <link href="/2022/20220429/"/>
    <url>/2022/20220429/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近被B站推送的一个视频：<a href="https://www.bilibili.com/video/BV1BY4y1i7TF">【容祖儿】唱到“没有”就切歌</a>逼疯了。</p><p>于是想看看粤语歌手的高频词是啥。</p><span id="more"></span><p>爬取了几个我喜欢粤语歌手的歌曲及歌词（来自QQ音乐，除去了live版等）。</p><p>最终用词云做出来的结果如下：</p><h2 id="陈奕迅"><a href="#陈奕迅" class="headerlink" title="陈奕迅"></a>陈奕迅</h2><p><img src="/images/love_%E9%99%88%E5%A5%95%E8%BF%85_%E6%AD%8C%E8%AF%8D.png" alt="love_陈奕迅_歌词"></p><h2 id="容祖儿"><a href="#容祖儿" class="headerlink" title="容祖儿"></a>容祖儿</h2><p><img src="/images/love_%E5%AE%B9%E7%A5%96%E5%84%BF_%E6%AD%8C%E8%AF%8D.png" alt="love_容祖儿_歌词"></p><h2 id="张敬轩"><a href="#张敬轩" class="headerlink" title="张敬轩"></a>张敬轩</h2><p><img src="/images/love_%E5%BC%A0%E6%95%AC%E8%BD%A9_%E6%AD%8C%E8%AF%8D.png" alt="love_张敬轩_歌词"></p><h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>可以看到陈奕迅比较高频的是：没有、我们、一个、什么、可以</p><blockquote><p>《我甚么都没有》居功甚伟 &#x2F;doge</p></blockquote><p>容祖儿比较高频的是：什么、没有、一个、我们、为何</p><blockquote><p>“为何”虽然很高频，但是我没什么印象，我只记得《十六号爱人》有“为何不可以，由我去决定谁”</p></blockquote><p>张敬轩比较高频的是：没有、自己、一个、怎么、可以</p><blockquote><p>“自己”这个高频词我也没有什么印象，好像没怎么听过</p></blockquote><p>完全可以弄个 张敬轩版”唱到’没有’就切歌“、陈奕迅版”唱到’没有’就切歌“。</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>走出焦虑的方法</title>
    <link href="/2022/20220417/"/>
    <url>/2022/20220417/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该笔记来自【如何才能不焦虑？【T君说心理】-哔哩哔哩】 <a href="https://b23.tv/1THZvnL">https://b23.tv/1THZvnL</a></p><span id="more"></span><p>总结一下方法：</p><h2 id="1-夸张式不攻自破法"><a href="#1-夸张式不攻自破法" class="headerlink" title="1. 夸张式不攻自破法"></a>1. 夸张式不攻自破法</h2><ol><li>夸张式悖论放大法：以毒攻毒，既然要从坏处想问题，那就想到底</li><li>夸张式任务拆解法：把一项任务拆分成若干个在几分钟，甚至几秒钟内完成的小任务。适合于拖延症患者。</li></ol><h2 id="2-洪水暴露疗法"><a href="#2-洪水暴露疗法" class="headerlink" title="2. 洪水暴露疗法"></a>2. 洪水暴露疗法</h2><p>当焦虑涉及到人际交往时适用。</p><p>有两种具体做法：</p><ol><li>把自己的失败和不足告诉自己尊敬的人。</li><li>把自己的焦虑和害怕暴露出来</li></ol><h3 id="社恐救星"><a href="#社恐救星" class="headerlink" title="社恐救星"></a>社恐救星</h3><p>循序渐进，例如：</p><ol><li>一周至少跟二十个对象微笑打招呼，对象可以先从没有威胁的开始</li><li>跟身边的人搭话，称赞对方的隐藏优势，对象可以由亲到疏</li><li>拒绝练习：提出一个你觉得一定会被拒绝的请求</li><li>自我揭露法：告诉别人自己在某一方面特别焦虑或害羞</li><li>大卫 · 莱特曼法：把对方放在聚光灯下，即鼓励对方发表意见</li></ol><h2 id="3-认知扭曲信念转变法"><a href="#3-认知扭曲信念转变法" class="headerlink" title="3. 认知扭曲信念转变法"></a>3. 认知扭曲信念转变法</h2><ol><li>写下你的消极核心信念评分（1-100）</li><li>从认知扭曲清单中识别你的认知扭曲</li></ol><blockquote><p>1.要么一切要么全无思想。你以黑白分明的范畴来看待事物，如果你的表现不够完美， 你<br>就会认为自己彻底失败。</p><p>2.过于概括。你把一个孤立的消极事件看做是一个永远会持续下去的失败模式。</p><p>3.心灵过滤。你选择一段消极细节，反复思考这段细节。结果，在你眼里，整个现实都 变<br>得黑暗起来，就像一滴墨水染黑了整杯水一样。</p><p>4.贬损积极的东西。你拒绝承认积极的经验，你会找这样那样的理由认为它们 “不算数” 。<br>这样你就可以坚持和你日常经验相矛盾的消极信念了。</p><p>5.跳跃式结论。即便没有确定的事实令人信服地支持你的结论，你也会对事情作出一个 消极的解释。<br>a. 测心术。你武断地认为别人对你作出消极的反映，你甚至不愿花工夫去检验一下。<br>b.先知错误。你预期事情会变糟，而且你坚信这个预言是一个已经成立的事实。</p><p>6.夸大与缩小。你夸大了事情的重要性（比如你弄糟了的事情或者别人的成绩），或者不合适地夸小事情，直到它们显得很小（你个人的优良品质或者别人的不足）。这种扭曲又被称 做<br>“双目镜把戏”。</p><p>7.情绪推理。你假定自己的消极情绪必然反映了事情的真实状况： “我这么感觉，所以它<br>肯定是真的。”</p><p>8.应该陈述。你试图用应该或不应该来激发自己，就好像在期望你做什么事之前应该先 鞭<br>笞你或惩罚你一样。 “必须”和“本该”同样也是罪魁祸首。这种情绪的结果是一种负罪感。当你<br>用应该陈述来要求别人时，你会体会到愤怒、灰心和怨恨。</p><p>9.贴标签与标签不当。贴标签是过于概括的一种极端形式。你不再描述你的错误，而是 为<br>你自己贴上一个消极的标签： “我是一个失败者。 ”当别人的行为以一种不当的方式与你发生关<br>系时，你也会给他贴上一个标签： “他是一个该死的讨厌鬼。 ”标签不当是指用高度主观的语言<br>或高度情绪化的语言来描述一件事情。</p><p>10.归己化。你会把自己看作是许多外界消极事件的原因，事实上你并不应该为这些事负<br>主要责任。</p></blockquote><ol start="3"><li>重新给你的消极核心信念打分</li></ol><p>最重要的是给自己创造<strong>积极思维</strong>。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>心理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>你该认清你的恐惧而不是目标</title>
    <link href="/2022/20220408/"/>
    <url>/2022/20220408/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>该演讲来自TED。<br>演讲者：Tim Ferriss<br>官网链接：<a href="https://www.ted.com/talks/tim_ferriss_why_you_should_define_your_fears_instead_of_your_goals/transcript">https://www.ted.com/talks/tim_ferriss_why_you_should_define_your_fears_instead_of_your_goals/transcript</a></p><span id="more"></span><p>我所找到的并被证实为最可靠安全的 情绪安全网 也正是 我用来做出最佳商业决定的工具。 但是这是次要的。 它就是斯多葛学派（stoicism）。</p><p>美国的奠基人， 托马斯·杰斐逊、约翰·亚当斯、 乔治·华盛顿 他们都是斯多葛学派的信奉者。</p><p>而在希腊罗马世界， 人们把斯多葛学派作为一个全面的系统 能解决很多事情。 于我们而言， 最主要的目的是训练我们自己 将可控和不可控的事情分开， 然后训练如何专注于于前者。 这将降低情绪的反应力， 这将成为一种超能力。</p><p>我找到一个改变人生的警句， ““折磨我們的往往是想像， 而不是真實”” 出自塞内卡， 他是著名的斯多葛学派作家。 </p><p> “premeditatio malorum,” 意思是在最坏情况来临前提前预想。 简而言之， 预想最坏的情景及你所恐惧的细节， 防止你采取任何行动， 因此你可以采取行动，来摆脱恐惧。 我当时头脑一片混乱， 充满着连续不断嘈杂的声音。 通过思考我的问题没有什么用处。 我需要把想法都写在纸上。 </p><h5 id="第一页"><a href="#第一页" class="headerlink" title="第一页"></a>第一页</h5><p>第一页是这样的。 “如果我…?” 这是你所恐惧的东西， 让你焦虑的东西， 被拖延的东西。 它可能是邀约某人， 结束一段关系， 提出升职，辞职或者创业。 它可以是任何事情。 与我而言，它是在工作 4年后第一次休假 我离开公司去伦敦休息一个月， 我可以免费住在伦敦朋友的房间里， 让我从生意的瓶颈中解放自己 或者结束它。</p><p>在第一栏“定义”中， 你写下所有你预想中 会发生的最坏的事情 如果你采取这一步行动。 你需要写下10到20个。 我不会每一个都详述， 但我举两个例子。 一个是如果我去伦敦， 伦敦在下雨的话，我会很沮丧。 整个旅程就是浪费时间。 第二个是我错过了美国国税局的信， 我将被查税 或者被抨击或者关闭等。</p><p>这时可以使用“预防”一栏。 在这一栏中，你写下答案： 我能做什么来预防这些事情发生， 或者至少降低发生的可能性？ 因此当我在伦敦觉得沮丧时， 我可以随身携带便携式蓝光 在早上使用15分钟。 我知道这会帮助我摆脱抑郁。 对于国税局， 我可以修改在国税局的邮寄地址， 因此文件到我的会计手上 而不是我的UPS地址。 超级简单。</p><p>接下来我们到“修复”一栏。 如果最坏的情况发生， 你能做什么来减轻损失， 或者你能向谁寻求帮助？ 因此第一个伦敦的例子， 我会多花点钱，去西班牙享受阳光， 来弥补损失，如果我陷入恐慌中。 如果我错过美国国税局的来件， 我可以给当律师的朋友打电话 或者咨询法学教授 他们的意见， 我将向他们请教 过去类似的情况是如何处理的。 在填写第一页时请谨记一个问题： 过去是否有人 不够聪明或者缺乏主动性 来弄清楚这些问题吗？ 答案是“是的”。</p><table><thead><tr><th>定义</th><th>预防</th><th>修复</th></tr></thead><tbody><tr><td>設定10-20個做下這個決定、行動後，你所恐懼的事情</td><td>你可以怎麼去避免或降低這些風險？</td><td>如果最糟的情況發生了， 你能做什麼來稍微 修復造成的損害？</td></tr></tbody></table><h5 id="第二页"><a href="#第二页" class="headerlink" title="第二页"></a>第二页</h5><p>第二页很简单： 一次尝试或部分成功会带来哪些好处？ 你可以看到我们直面恐惧 同时保持谨慎。 因此当你尝试你想做的事情的时候， 也许你可以建立自信， 提高情绪、经济等方面的技能。 一个安打（a base hit）能带来哪些好处？ 花10到15分钟时间思考下。</p><h5 id="第三页"><a href="#第三页" class="headerlink" title="第三页"></a>第三页</h5><p> 这很可能是最重要的，不要跳过。 “不行动的代价”。 </p><p>人类非常善于设想可能出错的事情 如果我们尝试新的事情，例如加薪。 我们通常忽视维持现状所付出的代价 什么都不改变。 因此你要扪心自问， 如果我错过这次行动或决定 以及类似的行动和决定， 6个月，12个月，3年后我的 生活会是什么样子？ 刚开始，这些变化非常细微。 但从情感、经济、身体等方面 再次仔细地思考。</p><p>这就是恐惧设置的三页纸。 </p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>之后，我意识到用1到10来评测， 1是最小的影响，10是最大的影响， 如果我踏上旅途，它将面对 1到3个短暂的可解决的苦恼， 还有8到10个能深刻改变我生活的 积极影响。 因此我选择了旅程。 然而我预想的灾难一个也没发生。 当然会有一些小问题。 我能将自己从生意中抽离出来。 最后我延长了那个环球旅行， 花了一年半的时间， 这也是我第一本书的素材来源， 最后让我今天站在了这里。</p><p>“Easy choices, hard life. Hard choices, easy life“</p><p>困难的选择， 我们最害怕去做的、问的、说的， 这些有可能正是我们最需要做的。 我们面对的最大挑战和困难是 永远不能通过一个轻松的谈话就能解决， 不管是你自我思考还是和别人探讨。</p><p>因此我鼓励你问自己： 你现在处在你人生中的哪个阶段 也许会让你看清恐惧而不是目标？ 请将Seneca的话铭记在心： “We suffer more often in imagination than in reality”</p>]]></content>
    
    
    
    <tags>
      
      <tag>演讲</tag>
      
      <tag>心理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《爱的艺术》摘抄</title>
    <link href="/2022/20220403/"/>
    <url>/2022/20220403/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《爱的艺术》是由弗罗姆创作的心理学著作。</p><p>弗洛姆也是《逃避自由》的作者，其实一开始我以为《逃避自由》和《爱的艺术》不是只是同名的作者。</p><p>《爱的艺术》很薄，但是很让人爱不释手。</p><span id="more"></span><hr><h3 id="爱的理论"><a href="#爱的理论" class="headerlink" title="爱的理论"></a>爱的理论</h3><h4 id="爱的解析"><a href="#爱的解析" class="headerlink" title="爱的解析"></a>爱的解析</h4><p>与共生性结合相反，成熟的爱是在保持自己的尊严和个性条件下的结合。爱是人的一种主动的能力，是一种突破使人与人分离的那些屏障的能力，一种把他和他人联合起来的能力。爱使人克服孤独和分离感，但爱承认人自身的价值，保持自身的尊严。在爱之中，存在着这样的矛盾状态：两个人成为一体而仍然保留着个人尊严和个性。</p><p>用最通俗的方式可以把爱的积极性表述为：爱主要是“给予”，而不是“接受”。对于具有创造性人格的人来说，“给予”是完全不同的意思。“给予”是潜力的最高表现。正是在“给予”行为中，我体会到自己的强大、富有、能干。这种增强的生命力和潜力的体验使我倍感快乐。我感到自己精力充沛、勇于奉献、充满活力，因此也欢欣愉悦。[6]“给予”比接受更令人快乐，这并不是因为“给予”是丧失、舍弃，而是因为我存在的价值正在于给予的行为。</p><p>在给予的因素之外，爱的主动特性明显地表现在这样的事实中，即所有形式的爱常常包含着共同的基本要素：<strong>关心、责任、尊重</strong>和<strong>了解</strong>。</p><p>**关心、责任：**爱是对所爱对象的生命和成长的积极关心。关注和关心包含了爱的另一方面，同时也是责任感的表现。</p><p>**尊重：**假如没有爱的第三种要素——尊重，那么责任有可能蜕变成支配和占有。尊重不是害怕和畏惧，根据该词词根来看，它表明按其本来面目发现一个人，认识其独特个性。尊重意指一个人对另一个人成长和发展应该顺其自身规律和意愿。尊重意为没有剥削。让被爱的人为他自己的目的去成长和发展，而不是为了服务于我。如果我爱另一个人，我感到与他或她很融洽，这是与作为他或她自己的他或她，而不是我需要使用的工具。很明显只有我独立了，只有我无须拐杖也无须支配和剥削任何人而立足和前进，尊重他或她才是有可能的。</p><p>**了解：**不了解一个人就不能尊重他，爱的责任若没有了解作为向导便是盲目的。了解若无关心为动力，便是一句空话。了解有多种层次，作为爱的一个方面的了解没有停留在表面上，而是深入到本质。只有当我能够超越对自己的关心而按其本来面目发现另一个人时，这种了解才有可能完成。比如即使他并未明显流露出自己的情绪，我就知道他生气了。我们还可以比这更深地了解他，因此我便知道他很急躁、忧虑重重，他感到孤独，感到内疚。然后，我知道他生气不过是由更深一层的某种原因引起的，在我看来与其说他是个易怒的人，不如说他是个遭受痛苦的人，他忧心忡忡，茫然不知所措。</p><p>了解“秘密”的另一个途径就是爱。爱是对他人的主动洞察力，在这种洞察之中，我了解秘密的渴望由结合而平息。在融洽的行为中，我了解你，我了解自己——我了解每个人——而我“一无”所知。</p><blockquote><p>最后一句其实不太懂，什么叫“一无所知”，以为是翻译的问题，翻了英文版也不甚明白。</p></blockquote><p>关心、责任、尊重和了解是相互依存的。只有在成熟的人身上才能找到这四者的交融形态；凡成熟的人都能创造性地发展自己的能力，他们放弃了自诩为无所不能的自恋的梦想，把已获得的谦恭置于真正的创造性活动产生的精神力量基础之上。</p><h4 id="父母之爱"><a href="#父母之爱" class="headerlink" title="父母之爱"></a>父母之爱</h4><p>我被爱。我之所以被爱，因为我是母亲的孩子，因为我无以自助，因为我惹人喜欢，因为母亲需要我。归纳成一个更一般的公式便是：我因我的样子被爱。或者也可以更为准确地说：我是我，所以被爱。这种被母亲所爱的感受是被动的。我不必为了被人爱而做任何事——母爱是无条件的。要我做的只是存在——是她的孩子。母爱是极乐，是安宁，它无须索取，不必报偿。但母爱的无条件性也有一方面被忽视了：它不仅无须报偿，也无法索取、制造和控制。如果它出现，便像是恩赐；如果它不存在，似乎像是一切美好事物都消失了——谁也无法创造它。</p><p>童稚的爱遵循这一原则：“我因被爱而爱。”成熟的爱遵循“我因爱而被爱”这一原则。不成熟的爱宣称：“我爱你，因为我需要你。”成熟的爱是：“我需要你，因为我爱你。</p><p>无条件的爱与一种最殷切的期望相一致，这种期望不仅存在于儿童，也存在于每个成年人；在另一方面，由于你的优点和你值得爱而被爱的话，总会留下疑问。也许我没能让我希望爱我的人高兴，也许说不清什么原因——总是存在一种爱可能消失的恐惧。而且，“值得的”爱容易留下自己不被爱的痛苦之感——你仅因为令人高兴而被爱，从最终的分析看，你根本不是被爱，而是被利用。无怪乎我们都一直渴望着母爱，不管是孩子还是成人。</p><p>父爱是有条件的爱。这种爱的原则是：“我爱你，因为你实现了我的愿望，因为你尽了职责，因为你像我。”在有条件的父爱中，我们发现与无条件的母爱一样，有消极的一面，也有积极的一面。消极的一面就是父爱必须有报答，如果你不按他所希望的去做便会失去他的爱。父爱的本质在于：服从成为主要的美德，不服从乃是主要的罪孽——以收回父爱作为惩罚。积极方面同样重要。既然父爱是有条件的，我们就可以想办法获得它，并为此而努力；他的爱不像母爱那样不为我们所控制。</p><h4 id="爱的对象"><a href="#爱的对象" class="headerlink" title="爱的对象"></a>爱的对象</h4><p>爱主要不是一种对某个特殊人的关系；它是一种态度，一种决定一个人对整个世界而不是对某个爱的“对象”的关系的性格倾向。如果一个人只爱某一个人，对其他同胞漠不关心，那么他的爱就不是真正的爱，而是共生性的依附，或是扩大了的自我主义。</p><p>如果我真正爱一个人，我就会爱所有人，爱这个世界，爱生活。如果我能对另一个人说“我爱你”，我就一定能够说：“我因为你而爱每个人，我通过你而爱这个世界，我由于你而爱我自己。”</p><blockquote><p>不明白</p></blockquote><h5 id="兄弟之爱"><a href="#兄弟之爱" class="headerlink" title="兄弟之爱"></a>兄弟之爱</h5><p>兄弟的爱，是构成各种爱的最基本的爱。这里，我所说的爱指责任感、关怀、尊重、对他人的了解、推动生活的愿望。</p><p>兄弟的爱是相互间平等的爱。</p><p>对无助者的爱，对穷人和陌生人的爱，是兄弟之爱的开端。</p><h5 id="母爱"><a href="#母爱" class="headerlink" title="母爱"></a>母爱</h5><p>与兄弟的爱和性爱——这两种爱是平等的人之间的爱——相反，母亲与孩子的关系就其本质而言是不平等的：一方事事需要帮助，另一方则给予帮助。正因为母爱这种利他的无私特性，所以被认为是最高层次的爱，是一切感情中最为神圣的。然而，母爱的真正伟大之处似乎并不在于母亲对婴儿的爱，而在于对成长着的孩子的爱。实际上，只要幼儿还小，还完全依靠她们，大多数母亲都是有仁爱之心的。多数女人想要孩子，为新生婴儿感到幸福，渴望关怀他。即使母亲们除了孩子脸上的微笑或满足的表情外没有得到任何报偿，也依然如故。这种爱的态度似乎部分地来源于本能。不管这种本能因素的分量多重，也有产生这种母爱的人类所特有的心理因素。在母爱中可以发现自恋的特征，因为幼儿仍被看作是她的一部分，她对他的爱和迷恋可能就是她自恋的一种满足。母亲对权力欲和占有欲的希冀是另一动因，幼弱的完全服从于她的意愿的孩子，对一个专横的渴望占有的女人来说，是让她得到满足的自然对象。</p><p>母爱的本质就在于关心孩子的成长，而这便意味着想让孩子离开她。它与性爱的根本区别就在于此。在性爱中，原本分离的两个人融为一体。在母爱中，原来融为一体的两个人分离了。母亲不仅必须容忍而且必须希望并支持孩子离开她。只有到这一阶段，母爱才成为如此困难的事。它要求毫无私心，要求具有“给予”一切，除了被爱者的幸福外一无所求的精神。</p><h5 id="性爱"><a href="#性爱" class="headerlink" title="性爱"></a>性爱</h5><p>兄弟的爱是平等的人之间的爱，母爱是对无助者的爱。尽管有区别，但是这两种爱的对象并没有囿于一个人，这一点是共同的。</p><p>性爱是对与另一异性的完全融合、结为一体的渴望。从其本性来说，它是排他的，不具有一般特性的爱。</p><p>如果说性爱是爱，那么它需要一个前提，那就是我从自身的存在本质出发去爱——并且也在他或她的存在本质中感受另一个人的爱。人的本质都是同一的。我们都是整体的一部分；我们就是整体。正因为如此，爱谁都不应有任何区别。爱本质上应是一种意志行为，是用自己的生命完全承诺另一个生命的决心。的确，这是隐蔽在婚姻背后的理论基础，是传统的诸多婚姻形式——两个伴侣不是自发选择，而是别人代为选择，却又指望相爱——的后盾。在当代西方，这种观点显然十分荒谬。爱应是一种自发的情感产物，是突然被一种不可抑制的情感所俘虏的产物。</p><p>爱上某人不只是一种强烈感情，还是一种决定、一种判断、一种承诺。如果爱仅是一种感情，便没有那种永远互爱的诺言的基础。感情可生亦可灭。当我们的行为不能囊括判断和决定时，怎么可能判断它将永驻呢？</p><p>性爱是一种排他的意愿与承诺的行为。因此从根本上说，爱的对象是谁，无关宏旨。不管婚姻是由他人撮合，还是由个人选择，一旦决定结婚，这种意愿行为就应保证爱的持久。</p><p>认为性爱完全是个人的吸引、是两个特定的人之间的特殊吸引的观点，和认为性爱就是一种意愿行为的观点，都是正确的——或者可以更确切地说，真理既非此也非彼。因此，人们认为一旦出现裂痕就应分道扬镳的观点，就如同无论如何都不能解除关系的观点一样错误。</p><h5 id="自爱"><a href="#自爱" class="headerlink" title="自爱"></a>自爱</h5><p>他的自私与自爱是一致的吗？或者说，难道自私不正是由于缺乏自爱引起的吗？</p><p>如果说我对邻人的爱是一种美德，那么，爱自己也一定是一种美德，而不是罪恶，因为我也是人的存在。没有任何不包括我自己的关于人的概念。任何一种宣扬这一排斥性的教条，论证本身都是自相矛盾的。《圣经》里表述的“爱邻如己”，暗含着对自身完整性和独特性的尊重。对自身的爱和理解与对另一个人的尊重、爱和理解是分不开的。对自身的爱与对另一个人的爱有不可分割的联系。</p><p>就“对象”与我们自身而言，爱在原则上是不可分割的。真正的爱意味着产生爱的能力，它蕴含着爱护、尊重、责任和了解。它并不是被某人所感动意义上的“情感”，而是一种为被爱者的成长和幸福所作的积极奋斗，它来源于爱的能力。</p><p>爱某个人是把爱的能力付诸实践，也是集中反映。爱是肯定所爱之人是根本人性的化身。爱一个人也意味着爱人类。威廉·詹姆斯所谓“爱的分工”，即有些人只爱自己的家庭，对“陌生人”没有感情，其实，这种现象是根本没有爱的能力的标志。对人类的爱，并不像人们常认为的那样，是对具体的人的爱的抽象化，而是其前提，尽管人类之爱总是通过爱在一个个具体的人中获得。</p><p>由此推论，我自身必定与他人一样是我爱的对象。人们对自己的生命、幸福、成长、自由的肯定来源于人们爱的能力，即来源于爱护、尊重、责任和了解。倘若一个人能够卓有成效地爱，他也会爱自己；倘若他仅能爱其他人，他便根本不会爱。</p><p>自私与自爱远不是一回事，实为水火不相容的对立物。自私者不是过于自爱，而是缺少自爱；他实际上恨自己。这种缺乏对自己的喜爱和关心，仅是他缺乏创造性能力的一种表现，留给他的是空虚和萎靡。他必然是不幸并焦虑不安地关注着从生活中攫取某种满足。这种满足限制了他自身的获取。从现象上看，他似乎过于关心自己，而实际上不过是枉费心机地试图掩盖关心真实自我的失败。自私者不能爱他人，因而也不能爱他们自己。</p><p>关于这个问题，没有谁比梅斯特·艾克哈特更好地概括了自爱的思想：“如果你爱自己，你也就会像爱自己那样爱每一个人，只要你爱自己多过爱他人，你便不会真正地爱自己。如果你同样地爱所有的人——包括自己，你便会把他们当成一个人来爱，这个人既是上帝，也是人。因此，这个人就是伟大正直的人，他像爱自己那样平等地爱其他所有人。”</p><h5 id="对上帝之爱"><a href="#对上帝之爱" class="headerlink" title="对上帝之爱"></a>对上帝之爱</h5><p>略。</p><h3 id="爱的实践"><a href="#爱的实践" class="headerlink" title="爱的实践"></a>爱的实践</h3><p>任何艺术的实践都有某些一般要求，不管是木工、医学，还是爱的艺术。首先，艺术的实践要求有<strong>规范</strong>。</p><p><strong>专心</strong>是掌握一门艺术的必要条件之一。</p><p>第三个因素是<strong>耐心</strong>。任何曾试图精通一门艺术的人都知道，成就任何事都需要有耐心。追求立竿见影，是永远不可能学到一门艺术的。</p><p>最后，对那门艺术予以<strong>最大关注</strong>是精通该艺术的一个条件。如果这门艺术不是什么重要的东西，艺徒不去学习它，那么他至多只是个好心而浅薄的涉猎者，而决不可能成为艺术大师。在这一点上，学习爱的艺术和学习任何其他艺术都是一样的。艺术大师和浅薄的涉猎者之间后者居多数，对爱的艺术而言似乎更是这种情况。</p><p>关于掌握一门艺术的一般条件还有一点必须指出，人们并不是直接而是间接地学习一门艺术。在开始学习一种艺术之前，必须学习大量其他东西——而且看起来经常是学一些无关的东西。木工学徒须从学习如何刨平木料开始，钢琴学徒从练习音阶开始，学射箭术的学徒从做运气练习[开始。你要成为一位艺术大师，就要将整个生命奉献给它，或者至少与它息息相关。你自己的身体变成了这门艺术实践中的器械，因而它必须保持健康，必须依据独特的功能完成任务。对爱的艺术而言，这就意味着，任何一个渴望成为这门艺术主人的人，都必须从其生活的每一方面由实现规范、专心、耐心开始做起。</p><p>的确，专心意味着孤独——而这种能力恰是具有爱的能力的一个条件。假如我因为不能自立而依附于另一个人，他或她可算是一个救命恩人，但这种关系不是一种爱的关系。相反，独立的能力是爱的能力的条件。任何试图独立的人都会发现，独处是多么困难。开始他会感到坐卧不安，心烦意乱，甚至感到忧心忡忡。他会认为这种实践毫无价值，不过是愚蠢的行为、浪费时间等等，因而容易把他不想继续进行这种实践的意愿合理化。</p><p>专心致志意味着此时此刻充实地生活，现在做某事时，不考虑下一步要做的事。无须说，相爱的人尤其应做到专心。他们必须学会彼此亲近，而不是采取通常采用的许多方式疏远。开始做时很困难，好像人们永远达不到目的似的。</p><p>爱的成功的主要条件乃是克服自恋。这种自恋表现为这样一种倾向性：人们感到真实的东西仅仅存在于自身的体验之中，外部世界的现象毫无真实性，而且总是从对人们有利或有害的观点上被感知。自恋的对立面是客观，那是一种按其本来面目认识人和事物并能够把客观现象与由于人的主观愿望和畏惧心理而形成的形象区分开来的本领。一切精神变态形式都表明不能客观地认识事物且已达到极端的程度。对精神错乱的人来说，唯一的真实存在就是他自身，就是他们畏惧和臆想的东西，他把外部世界看成是他内心世界的表现，看成他的创造物。</p><p>要有信仰就需要勇气和冒险的能力，甚至准备迎接厄运和挫折。谁若坚持把安全和保险视为生活的基本条件，那他就不可能有信仰；谁若把自己封闭在自己的系统中——在这里疏远和占有是他的保险工具——便会把自己变为囚犯。被爱和爱都需要勇气，需要判明作为最终关系的一定价值的勇气，并需要采取果断措施，为这些价值牺牲一切的勇气。</p><p>有办法培养信心和勇气吗？实际上，信心是随时都可以培养的。抚养小孩需要信心，入睡需要信心，开始做任何工作都需要信心。但是，我们对这种信心习以为常。谁若没有这种信心，谁便会为他的孩子过分焦虑，或患失眠症，或无力胜任任何创造性工作而积郁成疾，或疑心太重，或孤傲寡合，或鼠目寸光。坚持对一个人的判断，即使它似乎与公众意见或一些不可预知的事实相悖；坚持自己的信仰，即使它不为公众所接受，这些都需要信心和勇气。把生活中的困难、挫折和不幸视为只要克服它便会成为强者的一场挑战，而不是视为不应在我们身上发生的惩罚，这也都是要有信心和勇气的。</p><p>信心和勇气的培养要从日常生活中的小事开始。第一步是注意你何时何地失去信心，透过用以掩盖失去信心的借口，来认识你何处胆小怕事，又找到什么借口把它合理化。认识到缺乏信心会使人变得懦弱，而已有的懦弱又导致了新的灰心丧气，如此往复，恶性循环。那么你将认识到：当你意识上担心没有被爱的时候，实际上你害怕的是爱（尽管这常是不自觉的）。爱意味着在没有保证的条件下承诺自己，奉献自己，希望我们的爱能激起爱人心中的爱。爱是信心的行为，谁没有信心谁便没有爱。</p><p>对爱的艺术的实践来说，不可缺少的是至此还只是隐含提到的一种态度，那就是<strong>活动性</strong>。应该明确讨论它，因为它是爱的实践的基础。我前面已说过，活动并不是意味着“做某事”，而是一种内心活动，是人的能力富有成效的发挥。爱是一种活动，如果我在爱，我便常处于一种密切关心被爱的人的状态之中，而不仅仅是关心他或她。因为倘若我懒惰，倘若我不是常处在关注、机警、积极的状态，我便不能把自己与被爱的人积极地联系在一起。睡觉是唯一的不活动的状态；清醒则是懒惰没有容身之地的状态。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>验证集loss下降，准确率却上升的问题</title>
    <link href="/2022/20220328/"/>
    <url>/2022/20220328/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在做李宏毅的2022机器学习课程HW3时遇到了这个问题。</p><span id="more"></span><p>网上一搜，发现有很多人都遇到了这个问题。甚至有人在github专门开了一个<a href="https://github.com/thegregyang/LossUpAccUp">LossUpAccUp</a>项目讨论这一问题。</p><p>LossUpAccUp项目还讨论了一些解决方法：</p><p><strong>标准化</strong>。如果权重或者Logit标准化，可能不会出现这一问题。</p><p>Guo等人[2]认为这是一个错误的校准问题，即神经网络对其错误分类的输入变得过度自信。</p><p>Takashi Ishida等人提出了一个新的损失函数，来避免出现这一问题。（关键代码只有一行就能发顶会就很厉害。）</p><p>设原来的损失函数是$\mathcal{L}(\theta)$，现在改为$ \tilde{\mathcal{L}}(\theta)&#x3D;|\mathcal{L}(\theta)-b|+b$。其中b为预先设定的阈值。从而能使损失函数二次下降。</p><p>参考资料：</p><p>1.<a href="https://github.com/thegregyang/LossUpAccUp">LossUpAccUp</a></p><p>2.<a href="https://arxiv.org/abs/1706.04599">On Calibration of Modern Neural Networks</a></p><p>3.<a href="https://arxiv.org/pdf/2002.08709.pdf">Do We Need Zero Training Loss After Achieving Zero Training Error?</a></p><p>4.<a href="https://kexue.fm/archives/7643">我们真的需要把训练集的损失降低到零吗？</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>数据库系统概念</title>
    <link href="/2022/20220316/"/>
    <url>/2022/20220316/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h3 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h3><h4 id="1-1-Purpose-of-Database-Systems"><a href="#1-1-Purpose-of-Database-Systems" class="headerlink" title="1.1 Purpose of Database Systems"></a>1.1 Purpose of Database Systems</h4><span id="more"></span><p>In the early days, database applications were built directly on top of file systems.</p><p>Problems:</p><ul><li><p>Data redundancy and inconsistency     （冗余，不一致性）</p></li><li><p>数据访问困难（difficulty in accessing data）。Need to write a new program to carry out each new task (eg. Finding out the students who come from…)</p></li><li><p>数据孤立。Multiple files and formats</p></li><li><p>Integrity problems                                             （完整性问题）<br>（1）Integrity constraints  (e.g. account balance &gt; 0) become “buried” in program code rather than being stated explicitly<br>（2）当新的约束加入时，很难通过修改程序来体现这些新的约束。Hard to add new constraints or change existing ones</p></li><li><p>原子性问题（Atomicity of updates）<br>比如说把A系的账户余额中的500美元转入B系的账户余额中的这样一个程序。计入在程序的执行过程中发生了系统故障。操作必须是原子的——它要么全部发生要么根本不发生。在传统的文件处理系统中，保持原子性是很难做到的。</p></li><li><p>并发访问异常（concurrent-access anomaly）</p><p>Concurrent access by multiple users<br>Uncontrolled concurrent accesses may lead to inconsistencies<br>Example:  (a1) A &#x3D;  account<br>                 (a2) A &#x3D; A – 100<br>                                (b1) B &#x3D;  account<br>                 (a3) account &#x3D; A<br>                                (b2) B &#x3D; B – 100<br>                                (b3) account &#x3D; B</p></li><li><p>Security problems<br>Hard to provide user access to some, but not all, data<br>Database systems offer solutions to all the above problems<br>以上的问题促进了数据库系统的发展。</p></li></ul><h4 id="1-2-View-of-Data"><a href="#1-2-View-of-Data" class="headerlink" title="1.2 View of Data"></a>1.2 View of Data</h4><p>数据抽象的三个层次：</p><ul><li><p>物理层（physical level)。</p><p>（1）describes how data is actually stored.<br>（2）Database developers use this level.</p></li><li><p>Logical level:<br>（1）describes what data are stored in the database, and what relationships exist among those data<br>（2）Database administrators and application programmers use this level.</p></li><li><p>View level:<br>（1）simplifies the interaction with the system. Views can also hide information for security purposes.<br>（2）Computer users use this level.</p></li></ul><p><img src="C:\Users\李\AppData\Roaming\Typora\typora-user-images\image-20220318193447382.png" alt="image-20220318193447382"></p><h5 id="1-2-1-Instances-and-Schemas（实例和模式）"><a href="#1-2-1-Instances-and-Schemas（实例和模式）" class="headerlink" title="1.2.1 Instances and Schemas（实例和模式）"></a>1.2.1 Instances and Schemas（实例和模式）</h5><p>特定时刻存储在数据库中的信息的集合称作数据库的一个实例。而数据库的总体设计称为数据库模式。</p><p>根据前面我们所讨论的不同的抽象层次，数据库系统可以分为几种不同的模式。<strong>物理模式</strong>（physical schema）在物理层描述数据库的设计。<strong>逻辑模式</strong>（logical schema）则在逻辑层描述数据库的设计。数据库在视图层也可以由几种模式，有时称为<strong>子模式</strong>（subschema）。</p><p>应用程序如果不依赖于物理模式，它们就被称为是具有<strong>物理数据独立性</strong>（physical data independence），因此即使物理模式改变了它们也无需重写。</p><h5 id="1-2-2-数据模型（data-model）"><a href="#1-2-2-数据模型（data-model）" class="headerlink" title="1.2.2 数据模型（data model）"></a>1.2.2 数据模型（data model）</h5><p>数据模型提供了一种描述物理层、逻辑层以及视图层数据库设计的方式</p><p>可分为四类：</p><ul><li>关系模型（relational model）。</li></ul><p>关系模型用表的集合来表示数据和数据间的关系。每个表有多个列，每列有唯一的列名。</p><ul><li>实体-联系模型（entity-relationship model）。</li></ul><p>实体-联系（E-R）数据模型基于对现实世界的这样一种认识：现实世界由一组称作实体的基本对象以及这些对象间的联系构成。</p><ul><li>基于对象的数据模型（object-based data model）。</li></ul><p>面向对象的数据模型可以看成是E-R模型增加了封装、方法（函数）和对象标识等概念后的拓展。</p><ul><li>半结构化数据模型（semistructured data model）。</li></ul><p>半结构化数据模型允许那些相同类型的数据项含有不同的属性集的数据定义。</p><h4 id="1-3-数据库语言"><a href="#1-3-数据库语言" class="headerlink" title="1.3 数据库语言"></a>1.3 数据库语言</h4><p>数据库系统提供<strong>数据定义语言</strong>（data-definition language）来定义数据库模式，以及<strong>数据操纵语言</strong>（data-manipulation language）来表达数据库的查询和更新。</p><h5 id="1-3-1-数据操纵语言"><a href="#1-3-1-数据操纵语言" class="headerlink" title="1.3.1 数据操纵语言"></a>1.3.1 数据操纵语言</h5><p>DML能让用户可访问或操纵那些按照某种适当的数据模型组织起来的数据。有以下访问类型：</p><ul><li>对存储在数据库中的信息进行检索。</li><li>向数据库中插入新的信息。</li><li>从数据库中删除信息。</li><li>修改数据库中存储的信息</li></ul><p>通常分为两类：</p><ul><li><strong>过程化DML</strong>（procedural DML)要求用户指定与要什么数据以及如何获得这些数据。</li><li><strong>声明式DML</strong>（declarative DML)只要求用户指定需要什么数据，二不指明如何获得这些数据。</li></ul><p>查询（query)是要求对信息镜像检索的语句。DML中设计信息检索的部分称为查询语言。</p><h5 id="1-3-2-数据定义语言"><a href="#1-3-2-数据定义语言" class="headerlink" title="1.3.2 数据定义语言"></a>1.3.2 数据定义语言</h5><p>数据库系统所使用的的存储结构和访问方式是通过一系列特殊的DDL语句来说明的。这种特殊的DDL称作<strong>数据存储和定义</strong>(data storage and definition)语言。</p><p>存储在数据库中的数据值必须满足某些一致性约束（consistency constraint）。</p><h4 id="1-4-关系数据库"><a href="#1-4-关系数据库" class="headerlink" title="1.4 关系数据库"></a>1.4 关系数据库</h4><p>SQL（Structured Query Language）</p><h4 id="1-5数据库设计"><a href="#1-5数据库设计" class="headerlink" title="1.5数据库设计"></a>1.5数据库设计</h4><p>略</p><h4 id="1-6事务管理"><a href="#1-6事务管理" class="headerlink" title="1.6事务管理"></a>1.6事务管理</h4><p>事物（transaction）是数据库应用中完成单一逻辑功能的操作集合。每一个事务是一个既具原子性又具一致性的单元。</p><h4 id="1-7-数据库用户和管理员"><a href="#1-7-数据库用户和管理员" class="headerlink" title="1.7 数据库用户和管理员"></a>1.7 数据库用户和管理员</h4><ul><li><strong>无经验的用户</strong>(naive user)是默认经验的用户。</li><li><strong>应用程序员</strong>（application programmer）是编写应用程序的计算机专业人员。</li><li><strong>老练的用户</strong>（sophisticated user）不通过编写程序来同系统交互，而是用数据库查询语言或数据分析软件这样的工具来表达他们的要求。</li><li><strong>专门的用户</strong>（specialized user）是编写专门的、不适合于传统数据框架的数据库应用的富有经验的用户。</li></ul><h3 id="第二章-关系模型"><a href="#第二章-关系模型" class="headerlink" title="第二章 关系模型"></a>第二章 关系模型</h3><h4 id="2-1-关系模型介绍"><a href="#2-1-关系模型介绍" class="headerlink" title="2.1 关系模型介绍"></a>2.1 关系模型介绍</h4><p>在关系模型的术语中，关系（relation）来指代表，而元祖（tuple）用来指代行。属性（attribute）指代的是表中的列。</p><p>对于关系的每个属性，都存在一个允许取值的集合，称为该属性的域（domain）。</p><h4 id="2-2-码"><a href="#2-2-码" class="headerlink" title="2.2 码"></a>2.2 码</h4><p><strong>超码</strong>（superkey）是一个或多个属性的集合。这些属性的组合可以使我们在一个关系中唯一地表示一个元组。超码也有可能包含无关紧要的属性。</p><p><strong>候选码</strong>（candidate key)：任意真子集都不能成为超码</p><p><strong>主码</strong>（primary key）：被数据库设计者选中的、主要用来在一个关系中区分不同元组的<strong>候选码。</strong></p><p><strong>外码</strong>（foreign key)：一个关系模式（如$r_1$)可能在它的属性中包括另一个关系模式（如$r_2$）的主码。这个属性在$r_1$上称作参照$r_2$的<strong>外码</strong>。关系$r_1$也称为外码依赖的<strong>参照关系</strong>（referencing relation），$r_2$叫做外码的<strong>被参照关系</strong>（referenced relation）。</p><h4 id="2-3-模式图"><a href="#2-3-模式图" class="headerlink" title="2.3 模式图"></a>2.3 模式图</h4><p>一个含有主码和外码依赖的数据库模式可以用<strong>模式图</strong>（schema diagram）来表示。每一个关系用一个矩形来表示，关系的名字显示在矩形上方，矩形内列出各属性。主码属性用下划线标注。外码依赖用从参照关系的外码属性到被参照关系的主码属性之间的箭头来表示。</p><p><img src="C:\Users\李\AppData\Roaming\Typora\typora-user-images\image-20220318221451711.png" alt="image-20220318221451711"></p><h4 id="2-4-关系运算"><a href="#2-4-关系运算" class="headerlink" title="2.4 关系运算"></a>2.4 关系运算</h4><table><thead><tr><th>符号</th><th>例子</th></tr></thead><tbody><tr><td>$\sigma$    (选择)</td><td>$\sigma_{salary&gt;&#x3D;85000}(instructor)$</td></tr><tr><td>$\prod$   (投影，projection)</td><td>$\prod_{ID,salary}(instructor)$</td></tr><tr><td>$\bowtie$    (自然连接)</td><td>instructor$\bowtie$department</td></tr><tr><td>x      (笛卡尔积)</td><td>instructor x department</td></tr><tr><td>$\cup$   （并）</td><td></td></tr><tr><td>$\rho$  ()</td><td></td></tr></tbody></table><h3 id="第三章-SQL"><a href="#第三章-SQL" class="headerlink" title="第三章 SQL"></a>第三章 SQL</h3><h4 id="3-1-SQL数据定义"><a href="#3-1-SQL数据定义" class="headerlink" title="3.1 SQL数据定义"></a>3.1 SQL数据定义</h4><p>数据库中的关系集合必须由数据定义语言（DDL）指定给系统。SQL的DDL不仅能够定义一组关系，还能够定义每个关系的信息，包括：</p><ul><li>每个关系的模式</li><li>每个属性的取值类型</li><li>完整性约束</li><li>每个关系维护的索引集合</li><li>每个关系的安全性和权限信息</li><li>每个关系在磁盘上的物理存储结构</li></ul><h5 id="3-1-1-基本类型"><a href="#3-1-1-基本类型" class="headerlink" title="3.1.1 基本类型"></a>3.1.1 基本类型</h5><p>SQL标准支持多种固有类型，包括：</p><ul><li><strong>char(n)</strong>：固定长度的字符串，用户指定长度n。也可以使用全称<strong>character</strong>。</li><li><strong>varchar(n)</strong>：可变长度的字符串</li><li><strong>int</strong>：整数类型</li><li><strong>smallint</strong>：小整数类型（和机器相关的整数类型的子集）</li><li><strong>numeric(p,d)</strong>：定点数，精度由用户指定。这个数由p位数字（加上一个符号位），其中d位数字在小数点右边。所以在一个这种类型的字段上，numeric(3,1)可以精确储存44.5，但不能精确存储444.5或0.32.</li><li><strong>real,double precision</strong>：浮点数与双精度浮点数，精度与机器相关。</li><li><strong>float(n)</strong>：精度至少为n位的浮点数。</li></ul><h5 id="3-1-2-基本模式定义"><a href="#3-1-2-基本模式定义" class="headerlink" title="3.1.2 基本模式定义"></a>3.1.2 基本模式定义</h5><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">create table</span> r<br>(A1 D1,<br>     A2 D2,<br>     ...,<br>     An Dn,<br>     <span class="hljs-operator">&lt;</span>完整性约束<span class="hljs-number">1</span><span class="hljs-operator">&gt;</span>,<br>     ...,<br>     <span class="hljs-operator">&lt;</span>完整性约束k<span class="hljs-operator">&gt;</span>);<br><br></code></pre></td></tr></table></figure><p>SQL支持许多不同的完整性约束。</p><ul><li><strong>primary key</strong></li></ul><p>primary key 声明表示属性$A_{j1}、A_{j_2}…$构成关系的主码。主码属性必须非空其唯一，也就是说没有一个元组在主码属性上取空值，关系中也没有两个元组在所有主码属性上取值相同。</p><ul><li><strong>foreign key</strong></li></ul><p>foreign key 声明关系中任意元组在属性上的取值必须对应于关系s中某元组在主码属性上的取值。</p><ul><li><strong>not null</strong></li></ul><p>一个属性上的not null 约束表明在该属性上不允许空值。</p><h5 id="3-1-3-Drop-and-Alter-Table-Constructs"><a href="#3-1-3-Drop-and-Alter-Table-Constructs" class="headerlink" title="3.1.3 Drop and Alter Table Constructs"></a>3.1.3 Drop and Alter Table Constructs</h5><ul><li><p>To delete the table (the schema is also deleted): drop table   &lt; table name&gt;</p></li><li><p>To delete all contents of table (the schema is retained):</p><p> delete from  &lt; table name &gt;</p></li></ul><h4 id="3-2-SQL查询的基本结构"><a href="#3-2-SQL查询的基本结构" class="headerlink" title="3.2 SQL查询的基本结构"></a>3.2 SQL查询的基本结构</h4><h5 id="3-2-1-单关系查询"><a href="#3-2-1-单关系查询" class="headerlink" title="3.2.1 单关系查询"></a>3.2.1 单关系查询</h5><blockquote><p><strong>select</strong> name</p><p><strong>from</strong> instructor</p></blockquote><p>现在考虑另一个查询：“找出所有教师所在的系名”。</p><blockquote><p><strong>select</strong> dept_name</p><p><strong>from</strong> instructor</p></blockquote><p>因为一个系有多个教师，所以上述查询到结果是一个包含多个系名的关系。在关系模型的形式化数学定义中，关系是一个集合。因此，重复的元组不会出现在关系中。在实践中，去除重复是相当费时的。所以SQL允许出现重复。</p><p>如果我们想要删除重复，可在<strong>select</strong>后加入关键词<strong>distinct</strong>。</p><p>默认是不去除重复的，如果我们想要显式保留重复，可以在<strong>select</strong>后加入关键词<strong>all</strong>。</p><p>select子句还可带含义+、-、*、&#x2F;运算符的算术表达式。</p><p>where子句允许我们只选出那些在from子句的结果关系中满足特定谓词的元组。</p><h5 id="3-2-2多关系查询"><a href="#3-2-2多关系查询" class="headerlink" title="3.2.2多关系查询"></a>3.2.2多关系查询</h5><blockquote><p><strong>select</strong> A1，A2…,An</p><p><strong>from</strong> r1,r2…rm</p><p>where P;</p></blockquote><h5 id="3-2-3-自然连接"><a href="#3-2-3-自然连接" class="headerlink" title="3.2.3 自然连接"></a>3.2.3 自然连接</h5><blockquote><p> natural join</p></blockquote><p>另外，为了发扬自然连接的优点，同时避免不必要的相等属性带来的文献。SQL提供了一种自然连接的构造形式，允许用户来指定一个属性名列表——join … using …</p><h4 id="3-3-附加的基本运算"><a href="#3-3-附加的基本运算" class="headerlink" title="3.3 附加的基本运算"></a>3.3 附加的基本运算</h4><h5 id="3-3-1-更名运算"><a href="#3-3-1-更名运算" class="headerlink" title="3.3.1 更名运算"></a>3.3.1 更名运算</h5><p>as子句。可出现在select 子句中也可出现在from子句中。</p><h5 id="3-3-2-字符串运算"><a href="#3-3-2-字符串运算" class="headerlink" title="3.3.2 字符串运算"></a>3.3.2 字符串运算</h5><p>like子句来表示模式</p><blockquote><p><strong>select</strong> sept_name</p><p><strong>from</strong> department</p><p><strong>where</strong> building <strong>like</strong> ‘%Watson%’;</p></blockquote><p>模式：（大小写敏感）</p><ul><li>%：匹配任意子串</li><li>_：匹配任意一个字符</li></ul><p>在like 比较运算中使用<strong>escape</strong>关键词来定义转义字符。</p><p>SQL允许使用not like 比较运算符搜寻不匹配项。</p><h5 id="3-3-3-select-子句中的属性说明"><a href="#3-3-3-select-子句中的属性说明" class="headerlink" title="3.3.3 select 子句中的属性说明"></a>3.3.3 select 子句中的属性说明</h5><p>星号*可以用在select子句中表示所有的属性。</p><h5 id="3-3-4-排列元组的显示次序"><a href="#3-3-4-排列元组的显示次序" class="headerlink" title="3.3.4 排列元组的显示次序"></a>3.3.4 排列元组的显示次序</h5><p>order by 子句。默认使用升序，我们可以用<strong>desc</strong>表示降序，或者用asc表示升序。</p><blockquote><p><strong>select</strong> *</p><p><strong>from</strong> instructor</p><p><strong>order by</strong> salary <strong>desc</strong>, name <strong>asc</strong></p></blockquote><h5 id="3-3-5-where子句谓词"><a href="#3-3-5-where子句谓词" class="headerlink" title="3.3.5 where子句谓词"></a>3.3.5 where子句谓词</h5><p>SQL提供<strong>between</strong>比较运算符。</p><p>SQL允许我们用记号(v1,v2,…,vn)来表示一个n维元组，在元组上可以运用比较运算符，按字典顺序进行比较运算。</p><blockquote><p><strong>select</strong> name,course_id</p><p><strong>from</strong> instructor ,teaches</p><p><strong>where</strong> (instructor.ID,dep_name)&#x3D;(teaches.ID,’Biology’);</p></blockquote><h4 id="3-4-集合运算"><a href="#3-4-集合运算" class="headerlink" title="3.4 集合运算"></a>3.4 集合运算</h4><p>union,intersect和except运算对应于数学集合论中的$\cup、\cap、-$运算。</p><p>且自动去除重复。</p><h4 id="3-5-空值"><a href="#3-5-空值" class="headerlink" title="3.5 空值"></a>3.5 空值</h4><p>SQL将涉及空值的任何比较运算的结果视为<strong>unknown</strong>。</p><ul><li><strong>and:</strong></li></ul><p>true and unknown &#x3D;unknown.</p><p>false and unknown &#x3D;false.</p><p>unknown and unknown &#x3D;unknown.</p><ul><li><strong>or:</strong></li></ul><p>true or unknown &#x3D;true.</p><p>false or unknown &#x3D;unknown.</p><p>unknown or unknown &#x3D;unknown</p><ul><li>not</li></ul><p>null&#x3D;null返回unknown,而不是true。</p><h4 id="3-6-聚集函数"><a href="#3-6-聚集函数" class="headerlink" title="3.6 聚集函数"></a>3.6 聚集函数</h4><p>聚集函数是以值的一个集合（集或多重集）为输入、返回单个值的函数。SQL提供了五个固有聚集函数：</p><ul><li>平均值:avg</li><li>最小值:min</li><li>最大值:max</li><li>总和：sum</li><li>计数:count</li></ul><h5 id="3-6-1-分组聚集"><a href="#3-6-1-分组聚集" class="headerlink" title="3.6.1 分组聚集"></a>3.6.1 分组聚集</h5><p>group by 子句。在该子句中的所有属性取值相同的元组将被分在一个组中。</p><h5 id="3-6-2-having-子句"><a href="#3-6-2-having-子句" class="headerlink" title="3.6.2 having 子句"></a>3.6.2 having 子句</h5><p>用于分组限定条件。</p><h5 id="3-6-3-对空值和布尔值的聚集"><a href="#3-6-3-对空值和布尔值的聚集" class="headerlink" title="3.6.3 对空值和布尔值的聚集"></a>3.6.3 对空值和布尔值的聚集</h5><p>除了count外所有的聚集函数都忽略输入集合中的空值。</p><p>count:规定空集的值为0。</p><h4 id="3-7-嵌套子查询"><a href="#3-7-嵌套子查询" class="headerlink" title="3.7 嵌套子查询"></a>3.7 嵌套子查询</h4><p>select 、from、where语句都支持嵌套子查询。</p><p>A common use of subqueries is to perform tests for  </p><ul><li>set membership: in, not in</li><li>set comparisons: some, all </li><li>empty set: exists, not exists </li><li>set containment: not exists (B except A)</li></ul><p>返回true值</p><ul><li>duplicate tuples: unique, not unique</li></ul><h5 id="3-7-1-with-子句"><a href="#3-7-1-with-子句" class="headerlink" title="3.7.1 with 子句"></a>3.7.1 with 子句</h5><p>with 子句提供定义临时关系的方法。</p><blockquote><p><strong>with</strong> max_budget(value) <strong>as</strong></p><p>​(<strong>select</strong> <strong>max</strong> (budget)</p><p>​<strong>from</strong> department)</p><p><strong>select</strong> budget</p><p><strong>from</strong> department,max_budget</p><p><strong>where</strong> departmen.budget&#x3D;max_budget.balue;</p></blockquote><h5 id="3-7-2-标量子查询"><a href="#3-7-2-标量子查询" class="headerlink" title="3.7.2 标量子查询"></a>3.7.2 标量子查询</h5><p>SQL允许子查询出现在返回单个值的表达式能够出现的任何地方，只要该子查询只返回包含单个数学的单个元组；这样的子查询称为<strong>标量子查询</strong>(scalar subquery)。</p><h4 id="3-8-数据库的修改"><a href="#3-8-数据库的修改" class="headerlink" title="3.8 数据库的修改"></a>3.8 数据库的修改</h4><h4 id="3-8-1-删除"><a href="#3-8-1-删除" class="headerlink" title="3.8.1 删除"></a>3.8.1 删除</h4><p>删除与查询类似，我们只能删除整个元组，不能删除某些属性上的值</p><blockquote><p><strong>delete</strong> <strong>from</strong> r</p><p><strong>where</strong> P;</p></blockquote><h5 id="3-8-1-1-插入"><a href="#3-8-1-1-插入" class="headerlink" title="3.8.1.1 插入"></a>3.8.1.1 插入</h5><blockquote><p><strong>insert</strong> <strong>into</strong> student</p><p>​<strong>values</strong> (‘3003’,’Green’,’Finance’,null)</p></blockquote><h5 id="3-8-1-2-更新"><a href="#3-8-1-2-更新" class="headerlink" title="3.8.1.2 更新"></a>3.8.1.2 更新</h5><p>undate 语句</p><blockquote><p><strong>undate</strong> instructor</p><p><strong>set</strong> salary&#x3D;salary*1.05s</p></blockquote><h3 id="第四章-中级SQL"><a href="#第四章-中级SQL" class="headerlink" title="第四章 中级SQL"></a>第四章 中级SQL</h3><h4 id="4-1-连接表达式"><a href="#4-1-连接表达式" class="headerlink" title="4.1 连接表达式"></a>4.1 连接表达式</h4><h5 id="4-1-1-连接条件"><a href="#4-1-1-连接条件" class="headerlink" title="4.1.1 连接条件"></a>4.1.1 连接条件</h5><p>前面介绍了<strong>join…using</strong>子句</p><p>SQL还支持另外一种形式的连接。</p><p><strong>on</strong>条件允许在参与连接的关系上设置统一的谓词。该谓词的写法与<strong>where</strong>子句谓词类似。</p><blockquote><p><strong>select</strong> *</p><p><strong>from</strong> student <strong>join</strong> takes <strong>on</strong> student.ID &#x3D;takes.ID;</p></blockquote><h5 id="4-1-2-外连接"><a href="#4-1-2-外连接" class="headerlink" title="4.1.2 外连接"></a>4.1.2 外连接</h5><p>外连接（outer join ）运算与我们已经学过的连接运算类似，但通过在结果中创建包含空值元组的方式，保留了那些在连接中丢失的元组。</p><p>实际上有三种形式的外连接：</p><ul><li><strong>左外连接</strong>（left outer join）只保留出现在左外连接运算之前（左边）的关系中的元组。</li><li><strong>右外连接</strong>（right outer join）只保留出现在右外连接运算之前（右边）的关系中的元组。</li><li><strong>全外连接</strong>（full outer join）保留出现在两个关系中的元组。</li></ul><p>为了与外连接运算区分，前面我们学习的称为<strong>内连接</strong>。</p><p>on子句可以和外连接一起使用。on和where在外连接中的表现时不同的。on条件是外连接声明的一部分，但where不是。</p><p>outer join 的执行过程分为4步:</p><p>1、先对两个表执行交叉连接(笛卡尔积)</p><p>2、应用on筛选器</p><p>3、添加外部行</p><p>4、应用where筛选器</p><h4 id="4-2-视图"><a href="#4-2-视图" class="headerlink" title="4.2 视图"></a>4.2 视图</h4><p>在前面的例子中，我们一直都在逻辑模型层操作。</p><h5 id="4-2-1-视图定义"><a href="#4-2-1-视图定义" class="headerlink" title="4.2.1 视图定义"></a>4.2.1 视图定义</h5><p>我们在SQL中用<strong>create view</strong>命令定义视图。为了定义视图，我们必须给视图一个名称，并且必须提供计算视图的查询。</p><p>格式：</p><blockquote><p><strong>create</strong> <strong>view</strong> v <strong>as</strong> <quert expression></p></blockquote><p>为了创建一个视图，列出Physics 系在2009年秋季学期所开设的所有课程段，以及每个课程段在哪栋建筑的哪个房间授课的信息，我们可以写出：</p><blockquote><p><strong>create</strong> <strong>view</strong> physics_fall_2019 <strong>as</strong></p><p>​<strong>select</strong> course.course_id,sec_id ,building,room_number</p><p>​<strong>from</strong> course,section</p><p>​<strong>where</strong> course.course_id &#x3D;section.course_id</p><p>​<strong>and</strong> course.dept_name&#x3D;’phy’</p><p>​<strong>and</strong> section.semester&#x3D;’Fall’</p><p>​<strong>and</strong> section.year&#x3D;’2009’ </p></blockquote><h5 id="4-2-2-SQL查询中使用视图"><a href="#4-2-2-SQL查询中使用视图" class="headerlink" title="4.2.2 SQL查询中使用视图"></a>4.2.2 SQL查询中使用视图</h5><p>一旦定义了一个视图，我们就可以用视图名指代该视图生成的虚关系。</p><p>使用视图physics_fall_2009，我们可以这样查询</p><blockquote><p><strong>select</strong> cource_id</p><p><strong>from</strong> physics_fall_2009</p><p><strong>where</strong> building &#x3D; ‘watson’</p></blockquote><p>视图的属性名可以按下列方式显式指定：</p><blockquote><p><strong>create</strong> <strong>view</strong> department_total_salary(dept_name,total_salary) <strong>as</strong></p><p>​<strong>select</strong> dept_name,<strong>sum</strong>(salary)</p><p>​<strong>from</strong> instructor</p><p>​<strong>group</strong> <strong>by</strong> dept_name; </p></blockquote><h5 id="4-2-3-物化视图"><a href="#4-2-3-物化视图" class="headerlink" title="4.2.3 物化视图"></a>4.2.3 物化视图</h5><p>特定数据库系统允许存储视图关系，但是它们保证：如果用于定义视图的实际关系改变，视图也跟着修改。这样的视图被称为物化视图（materialized view)。</p><h4 id="4-3-事务"><a href="#4-3-事务" class="headerlink" title="4.3 事务"></a>4.3 事务</h4><p>事务（transaction）由查询和（或）更新语句的序列组成。SQL标准规定当一条SQL语句被执行，就隐式地开始了一个事务。下列SQL语句之一会结束一个事务：</p><ul><li>**Commit work:**提交当前事务，也就是将该事务所做的更新在数据库中持久保存。在事务被提交后，一个新的事务自动开始。</li><li><strong>Rollback work</strong>：回退当前事务，即撤销改事务中所有SQL语句对数据库的更新。这样，数据库就恢复到执行该事务第一条语句之前的状态。</li></ul><h4 id="4-4-完整性约束"><a href="#4-4-完整性约束" class="headerlink" title="4.4 完整性约束"></a>4.4 完整性约束</h4><p>完整性约束保证授权用户对数据库的修改不会破坏数据的一致性。因此，完整性约束防止的是对数据的意外破坏。</p><p>完整性约束的例子有：</p><ul><li>教师姓名不能为null</li><li>任意两位教师不能有相同的教师标识</li></ul><p>完整性约束常被看作是数据库模式设计过程的一部分。也可以通过使用<strong>alter</strong> <strong>table</strong> table-name <strong>add</strong> constaint 命令施加到已有关系上。</p><p><strong>create</strong> <strong>table</strong> 命令还可以包括完整性约束语句。</p><p>除了主码约束之外，还有其他命令。</p><p>比如：</p><ul><li>not null</li><li>unique</li><li>check(&lt;谓词&gt;)</li></ul><blockquote><p> check (semester in (’Fall’, ’Winter’, ’Spring’, ’Summer’))</p></blockquote><ul><li>初始值</li></ul><blockquote><p> tot_cred  numeric(3,0) <strong>default</strong> <strong>0</strong></p></blockquote><h5 id="4-4-1-参照完整性"><a href="#4-4-1-参照完整性" class="headerlink" title="4.4.1 参照完整性"></a>4.4.1 参照完整性</h5><p>我们常常希望保证在一个关系上给定属性集上的取值中出现。这种情况称为<strong>参照完整性</strong>（referential integrity）。</p><p>一般地，令关系r1和r2的属性集分别为R1和R2，主码分别为K1和K2。如果要求对r2中任意元组t2，均存在r1中元组使得t1.K1&#x3D;t2.$\alpha$，我们称R2的子集$\alpha$为参照关系r1中K1的外码。</p><p>这种要求称为<strong>参照完整性约束（referential-intergrity constraint)<strong>或</strong>子集依赖（subset dependency）</strong>。</p><blockquote><p>dept_name <strong>varchar</strong>(20) <strong>references</strong> department</p></blockquote><p>当违反参照完整性约束时，通常的处理是拒绝执行导致完整性破坏的操作（即镜像更新操作的事务被回滚）。</p><p>但是也可在foreign key子句中指明：如果被参照关系上的删除或更新动作违反了约束，那么系统必须采取一些步骤通过修改参照关系中的元组来恢复完整性约束，而不是拒绝这样的动作。</p><blockquote><p><strong>create</strong> <strong>table</strong> course</p><p>(…</p><p><strong>foreign</strong> <strong>key</strong> (dept_name) <strong>reference</strong> department</p><p>​<strong>on</strong> <strong>delete</strong> <strong>cascade</strong></p><p>​<strong>on</strong> <strong>update</strong> <strong>cascade</strong>,</p><p>…);</p></blockquote><p><strong>on delete cascade</strong> 子句 ：如果删除department 中的元组导致了此参照完整性约束被违反，则删除并不被系统拒绝，而是对course关系作“级联”删除，即删除参照了被删除系的元组。</p><p><strong>on</strong> <strong>update</strong> <strong>cascade</strong> 子句：若果更新被参照字段时违反了约束，则更新操作并不被系统拒绝，而是将course中参照的元组的dept_name字段也改为新值。</p><h5 id="4-4-2-事务中对完整性约束的违反"><a href="#4-4-2-事务中对完整性约束的违反" class="headerlink" title="4.4.2 事务中对完整性约束的违反"></a>4.4.2 事务中对完整性约束的违反</h5><p>事务可能包括几个步骤，在某一步之后完整性约束也许会暂时被违反，但是后面的某一步也许就会消除这个违反。例如，假设我们有一个主码name的person关系，还有一个属性时spouse，并且spouse是在person上的一个外码。也就是说，约束要求spouse属性必须包含在person表里出现的名字。</p><p>为了处理“插入第一个元组违反外码约束，但是插入第二个元组后，外码约束又满足”的情况，SQL标准允许将<strong>initially</strong> <strong>deferred</strong> 子句加入到约束声明中；这样完整性约束不是在事务的中间步骤上检查，而是在事务结束的时候检查。</p><blockquote><p>create table person</p><p>(namechar(15)     primary key,</p><p>spousechar(15),</p><p>foreign key spouse references person (name) <strong>initially deferred</strong>)</p></blockquote><h5 id="4-4-3-断言"><a href="#4-4-3-断言" class="headerlink" title="4.4.3 断言"></a>4.4.3 断言</h5><p>断言(assertion)就是一个谓词，它表达了我们希望数据库总能满足的一个条件。域约束和参考完整性约束是断言的特殊形式。</p><p>格式：</p><blockquote><p><strong>create</strong> <strong>assertion</strong> &lt; assertion-name &gt; <strong>check</strong> &lt; predicate &gt;;</p></blockquote><h4 id="4-5-SQL的数据类型与模式"><a href="#4-5-SQL的数据类型与模式" class="headerlink" title="4.5 SQL的数据类型与模式"></a>4.5 SQL的数据类型与模式</h4><h5 id="4-5-1-SQL中的日期和时间类型"><a href="#4-5-1-SQL中的日期和时间类型" class="headerlink" title="4.5.1 SQL中的日期和时间类型"></a>4.5.1 SQL中的日期和时间类型</h5><p>SQL除了前面介绍的基本数据类型以外，SQL标准还支持与日期和时间相关的几种数据类型：</p><ul><li><p><strong>date</strong>: 日历日期，包括年（四位）、月和日。</p></li><li><p><strong>time</strong>: 一天中的时间，包括小时、分和秒。可以用变量<strong>time</strong>(p)来表示秒的小数点后的数字位数（这里默认值为0）。通过指定<strong>time</strong> <strong>with</strong> <strong>timezone</strong>，还可以</p></li><li><p><strong>timestamp</strong>：<strong>date</strong> 和<strong>time</strong>的组合。可以用变量<strong>timestamp(p)<strong>来表示秒的小数点后的数字位数（这里默认值为6）。如果指定</strong>with timezone</strong>，则时区信息也会被存储。</p></li></ul><h5 id="4-5-2-大对象类型"><a href="#4-5-2-大对象类型" class="headerlink" title="4.5.2 大对象类型"></a>4.5.2 大对象类型</h5><p>SQL提供字符数据的大对象数据类（clob）和二进制数据的大对象数据类型（blob）。</p><h5 id="4-5-3-用户定义的类型"><a href="#4-5-3-用户定义的类型" class="headerlink" title="4.5.3 用户定义的类型"></a>4.5.3 用户定义的类型</h5><p>SQL支持两种形式的用户定义数据类型。第一种称为<strong>独特类型（distinct type)</strong>。另一种称为<strong>结构化数据类型</strong>（structured data type）。</p><p>独特类型：</p><p>使用**create type **子句来定义新类型</p><blockquote><p><strong>create type</strong> Dollars <strong>as numeric</strong> (12,2) <strong>final</strong></p><p><strong>create type</strong> RMB **as numeric **(12,2) <strong>final</strong></p></blockquote><p>也可以使用cast命令强制转换</p><blockquote><p><strong>cast</strong> (department.budget  <strong>to</strong> numeric(12, 2))</p></blockquote><p>SQL提供了<strong>drop type</strong>和<strong>alter type</strong> 子句来删除或修改以前创建过的类型。</p><p>SQL还有一个相似但稍有不同的概念：域（domain），它可以在基本类型上世家完整性约束。例如</p><blockquote><p><strong>create</strong> <strong>domain</strong> DDollars <strong>as</strong> <strong>numeric</strong>(12,2) <strong>not null</strong></p></blockquote><p>DDollars域可以用作属性类型。然而类型和域直接有两个重大的差别：</p><p>1.在域上可以声明约束，例如not null，也可以为域类型变量定义默认值，然而在用户定义类型上不能声明约束或默认值。设计用户定义类型不仅是用它来指定属性类型，而且还将它用在不能施加约束的地方对SQL进行过程扩展。</p><p>2.域并不是强类型的。因此一个域类型的值可以被赋给另一个域类型，只要他们的基本类型是相容的。</p><h4 id="4-6-授权"><a href="#4-6-授权" class="headerlink" title="4.6 授权"></a>4.6 授权</h4><p>我们可能会给一个用户在数据库的某些部分授予几种形式的权限。对数据的授权包括：</p><ul><li>授权读取数据</li><li>授权插入新数据</li><li>授权更新数据</li><li>授权删除数据</li></ul><p>我们使用grant语句来授予权限</p><blockquote><p>grant &lt; 权限列表 &gt;</p><p>on &lt; 关系吗或视图名&gt;</p><p>to &lt; 用户&#x2F;角色列表&gt;</p></blockquote><h5 id="4-6-1-角色"><a href="#4-6-1-角色" class="headerlink" title="4.6.1 角色"></a>4.6.1 角色</h5><p>角色：一组用户</p><p>在SQL创建角色如下所示：</p><p><strong>create role</strong> instructor</p><h5 id="4-6-2-权限的转移"><a href="#4-6-2-权限的转移" class="headerlink" title="4.6.2 权限的转移"></a>4.6.2 权限的转移</h5><p>默认方式下，被授予权限的用户&#x2F;角色无权把得到的权限再授予给另外的用户&#x2F;角色。如果我们在授权时允许接受者把得到的权限再传递给其他用户，我们可以在响应的grant命令后面附加with grant option子句。</p><blockquote><p><strong>grant</strong> <strong>select</strong> <strong>on</strong> department <strong>to</strong> Amit <strong>with grant option</strong></p></blockquote><h3 id="第五章-高级SQL"><a href="#第五章-高级SQL" class="headerlink" title="第五章 高级SQL"></a>第五章 高级SQL</h3><h4 id="5-1-使用程序设计语言访问数据库"><a href="#5-1-使用程序设计语言访问数据库" class="headerlink" title="5.1 使用程序设计语言访问数据库"></a>5.1 使用程序设计语言访问数据库</h4><p>可以通过以下两种方法从通用编程语言中访问SQL：</p><ul><li>动态SQL</li></ul><p>通用程序设计语言可以通过函数（对于过程式语言）或者方法（对于面对对象的语言）来连接数据库服务器并与之交互。利用动态SQL可以在运行时以字符串形式构建SQL查询，提交查询，然后把结果存入程序变量中，每次一个元组。动态SQL的SQL组件允许程序在运行时构建和提交SQL查询。</p><p>例如java语言的应用程序接口JDBC和另一种：ODBC</p><ul><li>嵌入式SQL</li></ul><p>如动态SQL类似，嵌入式SQL提供了另外一种使程序与数据库服务器交互的手段。然而，嵌入式SQL语句必须在编译时全部确定，并交给预处理器。预处理程序提交SQL语句到数据库系统进行预编译和优化，然后它把应用程序中的SQL语句替换成相应的代码和函数，最后调用程序语言的编译器进行编译。</p><h3 id="第六章-形式化关系查询语言"><a href="#第六章-形式化关系查询语言" class="headerlink" title="第六章 形式化关系查询语言"></a>第六章 形式化关系查询语言</h3><h4 id="6-1-关系代数"><a href="#6-1-关系代数" class="headerlink" title="6.1 关系代数"></a>6.1 关系代数</h4><p>关系代数是一种过程化查询语言。它包括一个运算的集合，这些运算以一个或两个关系为输入，产生一个新的关系作为结果。</p><h5 id="6-1-1-基本运算"><a href="#6-1-1-基本运算" class="headerlink" title="6.1.1 基本运算"></a>6.1.1 基本运算</h5><p><strong>（1）选择运算</strong></p><p><strong>（2）投影运算</strong></p><p>即选出某几列</p><p><strong>（3）关系运算的组合</strong></p><p><strong>（4）并运算</strong></p><p><strong>（5）集合差运算</strong></p><p><strong>（6）笛卡尔积运算</strong></p><p><strong>（7）更名运算</strong></p><h5 id="6-1-2-附加的关系代数运算"><a href="#6-1-2-附加的关系代数运算" class="headerlink" title="6.1.2 附加的关系代数运算"></a>6.1.2 附加的关系代数运算</h5><p>（1）集合交运算</p><p>（2）自然连接运算</p><p>（3）赋值运算</p><p>赋值用←表示</p><blockquote><p>temp1← R $\times$ S</p></blockquote><p>（4）外连接运算</p><h5 id="6-1-3-扩展的关系代数运算"><a href="#6-1-3-扩展的关系代数运算" class="headerlink" title="6.1.3 扩展的关系代数运算"></a>6.1.3 扩展的关系代数运算</h5><p><strong>（1）广义投影</strong></p><p>广义投影运算形式为：</p><blockquote><p>$\prod_{F1,F2…,Fn}(E)$</p></blockquote><p>其中E是任意关系代数表达式，而F1，F2，…，Fn中的每一个都是涉及常量以及E的模式中属性的算术表达式。最基本的情况下算术表达式可以仅仅是一个属性或常量。</p><p><strong>（2）聚集</strong></p><p>聚集函数像有sum、count等。</p><blockquote><p> $\mathcal{G}_{sum(salary)}(instructor)$</p></blockquote><p>关系代数运算$\mathcal{G}$表示聚集将被应用，它的下标说明采用的聚集运算。</p><p>有时，在计算聚集函数前我们必须去除重复值。如果我们想去除重复，我们仍然使用前面的函数名，但用连字符将”<strong>distinct</strong>“附加在函数名后（如<strong>count-distinct</strong>）</p><p>有时候我们希望对一组元组集合而不是单个元组集合执行聚集函数。</p><p>比如考虑查询“求出每个系的平均工资”。</p><blockquote><p>$<em>{dept_name}\mathcal{G}</em>{sum(salary)}(instructor)$</p></blockquote><p>聚集运算（aggregation operation）通常的形式如下：</p><blockquote><p>$<em>{G_1,G2,…,G_n}\mathcal{G}</em>{F_1(A_1),F_2(A_2),…,F_n{A_n}}{E}$</p></blockquote><p>其中E是任意关系代数表达式，$G_1,G_2,…,G_n$是用于分组的一系列属性；每个$F_i$是一个聚集函数，每个</p><p>$F_i$是一个聚集函数，每个$A_i$是一个属性名。运算的含义如下，表达式E的结果中元组以如下方式被分为若干组：</p><p>1.同一组中所有元组在$G_1,G2,…,G_n$上的值相同。</p><p>2.不同组中元组在$G_1,G2,…,G_n$上的值不同。</p><h3 id="第七章-待写"><a href="#第七章-待写" class="headerlink" title="第七章 待写"></a>第七章 待写</h3><p>待写</p><h3 id="第八章-关系数据库设计"><a href="#第八章-关系数据库设计" class="headerlink" title="第八章 关系数据库设计"></a>第八章 关系数据库设计</h3><h4 id="8-1-函数依赖"><a href="#8-1-函数依赖" class="headerlink" title="8.1 函数依赖"></a>8.1 函数依赖</h4><p>函数的定义：$y&#x3D;f(x)\ \ \ \ \ \ \ \ x\in X,y\in Y$</p><ul><li><p>$X→Y$</p></li><li><p>对于任意的x1,x2都满足x1&#x3D;x2→f(x1)&#x3D;f(x2)</p></li></ul><p>函数依赖的定义：</p><p>设R是一个关系模式，$\alpha \subseteq R$,$\beta \subseteq  R$</p><p>函数依赖：$\alpha \in \beta成立</p><p>当且仅当任意R的实例r中的任意二行t1和t2都满足：</p><p>t1[$\alpha$]&#x3D;t2[$\alpha$]→t2[$\beta$]&#x3D;t2[$\beta$]</p><p>称为：$\alpha$确定$\beta$，$\beta$函数依赖于$\alpha$,$\alpha$可推出$\beta$</p><p><strong>排除函数依赖：</strong></p><p>只需要一组反例，满足：存在s,t$\in$r,s[X]&#x3D;t[X],但s[Y]$\neq$t[Y]</p><p><strong>确认函数依赖</strong>:</p><p>对于R的任意一个可能的实例r，都满足：不存在s,t$\in$r,s[X]&#x3D;t[X],但s[Y]$\neq$t[Y]</p><p>**函数依赖集：**若干个函数依赖组成的集合</p><p>如果一个关系r没有违反函依赖集F，那么称关系r满足函数依赖集F（r satisfies F）</p><p>如果一个关系R的所有关系都满足函数依赖集F，那么称函数依赖集F在关系模式R上成立（F holds on R）</p><h4 id="8-2-属性集的闭包"><a href="#8-2-属性集的闭包" class="headerlink" title="8.2 属性集的闭包"></a>8.2 属性集的闭包</h4><p>定义：根据一个给定的函数依赖集F，由属性集$\alpha$可退出的所有属性组成的集合，称为$\alpha$的闭包，记作$\alpha^+$</p><blockquote><p> 计算$\alpha^+$的算法:</p><p>result:&#x3D;$\alpha$</p><p>repeat </p><p>​检查F中的每个函数依赖$\beta\in \gamma$</p><p>​若$\beta \subseteq result$,则把$\gamma$属性都加入result</p><p>until</p><p>​直到不能再添加属性到result</p></blockquote><p>属性集闭包的用途：</p><p>（1）判断一个属性集是否超码</p><ul><li>要判断$\alpha$是不是R的超码，只要看$\alpha^+$是不是包含R</li></ul><p>（2）判断一个函数依赖是否成立</p><ul><li>要判断$\alpha \in \beta$是不是成立，只要看$\beta \subseteq \alpha^+$是不是成立</li></ul><p>（3）简洁的计算函数依赖集的闭包 </p><h5 id="8-2-1-计算函数依赖集的闭包"><a href="#8-2-1-计算函数依赖集的闭包" class="headerlink" title="8.2.1 计算函数依赖集的闭包"></a>8.2.1 计算函数依赖集的闭包</h5><p>方法1：用Armstrong公理</p><ul><li><p>自反律：若$\beta \subseteq \alpha$那么$\alpha →\beta$</p></li><li><p>增广律：若$\alpha → \beta$，那么$\gamma\alpha → \gamma\beta$</p></li><li><p>传递律：若$\alpha → \beta$，$\beta → \gamma$，那么$\alpha → \gamma$</p></li><li><p>合并律：若$\alpha → \beta$，$\alpha → \gamma$，那么$\alpha → \beta\gamma$</p></li><li><p>分解律：若$\alpha → \beta\gamma$那么$\alpha→\beta$并且$\alpha → \gamma$</p></li><li><p>伪传递律：若$\alpha → \beta$并且$\gamma\beta → \delta$，那么$\gamma\alpha → \gamma\beta$</p></li></ul><p>方法二：利用属性集的闭包（穷举法）</p><ul><li>对关系模式R的每一个子集S进行以下操作</li></ul><p>为$S^+$的每一个子集T，输出函数依赖$S→ T$</p><h5 id="8-2-2-函数依赖集的等价性"><a href="#8-2-2-函数依赖集的等价性" class="headerlink" title="8.2.2 函数依赖集的等价性"></a>8.2.2 函数依赖集的等价性</h5><p>设F1和F2是两个函数依赖集，若由F1可以推出F2的所有函数依赖，由F2也可以推出F1的所有函数依赖，则说F1和F2是等价的。</p><ul><li>设F1和F2是两个函数依赖集，以下两个命题是等价<ul><li>F1、F2有相同的闭包（$F1^+&#x3D;F2^+$</li><li>F1和F2是等价的</li></ul></li></ul><h5 id="8-2-3-Canonical-最小覆盖-规范覆盖"><a href="#8-2-3-Canonical-最小覆盖-规范覆盖" class="headerlink" title="8.2.3 Canonical(最小覆盖&#x2F;规范覆盖)"></a>8.2.3 Canonical(最小覆盖&#x2F;规范覆盖)</h5><ul><li>函数依赖集中某些函数依赖是多余的。</li></ul><p>例如，{A→B，B→C，A→C}可以简化为{A→B，B→C}</p><ul><li>函数依赖中某些属性是多余的。</li></ul><p>例如，{A→B，B→C，AC→D}可以简化为{A→B，B→C，A→D}</p><p>把所有的多余属性和多余的函数依赖删掉，得到最小的函数依赖集合，称为F的最小覆盖或规范覆盖</p><h6 id="8-2-3-1-规范覆盖的定义："><a href="#8-2-3-1-规范覆盖的定义：" class="headerlink" title="8.2.3.1 规范覆盖的定义："></a>8.2.3.1 <strong>规范覆盖的定义：</strong></h6><p>函数依赖集F的最小覆盖FC满足：</p><ul><li>F和FC是等价的</li><li>FC的所有函数依赖都没有多余属性</li><li>FC的每个函数依赖的决定部分（左氏）都是唯一的</li></ul><p>X→Y</p><h6 id="8-2-3-2-多余的属性"><a href="#8-2-3-2-多余的属性" class="headerlink" title="8.2.3.2 多余的属性"></a>8.2.3.2 多余的属性</h6><p>考虑函数依赖集F中的某个函数依赖$\alpha→\beta$</p><p>左式属性$A\in \alpha$是多余属性，如果函数依赖集F在逻辑上蕴含：$(F-{\alpha→\beta })\cup ({\alpha -A }→\beta)$</p><p>右式属性$B\in \alpha$是多余属性，如果函数依赖集F在逻辑上蕴含：$(F-{\alpha→\beta })\cup (\alpha→ {\beta -B })$</p><h6 id="8-2-3-3-计算最小覆盖"><a href="#8-2-3-3-计算最小覆盖" class="headerlink" title="8.2.3.3 计算最小覆盖"></a>8.2.3.3 计算最小覆盖</h6><p>函数依赖集F的最小覆盖FC的算法：</p><p>FC&#x3D;F</p><p>repeat</p><p>​合并FC中有相同决定部分的函数依赖，即把$\alpha 1→\beta 1$和$\alpha 1→\beta 2$合并为$\alpha 1→\beta 1\beta 2$</p><p>​把FC中的多余属性删掉</p><p>until FC不再发生变化</p><hr><p>(PPT44）</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>数据库</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>kaggle竞赛小技巧——为什么四舍五入能提高分数</title>
    <link href="/2022/20220311/"/>
    <url>/2022/20220311/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>  该观点由<a href="https://www.kaggle.com/c/tabular-playground-series-jan-2022">tabular-playground-series-jan-2022</a>的冠军<a href="https://www.kaggle.com/ambrosm">AmbrosM</a>提出。</p><p>全文：<a href="https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/301249">https://www.kaggle.com/c/tabular-playground-series-jan-2022/discussion/301249</a></p><span id="more"></span><hr><p>这个观点基于三个假设：</p><p>1）真实的标签都是整数</p><p>2）比赛评分使用MAE（或SMAPE）</p><p>3）预测误差是连续单峰函数，最大概率密度在0附近，如下图（墙内下图可能未显示）所示</p><p><img src="/images/gXC61uC.png" alt="density"></p><p>绿色代表四舍五入减少误差，红色代表四舍五入增加误差。</p><p>易证绿色的面积会比红色的面积大。</p><p>虽然选择四舍五入会提高分数，但是四舍五入也会产生意想不到的效果：会引入不连续性，增加了结果的方差，使模型评估更加困难。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>kaggle</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>世另我（二）</title>
    <link href="/2022/me2/"/>
    <url>/2022/me2/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="密码错误.提示：无." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容">  <script id="hbeData" type="hbeData" data-hmacdigest="ceba6928d7387e53b125e7a90c4fd450ad66cfb56a0d7d22c673747afb9c27dd">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">请输入密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《被讨厌的勇气》笔记</title>
    <link href="/2022/20220228/"/>
    <url>/2022/20220228/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p> 《被讨厌的勇气》是由日本作家岸见一郎和古贺史健所著。以青年与哲人的对话呈现阿德勒心理学的思想与内容。</p><span id="more"></span><hr><h4 id="人可以改变"><a href="#人可以改变" class="headerlink" title="人可以改变"></a>人可以改变</h4><p>“任何经历本身并不是成功或者失败的原因。我们并非因为自身经历中的刺激——所谓的心理创伤——而痛苦，事实上我们会从经历中发现符合自己目的的因素。决定我们自身的不是过去的经历，而是我们自己赋予经历的意义。”</p><p>我们给过去的经历“赋予了什么样的意义”，这直接决定了我们的生活。人生不是由别人赋予的，而是由自己选择的，是自己选择自己如何生活。</p><p>我们不可能乘坐时光机器回到过去，也不可能让时针倒转。如果你成了原因论的信徒，那就会在过去的束缚之下永远无法获得幸福。</p><h4 id="“不幸”是自己的选择"><a href="#“不幸”是自己的选择" class="headerlink" title="“不幸”是自己的选择"></a>“不幸”是自己的选择</h4><p>你在人生的某个阶段里选择了“不幸”。这既不是因为你生在了不幸的环境中，也不是因为你陷入了不幸的境地中，而是因为你认为“不幸”对你自身而言是一种“善”。</p><p>尽管有些不方便、不自由，但你还是感觉现在的生活方式更好，大概是觉得一直这样不做改变比较轻松吧。 　　</p><p>如果一直保持“现在的我”，那么如何应对眼前的事情以及其结果会怎样等问题都可以根据经验进行推测，可谓是轻车熟路般的状态。即使遇到点状况也能够想办法对付过去。 　　</p><p>另一方面，如果选择新的生活方式，那就既不知道新的自己会遇到什么问题，也不知道应该如何应对眼前的事情。未来难以预测，生活就会充满不安，也可能有更加痛苦、更加不幸的生活在等着自己。也就是说，即使人们有各种不满，但还是认为保持现状更加轻松、更能安心。</p><p>要想改变生活方式需要很大的“勇气”。面对变化产生的“不安”与不变带来的“不满”，你一定是选择了后者。</p><p>阿德勒心理学就是勇气心理学。你之所以不幸并不是因为过去或者环境，更不是因为能力不足，你只不过是缺乏“勇气”，可以说是缺乏“获得幸福的勇气”。</p><h4 id="”脸红恐惧症“"><a href="#”脸红恐惧症“" class="headerlink" title="”脸红恐惧症“"></a>”脸红恐惧症“</h4><p>为什么只盯着缺点就是不肯去喜欢自己呢？那是因为你太害怕被他人讨厌、害怕在人际关系中受伤。</p><p>就像有脸红恐惧症的她害怕被男性拒绝一样，你也很害怕被他人否定。害怕被别人轻视或拒绝、害怕心灵受伤。你认为与其陷入那种窘境倒还不如一开始就不与任何人有关联。也就是说，你的“目的”是“避免在与他人的关系中受伤”。</p><p>那么，如何实现这种目的呢？答案很简单。只要变成一个只看自己的缺点、极其厌恶自我、尽量不涉入人际关系的人就可以了。如此一来，只要躲在自己的壳里就可以不与任何人发生关联，而且万一遭到别人的拒绝，还可以以此为理由来安慰自己。心里就会想：因为我有这样的缺点才会遭人拒绝，只要我没有这个缺点也会很讨人喜欢。</p><h4 id="自卑感"><a href="#自卑感" class="headerlink" title="自卑感"></a>自卑感</h4><p>自卑感是一种“自己没有价值或者只有一点儿价值”之类的感觉。</p><p>人都处于追求优越性这一“希望进步的状态”之中，树立某些理想或目标并努力为之奋斗。同时，对于无法达成理想的自己就会产生一种自卑感。例如，越是有远大志向的厨师也许就越会产生“还很不熟练”或者“必须做出更好的料理”之类的自卑感。</p><p>目前“自卑情结”这个词似乎在使用的时候与自卑感是一样的意思。就像“我为自己的单眼皮感到自卑”或者“他对自己的学历有自卑感”之类的描述中全都用“自卑情结”这个词来表示自卑感。其实，这完全是一种误用。自卑情结一词原本表示的是一种复杂而反常的心理状态，跟自卑感没有关系。例如，即使弗洛伊德提出的“俄狄浦斯情结”原本也是指一种对同性父母亲的反常对抗心理。</p><p>自卑感也可以成为促成努力和进步的契机。例如，虽然对学历抱有自卑感，但若是正因为如此，才下定“我学历低所以更要付出加倍的努力”之类的决心，那反而成了好事。 　　</p><p>而另一方面，自卑情结是指把自己的自卑感当作某种借口使用的状态。具体就像“我因为学历低所以无法成功”或者“我因为长得不漂亮所以结不了婚”之类的想法。像这样在日常生活中大肆宣扬“因为有A所以才做不到B”这样的理论，这已经超出了自卑感的范畴，它是一种自卑情结。</p><p>自卑情结的另一个侧面：那些用语言或态度表明自己的自卑情结的人和声称“因为有A所以才不能做到B”的人，他们的言外之意就是“只要没有A,我也会是有能力、有价值的人”。</p><p>拥有自卑感即感觉目前的“我”有所欠缺的状态。</p><p>如何去弥补自己欠缺的部分呢？最健全的姿态应该是想要通过努力和成长去弥补欠缺部分，例如刻苦学习、勤奋练习、努力工作等。 　　</p><p>但是，没有这种勇气的人就会陷入自卑情结。拿刚才的例子来讲，就会产生“我因为学历低所以无法成功”之类的想法，并且还会进一步通过“如果有高学历自己也很容易成功”之类的话来暗示自己的能力。意思就是“现在只不过是被学历低这个因素所埋没，’真正的我‘其实非常优秀”。</p><p>人生不是竞争。只要自己不断前进即可。当然，也 　　没有必要把自己和别人相比较。健全的自卑感不是来自与别人的比较，而是来自与“理想的自己”的比较。</p><h4 id="人生课题"><a href="#人生课题" class="headerlink" title="人生课题"></a>人生课题</h4><p>从孩提时代开始考虑人生这个词。孩提时代，我们在父母的守护下生活，即使不怎么劳动也可以生存下去。但是，很快就到了”自立“之时，不能继续依赖父母而必须争取精神性的自立这一点自不必说，即使在社会意义上也要自立，必须从事某些工作——这里不是指在企业上班之类狭义上的工作。 　　</p><p>此外，在成长过程中会遇到各种各样的朋友关系。当然，也会与某人结成恋爱关系甚至还有可能发展到结婚。如果是那样的话，就又会产生夫妻关系，一旦有了孩子还会出现亲子关系。 　　</p><p>阿德勒把这些过程中产生的人际关系分为”工作课题“”交友课题“和”爱的课题“这三类，又统称为”人生课题“。</p><h5 id="工作课题"><a href="#工作课题" class="headerlink" title="工作课题"></a>工作课题</h5><p>无论什么种类的工作，都没有一个人可以独立完成的。例如，我平时都在这个书房中写书稿。写作这项工作的确是无人能够代替、必须自己完成的作业。但即使如此，只有有了编辑的存在以及装订人员、印刷人员和经销或书店人员的协助，这项工作才能够成立。原则上来说，根本不可能存在不需要与他人合作完成的工作。</p><h5 id="交友课题"><a href="#交友课题" class="headerlink" title="交友课题"></a>交友课题</h5><p>这是指脱离了工作的、更广泛意义上的朋友关系。正因为没有了工作关系那样的强制力，所以也就更加难以开始和发展。</p><h5 id="爱的课题"><a href="#爱的课题" class="headerlink" title="爱的课题"></a>爱的课题</h5><p>这一点可以分成两个阶段：一个就是所谓的恋爱关系，而另一个就是与家人的关系，特别是亲子关系。在工作、交友和爱这三大课题中，爱之课题恐怕是最难的课题。 　　</p><p>例如，当由朋友关系发展成恋爱关系的时候，一些在朋友之间被允许的言行就不再被允许了。具体说来，例如不可以跟异性朋友一起玩儿，有时候甚至仅仅因为跟异性朋友打电话，恋人就会吃醋。像这样，距离近了，关系也深了。</p><p>但是，阿德勒不同意束缚对方这一点。如果对方过得幸福，那就能够真诚地去祝福，这就是爱。相互束缚的关系很快就会破裂。</p><p>如果在一起感到苦闷或者紧张，那即使是恋爱关系也不能称之为爱。当人能够感觉到”与这个人在一起可以无拘无束“的时候，才能够体会到爱。既没有自卑感也不必炫耀优越性，能够保持一种平静而自然的状态。真正的爱应该是这样的。 　　另一方面，束缚是想要支配对方的表现，也是一种基于不信任感的想法。与一个不信任自己的人处在同一个空间里，那就根本不可能保持一种自然状态。阿德勒说：”如果想要和谐地生活在一起，那就必须把对方当成平等的人。“</p><p>恋爱关系或夫妻关系还可以选择”分手“。即使常年一起生活的夫妻，如果难以继续维持关系的话，也可以选择分手。但是，亲子关系原则上就不可以如此。假如恋爱是用红色丝线系起来的关系的话，那亲子关系就是用坚固的锁链联结起来的关系。而且，自己手里只有一把小小的剪刀。亲子关系难就难在这里。</p><p>现阶段能说的就是不能够逃避。无论多么困难的关系都不可以选择逃避，必须勇敢去面对。即使最终发展成用剪刀剪断，也要首先选择面对。最不可取的就是在”这样“的状态下止步不前。 　　</p><p>人根本不可能一个人活着，只有在社会性的环境之下才能成为”个人“。因此，阿德勒心理学把作为个人的”自立“和在社会中的”和谐“作为重大目标。那么，如何才能实现这些目标呢？阿德勒说：”在这里必须要克服’工作，‘交友’和‘爱’这三大课题。“</p><h5 id="逃避"><a href="#逃避" class="headerlink" title="逃避"></a>逃避</h5><p>阿德勒把这种企图设立种种借口来回避人生课题的情况叫作”人生谎言‘。</p><p>对于自己目前所处的状态，把责任转嫁给别人，通过归咎于他人或者环境来回避人生课题。前面我提到的患脸红恐惧症的那个女学生也是一样——对自己撒谎，也对周围的人撒谎。仔细考虑一下，这的确是一个相当犀利的词语。</p><h4 id="寻求认可"><a href="#寻求认可" class="headerlink" title="寻求认可"></a>寻求认可</h4><p>阿德勒心理学的一个大前提。阿德勒心理学否定寻求他人的认可。你不是为了满足别人的期待而活着，我也不是为了满足别人的期待而活着。我们没必要去满足别人的期待。</p><p>过于希望得到别人的认可，就会按照别人的期待去生活。也就是舍弃真正的自我，活在别人的人生之中。而且，请你记住，假如说你“不是为了满足他人的期待而活”，那他人也“不是为了满足你的期待而活”。当别人的行为不符合自己的想法的时候也不可以发怒。这也是理所当然的事情。</p><h4 id="课题分离"><a href="#课题分离" class="headerlink" title="课题分离"></a>课题分离</h4><p>基本上，一切人际关系矛盾都起因于对别人的课题妄加干涉，或者自己的课题被别人妄加干涉。只要能够进行课题分离，人际关系就会发生巨大改变。</p><p>辨别究竟是谁的课题的方法非常简单，只需要考虑一下“某种选择所带来的结果最终要由谁来承担？”如果孩子选择“不学习”这个选项，那么由这种决断带来的后果一一例如成绩不好、无法上好学校等——最终的承担者不是父母，而是孩子。也就是说，学习是孩子的课题。</p><p>阿德勒心理学并不是推崇放任主义。放任是一种不知道也不想知道孩子在做什么的态度。而阿德勒心理学的主张不是如此，而是在了解孩子干什各的基础上对其加以守护。如果就学习而言，告诉孩子这是他自己的课题，在他想学习的时候父母要随时准备给予帮助，但绝不对孩子的课题妄加干涉。在孩子没有向你求助的时候不可以指手画脚。</p><p>关于自己的人生你能够做的就只有“选择自己认为最好的道路”。另一方面，别人如何评价你的选择，那是别人的课题，你根本无法左右。</p><p>课题分离并不是人际关系的最终目标，而是入口。</p><p>如果人际关系中有“回报思想”存在，那就会产生“因为我为你做了这些，所以你就应该给予相应回报”这样的想法。当然，这是一种与课题分离相悖的思想。我们既不可以寻求回报，也不可以受其束缚。</p><p>阿德勒心理学中有反常识的方面：否定原因论、否定精神创伤、采取目的论；认为人的烦恼全都是关于人际关系的烦恼；此外，不寻求认可或者课题分离也全都是反常识的理论。</p><p>为了满足别人的期望而活以及把自己的人生托付给别人，这是一种对自己撒谎也不断对周围人撒谎的生活方式。</p><h4 id="阿德勒心理学认为的自由"><a href="#阿德勒心理学认为的自由" class="headerlink" title="阿德勒心理学认为的自由"></a>阿德勒心理学认为的自由</h4><p>阿德勒心理学认为“一切烦恼皆源于人际关系”。也就是说，我们都在追求从人际关系中解放出来的自由。但是，一个人在宇宙中生存之类的事情根本不可能。</p><p>自由就是被别人讨厌。这是你行使自由以及活得自由的证据，也是你按照自我方针生活的表现。</p><p>这只是分离课题。即使有人不喜欢你，那也并不是你的课题。并且，“应该喜欢我”或者“我己经这么努力了还不喜欢我也太奇怪了”之类的想法也是一种干涉对方课题的回报式的思维。不畏惧被人讨厌而是勇往直前，不随波逐流而是激流勇进，这才是对人而言的自由。如果在我面前有“被所有人喜欢的人生”和“有人讨厌自己的人生”这两个选择让我选的话，我一定会毫不犹豫地选择后者。比起别人如何看自己，我更关心自己过得如何。也就是想要自由地生活。</p><h4 id="人际关系的终极目标"><a href="#人际关系的终极目标" class="headerlink" title="人际关系的终极目标"></a>人际关系的终极目标</h4><p>人际关系的终点是共同体感觉。不幸之源也在于人际关系，幸福之源也在于人际关系。共同体感觉叫作”socialinterest“。</p><p>把对自己的执著（selfinterest）变成对他人的关心（socialinterest）。</p><p>直面”人生课题“。也就是不回避工作、交友、爱之类的人际关系课题，要积极主动地去面对。如果你认为自己就是世界的中心，那就丝毫不会主动融入共同体中，因为一切他人都是”为我服务的人“，根本没必要由自己采取行动。</p><p>无论是你还是我，我们都不是世界的中心，必须用自己的脚主动迈出一步去面对人际关系课题；不是考虑”这个人会给我什么“，而是要必须思考一下”我能给这个人什么“。这就是对共同体的参与和融入。</p><h4 id="横向-纵向关系"><a href="#横向-纵向关系" class="headerlink" title="横向&#x2F;纵向关系"></a>横向&#x2F;纵向关系</h4><p>平等即”横向“关系。自卑感原本就是从纵向关系中产生的一种意识。只要能够对所有人都建立起”虽不同但平等“的横向关系，那就根本不会产生自卑情结。</p><p>重要的是不”评价“他人，评价性的语言是基于纵向关系的语言。如果能够建立起横向关系，那自然就会说出一些更加真诚地表示感谢、尊敬或者喜悦的话。</p><h4 id="自我接纳"><a href="#自我接纳" class="headerlink" title="自我接纳"></a>自我接纳</h4><p>自我肯定是明明做不到但还是暗示自己说”我能行“或者”我很强“，也可以说是一种容易导致优越情结的想法，是对自己撒谎的生活方式。 　　</p><p>而另一方面，自我接纳是指假如做不到就诚实地接受这个”做不到的自己“，然后尽量朝着能够做到的方向去努力，不对自己撒谎。 　　</p><p>说得更明白一些就是，对得了60分的自己说”这次只是运气不好，真正的自己能得100分“，这就是自我肯定；与此相对，在诚实地接受60分的自己的基础上努力思考”如何才能接近100分“，这就是自我接纳。</p><h4 id="人际关系的基础——信赖"><a href="#人际关系的基础——信赖" class="headerlink" title="人际关系的基础——信赖"></a>人际关系的基础——信赖</h4><p>阿德勒心理学认为人际关系的基础不应该是“信用”，而应该是“信赖”。</p><p>信赖：在相信他人的时候不附加任何条件。即使没有足以构成信用 的客观依据也依然相信，不考虑抵押之类的事情，无条件地相信。</p><p>无条件地相信他人有时也会遭遇背叛。就好比贷款保证人有时也会蒙受损失一样。即使如此却依然继续相信的态度就叫作信赖。</p><p>信赖的反义词是怀疑。假设你把人际关系的基础建立在“怀疑”之上。怀疑他人、怀疑朋友、甚至怀疑家人或恋人，生活中处处充满怀疑。那么，这样究竟会产生什么样的关系呢？对方也能够瞬时感觉到你怀疑的目光，会凭直觉认为“这个人不信赖我”。你认为这样还能建立起什么积极的关系吗？只有我们选择了无条件的信赖，才可以构筑更加深厚的关系。</p><p>这也是一种课题分离。决定背不背叛的不是你，那是他人的课题。你只需要考虑“我该怎么做”。“如果对方讲信用我也给予信任”，这只不过是一种基于抵押或条件的信用关系。</p><p>但是，并不代表我们应该信赖所有人，即使遭到欺骗依然继续相信，一直做个傻瓜式的老好人。如果你并不想与那个人搞好关系的话，也可以用手中的剪刀彻底剪断关系，因为剪断关系是你自己的课题。</p><h4 id="他者贡献"><a href="#他者贡献" class="headerlink" title="他者贡献"></a>他者贡献</h4><p>我们只有在感觉到自己的存在或行为对共同体有益的时候，也就是体会到“我对他人有用”的时候，才能切实感受到自己的价值。是这样吧？ 　　也就是说，他者贡献并不是舍弃“我”而为他人效劳，它反而是为了能够体会到“我”的价值而采取的一种手段。</p><h4 id="三位一体"><a href="#三位一体" class="headerlink" title="三位一体"></a>三位一体</h4><p>自我接纳、他者信赖、他者贡献这这三者是缺一不可的整体。正因为接受了真实的自我——也就是“自我接纳”——才能够不惧背叛地做到“他者信赖”；而且，正因为对他人给予无条件的信赖并能够视他人为自己的伙伴，才能够做到“他者贡献”；同时，正因为对他人有所贡献，才能够体会到“我对他人有用”进而接受真实的自己，做到“自我接纳”。</p><h4 id="人生和谐"><a href="#人生和谐" class="headerlink" title="人生和谐"></a>人生和谐</h4><p>人际关系中也会遭遇到诸多不愉快的事情。但是，在这里绝对不可以搞错这样一个事实：任何情况下都只是攻击我的“那个人”有问题，而绝不是“大家”的错。 　　具有神经质生活方式的人常常使用“大家”“总是”或者“一切”之类的词语。“大家都讨厌自己”“总是只有自己受损失”或者“一切都不对”等。如果你常常说这种一般化的词语，那就需要注意了。</p><p>犹太教教义中有这么一段话：“假如有10个人，其中势必会有1个人无论遇到什么事都会批判你。他讨厌你，你也不喜欢他。而且，10个人中也会有2个人能够成为与你互相接纳一切的好朋友。剩下的7个人则两者都不是。”</p><p>这种时候，是关注讨厌你的那个人呢？还是聚焦于非常喜欢你的那2个人？抑或是关注其他作为大多数的7个人？缺乏人生和谐的人就会只关注讨厌自己的那个人来判断“世界”。</p><h4 id="幸福即贡献感"><a href="#幸福即贡献感" class="headerlink" title="幸福即贡献感"></a>幸福即贡献感</h4><p>人们寻求认可的理由现在已经很清楚了吧。人们想要喜欢自己，想要感觉自己有价值，为此就想要拥有“我对他人有用”的贡献感，而获得贡献感的常见手段就是寻求他人认可。</p><p>但是幸福要以自由为前提。</p><h4 id="甘于平凡的勇气"><a href="#甘于平凡的勇气" class="headerlink" title="甘于平凡的勇气"></a>甘于平凡的勇气</h4><p>因为无法接受“普通的自己”。所以，在“特别优秀”的梦想受挫之后便非常极端地转为“特别差劲”。 　　</p><p>但是，普通和平凡真的不好吗？有什么不好呢？实际上谁都是普通人。没有必要纠结于这一点。</p><p>普通并不等于无能，我们根本没必要特意炫耀自己的优越性。</p><h4 id="人生是一连串的刹那"><a href="#人生是一连串的刹那" class="headerlink" title="人生是一连串的刹那"></a>人生是一连串的刹那</h4><p>把人生当作登山的人其实是把自己的人生看成了一条“线”。自降生人世那-瞬间便己经开始的线，画着大大小小形形色色的曲线到达顶点，最终迎来“死”这一终点。但是，这种把人生理解为故事的想法与弗洛伊德式的原因论紧密相关，而且会把人生的大半时光当作“在路上”。</p><p>人生是“现在”这一刹那的连续。我们只能活在“此时此刻”，我们的人生只存在于刹那之中。</p><p>为遥远的将来设定一个目标，并认为现在是其准备阶段。一直想着“真正想做的是这样的事情，等时机到了就去做”，是一种拖延人生的生活方式。只要在拖延人生，我们就会无所进展，只能每天过着枯燥乏味的单调生活。因为在这种情况下，人就会认为“此时此刻”只是准备阶段和忍耐阶段。 　　</p><p>但是，为了遥远将来的考试而努力学习的“此时此刻”却是真实的存在。</p><p>人生中最大的谎言就是不活在“此时此刻”。纠结过去、关注未来，把微弱而模糊的光打向人生整体，自认为看到了些什么。你之前就一直忽略“此时此刻”，只关注根本不存在的过去和未来。对自己的人生和无可替代的刹那撒了一个大大的谎言。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
      <tag>笔记</tag>
      
      <tag>心理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高考100天</title>
    <link href="/2022/20220227/"/>
    <url>/2022/20220227/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p> 今天离高考还有100天。</p><p>距离属于我的高考100天已经过了两年了。</p><p>没有发生变化的是仍然因为疫情不得不在家上网课。</p><span id="more"></span><p>回想起来，高考结束的下午和往日也没有什么太大不同，也没有意想中的解放后的欢呼雀跃</p>]]></content>
    
    
    
    <tags>
      
      <tag>纪念</tag>
      
      <tag>回忆</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《拖延症与摩托车维修艺术——“当代年轻人的精神困境”》笔记</title>
    <link href="/2022/20220225/"/>
    <url>/2022/20220225/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>演讲者是李松蔚</p><p>这个标题是为了致敬《禅与摩托车维修艺术》这本书。</p><h4 id="拖延症的由来及本质"><a href="#拖延症的由来及本质" class="headerlink" title="拖延症的由来及本质"></a>拖延症的由来及本质</h4><p>以前并没有拖延症的概念。由诺贝尔经济学家获得者提出了拖延症（Procrastination）的概念。（<strong>此处需补全</strong>）        </p><span id="more"></span><h5 id="拖延症就是懒吗？"><a href="#拖延症就是懒吗？" class="headerlink" title="拖延症就是懒吗？"></a>拖延症就是懒吗？</h5><p>并不是这么单纯</p><h5 id="生活原本该是什么样？"><a href="#生活原本该是什么样？" class="headerlink" title="生活原本该是什么样？"></a>生活原本该是什么样？</h5><p>《禅与摩托车修理艺术》：没有人会催促你，也不会担心浪费时间。</p><h5 id="当代青年的精神困境"><a href="#当代青年的精神困境" class="headerlink" title="当代青年的精神困境"></a>当代青年的精神困境</h5><p>演讲者认为，当代青年的精神困境其实在于屈辱感、自我贬低的这种乐趣，还有就是在这个自我贬低里面所隐藏的不甘心我们看看 就希望我们经常用的一些词。</p><h5 id="拖延症在暗示什么"><a href="#拖延症在暗示什么" class="headerlink" title="拖延症在暗示什么"></a>拖延症在暗示什么</h5><p>语言&#x3D;框架</p><ul><li>懒：谁叫你自己不争气</li><li>懒型人格：能努力也是一种天赋</li><li>懒癌：重病，放弃了</li><li>自控力缺乏：缺哪补哪，勤加锻炼</li></ul><p>现有的翻译暗示：拖着不做事就是有病</p><h5 id="拖延症的正确理解"><a href="#拖延症的正确理解" class="headerlink" title="拖延症的正确理解"></a>拖延症的正确理解</h5><p>演讲者提出了另一种翻译</p><p>：明日迷</p><ul><li>对“此刻”的逃避</li><li>对“明天”的幻想</li></ul><p>明日迷：日常焦虑，不断自责，认为现在过得不好，当下状态视为痛苦。渴望未来能得到解脱。害怕无聊。</p><p>很多这个明日迷经常会有一个矛盾的概念：他经常一边会说我没有时间，我希望我把我每分每秒的时间都用来做一些正事，我很怕我无聊 ，就哪怕我走个路我也恨不得听个英语单词。或者我哪怕吃个饭我也恨不得刷两条新闻，但是另一方面 他会大量的浪费时间。</p><p>明日迷这个状态其实讲的就是你会对于时间做一种非常不经济的一种使用。</p><table><thead><tr><th>“明日迷”的生活状态</th><th>正常的生活状态</th></tr></thead><tbody><tr><td>焦虑地做事，焦虑地玩</td><td>平常心做事，轻松地玩</td></tr><tr><td>沉浸于自责</td><td>沉浸于行动</td></tr><tr><td>将当下的状态视为痛苦</td><td>平常心看待当下</td></tr><tr><td>渴盼未来的解脱</td><td>对未来抱有希望，不执迷</td></tr><tr><td>总是害怕无聊</td><td>无事可做也怡然自得</td></tr><tr><td>浪费时间有强烈的罪恶感</td><td>无所谓何为“浪费”</td></tr><tr><td>时间表很满，但总是不能完成</td><td>时间表随意，但每天都有收获</td></tr><tr><td><strong>迫切希望改变以上状态</strong></td><td><strong>对现在的状态满意</strong></td></tr></tbody></table><h5 id="“旁观者”"><a href="#“旁观者”" class="headerlink" title="“旁观者”"></a>“旁观者”</h5><p>《禅与摩托车维修艺术》：”虽然他们看起来随和、友善、轻松自在，但是却没有投入工作。他们就像旁观者一样。你会觉得他们只是在那儿晃来晃去，然后接过别人递给他们的扳手。他们对自己的工作没有认同感，不会说：“我是修理师傅。”一旦到了下午五点，八个小时一满，你知道他们会立刻放下手中的工作，马上离开，然后尽可能地不去想他们的工作。“</p><h5 id="自我的爬山-奉献的爬山"><a href="#自我的爬山-奉献的爬山" class="headerlink" title="自我的爬山&amp;奉献的爬山"></a>自我的爬山&amp;奉献的爬山</h5><p>《禅与摩托车维修艺术》：“自我的爬山者就像一支失调的乐器，步伐不是太快就是太慢，也可能失去欣赏树梢上的美丽阳光的机会。在他步履蹒跚的时候却不休息，仍然继续前进。有的时候，刚刚观察过前面的情况，他会再看一遍。所以，他对周围环境的反应不是太快就是太慢。他谈论的话题永远是别的事和别的地方。他的人虽然在此地，他的心却不在。因为他拒绝活在此地，他想赶快爬到山顶，但是即使爬上去了，他却仍然不会快乐，因为那样的话，山顶就变成了“此地”。他追寻的、他想要的，都已经围绕在他的身边，但是他并不要这一切，因为这些“就在他身边”。<strong>于是在体力和精神上，他所跨出的每一步都很吃力，因为他总认为自己的目标在远方。</strong>”</p><p>“其他的朝圣者之所以能够到达山顶，是因为充分领受到了山的神圣，以至于每一步都是一种奉献的行为，是对这种神圣的心悦诚服。山神圣的一面融入了他们的心灵，因而使他们的耐力远远超过了体力所能负荷的。”</p><h4 id="幻想是今天的死敌"><a href="#幻想是今天的死敌" class="headerlink" title="幻想是今天的死敌"></a>幻想是今天的死敌</h4><p>幻想：“明日迷”（拖延症）的核心</p><p>今天要讲的不是如何避免拖延，而是：不把我们的目标放在远方而是放在我们脚下的一步；我们如何能够从对于明日、对于明天的这种幻想中摆脱出来，回到今天，回到现在。</p><h5 id="常见的幻想方式"><a href="#常见的幻想方式" class="headerlink" title="常见的幻想方式"></a><strong>常见的幻想方式</strong></h5><p>游戏、电影、动漫、小说（尤其是一些yy小说），可以提供给我们一些幻想的麻醉作用。</p><p>社交网络然我们构建另外的一个空间。</p><p>励志故事、学霸传奇。</p><p>积攒文献。我要写一篇论文 我先下载2000篇文件 然后把它们分文别类的整理好，然后觉得自己好像就占有了这样的一些文献， 自己这个论文还没开始，别人已经利用你下文献的时间已经早就完成了。</p><p>热血沸腾的计划表。就是一般我们在学期初的时候经常会见到这种方向， 他们会说 我现在列了一个非常完美的计划 ，我这个学期要做好几件事情 ，包括我要背多少单词，我要我要做实习，我要上多少课 什么，什么热血沸腾的计划表，然后这个计划在你脑子里面成形的那一刻。</p><p>发誓。如”我再拖延就剁手“。</p><p>还有一个<strong>最隐蔽的幻想。</strong></p><h5 id="最隐蔽的幻想"><a href="#最隐蔽的幻想" class="headerlink" title="最隐蔽的幻想"></a>最隐蔽的幻想</h5><p>战胜&#x2F;消灭&#x2F;干翻“拖延症”！努力是王道！</p><ul><li>战胜的念头，是症状的一部分</li><li>“生命最不可饶恕的是浪费时间”</li><li>“要利用好每一分钟”</li><li>“最可怕是比你聪明的人比你还要勤奋”</li><li>“以绝大多数人的努力程度，还轮不到拼天赋”</li><li>“……”</li></ul><p>《禅与摩托车维修艺术》：”仓促本身就是二十世纪最要不得的态度，当你做某件事的时候，一旦想要求快，就表示你再也不关心它，而想去做别的事。“</p><h5 id="“战拖”的陷阱"><a href="#“战拖”的陷阱" class="headerlink" title="“战拖”的陷阱"></a>“战拖”的陷阱</h5><ul><li>干”正事“等于争议！</li><li>何为“正事”？<ul><li>不伴随即刻欢愉的</li><li>具有长期回报的</li></ul></li><li>用意何在？<ul><li>成就最大化</li></ul></li><li>核心：<strong>工具理性</strong></li></ul><h4 id="如何摆脱“明日迷”"><a href="#如何摆脱“明日迷”" class="headerlink" title="如何摆脱“明日迷”"></a>如何摆脱“明日迷”</h4><p>行为层面：拖延→着手行动</p><p>想法层面：幻想战胜→接纳现状</p><h5 id="聚焦于当下"><a href="#聚焦于当下" class="headerlink" title="聚焦于当下"></a>聚焦于当下</h5><p><strong>心流（Flow)体验</strong></p><p>由心理学家Mihaly Csikszentmihalyi 提出</p><p>。</p><ul><li>将注意力完全投注在某种活动上的感觉，伴有高度的兴奋感及充实感，“物我两忘”</li><li>前提：充分的内在动机</li><li>三大原则：即时反馈、挑战与技能匹配、目标明确</li></ul><h5 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h5><h6 id="为什么要做眼前的事？"><a href="#为什么要做眼前的事？" class="headerlink" title="为什么要做眼前的事？"></a>为什么要做眼前的事？</h6><ul><li>为了将来：明日迷</li><li>为了现在：你在当下这一刻，收获了什么？<ul><li>焦躁、乏味、疲倦、挫败、自责、担忧…</li></ul></li></ul><p>Pain is inevitable.Suffering is optional.</p><p>Pain是感官的刺激，suffering是我们对这个刺激的理解。</p><h6 id="创造-专属感"><a href="#创造-专属感" class="headerlink" title="创造&amp;专属感"></a>创造&amp;专属感</h6><ul><li><p>属于自己的表达</p><p>当我们在跑或者当我们在做一件事情的时候，我们是否在创造一些东西，我们是不是在表达我们自己，我们是否会感觉到我们正在做的这件事情 哪怕是非常疼痛的，哪怕是会给我们带来很多负面感受的，但是它是不是也是一种我们跟这个世界的互动</p></li><li><p>模仿是一种真正的罪恶</p><p>《禅与摩托车维修艺术》：“学校教你去模仿，如果你不模仿，老师就给你很差的分数。而在大学里，情况就复杂多了，你必须要让老师觉得，虽然你实际是在模仿，但是表面上并没有模仿。你只是吸收老师指示的重点，然后再走自己的路，这样你就能得到高分。而原创的学生则可能从最高分到最低分都有，整个学校的评分制度都对原创不利。”</p></li><li><p>由于人类知识的范围太过复杂，每一个人都变成了专家，然而却造成了彼此之间的疏离。</p><p>一个例子：以前一个木匠，他可以制造一张完整的桌子，然后他会觉得这个桌子是他的作品，他可以在这个桌子上关于他的名字，然后这个桌子就可以就像他所创造出来的一部分一样去和这个世界发生互动，但是现在呢？没有这样的木匠，大家都是这个流水线上的一个生产者。我只知道这么多桌子里面都有我拧上的一颗螺丝钉，然后我就是那个不断的像机器人一样你螺丝钉的。然后，我不觉得我在创造，也不觉得我在表达，我也不觉得我跟别人会发生什么样的关系。</p></li></ul><h5 id="追求宁静"><a href="#追求宁静" class="headerlink" title="追求宁静"></a>追求宁静</h5><h6 id="追求宁静≠逃避感受"><a href="#追求宁静≠逃避感受" class="headerlink" title="追求宁静≠逃避感受"></a>追求宁静≠逃避感受</h6><p>我们追求宁静并不是说要你用打扫房间或者是玩游戏的方式来舒缓你的心情，而是说，当你觉得心里面不宁静的时候，停下来，然后审视你自己，看看到底是哪一块不宁静，我们把那个部分觉察到。保持内心的宁静在机械工作上并不是一件小事。他是工作的核心。</p><blockquote><p> 常见的宁静方法：</p><p>钓鱼。</p><p>聊天。</p><p>打坐。</p><p>放空五分钟。</p><p>修摩托车…</p></blockquote><p>但是非常要命的就是如果你要是一个明日迷的一个状态的话，你不太允许给自己时间。你会觉得我靠，我论文还没写完，你要去钓鱼，你有病吧，虽然你可能会整整花两个小时或者三个小时的时间看美剧。但你会觉得去钓鱼这个事情是不可饶恕的 或者你说去散步 每天拿一个小时散步。为什么为什么不能用那一个小时的时间去写论文呢，但你发现那个小说你没有拿来写论文，你只是拿来聊qq了。这种事情是经常会发生的，因为他们会误以为当我坐在宿舍里，或者在我的工作岗位上，我不去做别的事情，我就在做这种事，但其实他没有办法真的做正事，因为他的心并不宁静 ，而且他也无意去保持去追求这个宁静。他试图去逃避这种烦躁的感觉，所以他虽然坐在那里，电脑开着 ，甚至那个word文档都摆在他面前，但他就可能对着手机一刷刷一下午，刷到那个朋友圈都已经刷不出东西来了。他还非常敞然若失的去看很早以前的那个东西，他并不是真的在节约时间，他只是在幻想当中节约了时间而已，虽然他的时间会浪费的更多。但如果你要是在那个时候出去走一个小时或者跟人稍微聊一聊，或者哪怕就是拿五分钟时间把自己放空一下。让你的那个宁静的那个状态能够平复一下的时候 你可能就会发现，接下来你就又可以去做那件事情了。</p><h5 id="一份地图"><a href="#一份地图" class="headerlink" title="一份地图"></a>一份地图</h5><p><a href="https://imgtu.com/i/bZGMbd"><img src="https://s4.ax1x.com/2022/02/26/bZGMbd.jpg" alt="bZGMbd.jpg"></a></p><p>如果你要去开始一段旅途的话，你需要有一份地图。</p><p>你需要有一份什么样的地图，首先你要知道自己在哪里。</p><p>你现在的状态和你最糟的状态应该是不一样的，它应该会比你最糟的状态略好一点，但是当我们是一个明日迷的时候，我们会非常的排斥现在，所以我们会倾向于把现在想的非常糟。</p><p>你幻想的那部分是你理想的状态，是你觉得最好的一个状态，不管是功成名就还是赚大钱，但是那个状态并不是你应该去的地方。因为那是你的幻想，你应该把这个状态忘掉，然后去寻找在那个幻想跟他现状之间那个真实的。</p><p><strong>几个重要的区分：</strong></p><p>第一 现在的状态并不是最糟的状态 </p><p>第二 理想的状态并不是我们的目标</p><h5 id="允许“浪费”时间"><a href="#允许“浪费”时间" class="headerlink" title="允许“浪费”时间"></a>允许“浪费”时间</h5><p>但是这种浪费，必须是一种积极的、宁静的浪费，而不是一种非常浮躁、非常慌乱的浪费。 当我们试着去用玩手机或者是上网这样的方式来逃避的时候，其实我们那个时候不是真的在浪费时间，我们是在逃避现实。</p><p>允许你用现实的方式做一点别的事，简单说，你想玩就去玩呗，而是允许你用现实的方式做一点别的事，简单说，你想玩就去玩呗，而不是用玩的方式来逃避你要逃避的那个任务，那个任务没有那么紧张，没有要求你今天必须做完，所以你可以去玩，但是你没有必要在玩的过程当中让自己不开心，然后现在我们再讲一些。</p><h5 id="“卡住了”"><a href="#“卡住了”" class="headerlink" title="“卡住了”"></a>“卡住了”</h5><p>《禅与摩托车维修艺术》：“你需要一个解决的方法，然而传统科学不曾教导你如何自己摸索着解决。它让你清楚地知道身在何处，也能够验证你拥有的知识，但是它无法告诉你该往何处去，除非你的方向只是过去方向的延续。因此创意、原创力、发明、直觉、想象——换句话说就是“不被卡住”——全在它的研究范围之外。”</p><p>“卡住了“是你要必经的这条路上的一部分，而且当你卡住的时候，这个时候你千万不要跑掉，因为你一跑掉你就回不来了。但如果你能在里面多呆一会儿，再多呆一会儿，有可能你就会发现在这个卡住的过程当中蕴含了解决的办法。这个东西并不是通过你的理性去计算出来的，而是一种直觉性的东西。它需要的是一个很关键的品质。这个品质叫做关心，你要真的很在乎这件事情的时候，你跟这个事情保持一定时间的连接。它就会转动出来，向你招手，就是我们大部分的时候我们的灵感的来源。</p><h5 id="警惕：与现实的疏离"><a href="#警惕：与现实的疏离" class="headerlink" title="警惕：与现实的疏离"></a>警惕：与现实的疏离</h5><p>在我们处理完卡住的情形之后，还会出现各种各样的陷阱，这些陷阱会一次一次召唤我们明日迷的本性。让我们沉浸到幻想之中 然后跟我们的现实保持距离。</p><ul><li>不成熟的判断</li></ul><p>比如说你认定的问题就在这里但是结果证明不是。这个时候，你就傻住了，你必须要找到新的线索。但是在你找到这个线索之前，你必须要先摒弃旧的观念。如果你一直坚持自己原来的看法。就没有办法找到真正的答案，即使他就在你眼前。这些就是卡住的那个情形。那么你要做的是什么呢，你要做的不是脱离现实 而是重新回到现实当中。</p><ul><li>自视甚高</li></ul><p>第二个陷阱就是我们的自恋。当你在当下这一刻，如果你是一个自恋的人的时候，你就会跟现实脱钩。所以如果你发现自己的性格当中有自大的那一面的话，这个性格是颇为不利的。切记！当你在做一件事情的时候不要自恋，不要自我膨胀，甚至你装也要装的谦虚一点。</p><ul><li>过度担心</li></ul><p>过度担心指的是你会担忧假想当中的困扰，然后你就会产生各种荒谬的结论。你会因为自己的紧张而认定机器出了各种问题，然后一旦这个机器真的出现了某些问题，就更验证容易起初对自己的低估。</p><p>这种过度担心的品质，尤其是现在，因为我们这个信息实在是非常的丰富，所以你哪怕有一点点微小的担心 总是可以寻找到一些证据来支持你。比如说，某一天你可能会觉得肚子有点痛，然后你在百度上搜索一下肚子痛，然后你就会很吃惊，因为你会发现有若干种诊断，都会让你觉得很很符合，然后如果你要是不去医院看一看的话，就没办法解决这个担心。</p><p>你要做的事情是什么呢，努力地去研究。你越是研究就会越平静。你要记得 ：你追求的是内心的平静，而不仅仅是把机器修好。</p><p>我们工作的根本不是在于创造某种价值，而是在于获得我们内心的成绩。</p><ul><li>枯燥</li></ul><p>枯燥表示已经丧失了从新鲜角度看事情的能力。</p><p>如果你已经觉得这个事情没有意义了，那你就需要去小心，因为你已经开始厌倦。当你厌倦的时候，你很可能会沉浸入幻想这样的一个我们说的明日迷的最常见的一个品质，所以这个时候你不如暂时的离开那个机器去睡一觉。或者去散散步，或去钓鱼。如果你要是不能停下来的话 接下来很可能就会出问题。所以如果你开始觉得有枯燥的征兆的时候，你要学会调节自己。</p><ul><li>烦躁</li></ul><p>烦躁跟枯燥略有不同，烦躁是因为你低估了工作所需要的时间。然后比如说你认为这个工作需要七天的时间，然后你觉得比如说这篇论文你需要七天的时间，然后觉得你前两天可以用来写导言。但是你到了第四天的时候，发现他导言还没有写完，这个时候你很可能就会烦躁了，因为你不知道后边会怎么样。</p><p>增加你的工作时间，这个需要你的价值观增加一些弹性。通常就会牺牲掉一些进取心，但是这个牺牲是必须的，因为这总比因为烦躁而引发很多错误，最终导致进取心丧失殆尽要好得多。</p><p>所以我们需要给自己的计划多增加一些弹性。比如说有这个事情我三天可以做完，我确定我三天可以做完的事情，把计划定到七天 这样的话我可以用一些时间去干别的，然后它可以让我的生活变得更好。而不是有可能会让我面临着一些让我的进取性丧失殆尽的这样的可能</p><ul><li>最后一种陷阱</li></ul><p>大家都很焦虑。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>如何画一张美好的画？先让自己变得美好，再顺其自然地画出来。</p><p>要有一个正确态度 </p><h4 id="演讲问答"><a href="#演讲问答" class="headerlink" title="演讲问答"></a>演讲问答</h4><p>Q：不喜欢某专业，由于不喜欢专业造成了我的拖延</p><p>A：明确喜欢的不知道有什么理由会阻挡转换。真正喜欢的没什么可以不放弃的。</p><p>Q：如何开始第一步？</p><p>A：在某些时段来强行打断自己。当你选择三国杀来逃避的时候，打断自己。你会发现没什么事可做，去发呆，面对这一刻。</p><p>Q：幻想和梦想的区别？</p><p>A：幻想和梦想的区别——是否在做事。明日迷——做这件事而想另外一件事。</p><p>Q：我们的生活能受到自己的掌控吗？</p><p>A：不能，我们只能活在当下。有些人甚至认为能掌控自己的儿女的生活。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>演讲</tag>
      
      <tag>心理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>yolo三剑客</title>
    <link href="/2022/20220224/"/>
    <url>/2022/20220224/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>yolo&#x3D;you only look once</p><p>一个非常常用的目标检测算法</p><p>现在有5个版本，前三个由yolo之父Joseph Redmon领衔的研究人员研发。yolov3提出后不久，Joseph Redmon宣布退出CV界。故后两个版本均有另外的人研发。</p><span id="more"></span><h3 id="yolov1"><a href="#yolov1" class="headerlink" title="yolov1"></a>yolov1</h3><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p><strong>优点：</strong></p><p>1.速度快，性能优于其他检测方法，包括DPM和R-CNN。</p><p>YOLO在训练和测试期间看到整个图像，因此它隐式地编码有关类及其外观的上下文信息。Fast  R-CNN是一种顶级检测方法，它会将图像中的背景补丁误认为是物体，因为它看不到更大的背景。与Fast R-CNN相比，YOLO犯的背景错误不到一半。</p><p><strong>缺点：</strong></p><p>精确度不高，虽然它可以快速识别图像中的物体，但它很难精确定位某些物体，尤其是小物体。</p><p>由于模型从数据中学习预测边界框，因此很难将其推广到具有新的或不寻常的长宽比或配置的对象</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h4><p>算法将输入图像划分为S×S网格。如果物体的中心落入网格单元，该网格单元负责检测该物体。每个网格单元预测B个边界框和这些框的置信度分数。这些置信度分数反映了模型对盒子包含对象的置信度，以及它认为盒子预测的准确性。</p><p>在形式上，我们将置信度定义为<br>$$<br>Pr(物体)*IOU^{truth}_{pred}<br>$$<br>每个边界框由5个预测组成：x、y、w、h和置信度。</p><p>x，y坐标是预测框的中心坐标。w、h是预测框的宽度和高度。最后，置信度预测表示预测框和任何实际框之间的IOU。每个网格单元还预测C条件类概率$Pr（Class_i  | Object）$。这些概率取决于包含对象的网格单元。</p><h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p>网络结构基于 GoogLeNet。</p><p>![屏幕截图 2022-02-24 191727](&#x2F;images&#x2F;屏幕截图 2022-02-24 191727.png)</p><p>Fast YOLO使用的神经网络具有较少的卷积层（9层而不是24层）和较少的滤filter。除了网络的大小之外，YOLO和Fast  YOLO之间的所有训练和测试参数都是相同的。</p><p>为了避免过拟合，使用了Dropout 和数据增强（原始图像20%的随机缩放和旋转、在HSV上将曝光度和饱和度乘以若干不高于1.5的倍数）</p><h4 id="与其他目标检测算法的对比"><a href="#与其他目标检测算法的对比" class="headerlink" title="与其他目标检测算法的对比"></a>与其他目标检测算法的对比</h4><ul><li></li></ul><h3 id="yolov2"><a href="#yolov2" class="headerlink" title="yolov2"></a>yolov2</h3><p>又叫yolo9000，因为可以对9000个类别识别。</p><h4 id="改进之处"><a href="#改进之处" class="headerlink" title="改进之处"></a>改进之处</h4><p><strong>增加了BN层</strong></p><p><strong>增加了分辨率</strong></p><p><strong>使用了anchor</strong></p><p><strong>使用Kmeans选择anchor</strong></p><p>anchor的思想来自于Fast-RCNN。anchor是从数据集中统计得到的(Faster-RCNN中的Anchor的宽高和大小是手动挑选的)。</p><h3 id="yolov3"><a href="#yolov3" class="headerlink" title="yolov3"></a>yolov3</h3><h4 id="改进之处-1"><a href="#改进之处-1" class="headerlink" title="改进之处"></a>改进之处</h4><p><strong>网络结构发生变化</strong></p><p><img src="/images/v2-085b6d95dc53894e5de4fe95d2249b06_b.jpg" alt="img"></p><p><strong>softmax-&gt;交叉熵：</strong></p><p>YOLO v3使用多标签分类，用多个独立的logistic分类器代替softmax函数，以计算输入属于特定标签的可能性。在计算分类损失进行训练时，YOLO v3对每个标签使用二元交叉熵损失。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《正确写作美国大学生数学建模竞赛论文》笔记</title>
    <link href="/2022/20220201/"/>
    <url>/2022/20220201/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h5 id="写作规范："><a href="#写作规范：" class="headerlink" title="写作规范："></a>写作规范：</h5><h6 id="1-基本要求："><a href="#1-基本要求：" class="headerlink" title="1.基本要求："></a>1.基本要求：</h6><p>（1）使用第一人称复数代词</p><p>（2）使用现在时态</p><p>（3）使用主动语态</p><span id="more"></span><p>例：</p><p>The model produced a desirable conclusion.（√）</p><p>A desirable conclusion was produced by the model. (x)</p><p>（4）使用动词表示动作</p><p>例：</p><p>We give an analysis of the solution of the equation and arrive at the conclusion that our model is a good one.（x）</p><p>After we solve the equation and analyze the result,we conclude that our model is a good one. (√)</p><h6 id="2-段落和句子"><a href="#2-段落和句子" class="headerlink" title="2.段落和句子"></a>2.段落和句子</h6><p>（1）简短的段落</p><p>（2）使用简单句，不免用多个从句表达多个意思</p><p>（3）略去琐碎细节</p><p>如公式的详细中间过程可省略。</p><h6 id="3-单词和短语"><a href="#3-单词和短语" class="headerlink" title="3.单词和短语"></a>3.单词和短语</h6><p>（1）使用简单的单词和短语</p><p>另外，assume 和suppose这两个词后面应该加that</p><blockquote><p>简化复杂的短语：</p><p>at the present time -&gt; now</p><p>concerning the issue -&gt;about</p><p>currently -&gt; now</p><p>due to the fact that -&gt;because</p><p>for the purpose of -&gt; for</p><p>has the ability to -&gt; can</p><p>in the event that -&gt;if</p></blockquote><p>（2）使用有具体含义的词汇</p><p>比如方程应该用equation而不是expression(表达式)</p><p>（3）适当使用过渡词</p><p>（4）使用并列短语强调相似性</p><p>叙述相似的事件或相似的对象时，应该使用结构相似的句子。</p><p>（5）避免单调重复</p><p>First we solved an equation.Then we solved another equation.(X)</p><p>First we solved an equation.The solution of the first equation allowed us to solve the second equation.(√)</p><p>（6）避免使用同一词汇描述不同的对象</p><p>（7）代词所指的名词必须清晰</p><p>（8）正确使用that和which</p><h6 id="4-视觉效果"><a href="#4-视觉效果" class="headerlink" title="4.视觉效果"></a>4.视觉效果</h6><p>（1）让论文看上去简单</p><p>重要的论述和公式要看上去醒目。新定义的概念或词语应该用黑体字。重要的句子要放在段落开头。重要的公式以独行公式的形式展示。</p><p>（2）正确使用列表</p><p>一般不超过十项</p><p>在列表前正确使用冒号——完整句子用冒号，否则不用</p><p>（3）必要处使用表格</p><p>（4）必要处使用图</p><p>（5）不过分吹嘘论文结果，不用使用感叹号</p><h6 id="5-论文排版"><a href="#5-论文排版" class="headerlink" title="5.论文排版"></a>5.论文排版</h6><p>正文11号或12号字，一般使用正体Times New Roman</p><h5 id="论文写作"><a href="#论文写作" class="headerlink" title="论文写作"></a>论文写作</h5><h6 id="1-论文结构合理"><a href="#1-论文结构合理" class="headerlink" title="1.论文结构合理"></a>1.论文结构合理</h6><p>论文分为以下部分：</p><ul><li><p>摘要</p></li><li><p>问题重述</p></li><li><p>列出并解释所有的前提条件和假设</p></li><li><p>论证建模的合理性或给出建模的动机</p></li><li><p>模型设计</p></li><li><p>讨论如何检验模型，包括误差分析和稳定性测试（如对条件、敏感度等因素进行分析和测试）</p></li><li><p>模型的优缺点</p></li></ul><h6 id="2-引言部分"><a href="#2-引言部分" class="headerlink" title="2.引言部分"></a>2.引言部分</h6><p>浅显易懂，避免使用数学表达式和术语</p><h6 id="3-论文主体"><a href="#3-论文主体" class="headerlink" title="3.论文主体"></a>3.论文主体</h6><p>（1）假设条件和解释</p><p>在论文中应明确列出所有用到的假设条件，并解释其合理性。</p><p>（2）解释模型设计</p><p>应该从简单模型开始，逐步加工、修改及完善。</p><p>建模时，参赛小组应集中精力设计一个模型，或者最终能导出一个较好模型的一系列子模型。</p><p>另外，高水平的论文通常会把赛题看成是一般问题的一个特例。</p><p>（3）检验模型</p><p>应讨论参数值的微小变化对模型和结论会造成什么样的影响。</p><p>用简单的特例来验证模型的正确性。</p><p>（4）讨论模型的优缺点</p><p>缺点的根源肯来自：某些假设条件简化了问题，部分参数只是取近似值，甚至是估计出来的。</p><p>模型的优点应该在论文中明确指出。若模型的正确性可在一些简单的情形下得到验证，可以说“用本文的模型得到的结果与常识和经验相符合”。</p><p>若模型的某些优点虽然已经在前文提过，也应再次强调。</p><p>由于竞赛时间有限和计算条件的限制，赛题中的不少问题参赛小组可能无法在竞赛期间解决。如果有充足的资源，参赛小组将能够解决这些问题的话，则应该在论文中明确地指出来。</p><h6 id="4-摘要部分"><a href="#4-摘要部分" class="headerlink" title="4.摘要部分"></a>4.摘要部分</h6><p>第一句话尤其重要，应该用引人入胜的语言激发读者的兴趣。</p><p>但是不要包含太多的数学内容。</p><h5 id="如何撰写优胜论文"><a href="#如何撰写优胜论文" class="headerlink" title="如何撰写优胜论文"></a>如何撰写优胜论文</h5><h6 id="1-常见错误"><a href="#1-常见错误" class="headerlink" title="1.常见错误"></a>1.常见错误</h6><p>获奖不是唯一目的。</p><p>参赛目的很多，比如：</p><p>（1）获得经验</p><p>（2）培养团队合作精神</p><p>（3）完成竞赛后的成就感</p><p>等等等等。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>数学建模</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图神经网络</title>
    <link href="/2022/20220122/"/>
    <url>/2022/20220122/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>来自博客文章：A Gentle Introduction to Graph Neural Networks<br><a href="https://distill.pub/2021/gnn-intro/">https://distill.pub/2021/gnn-intro/</a></p><p>作者都来自Google Research </p><p>发表在 distill 网站(博客相比论文写作更自由)。 </p><span id="more"></span><hr><p><strong>一些符号：</strong></p><p>V 顶点</p><p>E 边</p><p>U 全局信息，整个图 </p><p>图可分为无向图和有向图。</p><p><strong>图片如何表示成图？</strong></p><p> 244 * 244 * 3通道，3维度的tensor</p><p>把图片看作一张图，一个像素是一个点；一个像素跟我是连接关系的话，像素之间连一条边。</p><h5 id="GNN"><a href="#GNN" class="headerlink" title="GNN"></a>GNN</h5><p>A GNN is  an optimizable transformation on all attributes of the graph (nodes, edges, global-contex ) that preserves graph symmetries (permuation invariance). </p><p>本文使用message passing NN </p><h5 id="GNN-对超参数比较敏感："><a href="#GNN-对超参数比较敏感：" class="headerlink" title="GNN 对超参数比较敏感："></a>GNN 对超参数比较敏感：</h5><p>多少层、attribute的embedding的维度、汇聚使用什么操作max average、怎样传递消息 </p><h5 id="GNN的假设："><a href="#GNN的假设：" class="headerlink" title="GNN的假设："></a>GNN的假设：</h5><p>图的对称性</p><h5 id="GCN（图卷积神经网络）"><a href="#GCN（图卷积神经网络）" class="headerlink" title="GCN（图卷积神经网络）"></a>GCN（图卷积神经网络）</h5><p>搜索</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet</title>
    <link href="/2022/20220121/"/>
    <url>/2022/20220121/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文：</p><p>ImageNet Classification with Deep Convolutional Neural Networks</p><p>——By Alex Krizhevsky, Ilya Sutskever and  Geoffrey E. Hinton  </p><p><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p><span id="more"></span><hr><p>过去的观点：过拟合代表深度学习的一个派别，深度学习说可以用很大的模型，通过正则使得不要过拟合</p><p>现在的观点：正则并不重要，神经网络结构更重要，使得你在没有很好的正则情况下也一样能训练出来</p><h5 id="文章亮点"><a href="#文章亮点" class="headerlink" title="文章亮点"></a>文章亮点</h5><p><strong>ReLu正则化：</strong></p><p>在过去有很好的作用，在现在的技术发展起来后，其实选哪个激活函数都一样，relu更简单。</p><p><strong>GPU训练：</strong></p><p>GPU训练速度快。</p><p><strong>Local Response Normalization:</strong></p><p><strong>Overlapping ：</strong></p><p><strong>Overall Architecture：</strong></p><p>输入是 224 × 224 × 3 224\times224\times3224×224×3 的图片，然后是5个卷积层，接着是3个全连接层，最后一层是softmax层，输出为1000个类别标签的预测概率分布。使用了两个GPU进行训练（现在训练网络一般可以不同分割模型），将网络模型切成两半分别在两个GPU中进行训练。第2个、第4个和第5个卷积层的输入为同一GPU上之前一层卷积层的输出，而第3个卷积层的输入为两个GPU上的第2个卷积层输出。每个全连接层的输入都为前一层网络的全部输出。可以看到，随着网络深度的增加，卷积层中图像大小在减少，而深度在不断增加。</p><h5 id="如何避免过拟合"><a href="#如何避免过拟合" class="headerlink" title="如何避免过拟合"></a>如何避免过拟合</h5><p>1.数据增强（Data Augmentation）</p><p>1)人工放大图片，随机crop 224*224。</p><p>2)通过PCA改变rgb</p><p>2.Dropout</p><p>当时以为是ensemble的技巧，实际上等价于一个L2正则项。</p><h5 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h5><p>使用SGD来训练，当时调参难调（当时常用的L-BFGS等调参更容易些）</p><p>weight decay很重要、</p><p>0.01方差的高斯分布来初始化权重，全连接层初始化为1。（虽然全初始化为0更常见）</p><p>使用了学习率衰减（每过一段时间除以10），现在的学习率使用warmup策略。</p><h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p><strong>奇怪的现象：</strong></p><p>GPU1学习到的东西和颜色无关，而GPU2学习到的东西和颜色有关。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何读论文——李沐</title>
    <link href="/2022/20220120/"/>
    <url>/2022/20220120/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文大致分为以下几个部分：</p><p>1.title<br>2.abstract<br>3.introduction<br>4.method<br>5.experiments<br>6.conclusion</p><p>论文读三遍。</p><span id="more"></span><h6 id="第一遍"><a href="#第一遍" class="headerlink" title="第一遍"></a>第一遍</h6><p>标题、摘要、结论。</p><p>然后看一看方法和实验部分重要的图和表。这样可以花费十几分钟时间了解到论文结果怎么样，质量怎么样，是否适合你的研究方向。</p><h6 id="第二遍"><a href="#第二遍" class="headerlink" title="第二遍"></a>第二遍</h6><p>确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要了解重要的图和表，知道每一个部分在干什么，圈出相关文献。</p><p>觉得文章太难，可以读引用的文献。</p><p>如果觉得不需要了解那么就可以停止了。</p><h6 id="第三遍"><a href="#第三遍" class="headerlink" title="第三遍"></a>第三遍</h6><p>每句话作者在干什么</p><p>提出什么问题，用什么方法来解决这个问题。实验是怎么做的。如果是我来做怎么实现？能不能做得更好？</p><p>合上文章，回忆每一个部分在讲什么</p><h6 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h6><p>第一遍 做海选</p><p>第二遍 做精选</p><p>第三遍 重点研读</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>BERT</title>
    <link href="/2022/20220119/"/>
    <url>/2022/20220119/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文：BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding</p><p><a href="https://arxiv.org/abs/1810.04805?context=cs">https://arxiv.org/abs/1810.04805?context=cs</a></p><span id="more"></span><hr><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p>BERT&#x3D;Bidirectional Encoder Representations from Transformers</p><p>pre-trained language representations 两类策略：</p><ul><li><p>基于特征的ELMo (构建和每一个下游任务相关的 NN 架构；训练好的特征（作为额外的特征） 和 输入 一起放进模型)</p></li><li><p>基于微调参数的 GPT</p></li></ul><p>ELMo 和 GPT 预训练时 使用 unidirectional langugage model，使用相同的目标函数</p><p>语言模型是单向的、预测未来。不是给第 一句、第三句，预测第二句</p><p>BERT 通过 MLM (带掩码的语言模型）作为预训练的目标，来减轻语言模型的单向约束。inspired by the Close task 1953 </p><blockquote><p>MLM：<br>每次随机选输入的词源 tokens, 然后 mask 它们，目标函数是预测被 masked 的词；类似挖空填词、完形填空。 </p></blockquote><h5 id="贡献："><a href="#贡献：" class="headerlink" title="贡献："></a>贡献：</h5><p>1  bidirectional 双向信息的重要性</p><p>2  BERT 首个 微调模型，在 sentence-level and token-level task效果好</p><p>好的预训练模型，不用对特定任务做一些模型架构的改动</p><p>3 BERT 开源，随便用 </p><h5 id="算法："><a href="#算法：" class="headerlink" title="算法："></a>算法：</h5><p>预训练 + 微调</p><p>pre-training: 使用 unlabeled data 训练 </p><p>fine-tuning: 微调的 BERT 使用 预训练的参数 初始化，所有的权重参数通过 下游任务的 labeled data 进行微调。</p><p>每一个下游任务会创建一个 新的 BERT 模型，（由预训练参数初始化），但每一个下游任务会根据自己任务的 labeled data 来微调自己的 BERT 模型。</p><p>预训练和微调不是BERT的创新，CV里用的比较多。 </p><p>模型调了 3 个参数：</p><p>L: transform blocks的个数</p><p>H: hidden size 隐藏层大小</p><p>A: 自注意力机制 multi-head 中 head 头的个数</p><p>分为 BERT_BASE （L&#x3D;12,H&#x3D;768,A&#x3D;12,1亿参数）和 BERT_LARGE （L&#x3D;24,H&#x3D;1024,A&#x3D;16,3.4亿参数） 两个模型。</p><p>BERT_base 的参数选取 和 GPT 差不多，比较模型；BERT_large 刷榜。</p><p>下游任务有处理一个句子 or 处理 2 个句子，BERT 能处理不同句子数量的下游任务，使输入可以是 a single sentence and a pair of sentences (Question answer)</p><p>a single sentence: 一段连续的文字，不一定是真正上的语义上的一段句子，它是我的输入叫做一个序列 sequence。</p><p>A “sequence” 序列可以是一个句子，也可以是两个句子。</p><p><strong>BERT 的输入和 transformer 区别：</strong></p><p>transformer 预训练时候的输入是一个序列对。编码器和解码器分别会输入一个序列。</p><p>BERT 只有一个编码器，为了使 BERT 能处理两个句子的情况，需要把两个句子并成一个序列。</p><p><strong>BERT 如何切词:</strong>﻿</p><p>WordPiece, 把一个出现概率低的词切开，只保留一个词出现频率高的子序列，30k token 经常出现的词（子序列）的字典。</p><p>否则，空格切词 –&gt; 一个词是一个 token。数据量打的时候，词典会特别大，到百万级别。可学习的参数基本都在嵌入层了。</p><p>BERT 的输入序列如何构成？ [ CLS ]  +  [ SEP ] </p><p>序列开始: [ CLS ] 输出的是句子层面的信息 sequence representation</p><p>BERT 使用的是 transformer 的 encoder，self-attention layer 会看输入的每个词和其它所有词的关系。</p><p>就算 [ CLS ] 这个词放在我的第一个的位置，他也是有办法能看到之后所有的词。所以他放在第一个是没关系的，不一定要放在最后。</p><p>区分 两个合在一起的句子 的方法：</p><p>每个句子后 + [ SEP ] 表示 seperate<br>学一个嵌入层 来表示 整个句子是第一句还是第二句</p><p> [ CLS ] [Token1] …… [Token n] [SEP] [Token1’] …… [Token m]</p><p>每一个 token 进入 BERT 得到 这个 token 的embedding 表示。</p><p>对于 BERT，输入一个序列，输出一个序列。</p><p>最后一个 transformer 块的输出，表示 这个词源 token 的 BERT 的表示。在后面再添加额外的输出层，来得到想要的结果。</p><p>For a given token, 进入 BERT 的表示 &#x3D; token 本身的表示 + segment 句子的表示 + position embedding 位置表示</p><p><img src="https://s4.ax1x.com/2022/01/21/7Rna8J.md.png" alt="7Rna8J.md.png"></p><p>一个词源的序列 –&gt; 一个向量的序列 –&gt; 进入 transformer 块</p><p>Token embeddings:  词源的embedding层，整成的embedding层， 每一个 token 有对应的词向量。</p><p>Segement embeddings: 这个 token 属于第一句话 A还是第二句话 B。</p><p>Position embeddings: 输入的大小 &#x3D; 这个序列最长有多长？ i.e., 1024 </p><p>Position embedding 的输入是 token 词源在这个序列 sequence 中的位置信息。从0开始 1 2 3 4 –&gt; 1024</p><p>BERT input representation &#x3D; token embeddings + segment embeddings + position embeddings </p><p>BERT 的 segment embedding （属于哪个句子）和 position embedding （位置在哪里）是学习得来的，transformer 的 position embedding 是给定的 </p><h5 id="Pre-training-BERT："><a href="#Pre-training-BERT：" class="headerlink" title="Pre-training BERT："></a>Pre-training BERT：</h5><p>预训练的 key factors: 目标函数，预训练的数据</p><p><strong>Task 1 MLM：</strong></p><p>由 WordPiece 生成的词源序列中的词源，它有 15% 的概率会随机替换成一个掩码。但是对于特殊的词源不做替换，i.e., 第一个词源 [ CLS ] 和中间的分割词源 [SEP]。</p><p>如果输入序列长度是 1000 的话，要预测 150 个词。</p><p>MLM 带来的问题：预训练和微调看到的数据不一样。预训练的输入序列有 15% [MASK]，微调时的数据没有 [MASK].</p><p>15% 计划被 masked 的词: 80% 的概率被替换为 [MASK], 10% 换成 random token,10% 不改变原 token。但 T_i 还是被用来做预测。</p><p>unchanged 和 微调中的数据应该是一样的 </p><p><strong>Task 2 NSP Next Sentence Prediction</strong></p><p>在问答和自然语言推理里都是句子对。</p><p>如果 BERT 能学习到 sentence-level 信息，很棒。</p><p>输入序列有 2 个句子 A 和 B，50% 正例，50%反例</p><p>50% B 在 A 之后，50% 是 a random sentence 随机采样的。</p><p>正例：这个人要去一个商店，然后他买了一加仑的牛奶。IsNext</p><p>反例：这个人去了商店，然后企鹅是一种不能飞的鸟。NotNext</p><p>flight ## less, flightless 出现概率不高，WordPiece 分成了 2 个出现频率高的子序列，## 表示 less 是 flightless 的一部分。</p><p><strong>Pre-training data</strong></p><p>2 个数据集：BooksCorpus (800 M) + English Wikipedia (2500 M)</p><p>使用一篇一篇文章，而不是随机打断的句子。 a document-level corpus rather than a shuffled sentence-level corpus</p><p>transformer 可以处理较长的序列，一整个文本的输入，效果会好一些。</p><h5 id="Fine-tuning-BERT"><a href="#Fine-tuning-BERT" class="headerlink" title="Fine-tuning BERT"></a>Fine-tuning BERT</h5><p>用 BERT 做微调的一般化的介绍。</p><p>BERT 和一些基于encoder-decoder的架构为什么不一样？transformer 是encoder-decoder。</p><p>整个句子对被放在一起输入 BERT，self-attention 能够在两个句子之间相互看。BERT 更好，但代价是 不能像 transformer 做机器翻译。</p><p>在encoder-decoder的架构，编码器看不到解码器的东西。</p><p><strong>BERT 做 下游任务</strong></p><p>根据下游任务，设计我们任务相关的输入和输出。</p><p>好处：模型不怎么变，加一个输出层 softmax 得到 标号 label</p><p>怎么样把输入改成想要的句子对？</p><p>有两个句子的话，当然就是句子 A 和 B。<br>只有一个句子的话，要做句子分类的话， B 没有。根据下游任务的要求，要么是 [CLS] representation is fed into an output layer for classification 拿到第一个词源 [CLS] 对应的输出做分类 such as entailment or sentiment analysis，或者是 the token representations are fed into an output layer for token-level tasks 拿到对应那些词源的输出做 sequence tagging or question answering 输出。</p><p>微调比预训练便宜。TPU 1 hour, GPU a few hours. </p><h5 id="对比实验（Ablation-studies）"><a href="#对比实验（Ablation-studies）" class="headerlink" title="对比实验（Ablation studies）"></a>对比实验（Ablation studies）</h5><p><strong>没有 NSP</strong></p><p>LTR 从左看到右（无 MLM ） &amp; 没有 NSP</p><p>LTR 从左看到右（无 MLM ） &amp; 没有 NSP + BiLSTM （从ELMo来的想法）</p><p>去掉任何一个组成部分，BERT的效果都会有打折，特别是 MRPC。</p><p><strong>Effect of Model Size</strong></p><p>BERT_base 110 M 可学习参数</p><p>BERT_large 340 M 可学习参数</p><p>NLP界认为 模型越大，效果越好。BERT 首先证明了大力出奇迹，引发了模型“大”战</p><p>现在：GPT-3 1000 亿可学习参数</p><p><strong>Feature-based Approach with BERT</strong></p><p>没有微调的 BERT，将pre-trained 得到的 BERT 特征作为一个静态的特征输入，效果没有 + 微调好</p><p>卖点：用 BERT 需要微调。 </p><p><strong>BERT 是否要选择  ‘bidirectional’  双向性呢</strong></p><p>可以写，但也要写 双向性带来的不足是什么？</p><p>选择有得有失。</p><p>GPT 用的是 decoder</p><p>BERT 用的是 encoder，不好做generative tasks：机器翻译、文本摘要。</p><p>分类问题在 NLP 更常见。</p><p>NLP 研究者喜欢 BERT，较容易的应用在 NLP 中自己想解决的问题。</p><p>BERT，完整的解决问题的思路 —- 大家对 DL 的期望</p><p>训练一个很深、很宽的模型，在一个很大的数据集上预训练好；训练好的模型参数可以解决很多小的问题，通过微调提升小数据集上的性能。</p><p>这个模型拿出来之后可以用在很多小的问题上，能够通过微调来全面提升这些小数据上的性能。这个在计算机视觉里面我们用了很多年了。</p><p>BERT 把 CV 的套路搬到了 NLP，1个3亿参数的模型，展示：模型越大、效果越好。大力出奇迹。</p><p><strong>为什么 BERT 被记住？</strong></p><p>BERT 用了 ELMo, GPT 更大的训练数据集，效果更好；BERE 也被更大的训练数据集和更大的模型超越。</p><p>BERT 的引用率是 GPT 的 10 倍，影响力 ✔ </p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ResNet</title>
    <link href="/2022/20220118/"/>
    <url>/2022/20220118/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>论文：何恺明等 - Deep Residual Learning for Image Recognition</p><span id="more"></span><hr><p>更深的网络可能面临误差变高的问题，但并不是过拟合（过拟合是训练误差变低，但测试误差变高，而更深的网络面临的问题是训练误差也变低。）</p><p>本文提出了一种框架（deep residual learning framework)来避免浅层神经网络到深度神经网络出现的这种情况。</p><p>要学的东西叫做H（x），假设现在已经有了一个浅的神经网络，他的输出是x，然后要在这个浅的神经网络上面再新加一些层，让它变得更深。新加的那些层不要直接去学H（x），而是应该去学H（x）-x，x是原始的浅层神经网络已经学到的一些东西，新加的层不要重新去学习，而是去学习学到的东西和真实的东西之间的残差，最后整个神经网络的输出等价于浅层神经网络的输出x和新加的神经网络学习残差的输出之和，将优化目标从H（x）转变成为了$F(x)&#x3D;H(x)-x $</p><p><img src="/images/360%E6%88%AA%E5%9B%BE20220118222138438.jpg" alt="360截图20220118222138438"></p><p>残差使用了Shortcut Connection等先人的技术。</p><p><strong>应用：</strong></p><p>残差连接如何处理输入和输出的形状是不同的情况：</p><p>第一个方案是在输入和输出上分别添加一些额外的0，使得这两个形状能够对应起来然后可以相加<br>第二个方案是之前提到过的全连接怎么做投影，做到卷积上，是通过一个叫做1<em>1的卷积层，这个卷积层的特点是在空间维度上不做任何东西，主要是在通道维度上做改变。所以只要选取一个1</em>1的卷积使得输出通道是输入通道的两倍，这样就能将残差连接的输入和输出进行对比了。在ResNet中，如果把输出通道数翻了两倍，那么输入的高和宽通常都会被减半，所以在做1*1的卷积的时候，同样也会使步幅为2，这样的话使得高宽和通道上都能够匹配上</p><p>图像处理：</p><p>短边随机裁剪到[256,480]。pre-pixel取均值。增强图像。</p><p>神经网络使用引用论文[13]（其实就是作者之前自己写的）的权重。</p><p>minibatch&#x3D;256,学习率0.1,当错误率不显著下降时每次除10。</p><p>训练了$60 *10^4$代。</p><p>没有使用dropout,因为没有全连接层。</p><p>测试的时候，使用了10-crop testing（就是给定一张测试图片，会在里面随机的或者是按照一定规则的去采样10个图片出来，然后再每个子图上面做预测，最后将结果做平均）。这样的好处是因为训练的时候每次是随机把图片拿出来，测试的时候也大概进行模拟这个过程，另外做10次预测能够降低方差。</p><p>在不同的分辨率上去做采样，工作量较大，一般不这么做。</p><p><strong>实验：</strong></p><p>神经网络架构。<img src="/images/360%E6%88%AA%E5%9B%BE20220118223707645.jpg" alt="360截图20220118223707645"></p><p>结果图：</p><p><img src="/images/360%E6%88%AA%E5%9B%BE20220118224259087.jpg" alt="360截图20220118224259087"></p><p>图中的误差急剧下降是因为学习率*0.1。</p><p>输入输出形状不一样的时候怎样做残差连接：</p><ul><li><p>A:填零</p></li><li><p>B:投影</p></li><li><p>C:所有的连接都做投影：就算输入输出的形状是一样的，一样可以在连接的时候做个1*1的卷积，但是输入和输出通道数是一样的，做一次投影</p></li></ul><p>B和C虽然差不多，但是计算复杂度更高，B对计算量的增加比较少，作者采用了B</p><p>bottleneck 设计：</p><p><img src="/images/360%E6%88%AA%E5%9B%BE20220118224930682.jpg" alt="360截图20220118224930682"></p><p><strong>为什么好：</strong></p><p>与没加残差相比，梯度能保证够大，保证能训练。</p><p>设浅层神经网络为$g(x)$，则深层神经网络可以看成f(g(x))</p><p>求导$\frac{\partial  f(g(x))}{\partial x}&#x3D;\frac{\partial  f(g(x))}{\partial g(x))} \frac{\partial  g(x)}{\partial x}$</p><p>残差：$\frac{\partial  (f(g(x))+g(x))}{\partial g(x))}&#x3D;\frac{\partial  f(g(x))}{\partial g(x))} \frac{\partial  g(x)}{\partial x}+\frac{\partial  g(x)}{\partial x}$</p><p><strong>另外：</strong><br>这篇文章的residual和gradient boosting是不一样的</p><ul><li>gradient boosting是在标号上做residual</li><li>这篇文章是在feature维度上</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Quasi-SVM</title>
    <link href="/2022/20220111/"/>
    <url>/2022/20220111/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>SVM是一个强有力的机器学习算法，很多算法（比如RCNN）的分类器也使用SVM作为分类器，帮助我获得了博弈杯准确率排行榜的第二名（我使用的是VGG+Quasi-SVM。</p><p>但困难的是，如何把SVM和神经网络合并起来，且共同训练。于是有了近似SVM，我们可以使用hingeloss。</p><p>更幸运的是，keras提供了另一种方法，Random Fourier Features。</p><span id="more"></span><p><strong>链接：</strong> </p><p><a href="https://keras.io/examples/keras_recipes/quasi_svm">https://keras.io/examples/keras_recipes/quasi_svm</a></p><p><strong>2025年更新：</strong> keras现在已经删掉了RandomFourierFeatures</p><h2 id="Random-Fourier-Features"><a href="#Random-Fourier-Features" class="headerlink" title="Random Fourier Features"></a>Random Fourier Features</h2><p>SVM的核心是核技巧和最大间隔，SVM具体实现我们不再赘述。</p><p>Random Fourier Features出自2007 年的《Random Features for Large-Scale Kernel Machines》，并于十年后获得了NeurIPS 2017的Test of Time Award。</p><p>他们的想法是使用一个随机化的特征图 $z: \mathbb{R}^D \to \mathbb{R}^d$ 来近似核函数 $k(x,y)$：</p><p>$$<br>k(x,y) &#x3D; \langle \phi(x), \phi(y) \rangle_V \approx z(x)^\top z(y)<br>$$</p><p>这里，$V$ 是一个（可能是无限维的）希尔伯特空间，$D$ 是输入空间的维度，$d$ 是随机特征空间的维度。关键在于，如果 $d \ll N$，那么在特征空间中工作的计算成本要低得多。</p><p>根据 <a href="http://www.argmin.net/2017/12/05/kitchen-sinks/">作者的blog</a>，它的想法受到了以下观察的启发，设w是一个随机的D维向量，$w\sim N_D(0,I)$</p><p>定义$h:x\to exp(iw^Tx)$</p><p>则<br>$$<br>\begin{align}<br>E_w[h(x)h(y)^*]&amp;&#x3D;E_w[exp(iw^T(x-y))]<br>\\<br>&amp;&#x3D;\int_{R^D}p(w)exp(iw^T(x-y))dw<br>\\<br>&amp;&#x3D;exp(-\frac{1}{2}(x-y)^T(x-y))<br>\end{align}<br>$$<br>即其期望值为高斯核。</p><p>实际上它是一个Bochner 定理（Rudin，1962 年）——更一般结果的特定实例，即</p><p>Bochner 定理：一个在 $R^D$ 上的连续核$k(x,y)&#x3D;k(x-y)$是正定的，当且仅当$k(\Delta)$ 是一个非负测度的傅里叶变换。</p><p>非负测度的傅里叶变换是$\int p(w)exp(iw\Delta)dw$</p><p>故有<br>$$<br>\begin{align}<br>k(x,y) &amp;&#x3D; k(x-y)<br>\\ &amp;&#x3D; \int p(w) \exp(i w^T (x-y)) dw<br>\\ &amp;&#x3D; \mathbb{E}_ {w}[\exp(i w^T (x-y))]<br>\\ &amp;\approx \frac{1}{R} \sum_ {r&#x3D;1}^{R} \exp(i w_r^T (x-y)) \\ &amp;&#x3D;<br>\begin{bmatrix}<br>\frac{1}{\sqrt{R}} \exp(i w_{1}^T x) \\ \frac{1}{\sqrt{R}} \exp(i w_{2}^T x) \\ \vdots \\ \frac{1}{\sqrt{R}} \exp(i w_{R}^T x)<br>\end{bmatrix}^T<br>\begin{bmatrix}<br>\frac{1}{\sqrt{R}} \exp(-i w_{1}^T y) \\ \frac{1}{\sqrt{R}} \exp(-i w_{2}^T y) \\ \vdots \\ \frac{1}{\sqrt{R}} \exp(-i w_{R}^T y)<br>\end{bmatrix}<br>\\ &amp;&#x3D; \mathbf{h}(x) \mathbf{h}(y)^{*} \end{align}<br>$$<br>为了避免虚数，我们定义：<br>$$<br>\begin{align}<br>w&amp;\sim p(w)<br>\\<br>b&amp;\sim Uniform(0,2\pi)<br>\\<br>z_w(x)&amp;&#x3D;\sqrt{2}cos(w^Tx+b)<br>\end{align}<br>$$</p><p>则<br>$$<br>\begin{align}<br>E_w(z_w(x)z_w(y))&amp;&#x3D;E_w[\sqrt{2}cos(w^Tx+b)\sqrt{2}cos(w^Ty+b)]<br>\\&amp;&#x3D;<br>E_w[cos(w^Tx+b)+2b]+E_w[cos(w^T(x-y))]<br>\\&amp;&#x3D;<br>0+E_w[cos(w^T(x-y))]<br>\end{align}<br>$$<br>故可定义:<br>$$<br>z(x) &#x3D;<br>\left[\begin{matrix} \frac{1}{\sqrt{R}} z_{w_1}(x) \\ \frac{1}{\sqrt{R}} z_{w_2}(x) \\ \vdots \\ \frac{1}{\sqrt{R}} z_{w_R}(x) \end{matrix}\right]<br>$$</p><p>因此<br>$$<br>\begin{align}<br>z(x)^Tz(y)&amp;&#x3D;\frac{1}{R}\sum_{r&#x3D;1}^R z_{w_r}(x)z_{w_r}(y)<br>\\<br>&amp;&#x3D;\frac{1}{R}\sum_{r&#x3D;1}^R 2cos(w_r^Tx+b_r)cos(w_r^Ty+b_r)<br>\\<br>&amp;&#x3D;\frac{1}{R}\sum_{r&#x3D;1}^R cos(w_r^T(x-y))<br>\\<br>&amp;\approx E_w[cos(w_r^T(x-y))]<br>\\<br>&amp;&#x3D;k(x,y)<br>\end{align}<br>$$</p><h2 id="应用——高斯核近似"><a href="#应用——高斯核近似" class="headerlink" title="应用——高斯核近似"></a>应用——高斯核近似</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span>   sklearn.metrics.pairwise <span class="hljs-keyword">import</span> rbf_kernel<br><span class="hljs-keyword">from</span>   sklearn.datasets <span class="hljs-keyword">import</span> make_s_curve<br><br>fig, axes = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)<br>fig.set_size_inches(<span class="hljs-number">15</span>, <span class="hljs-number">4</span>)<br>font = &#123;<span class="hljs-string">&#x27;fontname&#x27;</span>: <span class="hljs-string">&#x27;arial&#x27;</span>, <span class="hljs-string">&#x27;fontsize&#x27;</span>: <span class="hljs-number">18</span>&#125;<br><br>N    = <span class="hljs-number">1000</span><br>D    = <span class="hljs-number">3</span><br>X, t = make_s_curve(N, noise=<span class="hljs-number">0.1</span>)<br>X    = X[t.argsort()]<br><span class="hljs-comment"># The RBF kernel is the Gaussian kernel if we let \gamma = 1 / (2 \sigma^2).</span><br>K    = rbf_kernel(X, gamma=<span class="hljs-number">1</span>/<span class="hljs-number">2.</span>)<br><br>axes[<span class="hljs-number">0</span>].imshow(K, cmap=plt.cm.Blues)<br>axes[<span class="hljs-number">0</span>].set_title(<span class="hljs-string">&#x27;Exact RBF kernel&#x27;</span>, **font)<br>axes[<span class="hljs-number">0</span>].set_xticks([])<br>axes[<span class="hljs-number">0</span>].set_yticks([])<br><br><span class="hljs-keyword">for</span> R, ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1000</span>], axes[<span class="hljs-number">1</span>:]):<br>    W    = np.random.normal(loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>, size=(R, D))<br>    b    = np.random.uniform(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>*np.pi, size=R)<br>    B    = np.repeat(b[:, np.newaxis], N, axis=<span class="hljs-number">1</span>)<br>    norm = <span class="hljs-number">1.</span>/ np.sqrt(R)<br>    Z    = norm * np.sqrt(<span class="hljs-number">2</span>) * np.cos(W @ X.T + B)<br>    ZZ   = Z.T@Z<br><br>    ax.imshow(ZZ, cmap=plt.cm.Blues)<br>    ax.set_title(<span class="hljs-string">r&#x27;$\mathbf&#123;Z&#125; \mathbf&#123;Z&#125;^&#123;\top&#125;$, $R=%s$&#x27;</span> % R, **font)<br>    ax.set_xticks([])<br>    ax.set_yticks([])<br><br>plt.tight_layout()<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="/2022/20220111/rbf.png"></p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>1000 days</title>
    <link href="/2021/1000/"/>
    <url>/2021/1000/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>　　很高兴你能来到这里。<br>　　今天正好是博客诞生的1000天。（其实是把最早有准确时间记录的日子作为初始日，创立博客可能可以追溯到18年五月甚至更前）。很高兴你与我能共同见证这一时刻。</p><span id="more"></span><p>　　建博客的想法最初来自于知乎，当时觉得很cool（尽管blog对现在来说，是一个过时的产品，毕竟知乎、微博、豆瓣等等社交平台都是blog的替代品），github page也提供了一个易于操作的平台。于是乎，经过一个下午的探索，博客有了基本的雏形。<br>　　偶尔写写一些关于高考题的思路与做法的文章存放在博客上。随着高考结束，灵感枯竭，文章写得逐渐水了起来，博客从数学文章，慢慢转变成了文学分享(?)，写文章频率也慢慢缓了下来。<br>　　决定重拾博客并将其维持下去是在看了豆瓣上的一个想法之后。一位豆友将blog作为自己未来的墓志铭触动了我。我突然想，我应该找个日子写写我的日常，记录我的故事。<br>　　博客也逐渐多元化，也发了一些滥矫情的文章。博客功能也逐渐多样化，增加了“人间烟火”的相册，用以记录网络上、生活中的一些烟火味（现在停更了好久）；增加了说说页面，断断续续写些零散的文字，存放一些零散的、碎片化的情绪。<br>　　博客更自由，也无人倾听。<br>　　博客已经1000天了，博客不过也有少许的<span class="hint--info hint--rounded hint--top" data-hint="顺便一说,博客中直接能看到的内容只有长文其他部分需要间接看到或者需要去发现。" ontouchstart>数十篇长文</span><del>相当一部分还是笔记</del>还有一些隐蔽在角落的碎片。<br>　　待到下一个1000天、10000天，博客又是哪番模样呢？这一切只能交给时间去回答。<br>　　在自己喜欢的时间里，按照自己喜欢的方式，去做自己喜欢做的事，对我而言这便是自由人的定义。 ——村上春树<br>　　在最后，我想跟你道一声谢谢，谢谢你来到了我的博客，谢谢你看完了<del>有些流水账式的</del>文字。<br>　　最后的最后，附上一段纵贯线中我最喜欢的歌词：<br>…<br>喂 小子 我想我大概明白你的意思<br>那些发生在你身上的<br>曾经以不同的面貌<br>也在我生命里出现过好几次<br>对此 我并无更高明的解释<br>只是觉得今天说不定是个合适的日子<br>我们就各自用舒服的姿势<br>用擅长的方式 给人生我们的<br>不管是一种告解还是一份答辩词<br>人 再有本事也难抵抗命运的不仁慈<br>这道理再简单不过<br>接不接受是另外一回事<br>…<br>我们都不必在意未来的样子<br>像是精神病患写的诗<br>或是烟花绽放的节日<br>随它去吧 我们都只活一次<br>呼吸呼吸呼吸 呼 一切曳然而止<br>真理在荒谬被证实以前<br>都只是暗室里的装饰<br>只有眼前亮起来了以后<br>才有机会彰显它的价值<br>不是谁能决定的<br>该漫游还是冲刺<br>我们都在海里<br>我觉得我们像沙子<br>…</p><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>理应在这里放一个评论区<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><p><br><br></p><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><center>懒</center><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><br><p>本来想写得长一些，忙着复习，也没什么更多想写的。</p><p>就到这吧。</p><p align="right">2021/11/02 夜</p><center><b>End.</b></center><hr><p><img src="https://z3.ax1x.com/2021/11/03/IVVCnJ.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>纪念</tag>
      
      <tag>回忆</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《费尔巴哈和德国古典哲学的终结》</title>
    <link href="/2021/20211009/"/>
    <url>/2021/20211009/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>此为陕西师范大学公开课——哲学革命的理论总结——《费尔巴哈和德国古典哲学的终结》的笔记。</p><span id="more"></span><h3 id="第一讲-新世界观的萌发"><a href="#第一讲-新世界观的萌发" class="headerlink" title="第一讲 新世界观的萌发"></a>第一讲 新世界观的萌发</h3><h4 id="一、生成史"><a href="#一、生成史" class="headerlink" title="一、生成史"></a><strong>一、生成史</strong></h4><p>从时间角度：思想论战的产物——为了批评学术界与欧洲工人运动错误思想和阐明马克思思想</p><p>从逻辑角度：马克思思想新世界观（哲学）理论总结</p><p>序言讲述了写作目的：</p><p>1.阐明马克思哲学和德国古典哲学的关系。</p><p>2.理论斗争</p><p>3.直接目的：评论施达克对唯物主义评价</p><h4 id="二、学术背景"><a href="#二、学术背景" class="headerlink" title="二、学术背景"></a><strong>二、学术背景</strong></h4><p>学术界争论问题：</p><p>1.马克思有自己独立的马克思哲学吗？</p><p>部分学者认为只是黑格尔哲学的分支。</p><p>2.马克思是一个哲学的革命者吗？</p><p>有人认为马克思思想不过是黑格尔辩证法和机械唯物主义的一个奇怪的结合。</p><p>3.是否有两个马克思？</p><p>青年马克思（偏人道主义）和晚年马克思（偏科学主义）哪个能代表马克思。</p><p>4.历史唯物主义是机械决定论（或科学决定论）吗？</p><p>综合起来，其实就是马克思和恩格斯是否对立。</p><p>我们要关心《费尔巴哈和德国古典哲学的终结》对以上问题的看法。</p><h4 id="三、新世界观萌芽的“第一个文件”"><a href="#三、新世界观萌芽的“第一个文件”" class="headerlink" title="三、新世界观萌芽的“第一个文件”"></a><strong>三、新世界观萌芽的“第一个文件”</strong></h4><p><strong>首先，马克思确定了新哲学的理论基石。</strong></p><p>对旧哲学困境的批评：</p><p>旧唯物主义从直观的意义解释认识的本质，不理解实践对把握人的本质的重要性，不理解实践的批判功能。</p><p>唯心主义认识到主体的能动作用，却夸大了它的意义。</p><p>实践观：</p><p>实践是人的现实的、变革世界的感性活动。</p><p>实践是检验认识真理性的标准。</p><p>实践是社会生活的基础，也是认识的基础。</p><p>重要的问题不是解释世界，而是通过实践改变世界。</p><p><strong>其次，第一次勾画了唯物史论的基本框架。</strong></p><p>基本框架：</p><p>实践是实现人与环境统一的基础。</p><p>人的本质的规定。</p><p>社会意识对社会实践的依赖。</p><p><strong>最后，阐明了新哲学同旧哲学的本质区别。</strong></p><p>阶级基础不同。（工人阶级与资产阶级。）</p><p>对实践的态度不同。</p><h3 id="第二讲-新世界观产生的理论背景——阅读框架"><a href="#第二讲-新世界观产生的理论背景——阅读框架" class="headerlink" title="第二讲 新世界观产生的理论背景——阅读框架"></a>第二讲 新世界观产生的理论背景——阅读框架</h3><p>哲学方法论</p><p>我们把在思想史上引起视角发生重大变化的哲学思想称之为哲学革命。</p><p>哲学革命有两个问题值得关注：</p><p>1.哲学观（对什么是哲学这个问题的回答）发生了什么变化</p><p>2.哲学自身的内容（以什么方式回答面对的哲学问题）发生了什么变化</p><h4 id="一、笛卡尔的哲学观——百科全书式的知识体系"><a href="#一、笛卡尔的哲学观——百科全书式的知识体系" class="headerlink" title="一、笛卡尔的哲学观——百科全书式的知识体系"></a><strong>一、笛卡尔的哲学观——百科全书式的知识体系</strong></h4><p><strong>我思故我在</strong></p><p>寻求确定性（通过怀疑达到不怀疑）——我能够怀疑 是不可怀疑的——反对“盲目的顺从”，提倡“以自己的眼”看世界——思想是人的本质</p><p><strong>哲学是知识点大树</strong></p><p>追求真理是哲学的使命——形而上学是知识大树的根——形而上学的使命是探讨知识的原理</p><p>哲学就是知识体系，纯粹哲学就是探讨知识的基础。</p><p>上帝是观念指称的对象</p><p><strong>知识客观性的依据</strong></p><p>理性并没有给出知识可靠性的依据——认识论意义的上帝概念——知识真理性的两个逻辑依据（一个是承认对象自身的客观性；一个是要求立足于无条件性原则进行论证）</p><p><strong>评论：</strong></p><p>​   笛卡尔把哲学研究理解为通过批判为知识的确定性寻求理论依据的工作，理解为给人的道德生活提供依据的工作，这样的观念显然超越了古代哲学的哲学观。</p><p>​但是从他关于哲学体系的设计看，他没有超出亚里士多德的思想。从他对物质实体、灵魂实体以及上帝实体的讨论来看，它基本上是以近代机械论科学的概念分析他所提出的哲学问题。而这正是近代早起哲学后来走向实证化的缘由。</p><h4 id="二、康德的哲学观——建构知识的逻辑条件的理论讨论"><a href="#二、康德的哲学观——建构知识的逻辑条件的理论讨论" class="headerlink" title="二、康德的哲学观——建构知识的逻辑条件的理论讨论"></a><strong>二、康德的哲学观——建构知识的逻辑条件的理论讨论</strong></h4><p><strong>哲学的使命</strong></p><p>形而上学是纠纷不息、没有胜利者的战场——传统形而上学研究陷入困境的理论根源——“划分”是形而上学走出困境的出路。</p><p>康德探讨怎么理解“范畴”这个概念才能正确地认识世界。</p><p><strong>主体性原则</strong></p><p>思考主体性原则的两个纬度（神道主义与机械论）——“人为自然立法”和“人是道德的立法者”——自由原则是解决哲学问题的立足点。</p><p><strong>客观性原则</strong></p><p>“物自体”与“范畴”是知识客观性的两个基础——“物自体”不可知原则破坏了这个论证——形式的论证仍就是主观性的论证</p><p>康德贡献——从哲学观角度来讲：给哲学和科学划界</p><p>物自体为后世的相对主义、虚无主义打开了大门。</p><h4 id="三、黑格尔的哲学观——真理（精神）的自我运动和自我认识"><a href="#三、黑格尔的哲学观——真理（精神）的自我运动和自我认识" class="headerlink" title="三、黑格尔的哲学观——真理（精神）的自我运动和自我认识"></a><strong>三、黑格尔的哲学观——真理（精神）的自我运动和自我认识</strong></h4><p>思想的客观性：思想内容与内容所指之间的一致性</p><h4 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a><strong>四、总结</strong></h4><p>对什么是哲学的看法在一步一步推进。</p><p>”知识是什么“有分歧。</p><p><strong>19世纪中叶哲学面对的课题：</strong></p><p>​**如何理解哲学的使命（功能）：**哲学究竟是关于整体世界本质的研究，还是关于怎样研究世界的方法论（理论预设）的思考；哲学关注现实问题，是直接指出现实问题之所在并给予现实问题以可操作性的解决方案，还是分析各种解决方案的内在困境，为探索新的解决方案提供新的可能性。</p><p>​**如何理解人（精神、思想）的本性：**自由的真正含义是什么？自由的实现的现实条件？思想自由与现实自由的关系？真理性知识的存在方式？人的存在与人的自由的关系？</p><p>​**如何理解客观性问题：**客观性的真实含义是什么？为什么必须讲清楚思想与思想所表达事物之间的关系？为什么否认思想对象存在真实性一定会导致相对主义、虚无主义？</p><p><strong>阅读视域：</strong></p><p>思想历史性与永恒性；思想与社会生活的关系；生产方式变迁与社会生活的变化对思想的意义等。</p><h3 id="第三讲-恩格斯对旧形而上学的批评"><a href="#第三讲-恩格斯对旧形而上学的批评" class="headerlink" title="第三讲 恩格斯对旧形而上学的批评"></a>第三讲 恩格斯对旧形而上学的批评</h3><h4 id="一、形而上学概念的辨析"><a href="#一、形而上学概念的辨析" class="headerlink" title="一、形而上学概念的辨析"></a>一、形而上学概念的辨析</h4><p>狭义来讲，形而上学是哲学的一个部门。</p><p>后世把以存在为对象的学问称之为狭义意义的形而上学，其他的有时候称之为自然哲学。</p><p>康德认为，研究存在的目的是为人类认识世界提供一个理论前提。</p><p>阅读时，要注意作者对形而上学的概念的定义。</p><h4 id="二、应当摆脱旧哲学的思想桎梏"><a href="#二、应当摆脱旧哲学的思想桎梏" class="headerlink" title="二、应当摆脱旧哲学的思想桎梏"></a>二、应当摆脱旧哲学的思想桎梏</h4><p><strong>恩格斯批判旧哲学的视角</strong>：</p><p>或者是从“知识总汇”的角度，或者是从方法论的角度。</p><p><strong>旧形而上学的理论缺陷</strong>：</p><p><strong>企图获取绝对真理。</strong></p><p><strong>用臆想的方式勾画整体世界的途径。</strong></p><p><strong>脱离社会生活讨论社会问题。</strong></p><p>本书比较了费尔巴哈和黑格尔。费尔巴哈形式上看是唯物主义，但忽略了人与人之间的道德关系，实际上它是唯心的。而黑格尔正好相反，从形式上看是唯心的，从内容上看是非常接近于现实的。  </p><p>马克思思想的话来说，人的社会意识依赖于社会存在。</p><p><strong>非历史性的思维方式。</strong></p><p>看不到自然界是一个过程。</p><p>思维科学的清理工作还没做。 我们不可避免面对的一个问题：哲学研究还有什么意义？</p><p><strong>简单的总结：</strong></p><p>​恩格斯着重批评了旧形而上学（哲学）在哲学的使命、认识世界的路径、理解社会历史的方法论原则等方面的理论局限性。</p><p>​只有从超越“知识总汇”这种传统哲学观的角度进行思考，才能更深刻地把握马克思恩格斯思想与近代欧洲哲学发展的连续性，才能更深刻地领悟马克思恩格斯所实现的哲学变革的内涵与深远意义，才能更好地思考当代对马克思恩格斯批评的内涵和意义，才能更好地研究马克思主义哲学的当代发展问题。</p><h4 id="三、哲学的终结与新生"><a href="#三、哲学的终结与新生" class="headerlink" title="三、哲学的终结与新生"></a>三、哲学的终结与新生</h4><p>恩格斯说，哲学在黑格尔那里终结了。</p><p>理解：抛弃传统的哲学观念而用一种新的哲学观念来分析问题。</p><p><strong>恩格斯是不是一个实证主义者？</strong>（提出者：莱文）</p><p>本书提供的证据是否定的。</p><p>第一，恩格斯在本书中拒绝的不是哲学思维（形而上学）思考，</p><p>而是反对把哲学看做“知识总汇”的旧观念，反对僵化的思维方式。</p><p>第二，恩格斯认为，我们应该尊重科学的发展，然后依据辩证思维的原则去分析科学的材料。</p><p>第三，莱文的观点暗示恩格斯不关心人。恩格斯探讨了人的解放问题，但是并未过多地分析人的日常生活中生活状态的问题。恩格斯是关心人的，不过和某些人道主义者的方法和角度是不一样的，恩格斯是站在历史的宏观上讨论的。</p><h3 id="第四讲-辩证法的真实意义"><a href="#第四讲-辩证法的真实意义" class="headerlink" title="第四讲 辩证法的真实意义"></a>第四讲 辩证法的真实意义</h3><h4 id="一、辩证法概念的使用方式"><a href="#一、辩证法概念的使用方式" class="headerlink" title="一、辩证法概念的使用方式"></a>一、辩证法概念的使用方式</h4><p>​辩证法这一概念在不同时代和国度，其内涵和外延是不完全相同的：</p><p>​柏拉图时代，辩证法是指在对话中寻求真理的过程，没有给出一个确切的定义；</p><p>​中世纪的神学家、经院哲学家把辩证法理解为一种辨析的方法。</p><p><strong>黑格尔的理解</strong></p><p>​辩证法体现了精神存在的本质特征——精神（思想、观念）是在与其对立面不断斗争的过程中扬弃自身而得到发展的。</p><p>​辩证法是获取真理的方法——“从最初、最简单的精神现象，直接意识开始，进而从意识辩证进展（Dialektik）逐步发展”到真理。</p><p>​完成新逻辑体系的建构——这个体系的建构不是任意的编排，所有逻辑范畴都处在内在联系之中。</p><p><strong>阅读经典时要注意作者如何理解辩证法。</strong></p><h4 id="二、辩证法的精神实质"><a href="#二、辩证法的精神实质" class="headerlink" title="二、辩证法的精神实质"></a>二、辩证法的精神实质</h4><p>黑格尔强调了对象自身的特质，马恩也强调这一点。</p><p><strong>恩格斯角度：辩证法不是对象自身规定的描述，也不是解释对象和建构知识的方法，而是研究事物的方法论原则</strong></p><p><strong>· 恩格斯对黑格尔辩证法实质阐释</strong></p><p>“黑格尔哲学的实质及其革命意义就在于它永远结束了以为人的思维和行为的结果具有最终性质的看法。”</p><p>从认识领域看，真理不是一个教条，是一个过程，绝对真理存在于永恒的认识过程中。</p><p>从历史领域看，历史的任何一个阶段都只具有暂时的性质，完美的历史理想也不具有尽善尽美的性质，每个历史阶段都有其存在的理由，相对于新的更高的条件来讲，它必将灭亡而让位与其他历史阶段。因此，历史是一个永恒的向上发展的过程。</p><p>我们应该有变化、发展的观点去看待现实生活。</p><p>辩证法在承认永恒运动的同事，也承认事物相对于其存在条件的静止性。</p><p><strong>· 恩格斯观点的哲学史根据</strong></p><p><strong>近代哲学的独断性</strong>——人蝴蝶的知识就是对对象本性完美无缺的认识；人的理性具有不可怀疑的无限能力；认为科学是人的生命的全部内容。</p><p><strong>休谟对独断论的批评</strong>——观念与印象；我们的一切知识的来源都应当归之于经验；经验之外是什么，不知道。</p><p><strong>康德对知识有限性的讨论</strong>——理性的二律背反。人的知识的有限性（人的认识是受到限制的）。只有显现才是认知的对象；超验的物自体不是认知度对象；理性的限制性和诱导性功能。</p><p><strong>黑格尔的真理观</strong>——真理是个过程，绝对与相对在具体真理中的统一。</p><p><strong>· 恩格斯观点的现实意义</strong></p><p><strong>避免误解恩格斯的解释</strong></p><p>​恩格斯是从观察问题、分析问题、思考问题的视角这样的哲学观出发，分析黑格尔辩证法的实质。</p><p>​尽管恩格斯也讲辩证法是关于自然、历史与人类思维最一般、最普遍规律的科学。但是不应当从“哲学是知识的总汇”的意义上解读恩格斯。</p><h4 id="三、超越黑格尔辩证法"><a href="#三、超越黑格尔辩证法" class="headerlink" title="三、超越黑格尔辩证法"></a>三、超越黑格尔辩证法</h4><p><strong>拯救黑格尔辩证法</strong></p><p>​<strong>拯救工作的基本方法</strong>——这就是打破笼罩在辩证法身上的神秘外衣，恢复作为社会批判理论、精神批判理论的黑格尔辩证法的真实内容。</p><p>​<strong>超越体系结构的限制</strong>——僵硬的“骨架和脚手架”与现实的、”划时代作用“的思想。</p><p>​不要被体现结构所限制，应该去挖掘隐藏在这个大厦之中最合理的思想，比如，它在人类历史的讨论过程中对人类历史发展的研究，比如在《法哲学》中对人类自由精神的研究。</p><p>​<strong>历史唯物主义的理论价值</strong>——超越黑格尔和费尔巴哈的现实道路就是历史唯物主义所坚持的道路，“必须把这些人当做在历史中行动的人去研究”，不要把当成一个抽象的人。</p><h3 id="第五讲-哲学的社会述求"><a href="#第五讲-哲学的社会述求" class="headerlink" title="第五讲 哲学的社会述求"></a>第五讲 哲学的社会述求</h3><p>谁更懂得黑格尔？</p><p>恩格斯为什么要讨论黑格尔的阶级属性？——这是一个具有政治意义的问题。</p><h4 id="一、西方学术界的评论"><a href="#一、西方学术界的评论" class="headerlink" title="一、西方学术界的评论"></a>一、西方学术界的评论</h4><p><strong>同时代德国思想界的评价：</strong></p><p>弗里斯（黑格尔的同事，激进派的领袖人物之一）：长在粪堆上的毒草，哲学的保皇派、保皇哲学家</p><p>叔本华：江湖骗子、四分之三的胡说八道，四分之一的陈词滥调</p><p><strong>20世纪几位重要西方思想家的看法：</strong></p><p>罗素：在伦敦上空铁鹰的轰鸣声中感受到什么是绝对精神（暗指黑格尔应该为20世纪纳粹思想负责）</p><p>波普尔：普鲁士政府的辩护士，20世纪“极权主义政权”的思想根源</p><p><strong>黑格尔究竟代表着当时市民阶级的要求，还是代表着当时封建专制的要求？</strong></p><h4 id="二、一个典型的案例分析"><a href="#二、一个典型的案例分析" class="headerlink" title="二、一个典型的案例分析"></a>二、一个典型的案例分析</h4><p>恩格斯通过凡是命题来讨论。</p><p><strong>凡是现实的都是合理的，凡是合理的都是现实的</strong></p><p>· 这是一个招致人们激烈批评的命题</p><p>· 这是一个经常被人们误解和误用的命题</p><p>· 这是一个带有政治意味的学术命题</p><p>恩格斯认为黑格尔这个命题实际上是德国资产阶级走向政治舞台希望在政治上有自己发言权的一个宣言书。</p><p><strong>恩格斯对“凡是”命题的分析</strong></p><p>黑格尔所处时代的时代特征——资产阶级革命的时代</p><p>德国哲学革命的特点——德国哲学革命与法国哲学革命的同一性与表现形式的差异性；外表的灰色性（不像法国哲学家公开辩护）、语言的隐晦性、理论的深刻性（德国科学家对人性的论证、对自由平等的论证，其思想的深刻性远远超过了他的法国同事们和英国同事们，更注重人类型、整体性，不像英国哲学更多注重的是个体性）。</p><p>黑格尔辩证法真实的社会意义——被误解的命题（近视者）、现实性概念的真正含义（现实性——只有具有必然性的事物才是现实的事物；必然性——必然性不是一成不变的）、现存都应灭亡</p><p><strong>马克思认为</strong>：工业革命以后出现的资本主义制度，不管看起来有多强大，从历史趋势上看，终将会被一种新的社会制度所取代。</p><h4 id="三、黑格尔这个命题的意义"><a href="#三、黑格尔这个命题的意义" class="headerlink" title="三、黑格尔这个命题的意义"></a>三、黑格尔这个命题的意义</h4><p><strong>认识论意义</strong></p><p>​<strong>黑格尔对历史规律客观性的论证，确定了科学的社会历史学说研究的目标、使命和任务，对建构科学意义上的社会历史学说奠定了基本的理论基础</strong>。西方人认为，自然现象的背后有规律性存在，于是把知识的任务归结于寻找自然现象运动的规律性。历史的事件的不可重复性就成为构建社会历史科学的一个理论上的障碍。确认社会现象的规律性是社会知识走向科学的一个必要的理论条件。黑格尔的这个命题完成了这样一个任务。理性，一是指人类理性，人的精神，一是指规律性。</p><p>​当代哲学讨论的实质：反对按照机械必然性的方式理解规律，并且强调，应当关注人的创造性。但不应当成为否认社会规律性的理由。</p><p>​黑格尔、马克思、恩格斯都强调自然运动方式与社会运动方式的差异性，强调两者运动规律表现的不同。这指出了研究这一问题的基本思路。</p><p><strong>政治意义</strong></p><p>​近代理性概念的政治意义——理性主义的政治理想、近代理性概念二重性的政治解释力。</p><p>​德国资产阶级夺取政权的宣言书——当德国思想家宣布君主立宪制是最好的国家形式时，表明德国资产阶级登上政治舞台的时刻已经到来。、</p><p><strong>哲学的社会述求</strong></p><p>​哲学理论无论它有多么思辨，作为一个整体的理论必定包含着时代的内容，这就是哲学理论的社会述求问题。</p><p>​这种述求的表达方式是多种多样的。比如法国哲学家是公开地表达自己的社会述求，德国是相对隐晦的。</p><p>​对一种哲学理论社会述求的评价，需要依据文本提供的依据作具体分析，绝不能随意地贴标签。</p><p><strong>理想的作用</strong></p><p>第一，理想的东西成为我们批判现实生活中的一个尺子、一个镜子。我们对理想的思考才能使我们发现现实生活中存在的不合理的东西。</p><p>第二，有了理想性的思考，才会产生改变不合理现实的动力的要求</p><h3 id="第六讲-知识真理性的客观依据"><a href="#第六讲-知识真理性的客观依据" class="headerlink" title="第六讲 知识真理性的客观依据"></a>第六讲 知识真理性的客观依据</h3><h4 id="一、恩格斯论哲学基本问题"><a href="#一、恩格斯论哲学基本问题" class="headerlink" title="一、恩格斯论哲学基本问题"></a>一、恩格斯论哲学基本问题</h4><p>恩格斯：思维和存在的关系是哲学的基本问题。</p><p><strong>恩格斯对哲学基本问题提出的哲学史依据的分析</strong></p><p>远古时代，这个问题表现为灵魂与外部世界的关系问题；</p><p>古代世界，这个问题表现为世界本原与世界的关系问题；</p><p>中世纪，这个问题表现为神灵与自然和社会的关系问题；</p><p>近代，思维与存在关系这个问题以其本来的面目显现出来。</p><p><strong>哲学基本问题提出的理论依据</strong></p><p>​依据恩格斯的分析，可以看到，尽管不同时代人们关注的哲学问题有很大的差别，对哲学问题解读的角度有很大的泣别，但是，关注人的思维（精神、灵魂、意识、思想、意志、欲望等）同外部世界（自然与历史）的关系问题是蕴含在这些哲学问题中的普遍性问题。 </p><p>​人们对人与世界关系的看法在很大程度上依赖于对思想和世界关系的看法。因此恩格斯认为，思维与存在的关系问题构成贯穿哲学历史进程的一个核心问题。</p><p><strong>艾耶尔认为</strong></p><p>​我们应当“格外关心一批循环呈现的问题的演变。这些问题中最重要的也许就是客观性问题，它有时表现为实在论者与唯心论者分野的根源，有时表现为绝对真理说和相对真理说的争论点。这个基本问题就是，我们是否并且在何种程度上有可能不依赖事物与我们的关系而按照事物本来的面目描述事物。”</p><p><strong>哲学基本问题的内涵</strong></p><p>​<strong>思维与存在谁是本原的问题</strong>——逻辑上，而不是时间上的先后关系；核心问题是主体性原则与客观性原则何者为先；人存在的实践性、社会性和历史性。</p><p>​<strong>思维与存在是否具有同一性问题</strong>——思维能否正确地反映、表达世界的本性问题；正确性的程度有多大；争议的焦点是：人能否获取真理、真理获取是否是一个过程、真理性知识是否是永恒性与历史性的统一。</p><p>​<strong>哲学的党派性的含义问题</strong>——是指哲学家的基本哲学立场问题，而不是指其在社会问题上所持的立场问题。</p><p>​核心问题是我们怎么在理论上论证人类知识的可靠性。</p><h4 id="二、马克思主义哲学在哲学基本问题上的原则立场"><a href="#二、马克思主义哲学在哲学基本问题上的原则立场" class="headerlink" title="二、马克思主义哲学在哲学基本问题上的原则立场"></a>二、马克思主义哲学在哲学基本问题上的原则立场</h4><p>马克思主义哲学唯物主义立场的内涵</p><p>​<strong>不同解释视角有不同的表述</strong>——意识的生理基础、意识的直观表象、意识的内容、意识的社会基础。</p><p>​<strong>物质第一性的含义</strong>——指意识内容和内容之所指之间的对应关系，指人对外部世界的依赖（依存）关系，指实践对理解认识活动的本性及其成果的意义和价值。</p><p>​<strong>物质第一性和意识第二性原则表达的是一种逻辑关系</strong>——理论论证的关系，而不是时间关系。唯心主义强调的是人的主体性特征。</p><p>​<strong>思维过程是人的一种创造性活动</strong>——历史的具体的思维依赖于思维个体的经历、经验知识，依赖于人的创造性活动。</p><p>​<strong>唯物主义和唯心主义的区分标准的要义</strong>——以什么作为认识论研究的首要条件；是否应当按照事物自身的本来面目认识事物；是否坚持用思维主体的实践性分析认识的本性。</p><p>​<strong>思维与存在的关系问题是一个社会性、历史性范畴</strong>——对思维的本性的理解只有在人类历史性存在中得以实现。</p><p><strong>马克思主义哲学唯物主义立场的内涵</strong></p><p>​马克思唯物主义认识论的最基本的精神——尊重事实、尊重世界自身的本性与规律、尊重真理、避免一切主观随意性的决定和选择、随时准备抛弃错误的认识。</p><h4 id="三、两种视域下的唯物主义"><a href="#三、两种视域下的唯物主义" class="headerlink" title="三、两种视域下的唯物主义"></a>三、两种视域下的唯物主义</h4><p><strong>两种视域下的唯物主义和唯心主义</strong></p><p>”苏格拉底之问“——我为什么平静地坐在这等待着判决的执行？</p><p>“事实上如果我认为忍受城邦给予的判罚并不是正确的、最好还是逃掉的话，我想我的筋肉和骨头可能早就被什么是最好的信念带往麦加或者波埃提亚。可见把这类事情叫做原因是极其荒谬的。”</p><p>苏格拉底这个问题给我们带来了一个问题：如何解释人类行为的根据？就是人的行为是怎么产生的，到底是从人肉体的欲望或者是从人的精神角度来解释？</p><p>这就是西方哲学唯物主义和唯心主义的分歧。</p><p><strong>拯救旧哲学的出路是走向历史唯物主义</strong></p><p><strong>旧唯物主义的困境</strong>——旧唯物主义把自然科学的解释方式无条件推向人类社会，必定会遭遇伦理道德问题的质疑，这就会转向唯心主义。</p><p>马克思不是（像旧唯物主义那样）从人的肉体欲望，而是从人与人之间的社会关系来解释人类行为的最后根据。</p><p><strong>走向困境的出路</strong>——如果我们要在分析人类自身世界方面坚持唯物主义立场，那就必须超越旧唯物主义。这条道路就是马克思恩格斯开创的道路。</p><p>选择历史唯物主义角度，就是把人看做是在现实生活中实际存在的人，从社会关系入手来分析人的社会现象。</p><h3 id="第七讲-不可知论的历史评价"><a href="#第七讲-不可知论的历史评价" class="headerlink" title="第七讲 不可知论的历史评价"></a>第七讲 不可知论的历史评价</h3><h4 id="一、关于不可知论问题讨论的意义"><a href="#一、关于不可知论问题讨论的意义" class="headerlink" title="一、关于不可知论问题讨论的意义"></a>一、关于不可知论问题讨论的意义</h4><p><strong>争论的内涵——可知与不可知争论的核心问题是：</strong></p><p><strong>知识可靠性的根据问题</strong>——可知论：人的知识反映了事物自身的内容。</p><p><strong>知识真理性的绝对性（无条件性）和相对性（有条件性）的关系问题</strong>——通过实践获得的知识是相对性的，有限的；</p><p><strong>知识发展的无限性和存在的限度问题</strong></p><p><strong>知识问题与伦理道德问题的划界问题</strong>——人类仅仅有自然科学知识是不够的，还要关心道德问题 ；研究知识和遵守道德之间是什么关系</p><h4 id="二、恩格斯对不可知论问题的讨论"><a href="#二、恩格斯对不可知论问题的讨论" class="headerlink" title="二、恩格斯对不可知论问题的讨论"></a>二、恩格斯对不可知论问题的讨论</h4><p><strong>哲学史根据</strong></p><p>（1）古代</p><p>智者学派和其他学派的争论。</p><p>（2）休谟</p><p>（3）康德</p><p><strong>恩格斯对不可知论的批评</strong></p><p>批评的思路——针对“本质不可知”，属于真理问题的讨论。</p><p><strong>首先，肯定了黑格尔的批评的价值</strong>（首先，本质与现象是统一的，认识了现象“同时即认识了本质”。其次，批评康德低估思想的力量，看不到思想与事物之间的统一性。第三，黑格尔辩证法，从根本上否定了本质不可知这个问题的理论根据。</p><p>**其次，强调实践对批评不可知论的意义。**他以茜素的制作和海王星的发现为例说明，所谓的“不可知的自在之物”并不是不可知的。</p><p><strong>最后，批评当时新康德主义等经验主义学派在这个问题上的不足。</strong>（羞羞答答地引用唯物主义）</p><h4 id="三、关于不可知论（怀疑论）历史评价的思考"><a href="#三、关于不可知论（怀疑论）历史评价的思考" class="headerlink" title="三、关于不可知论（怀疑论）历史评价的思考"></a>三、关于不可知论（怀疑论）历史评价的思考</h4><p>传统的可知论中忽略了一个问题：人类已经取得的知识究竟受不受历史条件的限制。</p><p><strong>康德“不可知论”辩证</strong></p><p>​<strong>旧哲学“实体”概念的认识论困境</strong>——西方哲学把“实体“看做是一个整体性的概念。作为共相，从经验论的角度来看，是不可能在经验中出现的，这就产生一个问题，”实体“是否可以认知？</p><p>​<strong>康德的“物自体“概念</strong>——它或者绝对者（如上帝），或指感受的存在，或指思维主体自身，或指事物属性的载体，或指，同时隐含着本质的规定。</p><p>​<strong>物自体不可知具有重要意义</strong>——从上述几个规定中可以发现，“物自体”不可知这个命题具有不同的认识论意义。</p><p><strong>理论争论的焦点：</strong></p><p>​对知识真理性适用范围的质疑。人能否认识事物的本质或真相？能否获得绝对正确的知识？通常称之为真理问题。</p><p>​对科学的限度的质疑。科学是否是人生的唯一领域，道德信仰对人生的价值和意义是什么？通常称之为价值问题。</p><p><strong>不可知论的双重意义：</strong></p><p>​**不可知论的积极意义：**肯定了经验（实践）对认识的重要意义；确认了知识相对性原则；批判了独断论（教条主义）；确认了知识在经验领域内无限发展的权利；确认了道德信仰对人生的价值。</p><p>​**不可知论的消极意义：**否认认识本质的可能性，导致否认科学研究的价值；对知识真理性的怀疑，导致怀疑主义、虚无主义和无政府主义。</p><p><strong>结论:</strong></p><p>​我们看到，欧洲哲学史上，怀疑论（不可知论）往往同对教条主义的批判联系在一起的。但再往前走，就有两条路可选择：或者走向怀疑主义、虚无主义；或者走向清醒的理性主义。</p><p>​马克思主义哲学赞成后一种选择。在真理问题的讨论中，它扬弃了不可知论。它的贡献在于，指出了人的社会实践对克服虚无主义、相对主义的价值。</p><h3 id="第八讲-宗教问题的历史性"><a href="#第八讲-宗教问题的历史性" class="headerlink" title="第八讲 宗教问题的历史性"></a>第八讲 宗教问题的历史性</h3><p>恩格斯认为，费尔巴哈的宗教观、伦理观非常典型地表现了旧唯物主义哲学的唯心主义秘密。</p><h4 id="一、宗教源于对社会生活的反思"><a href="#一、宗教源于对社会生活的反思" class="headerlink" title="一、宗教源于对社会生活的反思"></a>一、宗教源于对社会生活的反思</h4><p>**十八世纪的启蒙学者：**宗教是愚昧无知的产物，是统治者统治被统治者的工具。</p><p>​这一切都是先由奸猾狡诈的阴谋家虚构出来，继而由伪预言家、骗子和江湖术士予以渲染扩大，而后由无知无识的人盲目地加以信奉，最后由世俗的国王和权贵用法律的形式加以维持和巩固。——梅叶</p><p>宗教之所以被想象出来，只是为了使君王和人民同样地成为教士的奴隶；教士的工作只是为国家的幸福不断地制造障碍。——霍尔巴赫</p><p><strong>黑格尔的观点</strong>——人类精神的一个环节；以崇拜（情感）的形式表现精神。</p><p><strong>费尔巴哈的观点</strong>——神是人的类本质的异化形式。</p><p>今天人们更倾向于<strong>多样性解释模式分析宗教问题</strong>，例如心理学方式、思想史的角度、文化史的角度。</p><p><strong>旧哲学在宗教问题上陷入理论问题的根源:</strong></p><p>​按照马克思主义的理解，脱离社会生活分析宗教问题，是旧哲学的根本缺陷。无论是把宗教看作是超自然存在者拯救人类的产物的观点，还是把宗教看作是“牧师和暴君”管制无知民众工商的观点，都没有看到宗教是社会现实生活的产物，他们在宗教问题上的理论困难都是由此而来的。</p><p><strong>宗教问题同人改变自身困境的需求密切相关</strong></p><p>​宗教是人类追求理性境界的精神特质的一种特殊表现。追求美好的未来是人类不同于动物的重要特征之一；理想境界的追求具有超验性的特征；当人生活在不人道的社会环境中，且依靠自己的力量又无法摆脱这种困境时，人的精神的反思就会导致对超自然、超人存在的敬仰与崇拜。当这种活动被固定化、形式化、教义化、规则化之后，人的精神以及世俗生活也就进入宗教的生活状态。</p><h4 id="二、恩格斯对费尔巴哈宗教观的批评"><a href="#二、恩格斯对费尔巴哈宗教观的批评" class="headerlink" title="二、恩格斯对费尔巴哈宗教观的批评"></a>二、恩格斯对费尔巴哈宗教观的批评</h4><p><strong>费尔巴哈对宗教的批判是不彻底的</strong></p><p>​这种不彻底性表现在他并不希望超越宗教，而只是使宗教更完善，希望哲学溶化在宗教中。</p><p>​同黑格尔的思想相比，是倒退了许多。</p><p><strong>费尔巴哈“爱”的宗教观的理论基础是错误的</strong></p><p>​<strong>对宗教本质的理解是错误的</strong>——费尔巴哈把宗教看作是人与人情感关系的表现，希望在人与人之间的“爱情关系”中寻找宗教的真理，这意味着他把人的“性爱关系”理解为新宗教借以实现的最高形式；</p><p>​<strong>抹杀了宗教的历史性</strong>——费尔巴哈将人情感的永恒性同宗教的永恒性混为一谈，这在实质上是否认了宗教的历史性。</p><p><strong>费尔巴哈夸大宗教变迁的历史作用</strong></p><p>​费尔巴哈以宗教变迁作为人类历史变迁的标志，这并不符合历史事实，传统部落宗教是伴随着部落的兴衰而存在的；世界三大宗教的变迁对人类历史有很大的影响，但这种影响实际上是现实经济关系变化的产物。当新阶级，如资产阶级，足够强大，它就会抛弃宗教的外衣，直接以政治、法律的形式表达自己阶级的要求。</p><p><strong>费尔巴哈宗教观的社会作用是消极的</strong></p><p>​恩格斯说，“在我们不得不生活于其中的、以阶级对立和阶级统治为基础的社会里，同他人交往时表现纯粹人类感情的可能性，今天已经被破坏得差不多了。我们没有理由去把这种感情尊崇为宗教，从而更多地破坏这种可能性。</p><p>​费尔巴哈的观点实质上进一步掩饰了这种对立的事实，结果只能阻碍人们对历史本质的认识和理解。</p><h4 id="三、关于宗教社会历史作用的反思"><a href="#三、关于宗教社会历史作用的反思" class="headerlink" title="三、关于宗教社会历史作用的反思"></a>三、关于宗教社会历史作用的反思</h4><p><strong>宗教与文化</strong></p><p>​宗教典籍和建筑都属于人类精神产品的重要成果。从哲学的角度看，我们从中能够领悟到的是历史上人类对自身命运的某种思考，对人生真谛的某种见解，对人自身局限性的某种批判。对我们今天进一步思考人生问题是有帮助的。</p><p><strong>宗教与政治</strong></p><p>​它既可能作为被压迫者反抗统治者的手段：如早期基督教，近代资产阶级反抗封建统治者的工具（新教改革）。</p><p>​也可能是统治者压迫人们的工具：中世纪基督教、近代传教士在殖民地半殖民地的活动等。</p><p><strong>宗教与伦理（道德）</strong></p><p>​宗教教义的伦理（道德）价值：新教的职业概念、积善行德的教诲（马克斯·韦伯的《新教伦理与资本主义精神》）。</p><p>​宗教教义终极思考的价值：对上帝的信仰表现信众们对至善问题的思考和追求，是他们寻找精神旧宿的努力（如何成为一个善良的人的问题）。</p><p><strong>宗教与人的自由</strong></p><p>​当社会阶级冲突异常尖锐的情况下，宗教的忍让说教，往往会模糊人们的阶级意识，遏制人的自我意识的觉醒。</p><p>​当“执着”态度转化为“痴迷”态度，“狂热”就会成为一种生存状态。此时，人的精神就被某种外在的观念所控制，人就会失去独立思考的能力，丧失批判的意识。</p><p>​人的自由的无条件性与人的有限自由之争论问题值得关注。</p><h3 id="第九讲-伦理规范的时代性"><a href="#第九讲-伦理规范的时代性" class="headerlink" title="第九讲 伦理规范的时代性"></a>第九讲 伦理规范的时代性</h3><h4 id="一、旧唯物主义走向唯心史观的理论原因"><a href="#一、旧唯物主义走向唯心史观的理论原因" class="headerlink" title="一、旧唯物主义走向唯心史观的理论原因"></a>一、旧唯物主义走向唯心史观的理论原因</h4><p>近代人道主义哲学最基本的精神就是弘扬人的自由精神。</p><p>**机械性：**用力学的观点解释一切自然现象和人类生活现象，看不到不同自然现象之间，以及人类生活同自然现象之间运动规律的重要区别。</p><p>**形而上学性（非历史观念）：**不能把自然现象理解为一个过程，把自然运动看作是周而复始的运动；没有把历史理解为普遍联系的运动过程，而把历史理解为单个事件的汇集。</p><p>走向<strong>历史唯心论</strong>是其逻辑的必然结局。</p><p><strong>马克思主义和旧哲学在伦理论问题上的根本区别</strong></p><p>第一点，人不是脱离社会关系而独立存在的人。</p><p>第二点，现实人在于它的社会性。</p><h4 id="二-、恩格斯对费尔巴哈伦理观的批评"><a href="#二-、恩格斯对费尔巴哈伦理观的批评" class="headerlink" title="二 、恩格斯对费尔巴哈伦理观的批评"></a>二 、恩格斯对费尔巴哈伦理观的批评</h4><p>费尔巴哈伦理观的理论基础（抽象的人性论）是不恰当的。</p><p>费尔巴哈的善恶观实际是抽象的。</p><p>费尔巴哈提出的道德准则实际上是一种超历史的、抽象的。</p><p><strong>如何理解“情爱”对人生的意义</strong></p><p>​恩格斯指出：“人与人之间的、特别是两性之间的感情关系，是自从有人类以来就存在的。性爱特别是在最近八百年间获得了这样的意义和地位，竟成了这个时期中一切诗歌必须环绕着旋转的轴心了”。</p><p>29分21秒处</p><h4 id="三、历史评价方法论问题反思"><a href="#三、历史评价方法论问题反思" class="headerlink" title="三、历史评价方法论问题反思"></a>三、历史评价方法论问题反思</h4>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>哲学</tag>
      
      <tag>演讲</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>广义线性模型</title>
    <link href="/2021/20210921/"/>
    <url>/2021/20210921/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>众所周知，线性模型是<span id="more"></span>：<br>$$<br>\begin{align}y &amp;&#x3D; \beta_0 + x_1 \beta_1 +x_2 \beta_2 +\cdots + x_p \beta_p  + \epsilon\\&amp;&#x3D; x^T \beta + \epsilon\end{align}<br>$$<br>那么广义线性模型（Generalized linear models,GLM）相当于把y变为了$g(y)$。</p><h2 id="指数族分布"><a href="#指数族分布" class="headerlink" title="指数族分布"></a>指数族分布</h2><p>可以写成<br>$$<br>p(y|\theta) &#x3D; \exp {\frac{\theta y - b(\theta)}{a(\phi)} + c(y,\phi)}<br>$$<br>a通常定义为，其中w是样本权重：<br>$$<br>a(\phi) &#x3D; \frac{\phi}{w_i}<br>$$</p><h2 id="广义线性模型"><a href="#广义线性模型" class="headerlink" title="广义线性模型"></a><strong>广义线性模型</strong></h2><p><img src="/34_1.jpg"></p><ol><li>一个线性预测器 $\eta&#x3D;\beta^Tx$，被称为系统组件(systematic component)。</li><li>一个指数族分布作为响应变量 Y概率分布$P（Y；\theta)$，被称为随机组件(random component)。</li><li>一个连接函数（link function） g 使得 $\eta&#x3D;g(\mu)$，$\mu$是Y 的期望，连接函数描述系统组件和随机组件之间的关系。</li></ol><p>PRML上称$g^{-1}$为激活函数（activation function）。</p><p>g要求是任意可微和可逆的，也就是说g是单调的。</p><p>但是我也找到一篇印度人写的《GENERALIZED LINEAR MODELS WITH NON-MONOTONIC LINK FUNCTION》，似乎是不用单调连接函数，但可惜没有找到pdf，未能浏览全文。</p><h2 id="如何优化"><a href="#如何优化" class="headerlink" title="如何优化"></a>如何优化</h2><p>最大似然估计</p><p>梯度法</p><p>牛顿法</p><p>等等</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.zhangzhenhu.com/glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">https://www.zhangzhenhu.com/glm/source/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html</a></p><p>PRML</p><hr>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《中县干部》</title>
    <link href="/2021/20210919/"/>
    <url>/2021/20210919/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><span id="more"></span><p>**主要是研究下面一些问题： **</p><p>1．中县县乡领导干部的基本特征 </p><p>2．中县领导干部的来源，他们进入党政机关的方式和机制 </p><p>3．领导干部的政治轨迹和生涯模式 </p><p>4．多出干部的机构和岗位 </p><p>5．政绩和领导干部的仕途 </p><p>6．关系和领导干部的仕途</p><p>中县副科级以上男女干部比例严重不对等，特别是女正科级一把手和女县领导。女干部除了自身的素质和能力之外，她们之所以能够脱颖而出，与 她们的家庭背景所拥有的政治资源息息相关。这其中有父辈的庇护，有丈夫的援助，也有组织部门的较早发现和刻意培养；或者几种因素兼而有之。忘掉性别，成为 男性丛林的一部分，这既是一些女干部的成长经验，也是一些女干部的成长之忧。</p><p>1978 年以来，中县领导干部获得干部身份主要有三个途径：大中专毕业生分配、军队转业安置， 以及各种形式的吸收录用。吸收录用机制是领导干部阶层安排子女就业的一种途径，是封闭的自我繁殖的机制，打上了深深的权力庇护的烙印。</p><p>有两个现象比较突出：一是单位系统自繁殖现象，无论好的和差的单位，系统内部子弟安排都比较普遍。二是关系资源的运作，无论自身人力资本如何， 都需要找关系、托门子，以获得工作安排。从这也可以看出，阶层自繁殖是一种基本的本能，而这 必然会导致阶层的劣化和腐朽。</p><p>没有市场化改革的国税所自繁殖现 象更加突出，但也有大中专毕业生分配和关系安排。而进行市场化改革的邮政所，基本通过招录来 补充工作人员，基本上根绝了自繁殖现象和关系安排，这说明，“逢进必考”、“公开招录”这些 市场化竞争机制是改变单位自繁殖和关系安排的两大制度设计。</p><p>从中县的现实来看，人力资本和 社会资本越雄厚，越是到好单位；人力资本和社会资本越稀薄，越是到坏单位。</p><p>中县官场存在“年轻”效应，即越年轻成为领导干部，优 势越大，晋升也可能越快。</p><p>是党务部门挤压了政府部门和事业单位的晋升空间，这同时也说明党务部门是 权力资源的集大成者，是干部资源的高地，也是执政党地位的一种体现。党管干部和党务部门向其 它部门输送干部是一脉相承的。</p><p>组织部为什么多出干部？</p><p>1．部门优势</p><p>2．高进高出。组织部进人也是再筛选机制，甄补干部比较严格。</p><p>农林、教育职能部门为什么产出干部相对较多？</p><p>1．院校分布和专业分配。其它系统则没有或者缺乏相应的精英学校</p><p>2．学历干部聚集。农林和教育系统是知识分子聚集的地方，是知识型干部扎堆的地方。</p><p>3．地方需要。</p><p>地位显赫的财政局、人劳局和计生委为什么出干部不多？</p><p>这是因为这些局权力和资源多，在县里地位高，待遇好，干部一旦进入，都不愿意出来，结果 导致干部积压，进步反而慢了。</p><p>有 59%的县领导，77%的乡镇领导，53%的县直领导，具有乡镇工作经历。乡镇为什么多出干部？</p><p>主要因素有：</p><p>1．组织部门用人导向。组织部门一直都有很明确的用人导向，就是从基层，从一线提拔干部。</p><p>2．乡镇职位多。</p><p>3．乡镇干部资源相对较少，积压也较少。</p><p>4．乡镇工作特点。乡镇是综合全面工作，可以使干部得到全方面的锻炼。乡镇直接面对群众， 经常面对急事、难事和麻烦事。</p><p>为什么秘书岗位多出干部？主要因素有：</p><p>1．多层筛选。质量有保障。</p><p>2．学历因素。</p><p>3．岗位特点。“秘书的快速晋升优势不仅在于其和首长亲近的关系纽带，还在于他们拥有适合中国政治系统 特性的技能。”（Li Wei and Pye,1992:928）</p><p>4．服务领导。也就是中县人所说的“大人手下长大人”，秘书是领导身边的人，一方面领导会 有意识的传授，另一方面秘书在日常服务中也会不断学习和感悟。同时，秘书和领导往往形成一种 庇护性关系，领导因为了解秘书，同时感念其服务的辛苦，往往会为秘书的提拔和晋升多多考虑和 筹划。</p>]]></content>
    
    
    
    <tags>
      
      <tag>社会</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>numpy-100知识点</title>
    <link href="/2021/20210829/"/>
    <url>/2021/20210829/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>以下<a href="https://github.com/rougier/numpy-100">numpy-100题</a>中涉及的一些知识点。</p><p>numpy-100涉及知识点广泛，适合复习和查漏补缺。</p><span id="more"></span><hr><p><strong>np.version, np.show_config:</strong><br>获取numpy的版本和配置信息</p><p><strong>np.info：</strong><br>获取说明文档<br>例：np.info(np.add)</p><p><strong>np.pad：</strong><br>对数组填充<br><a href="https://numpy.org/doc/stable/reference/generated/numpy.pad.html">详细可见</a></p><p><strong>np.nan：</strong><br>nan&#x3D;not a number<br>故：</p><table><thead><tr><th>输入</th><th>输出</th></tr></thead><tbody><tr><td>0 * np.nan</td><td>nan</td></tr><tr><td>np.nan &#x3D;&#x3D; np.nan</td><td>False</td></tr><tr><td>np.inf &gt; np.nan</td><td>False</td></tr><tr><td>np.nan - np.nan</td><td>nan</td></tr><tr><td>0.3 &#x3D;&#x3D; 3 * 0.1</td><td>False</td></tr></tbody></table><p><strong>np.unravel_index:</strong><br>例子：考虑一个 (6,7,8) 形状的数组，其第100个元素的索引(x,y,z)是什么?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(np.unravel_index(<span class="hljs-number">100</span>,(<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>)))<br></code></pre></td></tr></table></figure><p> <strong>np.tile：</strong><br>将原矩阵横向、纵向地复制。<br>例子：用tile函数去创建一个 8x8的棋盘样式矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Z = np.tile( np.array([[<span class="hljs-number">0</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">0</span>]]), (<span class="hljs-number">4</span>,<span class="hljs-number">4</span>))<br><span class="hljs-built_in">print</span>(Z)<br></code></pre></td></tr></table></figure><p><strong>np.dtype:</strong><br>查询属性。<br>此外，还有定义类型。<br>例 ：创建一个将颜色描述为(RGBA)四个无符号字节的自定义dtype？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">color = np.dtype([(<span class="hljs-string">&quot;r&quot;</span>, np.ubyte, <span class="hljs-number">1</span>),<br>                  (<span class="hljs-string">&quot;g&quot;</span>, np.ubyte, <span class="hljs-number">1</span>),<br>                  (<span class="hljs-string">&quot;b&quot;</span>, np.ubyte, <span class="hljs-number">1</span>),<br>                  (<span class="hljs-string">&quot;a&quot;</span>, np.ubyte, <span class="hljs-number">1</span>)])<br></code></pre></td></tr></table></figure><p><strong>np.copysign:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">numpy.copysign(array1,array2)       将第二个数组中值得符号复制给第一个数组中值<br></code></pre></td></tr></table></figure><p><strong>np.intersect1d：</strong><br>找到两个数组的交集。</p><p><strong>时间操作功能：</strong><br>np.datetime64, np.timedelta64。<br>例：today &#x3D; np.datetime64(‘today’, ‘D’)  查询今日日期<br>timedelta64数据类型用以补充datetime64。timedelta64的参数是一个数字，代表单位数，一个日期&#x2F;时间单位，例如（D）ay，（M）onth，（Y）ear，（h）ours，（m）inutes或（s）其次。timedelta64数据类型还接受字符串“ NAT”代替“ Not A Time”值的数字。</p><p><strong>取整：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span> (np.floor(Z))<br><span class="hljs-built_in">print</span> (np.cell(Z)-<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span> (Z.astype(<span class="hljs-built_in">int</span>))<br><span class="hljs-built_in">print</span> (np.trunc(Z))<br></code></pre></td></tr></table></figure><p><strong>np.ufunc.reduce:</strong><br>Reduces array’s dimension by one, by applying ufunc along one axis.<br>add.reduce()&#x3D;sum(),且在小数组求和中前者更快。</p><p><strong>np.allclose, np.array_equal：</strong><br>前者判断相近（相等），后者判断相等。</p><p><strong>flags.writeable</strong>：<br>把数组变为只读。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">Z = np.zeros(<span class="hljs-number">5</span>)<br>Z.flags.writeable = <span class="hljs-literal">False</span> <span class="hljs-comment">#把数组变为只读。</span><br>Z[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span> <span class="hljs-comment">#报错</span><br></code></pre></td></tr></table></figure><p><strong>打印数组中所有的值：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">np.set_printoptions(threshold=np.nan)<br>Z = np.zeros((<span class="hljs-number">16</span>,<span class="hljs-number">16</span>))<br><span class="hljs-built_in">print</span>(Z)<br></code></pre></td></tr></table></figure><p><strong>np.meshgrid()：</strong><br>生成网格点坐标矩阵。</p><p><strong>np.genfromtxt：</strong><br>读取txt。</p><p><strong>数组索引：</strong><br>np.ndenumerate：返回一个迭代器，生成数组坐标和值对。<br>np.ndindex:索引数组的N维迭代器对象。</p><p><strong>np.bincount:</strong><br>计数非负整数数组中每个值的出现次数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">np.bincount(np.array([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">7</span>]))<br>array([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p><strong>np.unique:</strong><br>去除数组中的重复数字,并进行排序之后输出。</p><p><strong>np.diag:</strong><br>当 np.diag(array) 中<br>array是一个1维数组时，结果形成一个以一维数组为对角线元素的矩阵。<br>array是一个二维矩阵时，结果输出矩阵的对角线元素。</p><p><strong>多维切片之冒号和三个点：</strong><br>None代表新增加一个维度。<br>…代表所有的冒号。</p><p><strong>np.repeat:</strong><br>将numpy数组重复。 numpy.repeat(a, repeats, axis&#x3D;None)</p><p><strong>np.cumsum:</strong><br>numpy.cumsum(a, axis&#x3D;None, dtype&#x3D;None, out&#x3D;None)<br>返回沿着给定轴的元素的累积和。<br>axis&#x3D;0，按照行累加。<br>axis&#x3D;1，按照列累加。<br>axis不给定具体值，就把numpy数组当成一个一维数组。</p><p>**注意：**cumsum(b)[-1] 可能不等于 sum(b)</p><p>因为sum直接将每个数字单独加到结果上。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">&gt;&gt;&gt;b = np.array([<span class="hljs-number">1</span>, <span class="hljs-number">2e-9</span>, <span class="hljs-number">3e-9</span>] * <span class="hljs-number">1000000</span>)<br>&gt;&gt;&gt;b.cumsum()[-<span class="hljs-number">1</span>]<br><span class="hljs-number">1000000.0050045159</span><br>&gt;&gt;&gt;b.<span class="hljs-built_in">sum</span>()                    <br><span class="hljs-number">1000000.0050000029</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>numpy</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>共产主义ABC</title>
    <link href="/2021/20210828/"/>
    <url>/2021/20210828/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><span id="more"></span><div class="row">    <embed src="/pdf/communism_abc.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    
    <tags>
      
      <tag>哲学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>变量替换巧获积分关系</title>
    <link href="/2021/20210825/"/>
    <url>/2021/20210825/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>题源：自动内卷人（来自Q群-卷明白）</p><p>证明: $\int_{0}^{1} \frac{\ln (1+x)}{x} d x&#x3D;\frac{\pi^{2}}{12}$, 并由此计算 $\int_{0}^{+\infty} \arctan x \cdot \ln \left(1+\frac{1}{x^{2}}\right) d x$.</p><span id="more"></span><hr><p>前半部分：略</p><p>后半部分：</p><p>计算$\int_{0}^{+\infty} \arctan x \cdot \ln \left(1+\frac{1}{x^{2}}\right) d x$并不算很困难，但<strong>由前半部分证明后半部分</strong>存在一定难度。</p><blockquote><p>相似的题：</p><p>SEEMOUS 2016 problem 4:</p><p> 整数$n \ge 1$。设$$I_n&#x3D;\int_0^{\infty}\frac{\arctan x}{(1+x^2)^n} dx.$$<br>证明<br>a) $\sum_{n&#x3D;1}^{\infty}\frac{I_n}{n}&#x3D;\frac{\pi ^2}{6}$<br>b)$\int_0^{\infty}\arctan\cdot \ln(1+\frac{1}{x^2})dx&#x3D;\frac{\pi ^2}{6}$</p></blockquote><p>$\int_0^\infty \arctan(x) \ln \left(1 + \frac{1}{x^2}\right) dx$</p><p>$&#x3D;\int_0^\infty  \ln \left(1 + \frac{1}{x^2}\right) d(xarctanx-\frac{1}{2}ln(1+x^2))$</p><p>$&#x3D; \int_0^\infty \left[\frac{2 \arctan{x}}{1+x^2} - \frac{\ln(1+x^2)}{x (1+x^2)}\right] dx $</p><p>$\hspace{-11pt}\stackrel{x &#x3D; \sqrt{\mathrm{e}^t - 1}}{&#x3D;} \frac{\pi^2}{4} - \frac{1}{2} \int_0^\infty \frac{t}{\mathrm{e}^t - 1} \mathrm{d} t $</p><p>设:</p><p>$I&#x3D;\int_{0}^{\infty} \frac{x}{e^{x}-1} d x$</p><p>则：</p><p>$I&#x3D;\int_{0}^{\infty} \frac{x}{e^{x}-1} d x&#x3D;-\int_{-\infty}^{0} \frac{-y}{e^{-y}-1} d y&#x3D;\int_{-\infty}^{0} \frac{y \cdot d^{y}}{y-1} d y&#x3D;\int_{0}^{1} \frac{u \cdot \ln (u)}{u \cdot(u-1)} d u$<br>$I&#x3D;\int_{0}^{1} \frac{\ln (u)}{u-1} d u&#x3D;4 \int_{0}^{1} \frac{v \cdot \ln (v)}{v^{2}-1} d \nu&#x3D;\int_{0}^{1} \frac{u \cdot \ln (u)}{u^{2}-1} d u+\int_{0}^{1} \frac{\ln (u)}{u^{2}-1} d u$ (1)</p><p>由此说明：</p><p>$\int_{0}^{1} \frac{\ln (x)}{x^{2}-1} d x&#x3D;3\int_{0}^{1} \frac{x\ln (x)}{x^{2}-1} d x$ （2）</p><blockquote><p>除此之外，还有一个有趣的等式：</p><p>$ \int_{0}^{1} \frac{\ln (u)}{u^{2}-1} d u$<br>$&#x3D;- \cdot \int_{\infty}^{1} \frac{\left(\frac{1}{w}\right)^{2} \cdot \ln \left(\frac{1}{w}\right)}{\left(\frac{1}{w}\right)^{2}-1} d w$<br>$&#x3D; \int_{1}^{\infty} \frac{\ln (w)}{w^{2}-1} d v$<br>$&#x3D; \int_{1}^{\infty} \frac{\ln (u)}{u^{2}-1} d u$</p></blockquote><p>设$J&#x3D;\int_{0}^{1} \frac{\ln (x+1)}{x} d x$，由题意我们知道$J&#x3D;\frac{\pi^{2}}{12}$.</p><p>$J&#x3D;\int_{0}^{1} \frac{\ln (x+1)}{x} d x$<br>$\hspace{-11pt}\stackrel{分部积分}{&#x3D;} -\int_{0}^{1} \frac{\ln (x)}{x+1} d x$<br>$&#x3D;-\int_{0}^{1} \frac{(x-1) \cdot \ln (x)}{(x-1) \cdot(x+1)} d x$<br>$&#x3D;\int_{0}^{1} \frac{\ln (x)}{x^{2}-1} d x-\int_{0}^{1} \frac{x \cdot \ln (x)}{x^{2}-1} d x$</p><p>代入（2）得：<br>$\quad \int_{0}^{1} \frac{\ln (x)}{x^{2}-1} d x&#x3D;\frac{\pi^{2}}{8} \quad \int_{0}^{1} \frac{x \ln (x)}{x^{2}-1} d x&#x3D;\frac{\pi^{2}}{24}$<br>再代入（1）得:$I&#x3D;\frac{\pi^{2}}{6}$</p><p>故原式$&#x3D;\frac{\pi^{2}}{4}-\frac{1}{2}*I&#x3D;\frac{\pi^{2}}{6}$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>辛普森公式</title>
    <link href="/2021/20210813/"/>
    <url>/2021/20210813/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>辛普森公式（Simpson‘s rule）公式是牛顿-科特斯公式当n&#x3D;2时的情形。</p><p>$\int_{a}^{b} f(x) d x \approx \frac{b-a}{6}\left[f(a)+4 f\left(\frac{a+b}{2}\right)+f(b)\right]$</p><span id="more"></span><p>辛普森公式的误差：</p><p>设$f(x)$在[a,b]上具有连续的四阶导数，则辛普森公式的误差为</p><p>$R_2(f)&#x3D;-\frac{(b-a)^5}{2880} f^{(4)}(\eta ),\eta \in [a,b].$</p><p>即：</p><p>$| \int_{a}^{b} f(x) d x - \frac{b-a}{6}\left[f(a)+4 f\left(\frac{a+b}{2}\right)+f(b)\right] | \leq \frac{1}{90} \left(\frac{b-a}{2}\right)^5 \sup_{[a,b]} |f^{(4)}|.$</p><p>证明：</p><p>设$x&#x3D;(a+b)&#x2F;2,h&#x3D;(b-a)&#x2F;2$</p><p>对$\frac{6\int_{x-h}^{x+h}f(t)dt-2h(f(x+h)+4f(x)+f(x-h))}{h^5}$</p><p>以h为主元，反复使用柯西中值定理：</p><p>$&#x3D;\frac{4f(x+h_1)-8f(x)+4f(x-h_1)-2h_1(f’(x+h_1)-f’(x-h_1))}{5h_1^4} $</p><p>$&#x3D;\frac{2f’(x+h_2)-2f’(x-h_2)-2h_2(f’’(x+h_2)+f’’(x-h_2))}{20h_2^3} $</p><p>$&#x3D;\frac{2h_3(f’’’(x+h_3)-f’’’(x-h_3))}{60h_3^2}&#x3D;\frac1{15}·\frac{f’’’(x+h_3)-f’’’(x-h_3)}{2h_3} $</p><p>$&#x3D;\frac1{15}·f^{(4)}(x+h_4) $</p><p>其中$0&lt;h_4&lt;h_3&lt;h_2&lt;h_1&lt;h$</p><p>证毕。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之建议与应用</title>
    <link href="/2021/20210805/"/>
    <url>/2021/20210805/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h5 id="机器学习的建议"><a href="#机器学习的建议" class="headerlink" title="机器学习的建议"></a>机器学习的建议</h5><p>面对模型的下一步操作：</p><p>我们可以</p><ol><li><p>获得更多的训练样本——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。</p></li><li><p>尝试减少特征的数量</p></li><li><p>尝试获得更多的特征</p></li><li><p>尝试增加多项式特征</p></li><li><p>尝试减少正则化程度$\lambda$</p></li><li><p>尝试增加正则化程度$\lambda$</p></li></ol><span id="more"></span><p>我们该怎么选择呢？我们应该运用一些方法——被称为”机器学习诊断法”——来帮助我们知道上面哪些方法对我们的算法是有效的。</p><h6 id="训练集与测试集"><a href="#训练集与测试集" class="headerlink" title="训练集与测试集"></a>训练集与测试集</h6><p>为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。</p><p>测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型。</p><h6 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h6><p>假设我们要在10个不同次数的二项式模型之间进行选择：</p><p><img src="http://www.ai-start.com/ml2014/images/1b908480ad78ee54ba7129945015f87f.jpg" alt="img"></p><p> 我们需要使用交叉验证集来帮助选择模型。 即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集。</p><p><img src="http://www.ai-start.com/ml2014/images/7cf1cd9c123a72ca4137ca515871689d.png" alt="img"></p><p><strong>模型选择的方法为：</strong></p><ol><li><p>使用训练集训练出10个模型</p></li><li><p>用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）</p></li><li><p>选取代价函数值最小的模型</p></li><li><p>用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）</p><p><em><strong>Train&#x2F;validation&#x2F;test error</strong></em></p><p><strong>Training error:</strong></p></li></ol><p>$J _{train}(\theta)&#x3D;\frac{1}{2m}\sum _\limits{i&#x3D;1}^{m}(h _{\theta}(x^{(i)})-y^{(i)})^2$</p><p><strong>Cross Validation error:</strong></p><p>$J _{cv}(\theta)&#x3D;\frac{1}{2m _{cv}}\sum _\limits{i&#x3D;1}^{m}(h _{\theta}(x^{(i)} _{cv})-y^{(i)} _{cv})^2$</p><p><strong>Test error:</strong></p><p>$J _{test}(\theta)&#x3D;\frac{1}{2m _{test}}\sum _\limits{i&#x3D;1}^{m _{test}}(h _{\theta}(x^{(i)} _{cv})-y^{(i)} _{cv})^2$</p><h6 id="偏差与方差"><a href="#偏差与方差" class="headerlink" title="偏差与方差"></a>偏差与方差</h6><p><img src="http://www.ai-start.com/ml2014/images/bca6906add60245bbc24d71e22f8b836.png" alt="img"></p><p><strong>Bias&#x2F;variance</strong></p><p><strong>Training error:</strong>               $J_{train}(\theta) &#x3D; \frac{1}{2m}\sum_\limits{i&#x3D;1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2$</p><p><strong>Cross Validation error:</strong></p><p>$J _{cv}(\theta) &#x3D; \frac{1}{2m _{cv}}\sum _\limits{i&#x3D;1}^{m}(h _{\theta}(x^{(i)} _{cv})-y^{(i)} _{cv})^2$</p><p>我们如何判断是方差还是偏差呢？根据上面的图表，我们知道:</p><p><img src="http://www.ai-start.com/ml2014/images/25597f0f88208a7e74a3ca028e971852.png" alt="img"></p><p>训练集误差和交叉验证集误差近似时：偏差&#x2F;欠拟合 交叉验证集误差远大于训练集误差时：方差&#x2F;过拟合</p><h6 id="正则化系数的选择"><a href="#正则化系数的选择" class="headerlink" title="正则化系数的选择"></a>正则化系数的选择</h6><p>我们选择一系列的想要测试的 $\lambda$ 值，通常是 0-10之间的呈现2倍关系的值（如：$0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10$共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。</p><p><img src="http://www.ai-start.com/ml2014/images/8f557105250853e1602a78c99b2ef95b.png" alt="img"></p><ol><li>用训练集训练出12个不同程度正则化的模型</li><li>用12个模型分别对交叉验证集计算的出交叉验证误差</li><li>选择得出交叉验证误差<strong>最小</strong>的模型</li><li>运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：</li></ol><p><img src="http://www.ai-start.com/ml2014/images/38eed7de718f44f6bb23727c5a88bf5d.png" alt="img"></p><p>• 当 $\lambda$ 较小时，训练集误差较小（过拟合）而交叉验证集误差较大<br><br>• 随着 $\lambda$ 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加</p><h6 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h6><p>学习曲线是将训练集误差和交叉验证集误差作为训练集样本数量（$m$）的函数绘制的图表。</p><p>即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。</p><p><img src="http://www.ai-start.com/ml2014/images/969281bc9b07e92a0052b17288fb2c52.png" alt="img"></p><p><img src="http://www.ai-start.com/ml2014/images/973216c7b01c910cfa1454da936391c6.png" alt="img"></p><p>（一）如何利用学习曲线识别高偏差&#x2F;欠拟合：</p><p>作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观：</p><p><img src="http://www.ai-start.com/ml2014/images/4a5099b9f4b6aac5785cb0ad05289335.jpg" alt="img"></p><p>也就是说在高偏差&#x2F;欠拟合的情况下，增加数据到训练集不一定能有帮助。</p><p>（二）如何利用学习曲线识别高方差&#x2F;过拟合：</p><p>假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。</p><p><img src="http://www.ai-start.com/ml2014/images/2977243994d8d28d5ff300680988ec34.jpg" alt="img"></p><p>也就是说在高方差&#x2F;过拟合的情况下，增加更多数据到训练集可能可以提高算法效果</p><h6 id="选择下一步"><a href="#选择下一步" class="headerlink" title="选择下一步"></a>选择下一步</h6><p>回顾前面中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择：</p><ol><li>获得更多的训练样本——解决高方差</li><li>尝试减少特征的数量——解决高方差</li><li>尝试获得更多的特征——解决高偏差</li><li>尝试增加多项式特征——解决高偏差</li><li>尝试减少正则化程度λ——解决高偏差</li><li>尝试增加正则化程度λ——解决高方差</li></ol><h6 id="误差分析（Error-Analysis）"><a href="#误差分析（Error-Analysis）" class="headerlink" title="误差分析（Error Analysis）"></a>误差分析（Error Analysis）</h6><p>即人工检查交叉验证集中我们算法中产生预测误差的样本，看看这些样本是否有某种系统化的趋势。</p><p>比如当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。</p><h6 id="类偏斜的误差度量"><a href="#类偏斜的误差度量" class="headerlink" title="类偏斜的误差度量"></a>类偏斜的误差度量</h6><p>训练集中有非常多同一类的样本，只有很少或者没有其他类的样本，这样的训练样本称为偏斜类。（skewed classes）</p><p>比如预测癌症是否恶性的 100 个样本中：95 个是良性的肿瘤，5 个恶性的肿瘤，假设我们在这个样本上对比以下 2 种分类算法的百分比准确度，即分类错误的百分比：人为把所有的样本都预测为良性，则分错了 5 个恶性的样本，错误率为 <code>5 / 100 = 0.05 = 5%</code></p><p>如果仅仅从错误率大小来判断算法的优劣是不合适的，</p><p>为了解决这个问题，使用**查准率（Precision）<strong>和</strong>查全率（Recall）**这 2 个误差指标，为了计算这 2 者，我们需要把算法预测的结果分为以下 4 种：</p><ol><li>真阳性（True Positive，TP）：预测为真，实际为真</li><li>真阴性（True Negative，TN）：预测为假，实际为假</li><li>假阳性（False Positive，FP）：预测为真，实际为假</li><li>假阴性（False Negative，FN）：预测为假，实际为真</li></ol><p>则：</p><p>查准率&#x3D;<strong>TP&#x2F;(TP+FP)</strong>。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，查准率越高越好。<br><br>查全率&#x3D;<strong>TP&#x2F;(TP+FN)</strong>。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比。查全率越高越好。<br><br>这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。</p><table><thead><tr><th></th><th></th><th><strong>预测值</strong></th><th></th></tr></thead><tbody><tr><td></td><td></td><td><strong>Positive</strong></td><td><strong>Negtive</strong></td></tr><tr><td><strong>实际值</strong></td><td><strong>Positive</strong></td><td><strong>TP</strong></td><td><strong>FN</strong></td></tr><tr><td></td><td><strong>Negtive</strong></td><td><strong>FP</strong></td><td><strong>TN</strong></td></tr></tbody></table><h6 id="均衡查准率和查全率"><a href="#均衡查准率和查全率" class="headerlink" title="均衡查准率和查全率"></a>均衡查准率和查全率</h6><p>高查准率意味着只在非常确信的情况下预测为真（肿瘤为恶性）。</p><p>高查全率意味着尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断。</p><p>面对多个阙值参数选择，我们应该使用F1值较高的那系列参数。</p><p><a href="https://imgtu.com/i/febRJg"><img src="https://z3.ax1x.com/2021/08/05/febRJg.jpg"></a></p><h6 id="几个关键问题"><a href="#几个关键问题" class="headerlink" title="几个关键问题"></a>几个关键问题</h6><p>首先，一个人类专家看到了特征值 $x$，能很有信心的预测出$y$值吗？因为这可以证明 $ y$ 可以根据特征值$x$被准确地预测出来。</p><p>其次，我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？</p><p>如果你能做到这两者，那么更多时候，你会得到一个性能很好的学习算法。</p><h6 id="Mini-Batch"><a href="#Mini-Batch" class="headerlink" title="Mini-Batch"></a>Mini-Batch</h6><p>Mini-Batch是介于批量梯度下降算法和随机梯度下降算法（Stochastic Gradient Descent）之间的算法，每计算常数次（b次）训练实例，便更新一次参数  。</p><h6 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h6><p>我们可以通过减小学习率来优化算法。</p><p>例如令$\alpha &#x3D;\frac{常数1}{迭代次数+常数2}$                                                                                                                                             </p><h6 id="在线学习（Online-Learning）"><a href="#在线学习（Online-Learning）" class="headerlink" title="在线学习（Online Learning）"></a>在线学习（Online Learning）</h6><p>在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。 </p><p>一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。</p><h6 id="映射化简和数据并行（Map-Reduce-and-Data-Parallelism）"><a href="#映射化简和数据并行（Map-Reduce-and-Data-Parallelism）" class="headerlink" title="映射化简和数据并行（Map Reduce and Data Parallelism）"></a>映射化简和数据并行（Map Reduce and Data Parallelism）</h6><p>如果我们能够将我们的数据集分配给多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化（Map Reduce）。</p><p>例如，我们有400个训练实例，我们可以将批量梯度下降的求和任务分配给4台计算机进行处理：</p><p><img src="http://www.ai-start.com/ml2014/images/919eabe903ef585ec7d08f2895551a1f.jpg" alt="img"></p><p>很多高级的线性代数函数库已经能够利用多核<strong>CPU</strong>的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。</p><h6 id="获取更多数据"><a href="#获取更多数据" class="headerlink" title="获取更多数据"></a>获取更多数据</h6><p>以文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。</p><p>另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。</p><p>有关获得更多数据的几种方法：</p><ol><li>人工数据合成</li><li>手动收集、标记数据</li><li>众包</li></ol><h6 id="上限分析（Ceiling-Analysis）"><a href="#上限分析（Ceiling-Analysis）" class="headerlink" title="上限分析（Ceiling Analysis）"></a>上限分析（Ceiling Analysis）</h6><p><img src="http://www.ai-start.com/ml2014/images/f1ecee10884098f98032648da08f8937.jpg" alt="img"></p><p> 我们的文字识别应用中，我们的流程图如下：</p><p><img src="http://www.ai-start.com/ml2014/images/55d41ee748680a62e755d6aa5b95b53c.png" alt="img"></p><p>流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。</p><p>如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。</p><p>接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。</p><p>最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。</p><h5 id="相关应用——图片文字识别-Photo-OCR"><a href="#相关应用——图片文字识别-Photo-OCR" class="headerlink" title="相关应用——图片文字识别(Photo OCR)"></a>相关应用——图片文字识别(Photo OCR)</h5><p>图像文字识别应用所作的事是，从一张给定的图片中识别文字。</p><p>了完成这样的工作，需要采取如下步骤：</p><ol><li>文字侦测（<strong>Text detection</strong>）——将图片上的文字与其他环境对象分离开来</li><li>字符切分（<strong>Character segmentation</strong>）——将文字分割成一个个单一的字符</li><li>字符分类（<strong>Character classification</strong>）——确定每一个字符是什么 可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小组来负责解决：</li></ol><p><img src="http://www.ai-start.com/ml2014/images/610fffb413d8d577882d6345c166a9fb.png" alt="img"></p><h6 id="滑动窗口（Sliding-Windows）"><a href="#滑动窗口（Sliding-Windows）" class="headerlink" title="滑动窗口（Sliding Windows）"></a>滑动窗口（Sliding Windows）</h6><p>滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。</p><p>一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。</p><p><img src="http://www.ai-start.com/ml2014/images/1e00d03719e20eeaf1f414f99d7f4109.jpg" alt="img"></p><p>滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。</p><p><img src="http://www.ai-start.com/ml2014/images/bc48a4b0c7257591643eb50f2bf46db6.jpg" alt="img"></p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一天学会深度学习</title>
    <link href="/2021/20210727/"/>
    <url>/2021/20210727/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>此为李宏毅（Hung-yi Lee）的Understanding Deep Learning in One Day 课程的笔记。</p><p>分为四个部分：</p><p>1.深度学习简介（Introduction of Deep Learning）</p><p>2.训练深度神经网络的技巧（ Tips for Training Deep Neural Network）</p><p>3.神经网络的变体（Variants of Neural Network）</p><p>4.深度学习下一股浪潮（Next Wave）</p><span id="more"></span><hr><h4 id="一、深度学习简介"><a href="#一、深度学习简介" class="headerlink" title="一、深度学习简介"></a>一、深度学习简介</h4><p>（这一章前段主要讲述了神经网络的一些基本知识，不再赘述。）</p><p>Universality Theorem告诉我们：任何连续函数$f:R^N \to R^M$都能通过具有一个隐藏层的神经网络实现（只要给定足够多的隐藏神经元）</p><p>相关资料：<a href="http://neuralnetworksanddeeplearning.com/chap4.html">Reference for the reason</a></p><p>那为什么我们选择使用深层（deep)神经网络而不是浅层但是更fat的神经网络呢？</p><p>使用深层神经网络具有更少的感知机，也表现得更好。</p><p>此外，深层也意味着模块化（Modularization）。比如一个人脸识别的神经网络，第一次可能是性别分类器，第二次可能是某某特征的分类器，诸如此类。模块化是从数据中自动学习的，无需手动设置。</p><p>（另外，也可参考吴恩达《深度学习》课程中“Why deep representations?”部分）</p><p>（接下来的后半段是Keras使用教程，不再赘述）</p><h4 id="二、训练深度神经网络的技巧"><a href="#二、训练深度神经网络的技巧" class="headerlink" title="二、训练深度神经网络的技巧"></a>二、训练深度神经网络的技巧</h4><p>首先虽然过拟合是一个常见问题，但不要总是责怪过拟合，有可能是训练集数据的问题。</p><p>为了解决过拟合，我们可以主动获取更多训练集，并创建一些训练集样本（比如手写识别中，我们可以选择将训练样本旋转一定角度）</p><p>作为实验，我们还会想测试集中添加噪声。</p><h5 id="深度学习的秘诀（Recipe-of-Deep-Learning）"><a href="#深度学习的秘诀（Recipe-of-Deep-Learning）" class="headerlink" title="深度学习的秘诀（Recipe of Deep Learning）"></a>深度学习的秘诀（Recipe of Deep Learning）</h5><h6 id="1、选择合适的代价函数"><a href="#1、选择合适的代价函数" class="headerlink" title="1、选择合适的代价函数"></a>1、选择合适的代价函数</h6><p>比如，当你使用softmax输出层时，请选择交叉熵（ cross entropy）</p><h6 id="2、使用Mini-batch"><a href="#2、使用Mini-batch" class="headerlink" title="2、使用Mini-batch"></a>2、使用Mini-batch</h6><p>Mini-batch相比原始的梯度下降法更快，但是不一定更准确，其核心思想其实就是并行计算。</p><h6 id="3、选择新的激活函数（activation-function）"><a href="#3、选择新的激活函数（activation-function）" class="headerlink" title="3、选择新的激活函数（activation function）"></a>3、选择新的激活函数（activation function）</h6><p>更深的神经网络并不一定更好，可能面对梯度消失（Vanishing Gradient Problem）等问题。</p><p><img src="https://z3.ax1x.com/2021/07/27/W4V66x.jpg"></p><p>2006年，人们普遍使用 RBM pre-training（Restricted Boltzmann Machine）。<br>2015年，更多的人使用ReLU。</p><blockquote><p>Rectified Linear Unit (ReLU)</p><p>使用原因：</p><p>1.计算更快</p><p>2.生物学原因</p><p>3.带有不同biases的无限(Infinite) sigmoid</p><p>4.有利于处理梯度消失的问题</p><p>ReLU 的变种：</p><p><img src="https://z3.ax1x.com/2021/07/27/W4VvNQ.jpg"></p><p>α也通过梯度下降法学习。</p></blockquote><p>此外还有Maxout，ReLU是Maxout的一个特例。</p><h6 id="4、自适应学习速率（Adaptive-Learning-Rate）"><a href="#4、自适应学习速率（Adaptive-Learning-Rate）" class="headerlink" title="4、自适应学习速率（Adaptive Learning Rate）"></a>4、自适应学习速率（Adaptive Learning Rate）</h6><p>一个流行且简单的想法是：</p><p>每隔几代（epochs）就降低学习率</p><p>比如可设置$\eta ^t&#x3D;\eta &#x2F; \sqrt{t+1}$</p><p>注意，学习率不能一刀切，不同参数不同学习率不一定相同。</p><p><strong>Adagrad算法：</strong></p><p><img src="https://z3.ax1x.com/2021/07/27/W4mDhQ.jpg"></p><p>设置全局学习率之后，每次的学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同。</p><p>当然还有其他算法。</p><blockquote><p>• Adagrad [John Duchi, JMLR’11] </p><p>• RMSprop </p><p>​• <a href="https://www.youtube.com/watch?v=O3sxAc4hxZU">https://www.youtube.com/watch?v=O3sxAc4hxZU</a> </p><p>• Adadelta [Matthew D. Zeiler, arXiv’12] </p><p>• “No more pesky learning rates” [Tom Schaul, arXiv’12] </p><p>• AdaSecant [Caglar Gulcehre, arXiv’14] </p><p>• Adam[Diederik P. Kingma, ICLR’15] </p><p>• Nadam </p><p>​• <a href="http://cs229.stanford.edu/proj2015/054_report.pdf">http://cs229.stanford.edu/proj2015/054_report.pdf</a></p></blockquote><h6 id="5、动量方法（Momentum）"><a href="#5、动量方法（Momentum）" class="headerlink" title="5、动量方法（Momentum）"></a>5、动量方法（Momentum）</h6><p>模拟自然界中的物理过程。仍然不能保证一定达到全局最小值，但有一定希望。</p><p><img src="https://z3.ax1x.com/2021/07/27/W4K4N8.jpg"></p><p><strong>Adam算法：</strong></p><p>相当于RMSProp (Advanced Adagrad) + Momentum </p><p><img src="https://z3.ax1x.com/2021/07/27/W5vFKJ.jpg"></p><h6 id="6、使用早停法（early-stopping）"><a href="#6、使用早停法（early-stopping）" class="headerlink" title="6、使用早停法（early stopping）"></a>6、使用早停法（early stopping）</h6><p><a href="https://keras.io/getting_started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore">keras关于早停的教程</a></p><h6 id="7、权重衰减-weight-decay"><a href="#7、权重衰减-weight-decay" class="headerlink" title="7、权重衰减(weight decay)"></a>7、权重衰减(<em>weight</em> <em>decay</em>)</h6><p>我们的大脑会清除了神经元之间无用的联系。</p><p>神经网络也应如此。</p><p>权值衰减是正则化的一种技巧。</p><p><img src="https://z3.ax1x.com/2021/07/28/WIXQC8.jpg"></p><p><a href="https://keras.io/api/layers/regularizers/">Keras关于权重衰减的部分</a></p><h6 id="8、使用Dropout算法"><a href="#8、使用Dropout算法" class="headerlink" title="8、使用Dropout算法"></a>8、使用Dropout算法</h6><p><strong>Dropout流程：</strong></p><p>每次更新参数前，每个神经元都有p%的可能性退出。</p><p>所以，Dropout算法训练一堆不同结构的神经网络。</p><p>那为什么Dropout会更好呢？</p><p>直观理解：</p><p>​比如当团队合作时，如果每个人都希望其他partner来完成工作，那么最终什么也做不成。<br>​然而，如果你知道你的partner会退出，你会做得更好。<br>​在测试时，其实没有人中途退出，所以最终取得了不错的成绩。</p><blockquote><p> 更多资料：<br>• More reference for dropout [Nitish Srivastava, JMLR’14] [Pierre Baldi, NIPS’13][Geoffrey E. Hinton, arXiv’12] </p><p>• Dropout works better with Maxout [Ian J. Goodfellow, ICML’13] </p><p>• Dropconnect [Li Wan, ICML’13]<br>​• Dropout delete neurons </p><p>​• Dropconnect deletes the connection between neurons </p><p>• Annealed dropout [S.J. Rennie, SLT’14] • Dropout rate decreases by epochs </p><p>• Standout [J. Ba, NISP’13] </p><p>​• Each neural has different dropout rate</p></blockquote><h4 id="三、神经网络的变体"><a href="#三、神经网络的变体" class="headerlink" title="三、神经网络的变体"></a>三、神经网络的变体</h4><p>这一部分主要讲述CNN和RNN。</p><h4 id="四、下一股浪潮"><a href="#四、下一股浪潮" class="headerlink" title="四、下一股浪潮"></a>四、下一股浪潮</h4><blockquote><p> 本节提纲:</p><p>•监督学习</p><p>​•超深层网络（Ultra Deep Network）</p><p>​•Attention Model</p><p>•强化学习</p><p>•无监督学习</p><p>​•图片:认识世界是什么样子</p><p>​•文本:理解词语的含义</p><p>​•音频:在没有监督的情况下学习人类语言</p></blockquote><h5 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h5><h6 id="超深网络"><a href="#超深网络" class="headerlink" title="超深网络:"></a>超深网络:</h6><p>近年来模型层数逐渐增加。</p><p>AlexNet (2012) ：8层</p><p>VGG (2014) ：19层</p><p>GoogleNet (2014)：22层</p><p>Residual Net (2015)：152层</p><p>超深层网络是多种深层网络的集合。</p><p>比如有篇论文：</p><p><a href="https://arxiv.org/abs/1605.06431">Residual Networks are Exponential Ensembles of Relatively Shallow Networks</a></p><p>Highway Network也是一种流行的超深层网络。其能自动选取所需层数。</p><h6 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h6><p><img src="https://z3.ax1x.com/2021/07/28/WoGm1P.jpg"><br><img src="https://z3.ax1x.com/2021/07/28/WoGept.jpg"></p><p>（吴恩达的《深度学习》课程关于这部分讲得更具体，可参考）</p><h5 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h5><p>强化学习是指智能体（Agent)以试错的方式进行学习。像Alpha Go就是监督学习+强化学习的产物。</p><h6 id="强化学习的一些困难"><a href="#强化学习的一些困难" class="headerlink" title="强化学习的一些困难"></a>强化学习的一些困难</h6><p>1）为了获得更多的长期奖励，牺牲即时奖励可能会更好</p><p>2）Agent的行为会影响它接收到的后续数据</p><h6 id="一些应用"><a href="#一些应用" class="headerlink" title="一些应用"></a>一些应用</h6><p> • Alpha Go, Playing Video Games, Dialogue </p><p>• Flying Helicopter </p><p>​• <a href="https://www.youtube.com/watch?v=0JL04JJjocc">https://www.youtube.com/watch?v=0JL04JJjocc</a> </p><p>• Driving </p><p>​• <a href="https://www.youtube.com/watch?v=0xo1Ldx3L5Q">https://www.youtube.com/watch?v=0xo1Ldx3L5Q</a> </p><p>• Google Cuts Its Giant Electricity Bill With DeepMind-Powered AI </p><p>​• <a href="http://www.bloomberg.com/news/articles/2016-07-">http://www.bloomberg.com/news/articles/2016-07-</a> 19&#x2F;google-cuts-its-giant-electricity-bill-with-deepmindpowered-ai</p><h6 id="强化学习相关资料"><a href="#强化学习相关资料" class="headerlink" title="强化学习相关资料"></a>强化学习相关资料</h6><p>• Lectures of David Silver </p><p>​• <a href="http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Te">http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Te</a> aching.html </p><p>​• 10 lectures (1:30 each) </p><p>• Deep Reinforcement Learning </p><p>​• <a href="http://videolectures.net/rldm2015_silver_reinfo">http://videolectures.net/rldm2015_silver_reinfo</a> rcement_learning&#x2F;</p><h5 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h5><h6 id="生成图像"><a href="#生成图像" class="headerlink" title="生成图像"></a>生成图像</h6><p>训练一个解码器来生成图像是无监督的。</p><p><strong>相关方法：</strong></p><p>• Variation Auto-encoder (VAE) </p><p>​• Ref: Auto-Encoding Variational Bayes, <a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a> </p><p>• Generative Adversarial Network (GAN) </p><p>​• Ref: Generative Adversarial Networks, <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a></p><h6 id="机器阅读"><a href="#机器阅读" class="headerlink" title="机器阅读"></a>机器阅读</h6><p>机器在没有监督的情况下，通过阅读大量的文件来学习单词的意思。</p><h6 id="声音学习"><a href="#声音学习" class="headerlink" title="声音学习"></a>声音学习</h6><p>相关模型：WaveNet (DeepMind)</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何只使用一个隐层，来实现包含n元输入的任意布尔函数？</title>
    <link href="/2021/20210724/"/>
    <url>/2021/20210724/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>此为吴恩达《深度学习》课程中其中一个（不起眼的）知识的拓展。</p><p>吴恩达举例说明深度神经网络时，给出了只使用一层隐藏层表示异或逻辑需要$O(2^n)$的隐藏单元。</p><span id="more"></span><p>结论看起来很显然，但该如何证明呢？</p><hr><p>包含n元输入的任意布尔函数可以唯一表示为析取范式（Disjunctive Normal Form，DNF）（由有限个简单合取式构成的析取式）的形式。</p><p>单个隐结点可以表示任意合取式。考虑任意布尔变量，假设$X_i$，若它 在合取范式中出现的形式为正（$X_i$)，则设权重为1；若出现的形式为非$\overline{X_i}$， 则设权重为−1；若没有在合取式中出现。设权重为0；并且偏置设为合区式中变量的总数取负之后再加1。可以看出，当采用ReLU激活函数之后，当且仅当所有出现的布尔变量均满足条件时，该隐藏单元才会被激活（输出1)，否则输出 0，这与合取范式的定义的相符的。然后，令所有隐藏单元到输出层的参数为1，并设输出单元的偏置为0。这样，当且仅当所有的隐藏单元都未被激活时，才会输出0，否则都将输出一个正数，起到了析取的作用。</p><p>由上可知，最多只需一层隐藏层，感知机就能表示任何布尔函数，只需表示为析取范式的形式。任何布尔函数都可以用真值表来表示。</p><p>例如:</p><p><img src="https://z3.ax1x.com/2021/07/26/WRrVr8.jpg"></p><p>那隐藏单元什么时候最多呢？异或函数。这时候的真值表如棋盘一样交叉相错。</p><p><img src="https://z3.ax1x.com/2021/07/26/WRWY4O.jpg"></p><p>这时候，隐藏单元为$2^{n-1}$个。</p><p>也如吴恩达所提到的，如果不限制隐藏层为单层，则隐藏单元数量可以降低到$O(n)$。</p><p>事实上，吴恩达提到的这个异或函数也被称为”n元奇偶校验问题“。文献3中通过引入隐层抑制神经元将隐元数目降至n。</p><p>此外，顺便提一下Shannon’theorem,即当n&gt;2时，存在一个n元输入的布尔函数至少需要$\frac{2^n}{n}$的门。</p><p>如果我们能找到用一个数量多项式大小的神经网络模型计算所有的布尔函数，则说明了P&#x3D;NP。</p><p>[参考资料：]</p><p>1.<a href="http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Spring.2018/www/slides/lec2.universal.pdf">cmu的deep learing ppt</a></p><p>2.《百面机器学习》</p><p>3.陆阳，杨娟，王强，黄镇谨.二进神经网络表达奇偶校验问题的隐元最小数目上界.中国科学：信息科学,2012</p><p>进一步阅读：</p><p>4.Setiono, R. (1997). On the solution of the parity problem by a single hidden layer feedforward neural network. Neurocomputing, 16(3), 225–235.</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>深度学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>费用为k的生成树</title>
    <link href="/2021/20210718/"/>
    <url>/2021/20210718/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>问题：输入无向图G和一个整数k。G中有n个顶点。每条边的费用非1 即2。请在图G中寻找一棵费用为k 的生成树。<br>(此题为数据结构与算法HW5的bouns题第一题)</p><span id="more"></span><p>算法流程：</p><p>先建立两棵生成树——一棵最小生成树，记为R；一棵最大生成树，记为B。</p><p>接下来:（该过程能将树R变形为树B）</p><p>(1)取一条在树B而不是树R中的边e;</p><p>(2)将边e加到树R上;</p><p>(3)因为R是一棵生成树，必然产生了一个环，环中也必然有些边不在B，去除其中的一条边</p><p>重复这三个步骤,直到满足条件。</p><p>时间复杂度：</p><p>算法的第一部分需要运行最小生成树算法两次，因此运行时间为O(Elog V)。</p><p>算法的第二部分最多重复插值算法V−1次。</p><p>插值算法的每次迭代都需要:(i)找到一条边，并将其加到R上，最多需要V次时间;(ii)在循环中寻找一条边从R中移除，这最多需要V个时间。因此，运行插值步骤的总时间最多为$O(V^2)$。因此，算法的总运行时间为$O(V^2+  ElogV)$。</p><p>快速解决方案:这个算法的缓慢部分在于，当我们将一棵生成树插入到另一棵生成树时，在生成树中寻找环。</p><p>下面是一种避免这种情况的方法:•</p><ul><li><p>首先，识别图中只有权值为1的边的连通分量。</p></li><li><p>其次，用权值为2的边连接上述连通分量。这可以使用并查集来检查红色连接组件之间的连接性，每次迭代一个权值为2的边。如果连接两个之前不连接的分量，则添加边。如果这个图最初是连通的，算法则是可行的。</p></li></ul><p>参考资料：</p><p><a href="https://www.comp.nus.edu.sg/~gilbert/CS4234/2015/psets/pset1-solutions.pdf">新加坡国立大学CS4234课程Problem Set 1</a></p><p>洛谷P3623</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之推荐系统</title>
    <link href="/2021/20210716/"/>
    <url>/2021/20210716/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>推荐系统是最重要的机器学习的应用之一。</p><span id="more"></span><p>我们从电影打分系统来进入推荐系统的学习。</p><p>假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。</p><p><img src="http://www.ai-start.com/ml2014/images/c2822f2c28b343d7e6ade5bd40f3a1fc.png" alt="img"></p><p>前三部电影是爱情片，后两部则是动作片，我们可以看出<strong>Alice</strong>和<strong>Bob</strong>似乎更倾向与爱情片， 而 <strong>Carol</strong> 和 <strong>Dave</strong> 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。</p><p>下面引入一些标记：</p><blockquote><p> $n_u$ 代表用户的数量</p><p>$n_m$ 代表电影的数量</p><p>$r(i, j)$ 如果用户j给电影 $i$ 评过分则 $r(i,j)&#x3D;1$</p><p>$y^{(i, j)}$ 代表用户 $j$ 给电影$i$的评分</p><p>$m_j$代表用户 $j$ 评过分的电影的总数</p></blockquote><h5 id="基于内容的推荐系统"><a href="#基于内容的推荐系统" class="headerlink" title="基于内容的推荐系统"></a>基于内容的推荐系统</h5><p>我们假设每部电影都有两个特征，如$x_1$代表电影的浪漫程度，$x_2$ 代表电影的动作程度。<br><img src="http://www.ai-start.com/ml2014/images/747c1fd6bff694c6034da1911aa3314b.png" alt="img"></p><p>则每部电影都有一个特征向量，如$x^{(1)}$是第一部电影的特征向量为[0.9 0]。</p><p>下面我们要基于这些特征来构建一个推荐系统算法。<br>假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如$\theta^{(1)}$是第一个用户的模型的参数。<br>于是，我们有：</p><p>$\theta^{(j)}$用户$j$的参数向量</p><p>$x^{(i)}$电影$i$的特征向量</p><p>对于用户$j$和电影$i$，我们预测评分为:$(\theta^{(j)})^T x^{(i)}$</p><p>代价函数</p><p>针对用户$j$，该线性回归模型的代价为预测误差的平方和，加上正则化项：<br>$$<br>\min_{\theta (j)}\frac{1}{2}\sum_{i:r(i,j)&#x3D;1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\left(\theta_{k}^{(j)}\right)^2<br>$$</p><p>其中 $i:r(i,j)$表示我们只计算那些用户 $j$ 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以$1&#x2F;2m$，在这里我们将$m$去掉。并且我们不对方差项$\theta_0$进行正则化处理。</p><p>上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：<br>$$<br>\min_{\theta^{(1)},…,\theta^{(n_u)}} \frac{1}{2}\sum_{j&#x3D;1}^{n_u}\sum_{i:r(i,j)&#x3D;1}\left((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\right)^2+\frac{\lambda}{2}\sum_{j&#x3D;1}^{n_u}\sum_{k&#x3D;1}^{n}(\theta_k^{(j)})^2<br>$$<br>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：</p><p>$$<br>\theta_k^{(j)}:&#x3D;\theta_k^{(j)}-\alpha\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)} \quad (\text{for} , k &#x3D; 0)<br>$$</p><p>$$<br>\theta_k^{(j)}:&#x3D;\theta_k^{(j)}-\alpha\left(\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda\theta_k^{(j)}\right) \quad (\text{for} , k\neq 0)<br>$$</p><h5 id="协同过滤："><a href="#协同过滤：" class="headerlink" title="协同过滤："></a>协同过滤：</h5><p>这两</p><p>我们的优化目标便改为同时针对$x$和$\theta$进行。<br>$$<br>J(x^{(1)},…x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})&#x3D;\frac{1}{2}\sum_{(i:j):r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i&#x3D;1}^{n_m}\sum_{k&#x3D;1}^{n}(x_k^{(j)})^2+\frac{\lambda}{2}\sum_{j&#x3D;1}^{n_u}\sum_{k&#x3D;1}^{n}(\theta_k^{(j)})^2<br>$$</p><p>对代价函数求偏导数的结果如下：</p><p>$$<br>x_k^{(i)}:&#x3D;x_k^{(i)}-\alpha\left(\sum_{j:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\theta_k^{j}+\lambda x_k^{(i)}\right)<br>$$</p><p>$$<br>\theta_k^{(i)}:&#x3D;\theta_k^{(i)}-\alpha\left(\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x_k^{(i)}+\lambda \theta_k^{(j)}\right)<br>$$</p><p>注：在协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。<br>协同过滤算法使用步骤如下：</p><ol><li><p>初始 $x^{(1)},x^{(1)},…x^{(nm)},\ \theta^{(1)},\theta^{(2)},…,\theta^{(n_u)}$为一些随机小值</p></li><li><p>使用梯度下降算法最小化代价函数</p></li><li><p>在训练完算法后，我们预测$(\theta^{(j)})^Tx^{(i)}$为用户 $j$ 给电影 $i$ 的评分</p></li></ol><p>协同过滤优化目标：</p><p>给定$x^{(1)},…,x^{(n_m)}$，估计$\theta^{(1)},…,\theta^{(n_u)}$：<br>$$<br>\min_{\theta^{(1)},…,\theta^{(n_u)}}\frac{1}{2}\sum_{j&#x3D;1}^{n_u}\sum_{i:r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{j&#x3D;1}^{n_u}\sum_{k&#x3D;1}^{n}(\theta_k^{(j)})^2<br>$$</p><p>给定$\theta^{(1)},…,\theta^{(n_u)}$，估计$x^{(1)},…,x^{(n_m)}$：</p><p>同时最小化$x^{(1)},…,x^{(n_m)}$和$\theta^{(1)},…,\theta^{(n_u)}$：<br>$$<br>J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})&#x3D;\frac{1}{2}\sum_{(i,j):r(i,j)&#x3D;1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\frac{\lambda}{2}\sum_{i&#x3D;1}^{n_m}\sum_{k&#x3D;1}^{n}(x_k^{(i)})^2+\frac{\lambda}{2}\sum_{j&#x3D;1}^{n_u}\sum_{k&#x3D;1}^{n}(\theta_k^{(j)})^2<br>$$</p><p>$$<br>\min_{x^{(1)},…,x^{(n_m)} \\ \theta^{(1)},…,\theta^{(n_u)}}J(x^{(1)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})<br>$$</p><h5 id="协同过滤算法的向量化实现"><a href="#协同过滤算法的向量化实现" class="headerlink" title="协同过滤算法的向量化实现"></a>协同过滤算法的向量化实现</h5><p>我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。</p><p>我们有五部电影，以及四位用户，那么 这个矩阵 $Y$ 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里：</p><table><thead><tr><th><strong>Movie</strong></th><th><strong>Alice (1)</strong></th><th><strong>Bob (2)</strong></th><th><strong>Carol (3)</strong></th><th><strong>Dave (4)</strong></th></tr></thead><tbody><tr><td>Love at last</td><td>5</td><td>5</td><td>0</td><td>0</td></tr><tr><td>Romance forever</td><td>5</td><td>?</td><td>?</td><td>0</td></tr><tr><td>Cute puppies of love</td><td>?</td><td>4</td><td>0</td><td>?</td></tr><tr><td>Nonstop car chases</td><td>0</td><td>0</td><td>5</td><td>4</td></tr><tr><td>Swords vs. karate</td><td>0</td><td>0</td><td>5</td><td>?</td></tr></tbody></table><p><img src="E:\geek\Coursera-ML-AndrewNg-Notes-master\Coursera-ML-AndrewNg-Notes-master\images\42a92e07b32b593bb826f8f6bc4d9eb3.png"></p><p>推出评分：</p><p><img src="http://www.ai-start.com/ml2014/images/c905a6f02e201a4767d869b3791e8aeb.png" alt="img"></p><p>找到相关影片：</p><p><img src="http://www.ai-start.com/ml2014/images/0a8b49da1ab852f2996a02afcaca2322.png" alt="img"></p><p>现在既然你已经对特征参数向量进行了学习，那么我们就会有一个很方便的方法来度量两部电影之间的相似性。例如说：电影 $i$ 有一个特征向量$x^{(i)}$，你是否能找到一部不同的电影 $j$，保证两部电影的特征向量之间的距离$x^{(i)}$和$x^{(j)}$很小，那就能很有力地表明电影$i$和电影 $j$ 在某种程度上有相似，至少在某种意义上，某些人喜欢电影 $i$，或许更有可能也对电影 $j$ 感兴趣。总结一下，当用户在看某部电影 $i$ 的时候，如果你想找5部与电影非常相似的电影，为了能给用户推荐5部新电影，你需要做的是找出电影 $j$，在这些不同的电影中与我们要找的电影 $i$ 的距离最小，这样你就能给你的用户推荐几部不同的电影了。</p><h5 id="均值归一化"><a href="#均值归一化" class="headerlink" title="均值归一化"></a>均值归一化</h5><p>让我们来看下面的用户评分数据：</p><p><img src="http://www.ai-start.com/ml2014/images/54b1f7c3131aed24f9834d62a6835642.png" alt="img"></p><p>如果我们新增一个用户 <strong>Eve</strong>，并且 <strong>Eve</strong> 没有为任何电影评分，那么我们以什么为依据为<strong>Eve</strong>推荐电影呢？</p><p>我们首先需要对结果 $Y $矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值：</p><p><img src="http://www.ai-start.com/ml2014/images/9ec5cb55e14bd1462183e104f8e02b80.png" alt="img"></p><p>然后我们利用这个新的 $Y$ 矩阵来训练算法。<br>如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测$(\theta^{(j)})^T x^{(i)}+\mu_i$，对于<strong>Eve</strong>，我们的新模型会认为她给每部电影的评分都是该电影的平均分。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>级数的若干反例</title>
    <link href="/2021/20210710/"/>
    <url>/2021/20210710/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（一）$\sum _{n&#x3D;1}^{\infty}a_n$收敛，但是$\sum _{n&#x3D;1}^{\infty}{a_n}^3$发散</p><p>例子：<br>$$<br>\sum _{n&#x3D;1}^{\infty}a_n&#x3D;1-1+\frac{1}{\sqrt[3]{2}}-\frac{1}{2\sqrt[3]{2}}-\frac{1}{2\sqrt[3]{2}}\<br>+…+\frac{1}{\sqrt[3]{k}}-\frac{1}{k\sqrt[3]{k}}(减去k个)-…<br>$$</p><span id="more"></span><p>（二）$\sum _{n&#x3D;1}^{\infty}a_n$收敛，但是$a_n \neq o(\frac{1}{n})$</p><p>例子：<br>$$<br>\sum _{n&#x3D;1}^{\infty}a_n&#x3D;1+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{4}+\<br>+\frac{1}{5^2}+\frac{1}{6^2}+\frac{1}{7^2}+\frac{1}{8^2}+\frac{1}{9}…<br>\<br>（即对k^2项改写，将\frac{1}{n^2}换成\frac{1}{n}）<br>$$<br>（三）$a_n &#x3D; o(\frac{1}{n})$，但是$\sum _{n&#x3D;1}^{\infty}a_n$不收敛<br>$$<br>a_n&#x3D;\frac{1}{nlnn}<br>$$</p><p>（四）$\sum _{n&#x3D;1}^{\infty}a_n+b_n$收敛，但是$\sum _{n&#x3D;1}^{\infty}a_n$、$\sum _{n&#x3D;1}^{\infty}a_n$均不收敛。</p><p>例子：<br>$$<br>a_n&#x3D;(-1)^n\ \ ,\ \  b_n&#x3D;(-1)^{n+1}<br>$$<br>（五）$\sum _{n&#x3D;1}^{\infty}a _{2n-1}+a _{2n}$收敛，但是$\sum _{n&#x3D;1}^{\infty}u_n$不收敛。</p><p>例子：<br>$$<br>a_n&#x3D;(-1)^n\ \ ,\ \  b_n&#x3D;(-1)^{n+1}<br>$$<br>（六）$\lim _{n\ \rightarrow \ \infty \ }\ na_n&#x3D;0$,但是$\sum _{n&#x3D;1}^{\infty}a _{2n-1}+a _{2n}$收敛。</p><p>例子：<br>$$<br>a_n&#x3D;\frac{1}{nlnn}<br>$$<br>（七）$\sum _{n&#x3D;1}^{\infty}a_n$为发散的交错级数，但是$\lim _{n\ \rightarrow \ \infty \ }\ a_n&#x3D;0$</p><p>例子：<br>$$<br>\sum _{n&#x3D;1}^{\infty}a_n&#x3D;1-\frac{1}{2}+\frac{1}{3^2}-\frac{1}{5^2}\<br>+…+\frac{1}{(2n-1)^2}-\frac{1}{2n}+…<br>$$<br>（八）$\sum _{n&#x3D;1}^{\infty}a_n$为发散的交错级数，但是$|a_n|&gt;|a _n+1|$</p><p>例子：<br>$$<br>\sum _{n&#x3D;1}^{\infty}a_n&#x3D;(-1)^n(1+\frac{1}{n})<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>巧用对称化三重积分为三次</title>
    <link href="/2021/20210708/"/>
    <url>/2021/20210708/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>$f(x)$ 在 $0\leq x\leq1$上连续。<br>证明:<br>$\int _0^1\ dx\int _x^1\ dy\int _x^y\ f\left(x\right)f\left(y\right)f\left(z\right)dz&#x3D;\frac{1}{3!}\left(\int _0^1\ f\left(t\right)dt\right)^3$</p><span id="more"></span><hr><p>证明如下：</p><p>$\left(\int_0^1 f(x)&gt;dx\right)^3&#x3D;6\int_{0\leq x\leq z\leq y\leq1}f(x)f(y)f(z)d(x,y,z)&#x3D;6\int_0^1\int_x^1\int_x^y f(x)f(y)f(z)dzdydx$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之降维算法</title>
    <link href="/2021/20210614/"/>
    <url>/2021/20210614/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>降维可以压缩数据，因而使用较少的计算机内存或磁盘空间，加快我们的学习算法。</p><p>在数据可视化中，降维也是常用的手段。</p><span id="more"></span><h5 id="PCA算法（Principal-Component-Analysis）"><a href="#PCA算法（Principal-Component-Analysis）" class="headerlink" title="PCA算法（Principal Component Analysis）"></a>PCA算法（Principal Component Analysis）</h5><p>在<strong>PCA</strong>中，我们要做的是找到一个方向向量（<strong>Vector direction</strong>），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p><p><img src="http://www.ai-start.com/ml2014/images/a93213474b35ce393320428996aeecd9.jpg" alt="img"></p><h5 id="主成分分析与线性回归的比较"><a href="#主成分分析与线性回归的比较" class="headerlink" title="主成分分析与线性回归的比较"></a>主成分分析与线性回归的比较</h5><p>尽管主成分分析与线性回归看上去很相似，但两者是两种不同的算法。主成分分析最小化的是投射误差（<strong>Projected Error</strong>），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p><p><img src="http://www.ai-start.com/ml2014/images/7e1389918ab9358d1432d20ed20f8142.png" alt="img"></p><p>上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。</p><h5 id="具体算法："><a href="#具体算法：" class="headerlink" title="具体算法："></a>具体算法：</h5><p><strong>PCA</strong> 减少$n$维到$k$维：</p><p>第一步是<strong>数据预处理</strong>。我们将进行均值归一化(mean normalization)&#x2F;特征缩放(feature scaling)。普遍的做法是计算出所有特征的均值，然后令 $x_j&#x3D; x_j-μ_j$。如果特征是在不同的数量级上，我们还需要将其除以标准差 $σ$。</p><p>第二步是计算<strong>协方差矩阵</strong>（<strong>covariance matrix</strong>）<br>$$<br>Σ：<br>\sum&#x3D;\dfrac {1}{m}\sum^{n}_{i&#x3D;1}( x^{(i)}) ( x^{(i)}) ^{T}<br>$$</p><p>第三步是计算协方差矩阵$Σ$的<strong>特征向量</strong>（<strong>eigenvectors</strong>）:</p><p>在 <strong>Octave</strong> 里我们可以利用<strong>奇异值分解</strong>（<strong>singular value decomposition</strong>）来求解，<code>[U, S, V]= svd(sigma)</code>。</p><p>对于一个 $n×n$维度的矩阵，上式中的$U$是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从$n$维降至$k$维，我们只需要从$U$中选取前$k$个向量，获得一个$n×k$维度的矩阵，我们用$U_{reduce}$表示，然后通过如下计算获得要求的新特征向量<br>$$<br>z^{(i)}:<br>z^{(i)}&#x3D;U^T_{reduce}*x^{(i)}<br>$$</p><p>其中$x$是$n×1$维的，因此结果为$k×1$维度。</p><h5 id="选择主成分的数量"><a href="#选择主成分的数量" class="headerlink" title="选择主成分的数量"></a>选择主成分的数量</h5><p>主要成分分析是减少投射的平均均方误差：</p><p>训练集的方差为：$\dfrac {1}{m}\sum^{m}_{i&#x3D;1}\left| x^{( i) }\right| ^{2}$</p><p>我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的$k$值。</p><p>如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。</p><p>我们可以先令$k&#x3D;1$，然后进行主要成分分析，获得$U_{reduce}$和$z$，然后计算比例是否小于1%。如果不是的话再令$k&#x3D;2$，如此类推，直到找到可以使得比例小于1%的最小$k$ 值（原因是各个特征之间通常情况存在某种相关性）。</p><p>还有一些更好的方式来选择$k$，当我们在<strong>Octave</strong>中调用“<strong>svd</strong>”函数的时候，我们获得三个参数：<code>[U, S, V] = svd(sigma)</code>。</p><p>其中的$S$是一个$n×n$的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：<br>$$<br>\dfrac {\dfrac {1}{m}\sum^{m}<em>{i&#x3D;1}\left| x^{( i) }-x^{( i) }</em>{approx}\right| ^{2}}{\dfrac {1}{m}\sum^{m}<em>{i&#x3D;1}\left| x^{(i)}\right| ^{2}}&#x3D;1-\dfrac {\Sigma^{k}</em>{i&#x3D;1}S_{ii}}{\Sigma^{m}<em>{i&#x3D;1}S</em>{ii}}\leq 1%<br>$$<br>也就是：<br>$$<br>\frac {\Sigma^{k}<em>{i&#x3D;1}s</em>{ii}}{\Sigma^{n}<em>{i&#x3D;1}s</em>{ii}}\geq0.99<br>$$<br>在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：<br>$$<br>x^{( i) }<em>{approx}&#x3D;U</em>{reduce}z^{(i)}<br>$$</p><h5 id="压缩重现"><a href="#压缩重现" class="headerlink" title="压缩重现"></a>压缩重现</h5><p><strong>PCA</strong>算法，我们可能有一个这样的样本。如图中样本$x^{(1)}$,$x^{(2)}$。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如$z^{(1)}$，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点$z^{(1)}$，我们怎么能回去这个原始的二维空间呢？$x$为2维，$z$为1维，$z&#x3D;U^{T}<em>{reduce}x$，相反的方程为：<br>$$<br>x</em>{appox}&#x3D;U_{reduce}\cdot z\ ,\ x_{appox}\approx x<br>$$</p><h5 id="几个错误使用"><a href="#几个错误使用" class="headerlink" title="几个错误使用"></a>几个错误使用</h5><p>（一）用来减少过拟合</p><p>这样做非常不好，效果也并不优于正则化处理。PCA只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。</p><p>(二)滥用PCA</p><p>我们经常不管三七二十一就将PCA算法归入我们的学习过程的一部分。吴恩达给出的建议是：当我们在使用PCA时要考虑不用PCA是否能达到我们的目标，只有做不到的时候采取考虑PCA算法。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>京都大学 2021 年入学試験問題（理系数学）部分解答</title>
    <link href="/2021/20210612/"/>
    <url>/2021/20210612/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>京都大学 2021 年入学试题略解：</p><span id="more"></span><p>1.略</p><p>2.性质：抛物线上的一点处的切线与该店的搅拌机的过相应焦点的垂线的交点的轨迹为抛物线准线。</p><p>3.解：根据欧拉公式：<br>$$<br>\sum_{n&#x3D;0}^\infty \frac{1}{2^n}\cos (\frac{n\pi}{6})\<br>&#x3D;Re(\sum_{n&#x3D;0} \left(\frac{e^\frac{i\pi}{6}}{2}\right)^n)\<br>&#x3D;Re(\lim_{n\to\infty}\frac{1-(\frac{e^{\frac{i\pi}{6}}}{2})^n}{1-\frac{e^{\frac{i\pi}{6}}}{2}})\<br>&#x3D;Re(\lim_{n\to\infty}\frac{1}{1-\frac{e^{\frac{i\pi}{6}}}{2}})\<br>&#x3D;\frac{1}{13}(14+3\sqrt{3})<br>$$<br>4.略。</p><ol start="5"><li></li></ol><p>(1)略</p><p>(2)利用性质O为△ABC的外心，H为垂心，求证：$\overrightarrow{OH}&#x3D;\overrightarrow{OA}+\overrightarrow{OB}+\overrightarrow{OC}$．(见《奥数教程》七（或者八）年级版)</p><ol start="6"><li></li></ol><p>(1)法一：</p><p>若n不是质数，设$n&#x3D;ab$，其中$a、b＞1$。则$3^a-2^a|3^n-2^n$.故$3^n-2^n$是合数，矛盾。</p><p>法二：</p><p>利用西格蒙德（Zsigmondy）定理可进一步证明:</p><p>若$a^n-b^n$为素数的幂，其中$n&gt;2，a、b&gt;1$，且$gcd(a,b)&#x3D;1$，则n也为素数。</p><p>(2)Hint:对a求导</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>世另我（一）</title>
    <link href="/2021/me/"/>
    <url>/2021/me/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉, 这个密码看着不太对, 请再试试." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容.">  <script id="hbeData" type="hbeData" data-hmacdigest="cbadec7fb45b0797fcefa4d4f7e7dca082e7789ba9f50b91ceef1c7788277829">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好, 这里需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2021新高考一卷压轴题</title>
    <link href="/2021/20210607/"/>
    <url>/2021/20210607/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>题目：<br>$$<br>f(x)&#x3D;x(1-lnx) \<br>(1)求单调性\<br>(2)a不等于b，且blna-alnb&#x3D;a-b\<br>证明：2&lt;\frac{1}{a}+\frac{1}{b}&lt;e<br>$$</p><span id="more"></span><p>（1）略</p><p>（2）</p><p>易证$\frac{1}{a}$和$\frac{1}{b}$是$f(x)$与$y&#x3D;n$的两个两个公共点的横坐标，记作$x_1、x_2$,易知,$0&lt;x_1&lt;1&lt;x_2&lt;e$.</p><p>左式解法一：（对称化构造）</p><p>令$h(x)&#x3D;f(2-x)-f(x)$，求导得其在$0&lt;x&lt;1$时单调递减，$f(2-x)&gt;f(x)$</p><p>化简可得所证。<br>左式解法二：（放缩）</p><p>设<br>$$<br>\begin{align}<br>x_1+a&#x3D;x_1lnx_1&gt;x_1*(\frac{1}{2}*(x_1-\frac{1}{x_1}))\\<br>x_2+a&#x3D;x_2lnx_2&lt;x_2*(\frac{1}{2}*(x_2-\frac{1}{x_2}))<br>\end{align}<br>$$<br>记$h(x)&#x3D;\frac{x^2}{2}-x-\frac{1}{2}$，则由上两式子得：$h(x_2)&gt;h(x_1)$.</p><p>注意到，$h(x)$是对称轴为x&#x3D;1的二次函数，故得证。</p><p>右式解法一：</p><p>设$h(x)&#x3D;\frac{1-lnx}{e-x}.$<br>故$h’(x)&#x3D;-\frac{-2x+xlnx+e}{x(e-x)^2}&lt;0$<br>$\frac{1-lnx_1}{e-x_1}&gt;\frac{1-lnx_2}{e-x_2}$<br>$\frac{x_1(1-lnx_1)}{x_1(e-x_1)}&gt;\frac{x_2(1-lnx_2)}{x_2(e-x_2)}$<br>$\frac{1}{x_1(e-x_1)}&gt;\frac{1}{x_2(e-x_2)}$<br>$(x_1-x_2)(x_1+x_2-e)&gt;0$</p><p>右式解法二：<br>$$<br>x_1&lt;x_1(1-lnx_1)&#x3D;x_2(1-lnx_2)&#x3D;x_2ln\frac{e}{x_2}\leq x_2(\frac{e}{x_2}-1)&#x3D;e-x_2<br>$$<br>得证。</p><p>右式解法三：(长风数学)</p><p>$x_1&lt;x_1(1-lnx_1)&#x3D;x_2(1-lnx_2)$</p><p>$x_1+x_2&lt;x_2(1-lnx_2)+x_2&lt;e$</p><p>右式解法四：（对称化构造）</p><p>略。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>高考</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之聚类算法</title>
    <link href="/2021/20210606/"/>
    <url>/2021/20210606/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签。聚类算法(<strong>Clustering</strong>）是一种非监督学习算法。</p><span id="more"></span><h5 id="K-means算法"><a href="#K-means算法" class="headerlink" title="K-means算法"></a>K-means算法</h5><p><strong>K-means</strong>是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</p><p><strong>K-means</strong>是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为:</p><p>首先选择$K$个随机的点，称为<strong>聚类中心</strong>（<strong>cluster centroids</strong>）；</p><p>对于数据集中的每一个数据，按照距离$K$个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。</p><p>计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。</p><p>重复步骤2-4直至中心点不再变化。</p><p><strong>K-均值</strong>算法的伪代码如下：</p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs pgsql">Repeat &#123;<br><br><span class="hljs-keyword">for</span> i = <span class="hljs-number">1</span> <span class="hljs-keyword">to</span> m<br><br>c(i) := <span class="hljs-keyword">index</span> (form <span class="hljs-number">1</span> <span class="hljs-keyword">to</span> K) <span class="hljs-keyword">of</span> <span class="hljs-keyword">cluster</span> centroid closest <span class="hljs-keyword">to</span> x(i)<br><br><span class="hljs-keyword">for</span> k = <span class="hljs-number">1</span> <span class="hljs-keyword">to</span> K<br><br>μk := average (mean) <span class="hljs-keyword">of</span> points assigned <span class="hljs-keyword">to</span> <span class="hljs-keyword">cluster</span> k<br><br>&#125;<br></code></pre></td></tr></table></figure><p>算法分为两个步骤，第一个<strong>for</strong>循环是赋值步骤，即：对于每一个样例$i$，计算其应该属于的类。第二个<strong>for</strong>循环是聚类中心的移动，即：对于每一个类$K$，重新计算该类的质心。</p><h5 id="K-means-的代价函数"><a href="#K-means-的代价函数" class="headerlink" title="K-means 的代价函数"></a>K-means 的代价函数</h5><p>K-means的代价函数（又称<strong>畸变函数</strong> <strong>Distortion function</strong>）为：<br>$$<br>J(c^{(1)},…,c^{(m)},μ_1,…,μ_K)&#x3D;\frac {1}{m}\sum^{m}<em>{i&#x3D;1}\left| X^{\left( i\right) }-\mu</em>{c^{(i)}}\right| ^2<br>$$</p><p>其中$\mu _c^{(i)}$代表与$x^{(i)}$最近的聚类中心点。</p><p>回顾刚才给出的:<br><strong>K-均值</strong>迭代算法，我们知道，第一个循环是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小$\mu _i$引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p><h5 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h5><p>为了避免使用K-means时出现陷入局部最优，我们通常需要多次（一般为50~1000次）运行<strong>K-均值</strong>算法，每一次都重新进行随机初始化，最后再比较多次运行<strong>K-均值</strong>的结果，选择代价函数最小的结果。这种方法在$K$较小的时候（2–10）还是可行的，但是如果$K$较大，这么做也可能不会有明显地改善。</p><h5 id="选择聚类数"><a href="#选择聚类数" class="headerlink" title="选择聚类数"></a>选择聚类数</h5><p>没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。</p><p>以下是一个可能会谈及的方法。</p><blockquote><p>肘部法则：</p><p>我们所需要做的是改变$K$值，也就是聚类类别数目的总数。我们用一个聚类来运行<strong>K均值</strong>聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数$J$。$K$代表聚类数字。</p><p><img src="https://i0.hdslb.com/bfs/album/ebba8111b6d86e6e594e5f78aa5faf5677c12ba1.png"></p><p>我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，$K&#x3D;3$之后就下降得很慢，那么我们就选$K&#x3D;3$。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。</p><p>但是通常我们得到的是右图，很难看出”肘部“的位置。所以“肘部法则”并不是一个非常常用的法则。</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之支持向量机</title>
    <link href="/2021/20210530/"/>
    <url>/2021/20210530/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>支持向量机（support vector machines,SVM)是一种二类分类模型。</p><span id="more"></span><h5 id="代价函数："><a href="#代价函数：" class="headerlink" title="代价函数："></a>代价函数：</h5><p>我们回忆一下逻辑回归。其代价函数如下：</p><p><img src="http://www.ai-start.com/ml2014/images/66facb7fa8eddc3a860e420588c981d5.png" alt="img"></p><p>我们要实现我们的目标，就要把$-\log(1-\frac{1}{1+e^{-z}})$一点一点修改，修改为下两图的紫色直线函数，左边的函数，称之为${\cos}t_1{(z)}$，同时，右边函数称它为${\cos}t_0{(z)}$。</p><p>最后SVM代价函数变为：</p><p><img src="http://www.ai-start.com/ml2014/images/5a63e35db410fdb57c76de97ea888278.png" alt="img"></p><p>最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数$\theta $时，支持向量机所做的是它来直接预测$y$的值等于1，还是等于0。因此，这个假设函数会预测1。当$\theta^Tx$大于或者等于0时，或者等于0时，所以学习参数$\theta $就是支持向量机假设函数的形式。</p><h5 id="大间距分类器"><a href="#大间距分类器" class="headerlink" title="大间距分类器:"></a>大间距分类器:</h5><p>支持向量机的目标是找到一个能正确划分数据集且间隔最大的超平面。</p><p><img src="http://www.ai-start.com/ml2014/images/e68e6ca3275f433330a7981971eb4f16.png" alt="img"></p><p>如该图所示，黑线比红线有着更大的距离。</p><h5 id="核函数："><a href="#核函数：" class="headerlink" title="核函数："></a>核函数：</h5><p>核函数可以看成是判断近似程度的函数。</p><p><img src="http://www.ai-start.com/ml2014/images/529b6dbc07c9f39f5266bd0b3f628545.png" alt="img"></p><p>为了获得上图所示的判定边界，我们的模型可能是<br>$$<br>\theta _0+\theta _1x_1+\theta _2x_2+\theta _3x_1x_2+\theta _4x_1^2+\theta _5x_2^2+\cdots<br>$$<br>的形式。</p><p>我们可以用一系列的新的特征$f$来替换模型中的每一项。例如令：<br>$$<br>f_1&#x3D;x_1,f_2&#x3D;x_2,f_3&#x3D;x_1x_2,f_4&#x3D;x_1^2,f_5&#x3D;x_2^2<br>$$</p><p>…得到:<br>$$<br>h_θ(x)&#x3D;\theta _1f_1+\theta _2f_2+…+\theta _nf_n<br>$$<br>。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造$f_1,f_2,f_3$？我们可以利用核函数来计算出新的特征。</p><p>给定一个训练样本$x$，我们利用$x$的各个特征与我们预先选定的<strong>地标</strong>(<strong>landmarks</strong>)$l^{(1)},l^{(2)},l^{(3)}$的近似程度来选取新的特征$f_1,f_2,f_3$</p><p><img src="http://www.ai-start.com/ml2014/images/2516821097bda5dfaf0b94e55de851e0.png" alt="img"></p><p>例如：<br>$$<br>f_1&#x3D;similarity(x,l^{(1)})&#x3D;e(-\frac{\left| x-l^{(1)} \right|^2}{2\sigma^2})<br>\<br>其中:\left| x-l^{(1)}\right|^2&#x3D;\sum{_{j&#x3D;1}^{n}}{(x_j-l_j^{(1)})^2}<br>$$<br>上例中的$similarity(x,l^{(1)})$就是核函数，具体而言，这里是一个<strong>高斯核函数</strong>(<strong>Gaussian Kernel</strong>)。 <strong>注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。</strong></p><p>这些地标的作用是什么？如果一个训练样本$x$与地标$l$之间的距离近似于0，则新特征 $f$近似于$e^{-0}&#x3D;1$，如果训练样本$x$与地标$l$之间距离较远，则$f$近似于$e^{-(一个较大的数)}&#x3D;0$。</p><h5 id="支持向量机参数的影响："><a href="#支持向量机参数的影响：" class="headerlink" title="支持向量机参数的影响："></a>支持向量机参数的影响：</h5><p>两个参数$C$和$\sigma$的影响：</p><blockquote><p>$C&#x3D;1&#x2F;\lambda$</p><p>$C$ 较大时，相当于$\lambda$较小，可能会导致过拟合，高方差；</p><p>$C$ 较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差；</p><p>$\sigma$较大时，可能会导致低方差，高偏差；</p><p>$\sigma$较小时，可能会导致低偏差，高方差。</p></blockquote><p>另外，支持向量机也可以不使用核函数，不使用核函数又称为<strong>线性核函数</strong>(<strong>linear kernel</strong>)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而样本非常少的时候，可以采用这种不带核函数的支持向量机。</p><h5 id="一些核函数"><a href="#一些核函数" class="headerlink" title="一些核函数"></a>一些核函数</h5><p>除了上文提到的高斯核函数外还有：</p><p>多项式核函数（<strong>Polynomial Kerne</strong>l）</p><p>字符串核函数（<strong>String kernel</strong>）</p><p>卡方核函数（ <strong>chi-square kernel</strong>）</p><p>直方图交集核函数（<strong>histogram intersection kernel</strong>）</p><h5 id="一些软件库和好的软件包"><a href="#一些软件库和好的软件包" class="headerlink" title="一些软件库和好的软件包"></a>一些软件库和好的软件包</h5><p>吴恩达推荐：<strong>liblinear</strong>和<strong>libsvm</strong></p><h5 id="多类分类问题："><a href="#多类分类问题：" class="headerlink" title="多类分类问题："></a>多类分类问题：</h5><p>我们同样也可以训练个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。</p><p>尽管你不去写你自己的<strong>SVM</strong>的优化软件，但是你也需要做几件事：</p><p>1、是提出参数的选择。我们在之前的视频中讨论过误差&#x2F;方差在这方面的性质。</p><p>2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的<strong>SVM</strong>（支持向量机），这就意味这他使用了不带有核函数的<strong>SVM</strong>（支持向量机）</p><p><strong>一些普遍使用的准则：</strong></p><p>$n$为特征数，$m$为训练样本数。</p><p>(1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。</p><p>(2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。</p><p>(3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。</p><p>值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。</p>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>辨析依概率收敛和以概率一收敛</title>
    <link href="/2021/20210514/"/>
    <url>/2021/20210514/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>依概率收敛和以概率一收敛是常见的收敛定义。但不易分清。</p><span id="more"></span><p>往往其常见的定义比较含糊。</p><p>可以转换一下，变得更加清晰。</p><p>定义如下：</p><p>Xn是一个随机变量序列，X是随机变量，对于任意自然数ε&gt;0</p><p>都有：</p><p><strong>依概率收敛：</strong></p><p>$$<br>\lim_{n\to\infty}P({\omega:|X_n(\omega)-X_0(\omega)|\geqε})&#x3D;0<br>$$<br><strong>以概率一收敛（几乎处处收敛）：</strong></p><p>$$<br>\lim_{n\to\infty}P({\omega:\sup_{m&gt;n}|X_m(\omega)-X_0(\omega)|\geqε})&#x3D;0<br>$$</p><hr><p>我们以一个图示来更加清晰地说明，我们把差值$|x_n-x|$绘成n的函数。为简单起见，把序列画成了曲线。于是曲线表示一特定的序列$|x_n(\xi )-x(\xi )|$。依概率收敛表示对于特定的$n&gt;n_o$，仅有一小部分曲线的坐标超过ε(图(a))。当然，对于每一个$n&gt;n_0$，可能甚至没有一条曲线始终小于ε,另一方面，几乎处处收敛则要求大多数曲线对每个$n&gt;n_0$都低于ε(图(b))。</p><p><img src="https://ae01.alicdn.com/kf/U06ed9e75205341088b128eac35fd404bt.jpg" alt="1"></p><p>阅读材料：</p><p>1.<a href="http://www.themattsimpson.com/2013/08/22/im-almost-sure-its-probably-converging-wait-what/">Matt Simpson-I’m Almost Sure it’s Probably Converging… Wait, What?</a></p><p>2.Papoulis A,Pillai S.U.《概率、随机变量与随机过程》</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之神经网络</title>
    <link href="/2021/20210509/"/>
    <url>/2021/20210509/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p><p>所以我们引入神经网络。</p><span id="more"></span><h5 id="一些神经网络术语"><a href="#一些神经网络术语" class="headerlink" title="一些神经网络术语"></a>一些神经网络术语</h5><p><a href="https://imgtu.com/i/WypJ74"><img src="https://z3.ax1x.com/2021/07/23/WypJ74.jpg" alt="WypJ74.jpg"></a></p><p>上图为神经元，神经元处理信号后，通过激活函数处理以产生神经元的输出。（常用的有Sigmoid函数、阶跃函数等）</p><p>下图为一个3层的神经网络，第一层成为输入层（<strong>Input Layer</strong>），最后一层称为输出层（<strong>Output Layer</strong>），中间一层成为隐藏层（<strong>Hidden Layers</strong>）。我们为每一层都增加一个偏差单位（<strong>bias unit</strong>）：</p><p><img src="http://www.ai-start.com/ml2014/images/8293711e1d23414d0a03f6878f5a2d91.jpg" alt="img"></p><h5 id="逻辑函数的一些例子"><a href="#逻辑函数的一些例子" class="headerlink" title="逻辑函数的一些例子"></a>逻辑函数的一些例子</h5><p>下图的神经元（三个权重分别为-30，20，20）可以被视为作用同于逻辑与（<strong>AND</strong>）：</p><p><img src="http://www.ai-start.com/ml2014/images/57480b04956f1dc54ecfc64d68a6b357.jpg" alt="img"></p><p>下图的神经元（三个权重分别为-10，20，20）可以被视为作用等同于逻辑或（<strong>OR</strong>）：</p><p><img src="http://www.ai-start.com/ml2014/images/7527e61b1612dcf84dadbcf7a26a22fb.jpg" alt="img"></p><p>下图的神经元（两个权重分别为 10，-20）可以被视为作用等同于逻辑非（<strong>NOT</strong>）：</p><p><img src="http://www.ai-start.com/ml2014/images/1fd3017dfa554642a5e1805d6d2b1fa6.jpg" alt="img"></p><p><strong>XNOR</strong>：</p><p>我们可以利用神经元来组合成更为复杂的神经网络以实现更复杂的运算。例如我们要实现<strong>XNOR</strong> 功能（输入的两个值必须一样，均为1或均为0），即<br>$$<br>\text{XNOR}&#x3D;( \text{x}_1, \text{AND}, \text{x}_2 ), \text{OR} ( ( \text{NOT}, \text{x}_1)\text{AND} ( \text{NOT}, \text{x}_2))<br>$$<br>首先构造一个能表达$( \text{NOT}, x_1 ) AND ( \text{NOT}, x_2 )$部分的神经元：</p><p><img src="http://www.ai-start.com/ml2014/images/4c44e69a12b48efdff2fe92a0a698768.jpg" alt="img"></p><p>然后将表示 <strong>AND</strong> 的神经元和表示$( \text{NOT}, \text{x}_1 ) \text{AND} ( \text{NOT}, \text{x}_2 )$的神经元以及表示 OR 的神经元进行组合：</p><p><img src="http://www.ai-start.com/ml2014/images/432c906875baca78031bd337fe0c8682.jpg" alt="img"></p><h5 id="神经网络的一些定义"><a href="#神经网络的一些定义" class="headerlink" title="神经网络的一些定义"></a>神经网络的一些定义</h5><p>假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的<strong>neuron</strong>个数($S_l$表示输出层神经元个数)，$S_L$代表最后一层中处理单元的个数。</p><p>将神经网络的分类定义为两种情况：二类分类和多类分类，</p><p>二类分类：$S_L&#x3D;0, y&#x3D;0, or, 1$表示哪一类；</p><p>$K$类分类：$S_L&#x3D;k, y_i &#x3D; 1$表示分到第$i$类；$(k&gt;2)$</p><p><img src="http://www.ai-start.com/ml2014/images/8f7c28297fc9ed297f42942018441850.jpg" alt="img"></p><h5 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h5><p>记<br>$$<br>h_\theta(x)\in \mathbb{R}^{K}{({h_\theta}(x))}_{i}&#x3D;{i}^{th} \text{output}<br>$$</p><p>则：</p><p><a href="https://imgtu.com/i/WyKJb9"><img src="https://z3.ax1x.com/2021/07/23/WyKJb9.jpg" alt="WyKJb9.jpg"></a></p><p>$h_\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行<strong>regularization</strong>的<strong>bias</strong>项处理所有参数的平方和。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Wolfram Mathematica识别猫种</title>
    <link href="/2021/20210415/"/>
    <url>/2021/20210415/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>自Wolfram Mathematica 11.3起逐渐开始支持神经网络等功能，</p><span id="more"></span><p>相关函数有<a href="https://reference.wolfram.com/language/ref/NetModel.html">NetModel</a>等。</p><p>也提供了一系列强大的<a href="https://resources.wolframcloud.com/NeuralNetRepository/">神经网络库</a>，有音频分析、分类、数据生成、特征提取、图像处理、语言建模、</p><p>对象检测、回归、语义分段、语音识别等107个库，还是比较全的。</p><p>我们检测猫的种类的图片用的是：</p><img src="http://www.hereinuk.com/wp-content/uploads/2016/06/14654596596466.jpg" alt="1" style="zoom:25%;" /><p>分别使用了：</p><p><a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/8d890763-4d74-4f89-90bf-88ef7c60f439/">Inception V3 Trained on ImageNet Competition Data</a>:</p><p>该模型由Google Inc.（也称为GoogLeNet）于2015年发布，这个模型建立在之前的Inception V1之上，使用不到100 MB的参数将性能提高了15%。它比它的前辈有一些架构上的改进，例如大滤波器卷积和非对称卷积级的分解。</p><p><a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/EfficientNet-Trained-on-ImageNet-with-AdvProp-and-AutoAugment/">EfficientNet Trained on ImageNet with AdvProp and AutoAugment</a>:</p><p>该模型于2019年发布，利用EfficientNet架构上的AdvProp和AutoAugment数据增强技术有效地执行图像分类。</p><p><a href="https://resources.wolframcloud.com/NeuralNetRepository/resources/ResNet-101-Trained-on-ImageNet-Competition-Data/">ResNet-101 Trained on ImageNet Competition Data</a>:</p><p>微软亚洲研究院于2015年发布的ResNet架构(包括ResNet-50，ResNet-101和ResNet-152),在ImageNet和MS-COCO竞赛中获得了非常成功的成绩。这些模型中所利用的核心思想残余连接(residual connections)极大地改善了梯度流动，从而允许训练更深入的模型，包括数十层甚至数百层。</p><blockquote><p>In[1]&#x3D;NetModel[“Inception V3 Trained on ImageNet Competition Data”][a, “TopProbabilities”]</p><p>Out[1]&#x3D;{tabbycat-&gt;0.211156,Egyptiancat-&gt;0.154156,tigercat-&gt;0.135589,bathtub-&gt;0.040887,vat-&gt;0.0263624}</p></blockquote><blockquote><p>In[2]&#x3D; NetModel[“EfficientNet Trained on ImageNet with AdvProp and AutoAugment”][a, “TopProbabilities”]</p><p>Out[2]&#x3D;{Egyptiancat-&gt;0.419587,tabbycat-&gt;0.101588}</p></blockquote><blockquote><p>In[3]&#x3D;NetModel[“ResNet-101 Trained on ImageNet Competition Data”][a, “TopProbabilities”]</p><p>Out[3]&#x3D;{Egyptiancat-&gt;0.735536,tabbycat-&gt;0.0787424}</p></blockquote><p>根据以上结果，这只猫大概率是埃及猫（Egyptiancat），也有可能是虎斑猫（tabbycat）。其中Inception V3给出了最多的可能答案，虽然两种不是猫（？）。而EfficientNet和ResNet-101给出Egyptiancat的概率均远大于其他。【虽然我觉得猫的种类结果都不太对（？）】</p><p>PS:没有这次识别，我也不清楚猫竟然有这么多种种类，有点孤陋寡闻。</p>]]></content>
    
    
    
    <tags>
      
      <tag>机器学习</tag>
      
      <tag>mathematica</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习之回归算法&amp;异常检测算法</title>
    <link href="/2021/20210411/"/>
    <url>/2021/20210411/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h5 id="机器学习的定义"><a href="#机器学习的定义" class="headerlink" title="机器学习的定义"></a>机器学习的定义</h5><p>（<strong>Arthur Samuel</strong>）在进行特定编程的情况下，给予计算机学习能力的领域。</p><p>（<strong>Tom Mitchell</strong>）一个程序被认为能从经验（Experience，<strong>E</strong>)中学习，解决任务(Task,<strong>T</strong>)，达到性能度量值(preference，<strong>P</strong>），有了经验<strong>E</strong>后，经过<strong>P</strong>评判，程序在处理T时的性能有所提升。</p><p>监督学习：监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。</p><p>无监督学习：数据集中没有任何的标签，没有提前告知算法一些信息。</p><span id="more"></span><h5 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h5><p>相关符号:</p><blockquote><p>m代表训练集中实例的数量</p><p>x代表特征&#x2F;输入变量</p><p>y代表目标变量&#x2F;输出变量</p><p>(x,y)代表训练集中的实例</p><p>$(x^{(i)},y^{(i)})$代表第$i$个观察实例</p><p>h为<strong>hypothesis</strong>（假设）,假设函数，在这一节记为$h_{\theta \ }(x)&#x3D;\theta \ _0+\theta \ _1x$</p></blockquote><p>为了找到最好的回归直线，我们要使得代价函数<br>$$<br>J(\theta \ _0,\theta \ _1)&#x3D;\frac{1}{2m}\sum <em>{i&#x3D;1}^{\ m}(h</em>{\theta \ }(x^{(i)})-y^{(i)}\ )^2<br>$$<br>最小。（这是回归中常用的代价函数，故又被称为平方误差代价函数，square error  function）</p><p>梯度下降法是一个比较常用的方法。</p><h5 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h5><p>我们引入一系列新的注释：</p><p>$n$ 代表特征的数量</p><p>$x^{( i )}$代表第 $i$ 个训练实例，是特征矩阵中的第$i$行，是一个<strong>向量</strong>（<strong>vector</strong>）。</p><p>在多变量线性回归中，<br>$$<br>h_{\theta}( x )&#x3D;\theta^TX&#x3D;{\theta_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+…+{\theta_{n}}{x_{n}}<br>$$</p><p>代价函数：<br>$$<br>J( {\theta_{0}},{\theta_{1}}…{\theta_{n}} )&#x3D;\frac{1}{2m}\sum\limits_{i&#x3D;1}^{m}{( h_\theta (X^{( i )})-{y}^{( i )} )}^{2}<br>$$</p><p>当使用梯度下降法时：</p><blockquote><p> 在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。</p></blockquote><p><img src="http://www.ai-start.com/ml2014/images/b8167ff0926046e112acf789dba98057.png" alt="img"></p><blockquote><p> 最简单的方法是令：${x_n}&#x3D;\frac{x_n-\mu_n}{s_n}$，其中 ${\mu_n}$是平均值，${s_n}$是标准差(极差也行)。</p></blockquote><p>使用正规方程：</p><blockquote><p>假设我们的训练集特征矩阵为 $X$（包含了 ${X_{0}}&#x3D;1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量:<br>$$<br>\theta &#x3D;( X^T X )^{-1}{X^T}y<br>$$</p></blockquote><p>梯度下降与正规方程的比较：</p><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率$\alpha$</td><td>不需要</td></tr><tr><td>需要多次迭代</td><td>一次运算得出</td></tr><tr><td>当特征数量$n$大时也能较好适用</td><td>需要计算$(X^TX )^{-1}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O( n^{3} )$，通常来说当$n$小于10000 时还是可以接受的</td></tr><tr><td>适用于各种类型的模型</td><td>只适用于线性模型，不适合逻辑回归模型等其他模型梯度下降与正规方程的比较：</td></tr></tbody></table><p>若$(X^T X)^-1$不可逆的情况:<br>(1)首先，看特征值里是否有一些多余的特征，像这些${x_1}$和${x_2}$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。<br>(2)正规化方法<br>(3)直接使用伪逆(pseudoinverse）,有时被称为广义逆（Generalized inverse）。定义：A* B* A&#x3D;A，则B是A的广义逆矩阵。所有的矩阵都存在逆矩阵。若一矩阵存在逆矩阵，则其逆矩阵即为其唯一的广义逆矩阵。Matlab的伪逆函数是pinv(逆矩阵函数为inv），可以证明：使用伪逆，以下式子任成立<br>$$<br>\theta &#x3D;( X^T X )^{-1}{X^T}y<br>$$</p><h5 id="逻辑回归-Logistic-Regression"><a href="#逻辑回归-Logistic-Regression" class="headerlink" title="逻辑回归 (Logistic Regression)"></a>逻辑回归 (<strong>Logistic Regression</strong>)</h5><p>简单而言，即将数据分类划分为离散的值{0,1}。</p><p>我们将因变量(<strong>dependent variable</strong>)可能属于的两个类分别称为负向类（<strong>negative class</strong>）和正向类（<strong>positive class</strong>），则因变量y∈ {0,1}，其中 0 表示负向类，1 表示正向类。</p><p>我们引入一个新的模型，逻辑回归，该模型的输出变量范围始终在0和1之间。<br>逻辑回归模型的假设是： $h_\theta ( x )&#x3D;g(\theta^{T}X )$<br>其中：<br>$X$ 代表特征向量<br>$g$ 代表逻辑函数（<strong>logistic function</strong>)是一个常用的逻辑函数为<strong>S</strong>形函数（<strong>Sigmoid function</strong>），公式为： $g( z )&#x3D;\frac{1}{1+e^{-z}}$。</p><p>该函数的图像为：</p><p><img src="http://www.ai-start.com/ml2014/images/1073efb17b0d053b4f9218d4393246cc.jpg" alt="img"></p><p>$h_\theta ( x )$的作用是，对于给定的输入变量，根据选择的参数计算输出变量&#x3D;1的可能性（<strong>estimated probablity</strong>）即$h_\theta ( x )&#x3D;P( y&#x3D;1|x;\theta )$<br>例如，如果对于给定的$x$，通过已经确定的参数计算得出$h_\theta ( x )&#x3D;0.7$，则表示有70%的几率$y$为正向类，相应地$y$为负向类的几率为1-0.7&#x3D;0.3。</p><p><strong>代价函数：</strong></p><p>若按照原来的公式，我们得到的代价函数将是一个非凸函数（<strong>non-convexfunction</strong>）。</p><p><img src="http://www.ai-start.com/ml2014/images/8b94e47b7630ac2b0bcb10d204513810.jpg" alt="img"></p><p>这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。</p><p>故我们重新定义逻辑回归的代价函数为:<br>$$<br>J(\theta )&#x3D;\frac{1}{m}\sum \limits_{i&#x3D;1}^m Cost(h _\theta( {x}^{( i )} ),y^{( i )})<br>$$<br>其中<img src="http://www.ai-start.com/ml2014/images/54249cb51f0086fa6a805291bf2639f1.png" alt="img"></p><p>${h_\theta}( x )$与 $Cost( {h_\theta}( x ),y )$之间的关系如下图所示：</p><p><img src="http://www.ai-start.com/ml2014/images/ffa56adcc217800d71afdc3e0df88378.jpg" alt="img"><br>将构建的 $Cost( {h_\theta}( x ),y )$简化如下：<br>$Cost( {h_\theta}( x ),y )&#x3D;-y\times log( {h_\theta}( x ) )-(1-y)\times log( 1-{h_\theta}( x ) )$<br>带入代价函数得到：<br>$J( \theta  )&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}{[-y^{(i)}\log ( {h_\theta}( x^{(i)} ) )-( 1-y^{(i)} )\log ( 1-{h_\theta}( x^{(i)} ) )]}$<br>即：$J( \theta  )&#x3D;-\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}{[y^{(i)}\log ( {h_\theta}( x^{(i)} ) )+( 1-y^{(i)} )\log ( 1-{h_\theta}( x^{(i)} ) )]}$</p><p><strong>一对多的情况：</strong></p><p><img src="http://www.ai-start.com/ml2014/images/b72863ce7f85cd491e5b940924ef5a5f.png" alt="img"></p><p>我们只需分别多次如右上角的操作，可化归为分成两类的逻辑回归。</p><h5 id="正规化"><a href="#正规化" class="headerlink" title="正规化"></a>正规化</h5><p>我们已经学了几种算法，如最小二乘法（Ordinary Least Squares,OLS)。我们所学的算法不足之处是随着特征维度的增加会出现线性模型的过度拟合。</p><p><img src="http://www.ai-start.com/ml2014/images/72f84165fbf1753cd516e65d5e91c0d3.jpg" alt="img"></p><p>过拟合解决方案：</p><ol><li><p>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如<strong>PCA</strong>）</p></li><li><p>正则化。 保留所有的特征，但是减少参数的大小（<strong>magnitude</strong>）。</p></li></ol><p>正则化：</p><p>我们可以修改代价函数，增加几个惩罚项。比如将代价函数写成：<br>$$<br>J\left( \theta  \right)&#x3D;\frac{1}{2m}[\sum\limits_{i&#x3D;1}^{m}({h_\theta}(x^{(i)})-y^{(i)})^{2}+\lambda \sum\limits_{j&#x3D;1}^{n}{\theta_j^2}]<br>$$</p><p>其中$\lambda $又称为正则化参数（<strong>Regularization Parameter</strong>）。 注：根据惯例，我们不对${\theta_{0}}$ 进行惩罚。事实上其也被称为岭回归：（Ridge Regression），由俄罗斯科学家Tikhonov 提出的对OLS的改进。$\lambda \sum\limits_{j&#x3D;1}^{n}{\theta_j^2}$被称为L2惩罚项（L2 Penalty）。</p><blockquote><p> 附：Lasso回归：</p><p>与岭回归类似：</p></blockquote><p>$$<br>\lambda \sum\limits_{j&#x3D;1}^{n}{\theta_j^2}改为\lambda \sum\limits_{j&#x3D;1}^{n}{|\theta_j|}<br>$$</p><blockquote><p>相应地，$\lambda \sum\limits_{j&#x3D;1}^{n}{|\theta_j|}$被称为L1惩罚项。</p></blockquote><p>在这里，我们展示只讨论岭回归。</p><p>我们也利用正规方程来求解正则化线性回归模型：<img src="http://www.ai-start.com/ml2014/images/71d723ddb5863c943fcd4e6951114ee3.png" alt="img"></p><p>图中的矩阵尺寸为 （n+1）*(n+1)。</p><p>同样对于逻辑回归，我们也给代价函数增加一个正则化的表达式，得到代价函数：</p><p>$$<br>J(\theta)&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^{m}{[-y^{(i)}\log (h_\theta( x^{(i)}) )-( 1-y^{(i)} )\log ( 1-{h_\theta}( x^{(i)}) )]}+\frac{\lambda }{2m}\sum\limits_{j&#x3D;1}^{n}\theta _j ^2<br>$$</p><h5 id="异常检测算法"><a href="#异常检测算法" class="headerlink" title="异常检测算法"></a>异常检测算法</h5><p>异常检测算法(<strong>Anomaly detection</strong>)主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。</p><p>正如其名，异常检测算法根据一堆无标签数据中的样本点检测另一个数据点是否异常。</p><p>异常检测算法用到了高斯分布。</p><p>算法流程：</p><p>我们可以利用已有的数据来预测总体中的$μ$和$σ^2$的计算方法如下：<br>$$<br>\mu&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^mx^{(i)}\<br>\sigma^2&#x3D;\frac{1}{m}\sum\limits_{i&#x3D;1}^m(x^{(i)}-\mu)^2<br>$$</p><p>一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：</p><p>$$<br>p(x)&#x3D;\prod\limits_{j&#x3D;1}^np(x_j;\mu_j,\sigma_j^2)&#x3D;\prod\limits_{j&#x3D;1}^1\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})<br>$$<br>当$p(x) &lt; \varepsilon$时，为异常。</p><p>下图是一个由两个特征的训练集，以及特征的分布情况：</p><p><img src="http://www.ai-start.com/ml2014/images/ba47767a11ba39a23898b9f1a5a57cc5.png" alt="img"></p><p>下面的三维图表表示的是密度估计函数，$z$轴为根据两个特征的值所估计$p(x)$值：</p><p><img src="http://www.ai-start.com/ml2014/images/82b90f56570c05966da116c3afe6fc91.jpg" alt="img"></p><p>接下来的具体评价方法如下：</p><ol><li>根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数</li><li>对交叉检验集，我们尝试使用不同的$\varepsilon$值作为阀值，并预测数据是否异常，根据$F1$值或者查准率与查全率的比例来选择 $\varepsilon$</li><li>选出 $\varepsilon$ 后，针对测试集进行预测，计算异常检验系统的$F1$值，或者查准率与查全率之比</li></ol><blockquote><p> F1值：</p><p>为了能够评价不同算法的优劣，在Precision和Recall的基础上提出了F1值的概念，来对Precision和Recall进行整体评价。F1的定义如下：</p><p>F1值 &#x3D; 正确率 * 召回率 * 2 &#x2F; (正确率 + 召回率)</p><p>$F_1&#x3D;{(\frac{recall^{-1}+precision^{-1}}{2})}^{-1}&#x3D;2* \frac{precision * recall}{precision+recall}$</p></blockquote><p>除此，还可以利用多元高斯分布</p><p>**多元高斯分布:</p><p>假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。</p><p>下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的<strong>X</strong>所代表的数据点很可能是异常值，但是其$p(x)$值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。</p><p><img src="http://www.ai-start.com/ml2014/images/598db991a7c930c9021cec5f6ab9beb9.png" alt="img"></p><p>在多元高斯分布模型中，我们将协方差矩阵形式的高斯分布函数，用所有的特征一起来计算 $p(x)$。</p><p>我们首先计算所有特征的平均值，然后再计算协方差矩阵：<br>$p(x)&#x3D;\prod_{j&#x3D;1}^np(x_j;\mu,\sigma_j^2)&#x3D;\prod_{j&#x3D;1}^n\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})$</p><p>$\mu&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^mx^{(i)}$</p><p>$\Sigma &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T&#x3D;\frac{1}{m}(X-\mu)^T(X-\mu)$</p><p>注:其中$\mu $ 是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的$p\left( x \right)$:<br>$p(x)&#x3D;\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^{\frac{1}{2}}}exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)$<br>其中：</p><p>$|\Sigma|$是定矩阵，在 <strong>Octave</strong> 中用 <code>det(sigma)</code>计算</p><p>$\Sigma^{-1}$ 是逆矩阵，下面我们来看看协方差矩阵是如何影响模型的：</p><p><img src="http://www.ai-start.com/ml2014/images/29df906704d254f18e92a63173dd51e7.jpg" alt="img"></p><p>上图是5个不同的模型，从左往右依次分析：</p><ol><li>是一个一般的高斯分布模型</li><li>通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差</li><li>通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差</li><li>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性</li><li>通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性</li></ol><p>多元高斯分布模型与原高斯分布模型的关系：</p><p>可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。</p><p>原高斯分布模型和多元高斯分布模型的比较：</p><table><thead><tr><th align="left">原高斯分布模型</th><th align="left">多元高斯分布模型</th></tr></thead><tbody><tr><td align="left">不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决</td><td align="left">自动捕捉特征之间的相关性</td></tr><tr><td align="left">计算代价低，能适应大规模的特征</td><td align="left">计算代价较高 训练集较小时也同样适用</td></tr><tr><td align="left"></td><td align="left">必须要有 ，不然的话协方差矩阵 不可逆的，通常需要  另外特征冗余也会导致协方差矩阵不可逆</td></tr></tbody></table><p>原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。</p><p>如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。</p><h5 id="异常检测与监督学习对比"><a href="#异常检测与监督学习对比" class="headerlink" title="异常检测与监督学习对比"></a>异常检测与监督学习对比</h5><p>两者比较：</p><table><thead><tr><th>异常检测</th><th>监督学习</th></tr></thead><tbody><tr><td>非常少量的正向类（异常数据 $y&#x3D;1$）, 大量的负向类（$y&#x3D;0$）</td><td>同时有大量的正向类和负向类</td></tr><tr><td>许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。</td><td>有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。</td></tr><tr><td>未来遇到的异常可能与已掌握的异常、非常的不同。</td><td></td></tr><tr><td>例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况</td><td>例如：邮件过滤器 天气预报 肿瘤分类</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习教程中文笔记——斯坦福大学2014（吴恩达）</title>
    <link href="/2021/20210410/"/>
    <url>/2021/20210410/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>吴恩达老师的机器学习课程，可以说是机器学习入门的第一课和最热门最经典的课程，在吴老师自己创办的coursera上可以观看。</p><p>以下是笔记部分：</p><span id="more"></span><p>相当一部分来自：<a href="https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes">https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes</a></p><p>目录:</p><ul><li><a href="https://spritmilk.github.io/2021/20210411/">回归</a></li></ul><p>一、 引言(<strong>Introduction</strong>) </p><p>1.1 欢迎 </p><p>1.2 机器学习是什么？ </p><p>1.3 监督学习 </p><p>1.4 无监督学习 </p><p>二、单变量线性回归**(Linear Regression with One Variable**) </p><p>2.1 模型表示 </p><p>2.2 代价函数 </p><p>2.3 代价函数的直观理解I </p><p>2.4 代价函数的直观理解II </p><p>2.5 梯度下降 </p><p>2.6 梯度下降的直观理解 </p><p>2.7 梯度下降的线性回归 </p><p>2.8 接下来的内容 </p><p>三、线性代数回顾(<strong>Linear Algebra Review</strong>) </p><p>3.1 矩阵和向量 </p><p>3.2 加法和标量乘法 </p><p>3.3 矩阵向量乘法 </p><p>3.4 矩阵乘法 </p><p>3.5 矩阵乘法的性质 </p><p>3.6 逆、转置</p><p>四、多变量线性回归(<strong>Linear Regression with Multiple Variables</strong>) </p><p>4.1 多维特征 </p><p>4.2 多变量梯度下降 </p><p>4.3 梯度下降法实践1-特征缩放 </p><p>4.4 梯度下降法实践2-学习率 </p><p>4.5 特征和多项式回归 </p><p>4.6 正规方程 </p><p>4.7 正规方程及不可逆性（选修） </p><p>五、Octave教程(<strong>Octave Tutorial</strong>) 【<strong>这部分无</strong>】</p><p>5.1 基本操作 </p><p>5.2 移动数据 </p><p>5.3 计算数据 </p><p>5.4 绘图数据 </p><p>5.5 控制语句：<strong>for</strong>，<strong>while</strong>，<strong>if</strong>语句 </p><p>5.6 向量化 88</p><p>5.7 工作和提交的编程练习 </p><p>六、逻辑回归(<strong>Logistic Regression</strong>) </p><p>6.1 分类问题 </p><p>6.2 假说表示 </p><p>6.3 判定边界 </p><p>6.4 代价函数 </p><p>6.5 简化的成本函数和梯度下降 </p><p>6.6 高级优化 </p><p>6.7 多类别分类：一对多 </p><p>七、正则化(<strong>Regularization</strong>) </p><p>7.1 过拟合的问题 </p><p>7.2 代价函数 </p><p>7.3 正则化线性回归 </p><p>7.4 正则化的逻辑回归模型 </p><ul><li><a href="https://spritmilk.github.io/2021/20210509/">神经网络</a></li></ul><p>第八、神经网络：表述(<strong>Neural Networks: Representation</strong>) </p><p>8.1 非线性假设 </p><p>8.2 神经元和大脑 </p><p>8.3 模型表示1 </p><p>8.4 模型表示2 </p><p>8.5 样本和直观理解1 </p><p>8.6 样本和直观理解II </p><p>8.7 多类分类 </p><p>九、神经网络的学习(<strong>Neural Networks: Learning</strong>) </p><p>9.1 代价函数 </p><p>9.2 反向传播算法 </p><p>9.3 反向传播算法的直观理解 </p><p>9.4 实现注意：展开参数 </p><p>9.5 梯度检验 </p><p>9.6 随机初始化 </p><p>9.7 综合起来 </p><p>9.8 自主驾驶 </p><ul><li><a href="https://spritmilk.github.io/2021/20210411/">机器学习之建议与应用</a></li></ul><p>十、应用机器学习的建议(<strong>Advice for Applying Machine Learning</strong>) </p><p>10.1 决定下一步做什么 </p><p>10.2 评估一个假设 </p><p>10.3 模型选择和交叉验证集 </p><p>10.4 诊断偏差和方差 </p><p>10.5 正则化和偏差&#x2F;方差 </p><p>10.6 学习曲线 </p><p>10.7 决定下一步做什么 </p><p>十一、机器学习系统的设计(<strong>Machine Learning System Design</strong>) </p><p>11.1 首先要做什么 </p><p>11.2 误差分析 </p><p>11.3 类偏斜的误差度量 </p><p>11.4 查准率和查全率之间的权衡 </p><p>11.5 机器学习的数据 </p><ul><li><a href="https://spritmilk.github.io/2021/20210530/">支持向量机</a></li></ul><p>十二、支持向量机(<strong>Support Vector Machines</strong>) </p><p>12.1 优化目标 </p><p>12.2 大边界的直观理解 </p><p>12.3 大边界分类背后的数学（选修） </p><p>12.4 核函数1 </p><p>12.5 核函数2 </p><p>12.6 使用支持向量机 </p><ul><li><a href="https://spritmilk.github.io/2021/20210614/">聚类</a></li></ul><p>十三、聚类(<strong>Clustering</strong>) </p><p>13.1 无监督学习：简介 </p><p>13.2 K-均值算法 </p><p>13.3 优化目标 </p><p>13.4 随机初始化</p><p>13.5 选择聚类数 </p><ul><li><a href="https://spritmilk.github.io/2021/20210614/">降维</a></li></ul><p>十四、降维(<strong>Dimensionality Reduction</strong>) </p><p>14.1 动机一：数据压缩 </p><p>14.2 动机二：数据可视化 </p><p>14.3 主成分分析问题 </p><p>14.4 主成分分析算法 </p><p>14.5 选择主成分的数量 </p><p>14.6 重建的压缩表示 </p><p>14.7 主成分分析法的应用建议 </p><ul><li><a href="https://spritmilk.github.io/2021/20210411/">异常检测</a></li></ul><p>十五、异常检测(<strong>Anomaly Detection</strong>) </p><p>15.1 问题的动机 </p><p>15.2 高斯分布 </p><p>15.3 算法 </p><p>15.4 开发和评价一个异常检测系统 </p><p>15.5 异常检测与监督学习对比 </p><p>15.6 选择特征 </p><p>15.7 多元高斯分布（选修） </p><p>15.8 使用多元高斯分布进行异常检测（选修） </p><ul><li><a href="https://spritmilk.github.io/2021/20210716/">推荐系统</a></li></ul><p>十六、推荐系统(<strong>Recommender Systems</strong>) </p><p>16.1 问题形式化 </p><p>16.2 基于内容的推荐系统 </p><p>16.3 协同过滤 </p><p>16.4 协同过滤算法 </p><p>16.5 向量化：低秩矩阵分解 </p><p>16.6 推行工作上的细节：均值归一化 </p><ul><li><a href="https://spritmilk.github.io/2021/20210411/">机器学习之建议与应用</a></li></ul><p>十七、大规模机器学习(<strong>Large Scale Machine Learning</strong>) </p><p>17.1 大型数据集的学习 </p><p>17.2 随机梯度下降法 </p><p>17.3 小批量梯度下降 </p><p>17.4 随机梯度下降收敛 </p><p>17.5 在线学习 </p><p>17.6 映射化简和数据并行 </p><p>十八、应用实例：图片文字识别(<strong>Application Example: Photo OCR</strong>) </p><p>18.1 问题描述和流程图</p><p>18.2 滑动窗口 </p><p>18.3 获取大量数据和人工数据 </p><p>18.4 上限分析：哪部分管道的接下去做 </p><p>十九、总结(<strong>Conclusion</strong>) </p><p>19.1 总结和致谢 </p><hr>]]></content>
    
    
    
    <tags>
      
      <tag>笔记</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>有趣的几道数院每日一题</title>
    <link href="/2021/20210405/"/>
    <url>/2021/20210405/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>今天的每日一题虽然难度不大，但比较有趣，事实上也无需大学的知识就能解决。</p><span id="more"></span><hr><p>数分：</p><blockquote><p>求极限：<br>$$<br>lim_{n\to \infty}sinsin….x.\ \ \ \ (n个sin)<br>$$</p></blockquote><p>几代：</p><blockquote><p>设f(x)是整系数多项式，若f(0)与f(1)均为奇数.证明：f(x)无整根。</p></blockquote><p>第一题：</p><p>事实上这个极限与$\frac{1}{\sqrt{n}}$同阶，更直接地说，这个极限与$\frac{\sqrt{3}}{\sqrt{n}}$等价。</p><p>详细可见论文： F. W. Hartmann.E2451(The Iterated Sine).The American Mathematical Monthly, Vol. 82, No. 1 (Jan., 1975), pp. 82-83</p><p>另解：</p><p>还可以利用：<br>$$<br>当x&gt;0时，sinx&lt;\frac{x}{\sqrt{1+\frac{x^2}{3}}}<br>$$</p><p>这个式子非常适合迭代。</p><p>第二题：</p><p>解法一：（反证法）</p><p>假设f(x)有整根n,则f(x)&#x3D;(x-n)g(x),g(x）也为整系数多项式,<br>因为f(0)&#x3D;-ng(0)为奇数,所以n为奇数,<br>又f(1)&#x3D;-(n-1)g(1)为奇数,所以n-1为奇数；所以,n-1、n都为奇数,矛盾.<br>所以,假设不成立,<br>所以,f(x)无整根.</p><p>解法二：</p><p>f(x)是整系数多项式，故f(n)只有整数之间加、减、乘运算。整数可分为奇数和偶数，且满足以下运算法则：</p><p>​奇数+奇数&#x3D;偶数</p><p>​奇数+偶数&#x3D;偶数</p><p>​偶数+偶数&#x3D;偶数</p><p>​奇数*奇数&#x3D;奇数</p><p>​奇数*偶数&#x3D;偶数</p><p>​偶数*偶数&#x3D;偶数</p><p>而且运算是封闭的。</p><p>f(0)为奇代表着f(n)为奇（n为偶数）</p><p>f(1)为奇代表着f(n)为奇（n为奇数）</p><p>故f(n)都为奇数，故不存在整根。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一些简单的串的模式匹配算法汇总</title>
    <link href="/2021/20210331/"/>
    <url>/2021/20210331/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>串的模式匹配算法在许多领域，包括爬虫、机器学习等都有重要应用。以下列了一些串的模式匹配算法：</p><span id="more"></span><hr><h3 id="1-Brute-Force算法（暴力算法）"><a href="#1-Brute-Force算法（暴力算法）" class="headerlink" title="1.Brute-Force算法（暴力算法）"></a><strong>1.Brute-Force算法（暴力算法）</strong></h3><p>正如其名，非常直接且简单。</p><p>首先将匹配串和模式串左对齐，然后从左向右一个一个进行比较，如果不成功则模式串向右移动一个单位。</p><p>例：</p><p>设置两个指针，一个匹配串指针i，一个模式串指针j</p><p>一开始i&#x3D;1,j&#x3D;1</p><p>第一个不符合，i++</p><p>…当i&#x3D;3时，</p><p>匹配串 ：ab<strong>cbc</strong>sdxzcxx</p><p>模式串：<strong>cbc</strong>ac</p><p>符合i++,j++，依旧符合i++,j++，直到i&#x3D;6,j&#x3D;4，不符合，i回退到3+1，j回退到1</p><p>…</p><p>该算法的缺陷是匹配串指针i会不断地回退，使时间复杂度较高。每次进行，对模式串的错误没有记忆，相当于每次模式串都是全新。接下来的算法基本都利用了模式串匹配的错误信息，从而避免了匹配串指针i不会一直回退。</p><h3 id="2-KMP算法"><a href="#2-KMP算法" class="headerlink" title="2.KMP算法"></a><strong>2.KMP算法</strong></h3><blockquote><p>相关论文：<br>Knuth D.E., Morris J.H., and Pratt V.R., Fast pattern matching in strings, SIAM Journal on Computing, 6(2), 323-350, 1977.</p></blockquote><p>以下来自：<a href="http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html">http://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html</a></p><ol><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050103.png" alt="img"></p><p>首先，字符串”BBC ABCDAB ABCDABCDABDE”的第一个字符与搜索词”ABCDABD”的第一个字符，进行比较。因为B与A不匹配，所以搜索词后移一位。</p><ol start="2"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050104.png" alt="img"></p><p>因为B与A不匹配，搜索词再往后移。</p><ol start="3"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050105.png" alt="img"></p><p>就这样，直到字符串有一个字符，与搜索词的第一个字符相同为止。</p><ol start="4"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050106.png" alt="img"></p><p>接着比较字符串和搜索词的下一个字符，还是相同。</p><ol start="5"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png" alt="img"></p><p>直到字符串有一个字符，与搜索词对应的字符不相同为止。</p><ol start="6"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050108.png" alt="img"></p><p>这时，最自然的反应是，将搜索词整个后移一位，再从头逐个比较。这样做虽然可行，但是效率很差，因为你要把”搜索位置”移到已经比较过的位置，重比一遍。</p><ol start="7"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png" alt="img"></p><p>一个基本事实是，当空格与D不匹配时，你其实知道前面六个字符是”ABCDAB”。KMP算法的想法是，设法利用这个已知信息，不要把”搜索位置”移回已经比较过的位置，继续把它向后移，这样就提高了效率。</p><ol start="8"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050109.png" alt="img"></p><p>怎么做到这一点呢？可以针对搜索词，算出一张《部分匹配表》（Partial Match Table）。这张表是如何产生的，后面再介绍，这里只要会用就可以了。</p><ol start="9"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050107.png" alt="img"></p><p>已知空格与D不匹配时，前面六个字符”ABCDAB”是匹配的。查表可知，最后一个匹配字符B对应的”部分匹配值”为2，因此按照下面的公式算出向后移动的位数：</p><blockquote><p>　　移动位数 &#x3D; 已匹配的字符数 - 对应的部分匹配值</p></blockquote><p>因为 6 - 2 等于4，所以将搜索词向后移动4位。</p><ol start="10"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050110.png" alt="img"></p><p>因为空格与Ｃ不匹配，搜索词还要继续往后移。这时，已匹配的字符数为2（”AB”），对应的”部分匹配值”为0。所以，移动位数 &#x3D; 2 - 0，结果为 2，于是将搜索词向后移2位。</p><ol start="11"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050111.png" alt="img"></p><p>因为空格与A不匹配，继续后移一位。</p><ol start="12"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050112.png" alt="img"></p><p>逐位比较，直到发现C与D不匹配。于是，移动位数 &#x3D; 6 - 2，继续将搜索词向后移动4位。</p><ol start="13"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050113.png" alt="img"></p><p>逐位比较，直到搜索词的最后一位，发现完全匹配，于是搜索完成。如果还要继续搜索（即找出全部匹配），移动位数 &#x3D; 7 - 0，再将搜索词向后移动7位，这里就不再重复了。</p><ol start="14"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050114.png" alt="img"></p><p>下面介绍《部分匹配表》是如何产生的。</p><p>首先，要了解两个概念：”前缀”和”后缀”。 “前缀”指除了最后一个字符以外，一个字符串的全部头部组合；”后缀”指除了第一个字符以外，一个字符串的全部尾部组合。</p><ol start="15"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050109.png" alt="img"></p><p>“部分匹配值”就是”前缀”和”后缀”的最长的共有元素的长度。以”ABCDABD”为例，</p><blockquote><p>　　－　“A”的前缀和后缀都为空集，共有元素的长度为0；</p><p>　　－　“AB”的前缀为[A]，后缀为[B]，共有元素的长度为0；</p><p>　　－　“ABC”的前缀为[A, AB]，后缀为[BC, C]，共有元素的长度0；</p><p>　　－　“ABCD”的前缀为[A, AB, ABC]，后缀为[BCD, CD, D]，共有元素的长度为0；</p><p>　　－　“ABCDA”的前缀为[A, AB, ABC, ABCD]，后缀为[BCDA, CDA, DA, A]，共有元素为”A”，长度为1；</p><p>　　－　“ABCDAB”的前缀为[A, AB, ABC, ABCD, ABCDA]，后缀为[BCDAB, CDAB, DAB, AB, B]，共有元素为”AB”，长度为2；</p><p>　　－　“ABCDABD”的前缀为[A, AB, ABC, ABCD, ABCDA, ABCDAB]，后缀为[BCDABD, CDABD, DABD, ABD, BD, D]，共有元素的长度为0。</p></blockquote><ol start="16"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050112.png" alt="img"></p><p>“部分匹配”的实质是，有时候，字符串头部和尾部会有重复。比如，”ABCDAB”之中有两个”AB”，那么它的”部分匹配值”就是2（”AB”的长度）。搜索词移动的时候，第一个”AB”向后移动4位（字符串长度-部分匹配值），就可以来到第二个”AB”的位置。</p><p>（完）</p><h3 id="3-Boyer-Moore算法"><a href="#3-Boyer-Moore算法" class="headerlink" title="3. Boyer-Moore算法"></a><strong>3. Boyer-Moore算法</strong></h3><blockquote><p>相关论文：<br>R.S.Boyer, J.S.Moore, A fast string searching algorithm , Communications of the ACM,20(10):762-772 ,1977<br>来自<a href="http://www.ruanyifeng.com/blog/2013/05/boyer-moore_string_search_algorithm.html">http://www.ruanyifeng.com/blog/2013/05/boyer-moore_string_search_algorithm.html</a></p></blockquote><ol><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050302.png" alt="img"></p><p>假定字符串为”HERE IS A SIMPLE EXAMPLE”，搜索词为”EXAMPLE”。</p><ol start="2"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050303.png" alt="img"></p><p>首先，”字符串”与”搜索词”头部对齐，从尾部开始比较。</p><p>这是一个很聪明的想法，因为如果尾部字符不匹配，那么只要一次比较，就可以知道前7个字符（整体上）肯定不是要找的结果。</p><p>我们看到，”S”与”E”不匹配。这时，**”S”就被称为”坏字符”（bad character），即不匹配的字符。**我们还发现，”S”不包含在搜索词”EXAMPLE”之中，这意味着可以把搜索词直接移到”S”的后一位。</p><ol start="3"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050304.png" alt="img"></p><p>依然从尾部开始比较，发现”P”与”E”不匹配，所以”P”是”坏字符”。但是，”P”包含在搜索词”EXAMPLE”之中。所以，将搜索词后移两位，两个”P”对齐。</p><ol start="4"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050305.png" alt="img"></p><p>我们由此总结出**”坏字符规则”**：</p><blockquote><p>　　后移位数 &#x3D; 坏字符的位置 - 搜索词中的上一次出现位置</p></blockquote><p>如果”坏字符”不包含在搜索词之中，则上一次出现位置为 -1。</p><p>以”P”为例，它作为”坏字符”，出现在搜索词的第6位（从0开始编号），在搜索词中的上一次出现位置为4，所以后移 6 - 4 &#x3D; 2位。再以前面第二步的”S”为例，它出现在第6位，上一次出现位置是 -1（即未出现），则整个搜索词后移 6 - (-1) &#x3D; 7位。</p><ol start="5"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050306.png" alt="img"></p><p>依然从尾部开始比较，”E”与”E”匹配。</p><ol start="6"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050307.png" alt="img"></p><p>比较前面一位，”LE”与”LE”匹配。</p><ol start="7"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050308.png" alt="img"></p><p>比较前面一位，”PLE”与”PLE”匹配。</p><ol start="8"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050309.png" alt="img"></p><p>比较前面一位，”MPLE”与”MPLE”匹配。**我们把这种情况称为”好后缀”（good suffix），即所有尾部匹配的字符串。**注意，”MPLE”、”PLE”、”LE”、”E”都是好后缀。</p><ol start="9"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050310.png" alt="img"></p><p>比较前一位，发现”I”与”A”不匹配。所以，”I”是”坏字符”。</p><ol start="10"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050311.png" alt="img"></p><p>根据”坏字符规则”，此时搜索词应该后移 2 - （-1）&#x3D; 3 位。问题是，此时有没有更好的移法？</p><ol start="11"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050309.png" alt="img"></p><p>我们知道，此时存在”好后缀”。所以，可以采用**”好后缀规则”**：</p><blockquote><p>　　后移位数 &#x3D; 好后缀的位置 - 搜索词中的上一次出现位置</p></blockquote><p>举例来说，如果字符串”ABCDAB”的后一个”AB”是”好后缀”。那么它的位置是5（从0开始计算，取最后的”B”的值），在”搜索词中的上一次出现位置”是1（第一个”B”的位置），所以后移 5 - 1 &#x3D; 4位，前一个”AB”移到后一个”AB”的位置。</p><p>再举一个例子，如果字符串”ABCDEF”的”EF”是好后缀，则”EF”的位置是5 ，上一次出现的位置是 -1（即未出现），所以后移 5 - (-1) &#x3D; 6位，即整个字符串移到”F”的后一位。</p><p>这个规则有三个注意点：</p><blockquote><p>　　（1）”好后缀”的位置以最后一个字符为准。假定”ABCDEF”的”EF”是好后缀，则它的位置以”F”为准，即5（从0开始计算）。</p><p>　　（2）如果”好后缀”在搜索词中只出现一次，则它的上一次出现位置为 -1。比如，”EF”在”ABCDEF”之中只出现一次，则它的上一次出现位置为-1（即未出现）。</p><p>　　（3）如果”好后缀”有多个，则除了最长的那个”好后缀”，其他”好后缀”的上一次出现位置必须在头部。比如，假定”BABCDAB”的”好后缀”是”DAB”、”AB”、”B”，请问这时”好后缀”的上一次出现位置是什么？回答是，此时采用的好后缀是”B”，它的上一次出现位置是头部，即第0位。这个规则也可以这样表达：如果最长的那个”好后缀”只出现一次，则可以把搜索词改写成如下形式进行位置计算”(DA)BABCDAB”，即虚拟加入最前面的”DA”。</p></blockquote><p>回到上文的这个例子。此时，所有的”好后缀”（MPLE、PLE、LE、E）之中，只有”E”在”EXAMPLE”还出现在头部，所以后移 6 - 0 &#x3D; 6位。</p><ol start="12"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050312.png" alt="img"></p><p>可以看到，”坏字符规则”只能移3位，”好后缀规则”可以移6位。所以，<strong>Boyer-Moore算法的基本思想是，每次后移这两个规则之中的较大值。</strong></p><p>更巧妙的是，这两个规则的移动位数，只与搜索词有关，与原字符串无关。因此，可以预先计算生成《坏字符规则表》和《好后缀规则表》。使用时，只要查表比较一下就可以了。</p><ol start="13"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050313.png" alt="img"></p><p>继续从尾部开始比较，”P”与”E”不匹配，因此”P”是”坏字符”。根据”坏字符规则”，后移 6 - 4 &#x3D; 2位。</p><ol start="14"><li></li></ol><p><img src="http://www.ruanyifeng.com/blogimg/asset/201305/bg2013050314.png" alt="img"></p><p>从尾部开始逐位比较，发现全部匹配，于是搜索结束。如果还要继续查找（即找出全部匹配），则根据”好后缀规则”，后移 6 - 0 &#x3D; 6位，即头部的”E”移到尾部的”E”的位置。</p><p>（完）</p><h3 id="4-Horspool算法"><a href="#4-Horspool算法" class="headerlink" title="4.Horspool算法"></a><strong>4.Horspool算法</strong></h3><blockquote><p>相关论文：</p><p>Horspool R.N., 1980, Practical fast searching in strings, Software - Practice &amp; Experience, 10(6):501-506</p></blockquote><p>Horspool算法的一个独特之处是它的模式串是尾匹配的（即从右到左）</p><p>例：</p><p>我们将在匹配串寻找字符使得模式串能从右往左匹配。（1）</p><p>模式串的尾是e，匹配到第一个e，这已经是匹配串的第11个字符。</p><p>匹配串 ：Here <strong>is a e</strong>xcellent example</p><p>模式串：      <strong>example</strong></p><p>这是我们从右往左匹配：</p><p>空格与l不匹配,模式串将从不匹配的那个字符开始从右向左寻找匹配串中不匹配的字符的位置，模式串里没有空格，所以移动一个模式串长度的单位</p><p>匹配串 ：Here is a e<strong>xcellen</strong>t example</p><p>模式串：      <strong>example</strong></p><p>不匹配，按（1）操作。</p><p>逐次类推。</p><p>（完）</p><h3 id="5-Sunday算法"><a href="#5-Sunday算法" class="headerlink" title="5.Sunday算法"></a>5.Sunday算法</h3><blockquote><p>相关论文：</p><p>Daniel M. Sunday, A very fast substring search algorithm, Communications of the ACM, v.33 n.8, p.132-142, Aug. 1990</p></blockquote><p>Sunday算法和Horspool算法类似，都是尾匹配，不过匹配方式略有不同，不是找匹配串中不匹配的字符在模式串的位置，而是直接找最右边对齐的右一位的那个字符在模式串的位置</p><p>例：</p><p>我们将在匹配串寻找字符使得模式串能从右往左匹配。</p><p>模式串的尾是e，匹配到第一个e，这已经是匹配串的第11个字符。</p><p>匹配串 ：Here <strong>is a e</strong>xcellent example</p><p>模式串：      <strong>example</strong></p><p>空格和l没有匹配上，寻找匹配串右一个字符在模式串的位置。</p><p>匹配串 ：Here is a <strong>excelle</strong>nt example</p><p>模式串：          <strong>example</strong></p><p>继续匹配，发现匹配串右一个字符在模式串里找不到，跳过去</p><p>匹配串 ：Here is a excellen<strong>t examp</strong>le</p><p>模式串：          <strong>example</strong></p><p>继续匹配，匹配串右一个字符为l</p><p>匹配串 ：Here is a excellent <strong>example</strong></p><p>模式串：          <strong>example</strong></p><p>（完）</p><h3 id="更高级的串模式匹配算法还有以下几种："><a href="#更高级的串模式匹配算法还有以下几种：" class="headerlink" title="更高级的串模式匹配算法还有以下几种："></a>更高级的串模式匹配算法还有以下几种：</h3><h4 id="6-有限自动机算法"><a href="#6-有限自动机算法" class="headerlink" title="6.有限自动机算法"></a><strong>6.有限自动机算法</strong></h4><h4 id="7-Rabin-Karp算法"><a href="#7-Rabin-Karp算法" class="headerlink" title="7.Rabin-Karp算法"></a>7.Rabin-Karp算法</h4><blockquote><p> 相关论文：</p><p> Rachard M.Karp and Michael O.Rabin.Efficient randomized pattern-matching algorithms.IBM Journal of Research and Development,31(2):249-260,1987.</p></blockquote><p>利用哈希值。</p><h4 id="8-Galil-Seiferas-算法"><a href="#8-Galil-Seiferas-算法" class="headerlink" title="8.Galil-Seiferas 算法"></a>8.Galil-Seiferas 算法</h4><blockquote><p>相关论文：</p><p>Zvi Galil and.Joel Seiferas. Time-Space-Optimal String Matching. Journal of Computer And System Sciences, 26(3),:280-294 (1983) </p></blockquote><p>该算法基于重复因子的字符串匹配。该算法是一个奇妙的线性时间字符串算法，除了匹配串和模式串所需的存储空间外，只需要O(1)的额外存储空间。</p><h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p>[1]<a href="https://blog.csdn.net/xiaodu93/article/details/39209421">https://blog.csdn.net/xiaodu93/article/details/39209421</a>.</p><p>[2]算法导论（Introduction to Algorithms）.</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>我没有为你伤春悲秋不配有憾事</title>
    <link href="/2021/20210326/"/>
    <url>/2021/20210326/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>听了张敬轩的《春秋》，本想提笔写写自己的感受，奈何在知乎上看到了一个写得非常非常好的评论，有种”眼前有景道不得,崔颢提诗在上头“的感觉。</p><span id="more"></span><hr><p>我其实已经不太记得你了。不记得你跟我说过的第一句话，也不记得我们说过的最后一句话。不记得你的生日，不记得那些曾珍而重之的细节。不记得你跟我熟识起来到底是哪一天，也不记得我们渐渐疏离是什么时候。但我记得我曾经坐在空房间整天整天的想你，我记得花了很长时间为你准备的礼物，我记得你遗失的背包，我记得你课本上的字迹。也记得我们在一起时总是笑，偶尔也会怄气，好长时间不说话。  我其实已经记不清你的样子，却还记得喜欢的感觉。就像我早已经不了解你，却因为曾经的亲密而沾沾自喜。  我把你当成你是我情感世界里最重要的一个。但其实，在世人眼里，你和我的关系恐怕还不如我和我的貌合神离的朋友。哪怕是向你表白也是借着酒意，以如此隐秘的方式，不留一丝痕迹。而你也是这样淡淡的拒绝，甚至扔像最好的朋友一样和我见面，跟我谈天说地。我的心不可能拒绝跟你的会面啊。明明你对我还是那么好，我没有理由逃避你，哪怕我如此爱你，而你只当我是朋友。  爱多数时候是自欺欺人，每个人都像小说家，替自己造梦。现实和梦的反差总要被当成是痛苦的根源。但说真的，能扮弱者玩失意，也是一种运气。至少能用自卑换一点你的关注，换一些两人相处的时光。但时机一过，真的只剩心酸罢了。“ 想心底 留根刺， 至少要见面上万次”，而我与你见过几面？到现在，你有了伴侣，我们或许也将天各一方。可能再见面只能是在梦里了。我这个曾经的“朋友”，凭什么在你心里留下一个坎，让你像留恋自己的情人一样留恋我呢？</p><p>你曾经害过我吗？你曾经背叛了我吗？都没有。既然都没有，我又有什么理由发泄自己的悲伤痛苦？我没有为你痛哭流涕过，也没有陪你走过你儿时走的那条小巷，我从未拥有你，也没多少厚重的回忆可以延续我们的故事。</p><p>如果你因为我的失意而感到不安，觉得是你害了我，那是我的大错！你找到了命中注定的爱人，我这个“朋友”凭什么有怨恨？我突然觉得即便是你的某某前任也比我幸运得多，至少有名目嫉妒。而我只能像天使一样，快活自在的跟你说祝福，没权让你多一丝负担。 </p><p> 时过境迁才感到后悔，原来我与你的联系，连一件实证都找不到。如果非要说的话，那年夏天被剪断的长发其实是为了你。但我不敢任何人说起，就像我不敢跟任何人说起我曾爱你一样。就这样飘渺到甚至连自己都已经不明朗的记忆，这样单薄的记忆，值得写成文字记录吗？ 后人如果来读，大概只觉得是一场杜撰。即便真实存在又如何呢？人生的最后如果要写一部回忆录的话，有多少东西可写啊。初恋的憧憬，新婚的期待，甚至中年的婚外情。而这渺渺两页没有痛点的过场呢？应该会被当做废纸撕去吧。  我们的故事其实是这么短，这么浅。不要说去媲美《春秋》的宏大，在平凡短暂的一生里都算不了是什么值得纪念的片段。那么我的眼泪有意义吗？我的想念有意义吗？其实真的极幼稚，只有孩子才有资格为一颗丢失的糖哭泣吧。我何以有幸为这样微不足道的记忆伤怀呢？我怎么能将自己的失意归咎于你呢？爱若是伟大到像罗密欧与朱丽叶，再怎么痛苦煎熬，也足够让人敞开胸襟轰轰烈烈。但是我对你的这份感情的实质和它看上去的样子其实是没有区别的。于青书史册也好，于你的人生也罢。都是那样轻，轻得不值一提。</p>]]></content>
    
    
    
    <tags>
      
      <tag>音乐</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>怎么去拯救Ta们？</title>
    <link href="/2021/20210313/"/>
    <url>/2021/20210313/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>她低头看着湖水，黑暗中仿佛空无一物，只有黑幽幽的颜色，一片巨大的虚无在她脚下铺展开来。没关系的，她告诉自己，然后，她就跨出小船，走进水中。</p><p>——《无声告白》</p><span id="more"></span><p>当初看完了这一段，叹息了许久。莉迪亚最终还是走向了另一种”新生“。父亲作为外来者的自卑和软弱，母亲作为失意者的不甘与歇斯底里，给莉迪亚施加了病态的爱，使莉迪亚陷入了无止境的痛苦。为了对父母的爱，莉迪亚“献祭”了自己，自愿成为牵线木偶，随着内斯的离开，最后的慰藉不复存在，成为了压垮莉迪亚的最后一根稻草，自杀成了莉迪亚离开人世的无声告白。</p><p>而现实中济南大学女生自杀更加令人扼腕长叹，太痛心了。在这样的花样年华，人生还很漫长，还有很多精彩，却选择了结束美好的生命。</p><p>这位父亲捅妻子一剪刀，把妻子打得鼻子骨折，无疑已经是家庭暴力的行为。《民法典》第一千零四十二条、《刑法》第二百六十条、《中华人民共和国治安管理处罚条例》第二十二条、《中华人民共和国反家庭暴力法》都制定了应对家庭暴力的相关法律法规。显然，这还不够，仅仅是惩戒还不够做到防范于未然。预防也必不可少，《中华人民共和国反家庭暴力法》第六条 提出了</p><p>“国家开展家庭美德宣传教育，普及反家庭暴力知识，增强公民反家庭暴力意识。</p><p>工会、共产主义青年团、妇女联合会、残疾人联合会应当在各自工作范围内，组织开展家庭美德和反家庭暴力宣传教育。</p><p>广播、电视、报刊、网络等应当开展家庭美德和反家庭暴力宣传。</p><p>学校、幼儿园应当开展家庭美德和反家庭暴力教育。”</p><p>可是是不是宣传做的还不够，从小到大，亲身经历过的反家庭暴力教育有几次？少之又少。</p><p>法律的制裁和组织的宣传就足够了吗？能否实施对父母的“义务教育”，达到一定标准才能为人父母？父母是世界上最容易的职业，成为父母不需要考试，也不需要经过孩子的同意。正如《罗生门》中所说：“‘憎恨他犯下的罪，但不憎恨他本人。‘这做起来并不一定很难。大多数孩子对大部分父母都很好地实现了这个格言。”利用父母的身份来情感绑架无疑是最好的控制工具，是许多不幸者因原生家庭走向悲剧的根源。“我觉得我活的像狗一样”道出了这位女生的心声。这种畸形，窒息的”爱“让人坍塌成了深渊，陷入了无边无际的黑暗。这是一种令人无法呼吸的束缚，令人深陷其中逃不出来的桎梏。懂事的孩子想让人开心，自己却没有让自己开心的能力。</p><p>这位女生何罪？无罪。无它，出生在了一个悲惨的家庭，碰上了一个糟糕的父亲罢了。出生从来没有选择权，出生是一种不公平，倘若她出生在另一个家庭，是不是她就能避免今日的惨剧呢？有多少人能被爱得“有恃无恐”，能被温柔对待？有多少人能出生在一个温馨氛围的家？在影片《何以为家》中，Zain发出来对父母的控诉：“我要起诉我的父母，因为他们生了我。” “我以为我们能活得体面，能被所有人爱。但上帝不希望我们这样，他宁愿我们做洗碗工。”法律只是对人们最低的道德要求，成为父母需要更高的道德。除了这位自杀的女生，还有多少人证备受煎熬，深陷深渊？</p><p>该怎么去拯救他们？该怎么去根绝这种现象？该怎么治愈在原生家庭中承受暴力中受到的创伤？这种家庭模式是不是不甚好？如何在现有家庭模式寻回确实的配位的正常的爱？能否采取社会化抚养的方式？是否采取另一种甚至更激进的方式更适合？该怎么办呢？该怎么办呢？</p><hr><p>愿逝者安息。</p>]]></content>
    
    
    
    <tags>
      
      <tag>社会</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>梯度下降法</title>
    <link href="/2021/20210309/"/>
    <url>/2021/20210309/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>梯度下降法是一种常见的无约束优化方法,其问题定义为<br>$$<br>min f(x),x\in R^n,f在R^n上连续可微<br>$$</p><span id="more"></span><p>其以负梯度为搜索方向，这是一种非常自然，正确的选择。</p><p>简述步骤如下：</p><p>给定初始点$x\in dom f$</p><p>重复进行：</p><p>1.$\Delta x:&#x3D;-\nabla f(x)$</p><p>2.**直线搜索。**确定步长t（通过精确或回溯直线）。</p><blockquote><p>通常t取<br>$$<br>t&#x3D;-\frac{\nabla f(x)^T\Delta x}{(\Delta x)^TA\Delta x}，此处A&#x3D;\nabla ^2 f(x)(即Hessian矩阵)<br>$$</p></blockquote><p>3.<strong>修改</strong>。$x:&#x3D;x+t\Delta x$</p><p>直到满足停止准则（大部分情况下，步骤1完成后就检验停止条件，，而不是在修改后才检验。</p><ul><li>可能会发现梯度下降法和最速下降法步骤相同，但事实上，两者并不完全一样，简单来说采用Euclid 范数的最速下降法就是梯度下降法。</li></ul><p>更多可见：</p><p>3Blue1Brown的《深度学习之梯度下降法》</p><p>Stephen Boyd ，Lieven Vandenberghe的《凸优化》</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《他者的消失》读书笔记</title>
    <link href="/2021/20210224/"/>
    <url>/2021/20210224/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《他者的消失》由“德国哲学界的一颗新星” 韩炳哲所作，有多篇小短文构成，对“他者”进行了剖析，有些晦涩，但也不乏好句。</p><span id="more"></span><hr><h3 id="摘抄："><a href="#摘抄：" class="headerlink" title="摘抄："></a>摘抄：</h3><h4 id="同质化的恐怖"><a href="#同质化的恐怖" class="headerlink" title="同质化的恐怖"></a>同质化的恐怖</h4><p>如今，感知（die Wahrnehmung）本身呈现出一种“狂看”（Binge Watching）的形式，即“毫无节制的呆视”（Komaglotzen）。它指的是无时间限制地消费视频和电影。人们持续不断地为消费者提供完全符合他们欣赏品位的、讨他们喜欢的电影和连续剧。消费者像牲畜一样，被饲以看似花样翻新实则完全相同的东西。如今社会的感知模式完全可以用这种“毫无节制的呆视”来概括。同质化的扩散不是癌症性质的，而是昏睡性质的。它并未遭遇免疫系统的抵抗。人们就这样呆视着，直至失去意识。</p><p>数字化的全联网（Totalvernetzung）和全交际（Totalkommunikation）并未使人们更容易遇见他者。相反，它恰恰更便于人们从陌生者和他者身边经过，无视他们的存在，寻找到同者、志同道合者，从而导致我们的经验视野日渐狭窄。它使我们陷入无尽的自我循环之中，并最终导致我们“被自我想象洗脑”[插图]。</p><p>然而如今的网络已变成一个特殊的共振空间，一个回音室，任何不同与陌生都被消除了。真正的共鸣以他者的切近为前提。如今，他者的切近让位于同者的无差别性（Abstandslosigkeit）。全球化交际只允许相同的他者（gleiche Andere）或其他的同者（andere Gleiche）存在。</p><p>我们就是被不知名的力量操纵的牵线木偶，没有一丝一毫是我们自己！”</p><h4 id="全球化与恐怖主义的暴力"><a href="#全球化与恐怖主义的暴力" class="headerlink" title="全球化与恐怖主义的暴力"></a>全球化与恐怖主义的暴力</h4><p>金钱是一个很糟糕的身份授予者，虽然它能代替身份，让拥有金钱的人至少获得安全感和平静。然而，那些一文不名的人是真的一无所有，既无身份也无安全。因此，没钱的人就只好走进虚幻之境，比如成为民族主义者，这会很快给他一个身份。与此同时，他也为自己创造了一个敌人。人们通过假想来构建免疫力，以获得有意义的身份。挥之不散的恐惧不知不觉地唤醒一种对敌人的渴求。敌人能快速给人以身份，哪怕是幻想中的敌人：“敌人勾勒出我们自身问题的形象，因此我必须与之横眉冷对，以获得自身的尺度、界线和轮廓。”[插图]想象弥补了现实的缺失。就连恐怖主义者也栖身于他们自己的想象之中。全球化让想象空间诞生，想象的空间却带来真实的暴力。</p><h4 id="真实性的恐怖"><a href="#真实性的恐怖" class="headerlink" title="真实性的恐怖"></a>真实性的恐怖</h4><p>自残不仅是自我惩罚的仪式，痛恨自己在面对如今功利的、追求完美的社会时和多数人一样力不从心，同时也是对爱的呼唤。</p><p>空虚感是抑郁和边缘性人格障碍的基本症状。边缘人通常无法感受到自己的存在。只有在自残的时候他们才终于有所感觉。抑郁的功能主体视自身为沉重负担。他厌倦自己，又沉湎于自己，完全无力从自身当中走出来，这一切都矛盾地导致自身的虚无和空洞。自我封闭、自我关押，失去一切与他者的关联。我触摸自己，却只能通过他者的触摸而感受到自己。他者是塑造稳定自我的根本途径。</p><p>自拍瘾（Selfie-Sucht）实际上跟虚荣心关系也不大，它无非就是孤独、自恋的自我在瞎忙。面对内心的空虚，人们徒劳地尝试着卖弄自己，博人眼球。唯有空虚在自我复制。自拍照是自身的空虚形态。自拍瘾加剧了空虚感。导致这一结果的不是虚荣心，而是自恋的自我关涉。自拍照是自身的美丽平面，而这自身空洞、不安。为了逃避空虚感的折磨，人们要么拿起刀片，要么拿起智能手机。自拍照是让空虚的自我短暂退隐的扁平表面。倘若把照片翻过来，人们就会撞见那伤痕累累的背面，汩汩流着血。自拍照的背后是伤口。</p><h4 id="恐惧"><a href="#恐惧" class="headerlink" title="恐惧"></a>恐惧</h4><p>[插图]。今天，我们都竭力逃离否定者，从不在其身边栖居。黏在肯定者身畔只能复制更多的同者。</p><p>自我对他人亦步亦趋，当自我觉得无法跟上他人步伐之时，便会慌乱无措。……他人如何看待我，他人觉得我又是如何看待他们，这些想象成为社会恐惧的来源。使个人不胜其烦、心力交瘁的并非客观的情况，而是感受到在与多数派的他者相比较时所产生的竞争。</p><p>如今，很多人饱受诸多恐惧的折磨，害怕拒绝，害怕失败，害怕被落下，害怕犯错误或者做错决定，害怕达不到自己的要求。在不停与他人比较的过程中，恐惧也不断加剧。这是与纵向恐惧背道而驰的横向恐惧。前者是面向全然他者、面向茫然失所和面向“无”之时才生出的恐惧。</p><h4 id="门槛"><a href="#门槛" class="headerlink" title="门槛"></a>门槛</h4><p>作为新兴生产方式，数字化交际彻底打破所有距离，以加速自身运行，所有保护性的距离也就此消失了。在超交际中万事万物皆熔于一炉，内在和外在之间的壁垒也愈发脆弱。如今，我们完全被外化为一个“纯粹的平面”[插图]，暴露在无孔不入的网络中。这种强制性的透明化克服了一切视觉与信息缺口，世间万物清晰可见。它不给人任何退路，令所有安全空间消失不见。万事万物汹涌而来，而我们却无遮无挡，亦无处可藏。我们本身也只是全球网络中的通道而已。透明化和超交际夺走了保护着我们的内心世界。是的，我们是自愿放弃了内心世界，甘于受数字化网络的奴役，任由它们穿透、照透、刺透我们。数字化的过度曝光与毫无遮掩带来一种潜在的恐惧，它并非源于他者的否定性，而是源于过多的肯定性。同质化的透明地狱并没有驱离恐惧，令人胆寒的正是同质化愈发震耳欲聋的轰鸣。</p><h4 id="异化"><a href="#异化" class="headerlink" title="异化"></a>异化</h4><p>如今我们生活在后马克思主义时代。在新自由主义的政制下，剥削不再以异化和自我现实化剥夺的面貌出现，而是披上了自由、自我实现和自我完善的外衣。这里并没有强迫我劳动、使我发生异化的剥削者。相反，我心甘情愿地剥削着我自己，还天真地以为是在自我实现。这是新自由主义的奸险逻辑。所以，过劳症（Burn-out）的初期表现恰恰是亢奋。干劲十足地投身于劳动之中，直至精疲力竭为止。自我实现，实现至死。自我完善，完善而亡。新自由主义的统治藏身于幻想中的自由背后。它与自由携手并立于我们面前之际，正是它大功告成之时。这种感觉上的自由消弭了任何反抗、革命的可能性，这才是它的致命之处。有什么可反抗的呢？已经没有人再压迫你了啊！珍妮·霍尔泽所说的“保护我免受我所欲之害”将这一范式转变表达得十分贴切。</p><h4 id="目光"><a href="#目光" class="headerlink" title="目光"></a>目光</h4><p>由于缺乏起到镇压作用的目光（这与纪律社会的监督策略有着本质区别），便产生了一种具有欺骗性的自由感。数字化全景监狱里的犯人并未觉得被凝视，也就是并未觉得被监控。因此，他们感到很自由，且自愿地去暴露自己。数字化全景监狱并非限制了自由，而是将其极尽利用。</p><h4 id="他者之语言"><a href="#他者之语言" class="headerlink" title="他者之语言"></a>他者之语言</h4><p>如今，我们为了成为被关注的焦点而无所不用其极。为了博取关注，我们彼此都成了橱窗。</p><h4 id="倾听"><a href="#倾听" class="headerlink" title="倾听"></a>倾听</h4><p>如今，我们越来越丧失倾听的能力。妨碍倾听的罪魁祸首便是日渐严重的自我聚焦，是社会的自恋化倾向。</p><p>我不必求助于某个“个人”对象，从网络上就能找到信息。我也不必前往公共领域去获取信息或购买商品，而是让它们被送上门来。数字化交际将我联入网中，但同时也使我孤立于他人。它虽然消灭了距离，然而，“无距离”却产生不了人与人的切近。</p><p>人们在脸书（Facebook）上不会提及关乎我们大家的、可以讨论的问题，而主要是发布一些不需要讨论的、只为凸显发布者形象的广告。在那里，人们不会想到，他者可能有着烦恼和痛苦。在“点赞”的共同体中人们只会遇到自己，或者和自己相同的人。那里也不可能形成讨论。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《概率导论》补充答案</title>
    <link href="/2021/20210122(1~5)/"/>
    <url>/2021/20210122(1~5)/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《概率导论》第二版答案补充，书上已给解答的不再赘述。</p><span id="more"></span><p>部分来自：</p><p> <a href="https://github.com/NeilKleistGao/answer-to-introduction-to-probability">answer-to-introduction-to-probability</a> （NeilKleistGao&#x2F;雪岛&#x2F;HumphreyYang）</p><p><a href="http://athenasc.com/prob-solved_2ndedition.pdf">http://athenasc.com/</a> （Athena Scientific, Belmont, Massachusetts）-</p><hr><h3 id="第一章-样本空间与概率"><a href="#第一章-样本空间与概率" class="headerlink" title="第一章 样本空间与概率"></a>第一章 样本空间与概率</h3><h4 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h4><p>令事件A为掷出偶数，即$A &#x3D; {2, 4, 6}$</p><p>令B表示点数大于3的事件，即$B &#x3D; {4, 5, 6}$</p><p>那么：</p><p>$A^c &#x3D; {1, 3, 5}, B^c &#x3D; {1, 2, 3}$</p><p>$A \cup B &#x3D; {2, 4, 5, 6}, A \cap B &#x3D; {4, 6}$</p><p>所以$(A \cup B)^c &#x3D; {1, 3} &#x3D; A^c \cap B^c$</p><p>$(A \cap B)^c &#x3D; {1, 2, 3, 5} &#x3D; A^c \cup B^c$</p><h4 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h4><h5 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h5><p>设全集为S，由代数性质：</p><p>$\begin{array}{lcr}<br>                A^c &#x3D; \<br>            A^c \cap S &#x3D; \<br>            A^c \cap (B \cup B^c) &#x3D; \<br>            (A^c \cap B) \cup (A^c \cap B^c)<br>            \end{array}$</p><p>对$B^c$的结论同理，证毕。</p><p>也可以通过韦恩图进行证明：任意一个集合中的元素，要么属于$B$，要么属于$B^c$。上式就是全概率定理的特殊情况。</p><h5 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h5><p>设全集为S，由德摩根律：</p><p>$\begin{array}{lcr}<br>                (A \cap B)^c &#x3D; \<br>                A^c \cup B^c &#x3D; \<br>                (A^c \cap S) \cup (B^c \cap S) &#x3D; \<br>                (A^c \cap B) \cup  (A^c \cap B^c) \cup (A^c \cap B^c) \cup (A \cap B^c) &#x3D; \<br>                (A^c \cap B) \cup  (A^c \cap B^c) \cup (A \cap B^c)<br>            \end{array}$</p><p>证毕。</p><p>也可以通过韦恩图进行证明：A交B的补集中，只有三种可能：属于A但不属于B、属于B但不属于A、既不属于A也不属于B，整理即可得到上式。</p><h5 id="c"><a href="#c" class="headerlink" title="(c)"></a>(c)</h5><p>令事件A为掷出偶数，即$A &#x3D; {2, 4, 6}$</p><p>令B表示点数大于3的事件，即$B &#x3D; {1, 2, 3}$</p><p>则$(A \cap B)^c &#x3D; {1, 3, 4, 5, 6}$</p><p>$(A^c \cap B) \cup  (A^c \cap B^c) \cup (A \cap B^c) &#x3D; {1, 3} \cup {5} \cup {4, 6} &#x3D; {1, 3, 4, 5, 6}$</p><p>所以$(A \cap B)^c &#x3D; (A^c \cap B) \cup  (A^c \cap B^c) \cup (A \cap B^c)$。</p><h4 id="3"><a href="#3" class="headerlink" title="3."></a>3.</h4><p>略。</p><h4 id="4"><a href="#4" class="headerlink" title="4."></a>4.</h4><p>证明略，这里给出一个实例。假设[0, 1]区间中的数可数，那么我们可以列出：<br>$$\begin{array}{rcl}<br>            x_1 &amp;&#x3D; 0.3154… \<br>            x_2 &amp;&#x3D; 0.114514… \<br>            x_3 &amp;&#x3D; 0.2333… \<br>            …<br>        \end{array}$$<br>我们可以找到一个y，使得$y &#x3D; 0.121…$。由于y的第n位与$x_n$不同，于是这个y不可能在上面的数中。</p><p>当然找到y的方式并不止书上一种。</p><h4 id="5"><a href="#5" class="headerlink" title="5."></a>5.</h4><p>设挑选一个学生，这个学生是天才为事件A，这个学生喜欢巧克力为事件B，那么：</p><p>$P(A) &#x3D; 0.6, P(B) &#x3D; 0.7, P(A \cap B) &#x3D; 0.4$</p><p>所以$P(\bar A \cap \bar B) &#x3D; 1 - P(A) - P(B) + P(A \cap B) &#x3D; 0.1$。</p><p>这题是12题容斥原理的应用。</p><h4 id="6"><a href="#6" class="headerlink" title="6."></a>6.</h4><p>因为偶数的概率是奇数的两倍，又因为$P(even) + P(odd) &#x3D; 1$，所以$P(even) &#x3D; \frac{2}{3}, P(odd) &#x3D; \frac{1}{3}$。</p><p>因为不同偶数&#x2F;奇数面出现的概率相同，于是：</p><p>$$\begin{array}{rcl}<br>            P(1) &amp;&#x3D; \frac{1}{9} \<br>            P(2) &amp;&#x3D; \frac{2}{9} \<br>            P(3) &amp;&#x3D; \frac{1}{9} \<br>            P(4) &amp;&#x3D; \frac{2}{9} \<br>            P(5) &amp;&#x3D; \frac{1}{9} \<br>            P(6) &amp;&#x3D; \frac{2}{9}<br>        \end{array}$$</p><p>所以点数小于4的概率为$P(1) + P(2) + P(3) &#x3D; \frac{4}{9}$。</p><h4 id="7"><a href="#7" class="headerlink" title="7."></a>7.</h4><p>样本空间为：第一次时停止，第二次时停止，第三次时停止……</p><h4 id="8"><a href="#8" class="headerlink" title="8."></a>8.</h4><p>不妨设第n局获胜的概率为$P_n$，那么最终获胜的概率：</p><p>$P &#x3D; P_1(1 - P_2)P_3 + P_1P_2 + (1 - P_1)P_2P_3$</p><p>化简上式得到$P &#x3D; P_1P_3 + P_2(P_1 + P_3 - 2P_1P_3)$。</p><p>对于函数$f(x, y) &#x3D; x + y - 2xy$在x和y都属于[0,<br>1]区间内时，$x \geq x^2, y \geq y^2, (x - y)^2 \geq 0$，所以$x + y - 2xy \geq x^2 + y^2 -2xy &#x3D; (x - y)^2 \geq 0$</p><p>所以对于$(P_1 + P_3 - 2P_1P_3)$，这个值永远非负，故$P_2$最大时整个式子最大，且与$P_1,P_3$的顺序无关。</p><p>证毕。</p><h4 id="9"><a href="#9" class="headerlink" title="9."></a>9.</h4><h5 id="a-1"><a href="#a-1" class="headerlink" title="(a)"></a>(a)</h5><p>$P(A) &#x3D; P(A \cap \Omega) &#x3D; P(A \cap \bigcup_{i &#x3D; 1}^nS_i) &#x3D; P(\bigcup_{i &#x3D; 1}^n(A \cap S_i))$</p><p>由于${S_1, S_2, …, S_n}$互不相容，故$P(A) &#x3D; \sum_{i &#x3D; 1}^nP(A \cap S_i)$</p><p>证毕。</p><h5 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h5><p>不难发现，$B^c \cap C^c, B \cap C^c, B^c \cap C, B \cap C$互不相容</p><p>所以$P(A) &#x3D; P(A \cap B^c \cap C^c) + P(A \cap B \cap C^c) + P(A \cap B^c \cap C) + P(A \cap B \cap C)$</p><p>由容斥原理，$P(A) &#x3D; P(A \cap B^c \cap C^c) + P(A \cap B) + P(A \cap C) - P(A \cap B \cap C)$</p><p>证毕。</p><h4 id="10"><a href="#10" class="headerlink" title="10."></a>10.</h4><p>$P(A) + P(B) - 2P(A \cap B) &#x3D; P(A \cup B) - P(A \cap B) &#x3D; P((A \cap B^c) \cup (A^c \cap B))$</p><p>证毕。</p><h4 id="11"><a href="#11" class="headerlink" title="11."></a>11.</h4><h5 id="a-2"><a href="#a-2" class="headerlink" title="(a)"></a>(a)</h5><p>原文给出的等式应为$P(A \cup B) &#x3D; P(A) + P(B) - P(A \cap B)$，似乎存在笔误。</p><h5 id="b-2"><a href="#b-2" class="headerlink" title="(b)"></a>(b)</h5><p>略。</p><h4 id="12"><a href="#12" class="headerlink" title="12."></a>12.</h4><h5 id="a-3"><a href="#a-3" class="headerlink" title="(a)"></a>(a)</h5><p>略。</p><h5 id="b-3"><a href="#b-3" class="headerlink" title="(b)"></a>(b)</h5><p>当n &#x3D; 3时，由(a)可知等式成立。</p><p>假定n &#x3D; m时等式依然成立，那么当n &#x3D; m + 1时：</p><p>$P(\bigcup_{k &#x3D; 1}^{m + 1}) &#x3D; P(A_{m + 1} \cup \bigcup_{k &#x3D; 1}^m A_k) &#x3D;<br>            P(A_{m + 1}) + \sum_{i \in S_1}P(A_i)<br>            - \sum_{(i_1, i_2) \in S_2}P(A_{i_1} \cap A_{i_2}) + … + (-1)^{n - 1}P(\bigcap_{k &#x3D; 1}^{n}A_k) -P(A_{m + 1} \cap \bigcup_{k &#x3D; 1}^m A_k)$</p><p>整理上式可得：</p><p>$P(\bigcup_{k &#x3D; 1}^{m + 1}) &#x3D; \sum_{i \in S_1}P(A_i) - \sum_{(i_1, i_2) \in S_2}P(A_{i_1} \cap A_{i_2}) + … + (-1)^nP(\bigcap_{k &#x3D; 1}^{m + 1}A_k)$</p><p>归纳完毕。</p><h4 id="13"><a href="#13" class="headerlink" title="13."></a>13.</h4><p>略。</p><h4 id="14"><a href="#14" class="headerlink" title="14."></a>14.</h4><h5 id="a-4"><a href="#a-4" class="headerlink" title="(a)"></a>(a)</h5><p>仅有(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6,<br>6)满足情况，故$P &#x3D; \frac{1}{6}$</p><h5 id="b-4"><a href="#b-4" class="headerlink" title="(b)"></a>(b)</h5><p>若总和小于等于4，则有(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (3,<br>1)六种情况，其中一对的事件有两个，故$P &#x3D; \frac{1}{3}$。</p><h5 id="c-1"><a href="#c-1" class="headerlink" title="(c)"></a>(c)</h5><p>设两个骰子都是6为事件A，恰有一个骰子为6为事件B，则$P(A) + P(B) &#x3D; \frac{1}{36} + \frac{10}{36} &#x3D; \frac{11}{36}$即为所求。</p><h5 id="d"><a href="#d" class="headerlink" title="(d)"></a>(d)</h5><p>不同的抛掷结果一共有30种，而至少一个骰子为6（也只能有一个骰子为6）有10种情况，所以$P &#x3D; \frac{1}{3}$</p><h4 id="15"><a href="#15" class="headerlink" title="15."></a>15.</h4><p>在已知第一次为正面的情况下，两次均正面朝上的概率只取决于第二次抛掷的情况，所以此时的概率为$\frac{1}{2}$。</p><p>而两次中至少有一次正面朝上的概率为$\frac{3}{4}$，在已知该前提下，两次正面的概率为$\frac{1}{3}$，所以爱丽丝的结论是正确的。</p><p>假定硬币不对称，即$p \ne 1 - p$，那么第一种情况的概率变为$p$；而第二种情况下至少一次正面朝上的概率变为了$1 - (1 - p)^2 &#x3D; 2p - p^2$，<br>在该前提下两次正面朝上的概率就应该是$\frac{p}{2 - p}$。因为$p \leq 1$，所以仅当$p &#x3D; 1$或$p &#x3D; 0$，即正面始终朝上或朝下时，两个条件概率相等，<br>其他条件下，前者的概率都要比后者大。</p><p>可以将这个结论推广到任意n$(n \geq 2)$次游戏：将前n -<br>1次游戏抛得正面条件下得到n次正面的概率与至少n-1次得到正面条件下的概率相差，<br>得到$\frac{(n - 1)p + (1 - n)p^2}{p + n - np}$，由于$p \geq p^2$，可以得到和上面一样的结论。</p><h4 id="16"><a href="#16" class="headerlink" title="16."></a>16.</h4><p>所有抛掷可以得到正面的概率为$\frac{1}{3} + \frac{1}{6} &#x3D; \frac{1}{2}$，而选中正常硬币并抛掷得到正面的概率为$\frac{1}{3} \times \frac{1}{2} &#x3D; \frac{1}{6}$，<br>于是由条件概率可得$\frac{\frac{1}{6}}{\frac{1}{2}} &#x3D; \frac{1}{3}$。</p><p>也可以考虑所有抛掷的结果：一共是三正三反，而三个正面朝上的情况中只有其中一个是来自于正常硬币，结果也是$\frac{1}{3}$。</p><h4 id="17"><a href="#17" class="headerlink" title="17."></a>17.</h4><p>设这批产品被接受为事件A，则$P(A) &#x3D; \frac{C_{96}^{5}}{C_{100}^5}$，那么被拒绝的概率就为$1 - P(A) \approx 0.19$</p><h4 id="18"><a href="#18" class="headerlink" title="18."></a>18.</h4><p>$P(A \cap B|B) &#x3D; \frac{P(A \cap B \cap B)}{P(B)} &#x3D; \frac{P(A \cap B)}{P(B)} &#x3D; P(A|B)$</p><p>证毕。</p><h4 id="19"><a href="#19" class="headerlink" title="19."></a>19.</h4><p>若$j \ne i$，则在第i个抽屉内找但没有找到的概率为$1 - p_id_i$（要么不在这个抽屉，要么在但是没有找到），而报告在第j个抽屉的概率为$p_j$，<br>故结果为$\frac{p_j}{1 - p_id_i}$。</p><p>若$j &#x3D; i$，在第i个抽屉内找但没有找到的概率同上，而报告恰好就在第i个抽屉并且没有被找到的概率为$P_i(1 - d_i)$，<br>故结果为$\frac{p_i(1 - d_i)}{1 - p_id_i}$</p><p>证毕。</p><h4 id="20"><a href="#20" class="headerlink" title="20."></a>20.</h4><h5 id="a-5"><a href="#a-5" class="headerlink" title="(a)"></a>(a)</h5><h5 id="i"><a href="#i" class="headerlink" title="(i)"></a>(i)</h5><p>进攻风格下，和局的概率为0，所以我们只需要考虑两种情况：<br>前两局鲍里斯均获胜，前两局平局，第三局鲍里斯获胜。</p><p>所以获胜概率为：$p_w^2 + p_w^2(1 - p_w)$。</p><h5 id="ii"><a href="#ii" class="headerlink" title="(ii)"></a>(ii)</h5><p>保守风格下，获胜的概率为0，所以获胜情况只有一种：<br>前两局和局，第三局获胜。</p><p>所以获胜概率为：$p_d^2p_w$。</p><h5 id="iii"><a href="#iii" class="headerlink" title="(iii)"></a>(iii)</h5><p>由全概率定理，整理出所有获胜情况：</p><ul><li><p>第一局获胜，第二局平局</p></li><li><p>第一局获胜，第二局失败，第三局获胜</p></li><li><p>第一局失败，第二局获胜，第三局获胜</p></li></ul><p>所以获胜概率为：$p_wp_d + p_w^2(1 - p_d) + p_w^2(1 - p_w)$</p><h5 id="b-5"><a href="#b-5" class="headerlink" title="(b)"></a>(b)</h5><p>由(a)(iii)可知，策略三的获胜概率为$p_w(p_d + 2p_w - p_w^2 - p_wp_d)$。</p><p>不难发现若$p_w &#x3D; 0.49$时，$\exists p_d \approx 0.56$，使得上式可以大于0.5。</p><h4 id="21"><a href="#21" class="headerlink" title="21."></a>21.</h4><ul><li><p>若在第一回合获胜，概率为$\frac{m}{m + n}$</p></li><li><p>若在第二回合获胜，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}\frac{m}{m + n - 2}$</p></li><li><p>若在第三回合获胜，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}\frac{n - 2}{m + n - 2}\frac{n - 3}{m + n - 3}\frac{m}{m + n - 4}$</p></li><li><p>...</p></li></ul><p>从上面的式子并不容易看出递推式来，但如果稍作一些改动：</p><ul><li><p>若第一回合前无法决出胜负，概率为$1$</p></li><li><p>若第二回合前无法决出胜负，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}$</p></li><li><p>若第三回合前无法决出胜负，概率为$\frac{n}{m + n}\frac{n - 1}{m + n - 1}\frac{n - 2}{m + n - 2}\frac{n - 3}{m + n - 3}$</p></li><li><p>...</p></li></ul><p>设第i回合前无法决出胜负的概率为$p_i$，那么我们可以得到$p_1 &#x3D; 1$，递推式$p_i &#x3D; p_{i - 1}\frac{n - 2i + 4}{m + n - 2i + 4}\frac{n - 2i + 3}{m + n -2i + 3}$</p><p>所以获胜概率为：$\sum_{i &#x3D; 1}^{1 + \lfloor \frac{n}{2} \rfloor}p_i \frac{m}{m + n - 2i + 2}$</p><h4 id="22"><a href="#22" class="headerlink" title="22."></a>22.</h4><p>当$k &#x3D; 1$时，取出白球的概率为$\frac{m}{m + n}$显然成立。</p><p>不妨假设当$k &#x3D; p$时，在第k个罐子里取出白球的概率依然是$\frac{m}{m + n}$，<br>则对于$k &#x3D; p + 1$时，由全概率定理， 在第k个罐子里取出白球的概率为</p><p>$\frac{m}{m + n}\frac{m + 1}{m + n + 1} + \frac{n}{m + n}\frac{n}{m + n + 1}$</p><p>整理后得到$\frac{m}{m + n}$，归纳完毕。</p><p>证毕。</p><h4 id="23"><a href="#23" class="headerlink" title="23."></a>23.</h4><p>不妨设罐子里的球的个数为n，分两种情况讨论：</p><ul><li><p>第一次交换两个球，第二次再交换回来，第三第四次亦如此，这样做的概率为$(\frac{1}{n})^4$</p></li><li><p>第一次第二次一共交换了两组球，第三第四次再交换回来，这样做的概率为$\frac{4(n - 1)^2}{n^6}$</p></li></ul><p>总的概率为二者之和。</p><h4 id="24"><a href="#24" class="headerlink" title="24."></a>24.</h4><p>$\frac{2}{3}$是先验概率，而$\frac{1}{2}$是后验概率。</p><h4 id="25"><a href="#25" class="headerlink" title="25."></a>25.</h4><p>不妨设两个信封内的钱数分别为S和L，且$L &gt; S$。</p><p>对于S，选中的概率为$\frac{1}{2}$，且只有抛硬币的次数不少于S次时才会换信封。<br>若第一次选中了该信封，则最后得到的钱数为L的概率为</p><p>$\frac{1}{2}(1 - \sum_{i &#x3D; 1}^{S - 1}(\frac{1}{2})^i) &#x3D; (\frac{1}{2})^S$</p><p>对于L，选中的概率也是$\frac{1}{2}$，且只有抛硬币的次数小于L次时才会选择不换信封。<br>若第一次选中了该信封，则最后得到的钱数为L的概率为</p><p>$\frac{1}{2}\sum_{i &#x3D; 1}^{L - 1}(\frac{1}{2})^i &#x3D; \frac{1}{2} - (\frac{1}{2})^L$</p><p>所以最后的概率为$\frac{1}{2} - (\frac{1}{2})^L + (\frac{1}{2})^S$。因为$L &gt; S$，所以这个概率大于$\frac{1}{2}$。</p><h4 id="26"><a href="#26" class="headerlink" title="26."></a>26.</h4><h5 id="a-6"><a href="#a-6" class="headerlink" title="(a)"></a>(a)</h5><p>由于A和B独立，由条件概率$P(A|B) &#x3D; \frac{p(1 - q)}{1 -q} &#x3D; p$。</p><h5 id="b-6"><a href="#b-6" class="headerlink" title="(b)"></a>(b)</h5><p>$P(C) &#x3D; q(p + \frac{1 - p}{2}) &#x3D; \frac{q(1 + p)}{2}$</p><p>由贝叶斯准则：$P(A|C) &#x3D; \frac{P(A)P(C|A)}{P(C)} &#x3D; \frac{2p}{p + 1}$</p><h4 id="27"><a href="#27" class="headerlink" title="27."></a>27.</h4><ul><li><p>当鲍勃的所有硬币均正面朝上时，爱丽丝无论怎么抛都是失败，一共有$2^n$种情况</p></li><li><p>当鲍勃有一枚硬币朝下时，对于鲍勃来说有$C^1_{n+1}$种可能，而对于爱丽丝来说，除了全正以外都是鲍勃获胜，故有$2^n - C^0_n$种情况</p></li><li><p>当鲍勃有两枚硬币朝下时，对于鲍勃来说有$C^2_{n+1}$种可能，对于爱丽丝来说，有$2^n - C^0_n - C^1_n$种情况</p></li><li><p>…</p></li></ul><p>综上，所有鲍勃获胜的情况的可能数一共是<br>$2^n + C_{n + 1}^1(2^n - C_n^0) + C_{n + 1}^2(2^n - C_n^0 - C_n^1) + \dots + C_{n + 1}^n(2^n - C_n^0 - \dots - C_n^{n - 1})$</p><p>上式化简后得到$2^{2n + 1} - 2^n - \sum_{i &#x3D; 1}^nC_{n + 1}^i(\sum_{j &#x3D; 0}^{i - 1}C_n^j)$。</p><p>注意到$2^{2n + 1} &#x3D; 2^{n + 1} \times 2^n &#x3D; (\sum_{i &#x3D; 0}^{n + 1}C_{n + 1}^i)(\sum_{i &#x3D; 0}^{n}C_n^i)$</p><p>所以$(\sum_{i &#x3D; 0}^{n + 1}C_{n + 1}^i)(\sum_{i &#x3D; 1}^{n}C_n^i) &#x3D; 2^{2n + 1} - 2^{n + 1}$</p><p>因为$\sum_{i &#x3D; 1}^{n}C_{n + 1}^i(\sum_{j &#x3D; 0}^{i - 1}C_n^j) &#x3D; \sum_{i &#x3D; 1}^{n}C_{n + 1}^i(\sum_{j &#x3D; i}^{n}C_n^j)$</p><p>所以<br>$2\sum_{i &#x3D; 1}^{n}C_{n + 1}^i(\sum_{j &#x3D; 0}^{i - 1}C_n^j) &#x3D; \sum_{i &#x3D; 1}^{n}C_{n + 1}^i(\sum_{j &#x3D; 0}^{i - 1}C_n^j) + \sum_{i &#x3D; 1}^{n}C_{n + 1}^i(\sum_{j &#x3D; i}^{n}C_n^j)$</p><p>上式就等于$(\sum_{i &#x3D; 1}^{n}C_{n + 1}^i)(\sum_{i &#x3D; 0}^{n}C_n^i)$，即$2^{2n + 1} - 2^{n + 1}$</p><p>带回原式可知鲍勃获胜的情况总共有$2^{2n}$种，所有情况一共有$2^{2n + 1}$种，故获胜的概率为$\frac{1}{2}$。</p><p>证毕。</p><h4 id="28"><a href="#28" class="headerlink" title="28."></a>28.</h4><p>略。</p><h4 id="29"><a href="#29" class="headerlink" title="29."></a>29.</h4><p>略。</p><h4 id="30"><a href="#30" class="headerlink" title="30."></a>30.</h4><p>如果两头猎犬均选择了正确方向，概率为$p^2$；<br>如果只有一头选择了正确方向，概率为$2p(1 - p)$，此时随机选择到正确方向的概率是$p(1 - p)$，<br>故在这个策略下选择正确方向的概率为$p^2 + p - p^2 &#x3D; p$，并不会比只让一条猎犬选择更优。</p><h4 id="31"><a href="#31" class="headerlink" title="31."></a>31.</h4><h5 id="a-7"><a href="#a-7" class="headerlink" title="(a)"></a>(a)</h5><p>由于不同信号之间独立，第k个信号正确传输的概率为$p(1 - \epsilon_0) + (1 - p)(1 - \epsilon_1)$</p><h5 id="b-7"><a href="#b-7" class="headerlink" title="(b)"></a>(b)</h5><p>1011一共含有三个1和一个0，且彼此独立，所以正确传输的概率为$(1 - \epsilon_0)(1 - \epsilon_1)^3$</p><h5 id="c-2"><a href="#c-2" class="headerlink" title="(c)"></a>(c)</h5><p>只有以下的情况数据可以被正常传输：</p><ul><li><p>3个0均没有错误，概率为$(1 - \epsilon_0)^3$</p></li><li><p>只有一个0发生错误，概率为$C^1_3\epsilon_0(1 - \epsilon_0)^2$</p></li></ul><p>综上，0被正确传输的概率为$(1 - \epsilon_0)^3 + 3\epsilon_0(1 - \epsilon_0)^2$</p><h5 id="d-1"><a href="#d-1" class="headerlink" title="(d)"></a>(d)</h5><p>将(c)中的式子变形为$1^3 - 3\epsilon_0^2(1 - \epsilon_0) - \epsilon_0^3 &#x3D; 1 - 3\epsilon_0^2 + 2\epsilon_0^3$</p><p>对上式求导得到$-6\epsilon_0 + 6\epsilon_0^2$,由于$0 \leq \epsilon_0 \leq 1$，所以函数在$\epsilon_0 &#x3D; 0$时取到最大值。<br>即：整个传输过程完全不要出错是坠吼的。</p><h5 id="e"><a href="#e" class="headerlink" title="(e)"></a>(e)</h5><p>由贝叶斯准则，对方发出的数据为0的概率为$\frac{P(0)}{P(1) + P(0)} &#x3D; \frac{\epsilon_0^2}{\epsilon_1 + \epsilon_0^2}$</p><h4 id="32"><a href="#32" class="headerlink" title="32."></a>32.</h4><ul><li><p>因为各次生育是独立的，所以国王的性别并不会影响他的兄弟姐妹，而生男的概率为$\frac{1}{2}$，故国王有一个兄弟的概率为$\frac{1}{2}$</p></li><li><p>国王的母亲一共有两个孩子，并且两个都是男性，所以国王有一个兄弟的概率为$\frac{1}{4}$</p></li></ul><h4 id="33"><a href="#33" class="headerlink" title="33."></a>33.</h4><p>投一次硬币已经没办法解决这个问题了，那我们考虑投两次。投两次硬币的全部概率为$1^2 &#x3D; (1 - p + p)^2 &#x3D; (1 - p)^2 + 2p(1 - p) + p^2$。注意到里面有一个2，<br>那么我们不妨投两次硬币，如果两次结果相同，则重新开始；剩余的两种情况每个人获胜的概率均为$\frac{1}{2}$。</p><h4 id="34"><a href="#34" class="headerlink" title="34."></a>34.</h4><ul><li><p>第一个子系统有效的概率为$p$</p></li><li><p>第二个子系统有效的概率为$p + 3p^2 - 5P^3 + 2p^4$</p></li><li><p>第三个子系统有效的概率为$2p - p^2$</p></li></ul><p>由于三个子系统独立，整个系统的有效概率为$p(p + 3p^2 - 5p^3 + 2p^4)(2p - p^2) &#x3D; 2p^3 + 5p^4 -13p^5 + 9p^6 -2p^7$</p><h4 id="35"><a href="#35" class="headerlink" title="35."></a>35.</h4><p>由独立性： $\sum_{i &#x3D; k}^nC^i_np^i(1 - p)^{n - i}$</p><h4 id="36"><a href="#36" class="headerlink" title="36."></a>36.</h4><h5 id="a-8"><a href="#a-8" class="headerlink" title="(a)"></a>(a)</h5><p>只有所有电厂均中断的时候全市才会停电，故由独立性可得：<br>$\Pi_{i &#x3D; 1}^np_i$</p><h5 id="b-8"><a href="#b-8" class="headerlink" title="(b)"></a>(b)</h5><p>如果所有电厂中断，或者只有不到三个电厂在供电，全市会处在停电状态。<br>设$P(i), P(i, j)$分别为仅第i个电厂工作的概率和仅第i，j个电厂在工作的概率，<br>即$P(i) &#x3D; \frac{(1 - p_i)\Pi_{j &#x3D; 1}^np_j}{p_i}, P(i, j) &#x3D; \frac{(1 - p_i)(1 - p_j)\Pi_{k &#x3D; 1}^np_k}{p_ip_j}$，<br>全市停电的概率为$\Pi_{i &#x3D; 1}^n + \sum_{i &#x3D; 1}^nP(i) + \sum_{1 \leq i &lt; j \leq n}P(i, j)$。</p><h4 id="37"><a href="#37" class="headerlink" title="37."></a>37.</h4><p>$\sum_{i &#x3D; 0}^{n_1}\sum_{j &#x3D; 0}^{n_2}\left[r_1i + r_2j &gt; c\right]C_{n_1}^ip_1^i(1 - p_1)^{n_1 - i}C_{n_2}^jp_2^j(1 - p_2)^{n_2 - j}$</p><h4 id="38"><a href="#38" class="headerlink" title="38."></a>38.</h4><p>考虑泰里思领先的概率$p_T$：只有在剩下的比赛中得到至少6分，才能保证泰里思领先。<br>所以$p_T &#x3D; \sum_{i &#x3D; 6}^8C_8^ip^i(1 - p)^{8 - i}$。</p><p>同理对于温迪来说，$P_W &#x3D; \sum_{i &#x3D; 4}^8C_8^i(1 - p)^ip^{8 - i} &#x3D; \sum_{i &#x3D; 0}^4C_8^ip^i(1 - p)^{8 - i}$。</p><p>所以泰里思可以分到的钱为$\frac{10p_T}{p_T + p_W} &#x3D; \frac{10p_T}{1 - p^5(1 - p)^3}$</p><h4 id="39"><a href="#39" class="headerlink" title="39."></a>39.</h4><p>设这天是好天气的概率为$p_w$，若当天每个学生的出勤概率为$t$，<br>则当天出勤人数不少于k的概率$P_k(t) &#x3D; \sum_{i &#x3D; k}^nC_n^it^i(1 - t)^{n - i}$，<br>那么教授可以讲课的概率为$p_wP_k(p_g) + (1 - p_w)P_k(p_b)$。</p><h4 id="40"><a href="#40" class="headerlink" title="40."></a>40.</h4><p>当$n &#x3D; 0$时，$p_n &#x3D; 1$。</p><p>当$n &gt; 0$时，$p_n &#x3D; p(1 - p_{n - 1}) + p_{n - 1}(1 - p)$。</p><p>所以$p_n &#x3D; p_{n - 1}(1 - 2p) + p$。令$p_n’ &#x3D; p_n - \frac{1}{2}$，则$p_n’ &#x3D; (1 - 2p)p_{n - 1}’$，<br>由等比数列通项公式：$p_n’ &#x3D; \frac{(1 - 2p)^n}{2}$，所以$p_n &#x3D; \frac{1 + (1 - 2p)^n}{2}$。</p><h4 id="41"><a href="#41" class="headerlink" title="41."></a>41.</h4><p>假设第一个人转出来的数字为$p$，则在某一回合内，这个人被淘汰的概率为$p$。<br>所以第一个人在第n个回合被淘汰的概率等于前n-1回合没被淘汰的概率乘上这个回合被淘汰的概率，<br>即$P(N &#x3D; n) &#x3D; (1 - p)^{n - 1}p$。</p><h4 id="42"><a href="#42" class="headerlink" title="42."></a>42.</h4><p>略。这个问题在第七章马尔可夫链中还会再遇到。</p><h4 id="43"><a href="#43" class="headerlink" title="43."></a>43.</h4><p>略。</p><h4 id="44"><a href="#44" class="headerlink" title="44."></a>44.</h4><h5 id="a-9"><a href="#a-9" class="headerlink" title="(a)"></a>(a)</h5><p>略。</p><h5 id="b-9"><a href="#b-9" class="headerlink" title="(b)"></a>(b)</h5><p>由(a)可知：若A和B独立，则$A$和$B^c$独立，<br>即$P(A \cap B^c) &#x3D; P(A)P(B^c)$。</p><p>对于事件$B^c$，由全概率公式：<br>$P(B^c) &#x3D; P(A \cap B^c) + P(A^c \cap B^c) &#x3D; P(A)P(B^c) + P(A^c \cap B^c)$，<br>所以$P(A^c \cap B^c) &#x3D; (1 - P(A))P(B^c) &#x3D; P(A^c)P(B^c)$，所以$A^c$和$B^c$独立。</p><p>证毕。</p><h4 id="45"><a href="#45" class="headerlink" title="45."></a>45.</h4><p>略。</p><h4 id="46"><a href="#46" class="headerlink" title="46."></a>46.</h4><p>略。</p><h4 id="47"><a href="#47" class="headerlink" title="47."></a>47.</h4><p>略。</p><h4 id="48"><a href="#48" class="headerlink" title="48."></a>48.</h4><p>略。</p><h4 id="49"><a href="#49" class="headerlink" title="49."></a>49.</h4><p>第一次投掷：</p><hr><p>  点数   1               2               3               4               5               6<br>  概率   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$   $\frac{1}{6}$</p><hr><p>第二次投掷：</p><hr><p>  点数和   2                3                4                5                6                7                8                9                10               11               12<br>  概率     $\frac{1}{36}$   $\frac{2}{36}$   $\frac{3}{36}$   $\frac{4}{36}$   $\frac{5}{36}$   $\frac{6}{36}$   $\frac{5}{36}$   $\frac{4}{36}$   $\frac{3}{36}$   $\frac{2}{36}$   $\frac{1}{36}$</p><hr><p>第三次投掷：</p><hr><p>  点数和   3                 4                 5                 6                  7                  8                 9                  10                 11                 12                 13                 14                 15                 16                17                18<br>  概率     $\frac{1}{216}$   $\frac{3}{216}$   $\frac{6}{216}$   $\frac{10}{216}$   $\frac{15}{216}$   $\frac{21}{36}$   $\frac{25}{216}$   $\frac{27}{216}$   $\frac{27}{216}$   $\frac{25}{216}$   $\frac{21}{216}$   $\frac{15}{216}$   $\frac{10}{216}$   $\frac{6}{216}$   $\frac{3}{216}$   $\frac{1}{216}$</p><hr><p>从上表来看和为11的概率要大于和为12。</p><h4 id="50"><a href="#50" class="headerlink" title="50."></a>50.</h4><p>如果$n &gt; 365$，那么根据鸽巢原理，一定有两个人的生日在同一天，所以这个概率是0。</p><p>如果$1 &lt; n \leq 365$，那么没有任何两个人在同一天生日的概率为$\frac{C_{365}^n}{365^n}$（每个人选其中的某一天作为自己的生日，可选的所有可能是$365^n$种）。</p><h4 id="51"><a href="#51" class="headerlink" title="51."></a>51.</h4><h5 id="a-10"><a href="#a-10" class="headerlink" title="(a)"></a>(a)</h5><p>样本空间：抽走两个红球（设为事件A），抽走两个白球（设为事件B），抽走一个红球和一个白球（设为事件C）。</p><p>基于离散均匀分布律的计数方法，我们有：<br>$$P(A) &#x3D; \frac{C_m^2}{C_{m + n}^2}, P(B) &#x3D; \frac{C_n^2}{C_{m + n}^2},<br>             P(C) &#x3D; \frac{C_m^1C_n^1}{C_{m + n}^2}$$</p><p>设第一次抽到红球为事件D，第一次抽到白球为事件E，基于乘积规则，我们也可以得到：<br>$$P(D) &#x3D; \frac{m}{m + n}, P(E) &#x3D; \frac{n}{m + n}$$</p><p>那么：<br>$$P(A) &#x3D; \frac{m - 1}{m + n - 1}P(D) &#x3D; \frac{C_m^2}{C_{m + n}^2}$$<br>$$P(B) &#x3D; \frac{n - 1}{m + n - 1}P(E) &#x3D; \frac{C_n^2}{C_{m + n}^2}$$<br>$$P(C) &#x3D; \frac{m}{m + n - 1}P(E) + \frac{n}{m + n - 1}P(D) &#x3D; \frac{C_m^1C_n^1}{C_{m + n}^2}$$</p><h5 id="b-10"><a href="#b-10" class="headerlink" title="(b)"></a>(b)</h5><ul><li><p>出现1的时候，全是红球的概率为$\frac{C_m^1}{C_{m + n}^1}$</p></li><li><p>出现2的时候，全是红球的概率为$\frac{C_m^2}{C_{m + n}^2}$</p></li><li><p>出现3的时候，全是红球的概率为$\frac{C_m^3}{C_{m + n}^3}$</p></li></ul><p>由全概率公式，取出的球全是红色的概率为$\frac{1}{3}(\frac{C_m^1}{C_{m + n}^1} + \frac{C_m^2}{C_{m + n}^2} + \frac{C_m^3}{C_{m + n}^3})$。</p><h4 id="52"><a href="#52" class="headerlink" title="52."></a>52.</h4><p>第13张牌正好是老K，意味着前12张牌均不能是老K，故概率为$\frac{C_{48}^{12}}{C_{52}^{12}}\frac{C_4^1}{C_{40}^{1}}$。</p><h4 id="53"><a href="#53" class="headerlink" title="53."></a>53.</h4><p>我们把90个学生排成一排，前30个学生在一个班，中间30个学生在一个班，最后30个学生在一个班。这样所有的可能有$90!$种。<br>对于乔和简，他们两个人可以先确定在三个班中的某一个，然后在这个班的30个位置中任意占据两个，其他人再随机排列。<br>这样满足条件的排列数是$C_3^1C_{30}^298!$。所以分到一个班的概率为$\frac{C_3^1C_{30}^298!}{90!}$。</p><h4 id="54"><a href="#54" class="headerlink" title="54."></a>54.</h4><h5 id="a-11"><a href="#a-11" class="headerlink" title="(a)"></a>(a)</h5><p>$\frac{20!}{10!10!} &#x3D; 184756‬$</p><h5 id="b-11"><a href="#b-11" class="headerlink" title="(b)"></a>(b)</h5><p>不难发现只有两种排列方式满足要求（”美国车，外国车，美国车，…，外国车”和”外国车，美国车，外国车，…，美国车”）。<br>所以这个概率为$\frac{2}{184756}$。</p><h4 id="55"><a href="#55" class="headerlink" title="55."></a>55.</h4><p>总共可以摆放的方案一共是$C_{64}^8$种。考虑合法的情况，每一个车一定单独占据一行，第一行的车有8种选择，第二行的车只有7种选择……<br>所以合法的概率为$\frac{8!}{C_{64}^8}$。</p><h4 id="56"><a href="#56" class="headerlink" title="56."></a>56.</h4><h5 id="a-12"><a href="#a-12" class="headerlink" title="(a)"></a>(a)</h5><p>$C_8^4C_{10}^3 &#x3D; 1680$</p><h5 id="b-12"><a href="#b-12" class="headerlink" title="(b)"></a>(b)</h5><ul><li><p>假定所选的所有高水平课程均在$H_1, \dots, H_5$中，那么所有的可能有$C_7^3C_5^3 &#x3D; 350$种；</p></li><li><p>假定所选的所有高水平课程均在$H_6, \dots, H_10$中，那么所有的可能有$C_6^2C_5^3 &#x3D; 150$种；</p></li><li><p>假设二者均有涉及，那么三门先修课程均需要开设，所有的可能有$C_5^1\sum_{i &#x3D; 1}^2C_5^iC_5^{3 - i} &#x3D; 500$种</p></li></ul><p>所以一共有1000种。</p><h4 id="57"><a href="#57" class="headerlink" title="57."></a>57.</h4><p>将问题换一个方式陈述：将一个26个不同字母随机排列组成的字符串划分为6个部分，有多少种划分方式？</p><p>26个字母的总排列数是$26!$，将长度为26的字符串划分为6个部分，只需要在其中的25个间隔处插入空格即可，且不能有连续的空格。<br>所以最后的结果为$26!C_{25}^5$。</p><h4 id="58"><a href="#58" class="headerlink" title="58."></a>58.</h4><p>接下来的问题中我们假设一共有52张牌。</p><h5 id="a-13"><a href="#a-13" class="headerlink" title="(a)"></a>(a)</h5><p>$\frac{C_4^3C_{48}^4}{C_{52}^7}$</p><h5 id="b-13"><a href="#b-13" class="headerlink" title="(b)"></a>(b)</h5><p>$\frac{C_4^2C_{48}^5}{C_{52}^7}$</p><h5 id="c-3"><a href="#c-3" class="headerlink" title="(c)"></a>(c)</h5><p>$\frac{C_4^3C_{48}^4}{C_{52}^7} + \frac{C_4^2C_{48}^5}{C_{52}^7} - \frac{C_4^2C_4^3C_{44}^2}{C_{52}^7}$</p><h4 id="59"><a href="#59" class="headerlink" title="59."></a>59.</h4><p>$\frac{C_k^nC_{100 - k}^{m - n}}{C_{100}^m}$</p><p>柠檬法案：柠檬法（Lemon<br>Laws）是一种美国的消费者保护法，主要是在保障汽车买主的权益。柠檬法的名称起源于美国经济学家乔治·阿克罗夫（George<br>A.<br>Akerlof）所发表的一篇经济学论文，因为这缘故，对于出厂后有瑕疵问题的汽车，通常也会称呼其为柠檬车（Lemon<br>Car）或直接就称为柠檬。</p><h4 id="60"><a href="#60" class="headerlink" title="60."></a>60.</h4><p>我们可以使用类似于53题的思路，得到$\frac{13^4 \times \frac{48!}{(4!)^{12}}}{\frac{52!}{(4!)^{13}}} \approx 0.105$</p><h4 id="61"><a href="#61" class="headerlink" title="61."></a>61.</h4><p>略。</p><h4 id="62"><a href="#62" class="headerlink" title="62."></a>62.</h4><p>略。</p><h3 id="第二章-离散随机变量"><a href="#第二章-离散随机变量" class="headerlink" title="第二章 离散随机变量"></a>第二章 离散随机变量</h3><h4 id="1-1"><a href="#1-1" class="headerlink" title="1."></a>1.</h4><hr><p>  得分   0      1      2      3      4<br>  概率   0.18   0.27   0.34   0.14   0.07</p><hr><h4 id="2-1"><a href="#2-1" class="headerlink" title="2."></a>2.</h4><p>假设有k个人与你的生日相同，由二项分布：<br>$$P_X(k) &#x3D; C_{500}^kp^k(1 - p)^{500 - k}$$<br>其中$p &#x3D; \frac{1}{365}$，则有人生日与你相同的概率为$1 - P_X(0) &#x3D; 1 - (1 - \frac{1}{365})^{500} \approx 0.75$。</p><p>若使用泊松分布逼近，$\lambda &#x3D; np &#x3D; \frac{500}{365}$，<br>那么$1 - P_X(0) &#x3D; 1 - e^{-\lambda}\frac{\lambda^0}{0!} \approx 0.75$。</p><h4 id="3-1"><a href="#3-1" class="headerlink" title="3."></a>3.</h4><h5 id="a-14"><a href="#a-14" class="headerlink" title="(a)"></a>(a)</h5><p>设k为比赛连续平局的局数，则$P(k) &#x3D; 0.3^k$，那么赢得比赛的概率为$0.4\sum_{i &#x3D; 0}^9P(i) &#x3D; 0.5714251972‬$。</p><h5 id="b-14"><a href="#b-14" class="headerlink" title="(b)"></a>(b)</h5><p>下棋的前9局数满足几何分布，所以$P_X(k) &#x3D; (1 - p)^{k - 1}p &#x3D; 0.3^{k - 1}0.7$，<br>第10局后比赛一定结束， 所以分布列为：</p><hr><p>  局数   1     2      3       4         5<br>  概率   0.7   0.21   0.063   0.0189‬   0.00567‬</p><hr><hr><p>  局数   6           7            8             9              10<br>  概率   0.001701‬   0.0005103‬   0.00015309‬   0.000045927‬   0.000019683‬</p><hr><h4 id="4-1"><a href="#4-1" class="headerlink" title="4."></a>4.</h4><h5 id="a-15"><a href="#a-15" class="headerlink" title="(a)"></a>(a)</h5><p>在使用者的数量小于等于50的时候，调制解调器的数量分布满足二项分布；<br>使用者数量大于50的时候，由于调制解调器数量不足，故一定是50个。</p><p>所以分布列为：</p><hr><p>  人数   $k \leq 50$                         $k &gt; 50$<br>  概率   $C_{1000}^k0.01^k0.99^{1000 - k}$   $1 - \sum_{i &#x3D; 0}^{50}C_{1000}^i0.01^i0.99^{1000 - i}$</p><hr><h5 id="b-15"><a href="#b-15" class="headerlink" title="(b)"></a>(b)</h5><p>设使用者人数为k，由泊松分布有$P_X(k) &#x3D; e^{-\lambda}\frac{\lambda^k}{k!} &#x3D; e^{-10}\frac{10^k}{k!}$</p><h5 id="c-4"><a href="#c-4" class="headerlink" title="(c)"></a>(c)</h5><p>利用精确分布，$P &#x3D; 1 - \sum_{i &#x3D; 0}^{50}C_{1000}^i0.01^i0.99^{1000 - i}$，而利用泊松分布，$P &#x3D; 1 - \sum_{i &#x3D; 0}^{50}e^{-10}\frac{10^i}{i!}$</p><h4 id="5-1"><a href="#5-1" class="headerlink" title="5."></a>5.</h4><h5 id="a-16"><a href="#a-16" class="headerlink" title="(a)"></a>(a)</h5><p>第一时段结束时：</p><hr><p>  信息包数量k   $k &lt; b$                              $k &#x3D; b$<br>  概率          $e^{-\lambda}\frac{\lambda^k}{k!}$   $1 - \sum_{i &#x3D; 0}^{b - 1}e^{-\lambda}\frac{\lambda^i}{i!}$</p><hr><p>第二时段结束时：</p><hr><p>  信息包数量k   $k &#x3D; 0$                                             $k &gt; 0$<br>  概率          $\sum_{i &#x3D; 0}^c e^{-\lambda}\frac{\lambda^i}{i!}$   $\sum_{i &#x3D; c + 1}^{b - 1} e^{-\lambda}\frac{\lambda^i}{i!} + 1 - \sum_{i &#x3D; 0}^{b - 1}e^{-\lambda}\frac{\lambda^i}{i!}$</p><hr><h5 id="b-16"><a href="#b-16" class="headerlink" title="(b)"></a>(b)</h5><p>如果到达的信息包的数量不超过b，是不会发生丢包的，<br>所以丢包的概率为$1 - \sum_{i &#x3D; 0}^{b}e^{-\lambda}\frac{\lambda^i}{i!}$。</p><h4 id="6-1"><a href="#6-1" class="headerlink" title="6."></a>6.</h4><h5 id="a-17"><a href="#a-17" class="headerlink" title="(a)"></a>(a)</h5><p>对于$n &#x3D; 5$，凯尔特人队获胜的概率为$p^3 + C_3^1p^3(1 - p) + C_4^2p^3(1 - p)^2 &#x3D; 10p^3 - 15p^4 + 6p^5$；</p><p>对于$n &#x3D; 3$，凯尔特人队获胜的概率为$p^2 + C_2^1p^2(1 - p) &#x3D; 3p^2 - 2p^3$</p><p>所以当$10p^3 - 15p^4 + 6p^5 &gt; 3p^2 - 2p^3$，时，$n &#x3D; 5$比$n &#x3D; 3$合算。<br>将所有式子移到左侧，消去$p^2(p &gt; 0)$并求导得到$18p^2 - 30p + 12$，不难发现当$p &#x3D; \frac{1}{2}$或$p &#x3D; 1$时，原式为0，<br>且该函数在$[0.5, 1]$上先增后减，所以当$p \in (0.5, 1)$时$n &#x3D; 5$比$n &#x3D; 3$合算。</p><h5 id="b-17"><a href="#b-17" class="headerlink" title="(b)"></a>(b)</h5><p>不难得到$P_N(n &#x3D; 2k + 1) &#x3D; p^{k + 1} + C_{k + 1}^1P^{k+1}(1 - p) + \dots + C_{2k}^kp^{k + 1}(1 - p)^k$，<br>并且对于不同的k，$P_N(0) &#x3D; 0, P_N(1) &#x3D; 1, P_N(\frac{1}{2}) &#x3D; \frac{1}{2}$恒成立（0和1十分显然，对于0.5的证明我们额外放在(c)部分讲解）。</p><p>不妨做差$P_N(2k + 3) - P_N(2k + 1)$并对这个式子求导，不难验证导数在$\frac{1}{2}$处大于0。<br>由罗尔定理导函数在$[0.5, 1]$之间一定存在值为0的点，且仅有一个（这是一个$2k + 1$重多项式，其中0为k重根，1为k重根，$\frac{1}{2}$为1重根）。<br>所以导函数在区间上先增后减，原来的差值函数保证大于0，故$p \in (0.5, 1)$。</p><h5 id="c-5"><a href="#c-5" class="headerlink" title="(c)"></a>(c)</h5><p>原题不存在，这部分是(b)中$P_N(n)$在$p &#x3D; \frac{1}{2}$为常数的证明。</p><p>因为$P_N(n &#x3D; 2k + 1) &#x3D; p^{k + 1} + C_{k + 1}^1p^{k+1}(1 - p) + \dots + C_{2k}^kp^{k + 1}(1 - p)^k$,<br>将$\frac{1}{2}$带入后$P_N(2k + 1) &#x3D; (\frac{1}{2})^{k + 1} + C_k^1(\frac{1}{2})^{k + 2} + \dots + C_{2k}^k(\frac{1}{2})^{2k + 1}$。</p><p>当$k &#x3D; 0$时，原式显然成立。假定$k &#x3D; m$时原式也成立，则$k &#x3D; m + 1$时，<br>我们不妨计算$2^{k+2}P_N(2m + 3), 2^{k + 1}P_N(2m + 1)$，将两个式子逐项做差，得到$2^{k + 1}P_N(2m + 1)$，<br>则$2^{k+2}P_N(2m + 3) &#x3D; 2^{k + 2}P_N(2m + 1)$，故原式也为$\frac{1}{2}$，归纳完毕。</p><p>证毕。</p><h4 id="7-1"><a href="#7-1" class="headerlink" title="7."></a>7.</h4><h5 id="a-18"><a href="#a-18" class="headerlink" title="(a)"></a>(a)</h5><h5 id="1-2"><a href="#1-2" class="headerlink" title="(1)"></a>(1)</h5><p>由于失败后会做记号，所以尝试的次数最多只有四次（第四次即便失败，剩下的钥匙也没必要再试一次了）。</p><p>分布列：</p><hr><p>  尝试次数   1               2               3               4<br>  概率       $\frac{1}{5}$   $\frac{1}{5}$   $\frac{1}{5}$   $\frac{2}{5}$</p><hr><h5 id="2-2"><a href="#2-2" class="headerlink" title="(2)"></a>(2)</h5><p>随机尝试的情况下，尝试次数满足几何分布，所以分布律为：$P(k) &#x3D; (\frac{4}{5})^{k - 1}(\frac{1}{5})^k$。</p><h5 id="b-18"><a href="#b-18" class="headerlink" title="(b)"></a>(b)</h5><h5 id="1-3"><a href="#1-3" class="headerlink" title="(1)"></a>(1)</h5><hr><p>  尝试次数   1               2                3                4                5               6                7                8<br>  概率       $\frac{1}{5}$   $\frac{8}{45}$   $\frac{7}{45}$   $\frac{2}{15}$   $\frac{1}{9}$   $\frac{4}{45}$   $\frac{1}{15}$   $\frac{1}{15}$</p><hr><h5 id="2-3"><a href="#2-3" class="headerlink" title="(2)"></a>(2)</h5><p>$P(k) &#x3D; (\frac{4}{5})^{k - 1}(\frac{1}{5})^k$</p><h4 id="8-1"><a href="#8-1" class="headerlink" title="8."></a>8.</h4><p>二项随机变量满足$P_X(k) &#x3D; C_n^kp^k(1 - p)^{n - k}$。</p><p>所以 $$P_X(k + 1) &#x3D; C_n^{k + 1}p^{k + 1}(1 - p)^{n - k - 1} &#x3D;$$<br>$$\frac{n!}{(k + 1)!(n - k -1)!}p^{k + 1}(1 - p)^{n - k - 1} &#x3D;$$<br>$$\frac{n - k}{k + 1}\frac{n!}{k!(n - k)!}\frac{p}{1 - p}p^k(1 - p)^{n - k} &#x3D; \frac{p}{1 - p}\frac{n - k}{k + 1}P_X(k)$$</p><p>证毕。</p><h4 id="9-1"><a href="#9-1" class="headerlink" title="9."></a>9.</h4><p>由第8题，我们可以得到：<br>$P_X(k + 1) - P_X(k) &#x3D; (\frac{p}{1 - p}\frac{n - k}{k + 1} - 1)P_X(k)$。</p><p>由于$P_X(k) \geq 0$，我们不妨令$f(k) &#x3D; \frac{p}{1 - p}\frac{n - k}{k + 1} - 1$，<br>则$f’(k) &#x3D; \frac{-p(n + 1)}{(1 - p)(k + 1)^2}$。</p><p>不难发现导函数恒为负，且当$k &#x3D; p(n + 1) - 1$时，原函数为0。所以当$k &lt;&#x3D; (n + 1)p$时，$P_X(k)$是非降的，而在$k &gt; (n + 1)p$时是单调递减的。</p><p>证毕。</p><h4 id="10-1"><a href="#10-1" class="headerlink" title="10."></a>10.</h4><p>因为泊松分布$P_X(k) &#x3D; e^{-\lambda}\frac{\lambda^k}{k!}$，<br>所以$P_X’(k) &#x3D; \frac{\lambda^ke^{-\lambda}}{(k!)^2}(ln\lambda\Gamma(k + 1) - \Gamma’(k + 1))$。</p><p>由于括号外的部分恒正，我们解括号内的部分等于0，即$\frac{\Gamma’(k + 1)}{\Gamma(k + 1)} &#x3D; \Psi(k + 1) &#x3D; ln\lambda$。</p><p>解得近似解为$k &#x3D; \lambda$。由于$\Psi(x)$是单调递增的，故导函数在$[0, \lambda]$上大于0，在$(\lambda, \infty)$上小于0。</p><p>也可以简单地通过$\frac{P_X(k + 1)}{P_X(k)} &#x3D; \frac{\lambda}{k + 1}$看出单调性。</p><p>证毕。</p><h4 id="11-1"><a href="#11-1" class="headerlink" title="11."></a>11.</h4><p>略。</p><h4 id="12-1"><a href="#12-1" class="headerlink" title="12."></a>12.</h4><p>略。</p><h4 id="13-1"><a href="#13-1" class="headerlink" title="13."></a>13.</h4><hr><p>  数量   2                3                4                 5                 6                7<br>  概率   $\frac{1}{32}$   $\frac{5}{32}$   $\frac{10}{32}$   $\frac{10}{32}$   $\frac{5}{32}$   $\frac{1}{32}$</p><hr><h4 id="14-1"><a href="#14-1" class="headerlink" title="14."></a>14.</h4><h5 id="a-19"><a href="#a-19" class="headerlink" title="(a)"></a>(a)</h5><hr><p>  Y   0               1                2<br>  P   $\frac{2}{5}$   $\frac{3}{10}$   $\frac{3}{10}$</p><hr><h5 id="b-19"><a href="#b-19" class="headerlink" title="(b)"></a>(b)</h5><hr><p>  Y   0               1               2                5<br>  P   $\frac{1}{5}$   $\frac{1}{5}$   $\frac{1}{10}$   $\frac{1}{2}$</p><hr><h4 id="15-1"><a href="#15-1" class="headerlink" title="15."></a>15.</h4><p>因为$Y &#x3D; ln(X), X &#x3D; a^{|k|}$， 所以$Y &#x3D; |K|lna$。<br>又因为$K$是$[-n, n]$上的均匀分布，<br>所以$P_Y(y &#x3D; |k|lna) &#x3D; {\begin{array}{lcr}<br>            \frac{2}{2n + 1} &amp; &amp; k \ne 0\<br>            \frac{1}{2n + 1} &amp; &amp; k &#x3D; 0<br>        \end{array}$</p><h4 id="16-1"><a href="#16-1" class="headerlink" title="16."></a>16.</h4><h5 id="a-20"><a href="#a-20" class="headerlink" title="(a)"></a>(a)</h5><p>由归一性可知$2(P_X(3) + P_X(2) +P_X(1)) + P_X(0) &#x3D; 1$，<br>解之得$a &#x3D; \frac{1}{28}$。</p><p>由于$P_X$及其定义域关于$x &#x3D; 0$对称，所以$E[x] &#x3D; 0$。</p><h5 id="b-20"><a href="#b-20" class="headerlink" title="(b)"></a>(b)</h5><p>因为$E[x] &#x3D; 0$，所以$Z &#x3D; X^2$， 故$P_Z(z) &#x3D; {\begin{array}{lcr}<br>                \frac{2z}{28} &amp; &amp; z &#x3D; 0, 1, 4, 9 \<br>                0 &amp; &amp; otherwise<br>            \end{array}$</p><h5 id="c-6"><a href="#c-6" class="headerlink" title="(c)"></a>(c)</h5><p>由(b)得，$var(X) &#x3D; E[Z] &#x3D; \frac{2}{28} + \frac{32}{28} + \frac{162}{28} &#x3D; 7$。</p><h5 id="d-2"><a href="#d-2" class="headerlink" title="(d)"></a>(d)</h5><p>$var(X) &#x3D; \sum_{x}(x - E[X])^2p_X(x) &#x3D; 2(\frac{1}{28} + \frac{16}{28} + \frac{81}{28}) &#x3D; 7$</p><h4 id="17-1"><a href="#17-1" class="headerlink" title="17."></a>17.</h4><p>因为$F &#x3D; 1.8C + 32$，所以$E[F] &#x3D; 1.8E[C] + 32 &#x3D; 40$，$var(F) &#x3D; 1.8^2var(C) &#x3D; 324$，所以$\sigma_F &#x3D; 18$。</p><h4 id="18-1"><a href="#18-1" class="headerlink" title="18."></a>18.</h4><p>$E[X] &#x3D; \int_a^b\frac{2^x}{b - a}dx &#x3D; \frac{2^b - 2^a}{(b - a)ln2}$</p><p>$var(X) &#x3D; \int_a^b(2^x - E[X])^2 &#x3D; \frac{2^{2b} - 2^{2a}}{2ln2} - \frac{b - a}{ln2} + (2^b - 2^a)^2(b - a)$</p><h4 id="19-1"><a href="#19-1" class="headerlink" title="19."></a>19.</h4><p>略。</p><h4 id="20-1"><a href="#20-1" class="headerlink" title="20."></a>20.</h4><p>设需要购买的数量为随机变量X，则X满足几何分布。所以$P_X(x) &#x3D; (1 - p)^{x - 1}p$，<br>故期望$E[X] &#x3D; \sum_{x &#x3D; 1}^{\infty}x(1 - p)^{x - 1}p$，计算$E[x] - (1 - p)E[x]$后求得期望为$\frac{1}{p}$。</p><p>首先计算$E[X^2]$，然后使用$var(X) &#x3D; E[X^2] - E[X]^2$计算方差。<br>因为$\E[X^2] &#x3D; \sum_{x &#x3D; 1}^{\infty}x^2(1 - p)^{x - 1}p$，<br>不难发现可以利用积分得到类似$E[X]$的形式。最后求得$E[X^2] &#x3D; \frac{2 - p}{p^2}$，<br>故$var(X) &#x3D; \frac{1 - p}{p^2}$。</p><p>几何分布的期望也可以通过第6节的全期望定理完成。</p><h4 id="21-1"><a href="#21-1" class="headerlink" title="21."></a>21.</h4><p>由题意得，这是一个几何分布，所以$P_N(n) &#x3D; (\frac{1}{2})^n$。<br>于是我们可以求得期望$E[X] &#x3D; \sum_{x &#x3D; 1}^{\infty}(\frac{1}{2})^x2^x &#x3D; \infty$。</p><p>这个级数并不收敛，所以理论上你可以得到无穷多的钱，但是事实上这并不可能发生。</p><h4 id="22-1"><a href="#22-1" class="headerlink" title="22."></a>22.</h4><h5 id="a-21"><a href="#a-21" class="headerlink" title="(a)"></a>(a)</h5><p>$$P_X(x) &#x3D; (1 - p - q + 2pq)^{x - 1}(p + q - 2pq)$$<br>$$E[X] &#x3D; \frac{1}{p + q -2pq}$$<br>$$var(X) &#x3D; \frac{1 - p - q + 2pq}{(p + q - 2pq)^2}$$</p><h5 id="b-21"><a href="#b-21" class="headerlink" title="(b)"></a>(b)</h5><p>由条件概率得： $\frac{p(1 - q)}{p + q - 2pq}$</p><h4 id="23-1"><a href="#23-1" class="headerlink" title="23."></a>23.</h4><h5 id="a-22"><a href="#a-22" class="headerlink" title="(a)"></a>(a)</h5><p>抛掷k次，前$k - 1$次必定是正反交替，最后一次和倒数第二次相同。抛掷k次结束可以有两种情况（最后一次为正面和最后一次为反面），<br>所以分布列为$P_X(k) &#x3D; (\frac{1}{2})^{k - 1}$。</p><p>期望值$E[X] &#x3D; \sum_{x &#x3D; 2}^{\infty}x(\frac{1}{2})^{x - 1}$，利用积分可得$E[X] &#x3D; 3$。</p><p>同理方差$var(X) &#x3D; E[X^2] - E[X]^2 &#x3D; 12 - 9 &#x3D; 3$。</p><p>这个问题在条件一节的习题33中有对任意p值下期望值公式的推广。</p><h5 id="b-22"><a href="#b-22" class="headerlink" title="(b)"></a>(b)</h5><p>抛掷k次，一定先出现若干次（可以为0）反面朝上，接下来若干次正面朝上I（至少有一次），最后一次反面朝上。<br>所以分布列为$P_X(k) &#x3D; (k - 1)(\frac{1}{2})^k$。</p><p>期望值$E[X] &#x3D; \sum_{x &#x3D; 2}^{\infty}x(x - 1)(\frac{1}{2})^x &#x3D; 4$。</p><p>方差$var(X) &#x3D; E[X^2] - E[X]^2 &#x3D; 80 - 16 &#x3D; 64$</p><h4 id="24-1"><a href="#24-1" class="headerlink" title="24."></a>24.</h4><h5 id="a-23"><a href="#a-23" class="headerlink" title="(a)"></a>(a)</h5><p>由题意得：X，Y是$-2 \leq x \leq 4, x - 1 \leq y \leq x + 1$上的均匀分布，所以：</p><hr><p>  Y&#x2F;X   -2               -1               0                1                2                3                4<br>  -3    $\frac{1}{21}$   0                0                0                0                0                0<br>  -2    $\frac{1}{21}$   $\frac{1}{21}$   0                0                0                0                0<br>  -1    $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0                0                0                0<br>  0     0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0                0                0<br>  1     0                0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0                0<br>  2     0                0                0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$   0<br>  3     0                0                0                0                $\frac{1}{21}$   $\frac{1}{21}$   $\frac{1}{21}$<br>  4     0                0                0                0                0                $\frac{1}{21}$   $\frac{1}{21}$<br>  5     0                0                0                0                0                0                $\frac{1}{21}$</p><hr><p>所以边缘分布列$$P_X(x) &#x3D; \left{\begin{array}{lcr}<br>                \frac{1}{7} &amp; &amp; -2 \leq x \leq 4 \<br>                0 &amp; &amp; otherwise<br>            \end{array}\right.$$</p><p>$$P_Y(y) &#x3D; \left{\begin{array}{lcr}<br>                \frac{1}{21} &amp; &amp; y &#x3D; -3&#x2F;y &#x3D; 5\<br>                \frac{2}{21} &amp; &amp; y &#x3D; -2&#x2F;y &#x3D; 4\<br>                \frac{1}{7} &amp; &amp; -1 \leq y \leq 3\<br>                0 &amp; &amp; otherwise<br>            \end{array}\right.$$</p><p>期望$E[X] &#x3D; 1, E[Y] &#x3D; 1$。</p><h5 id="b-23"><a href="#b-23" class="headerlink" title="(b)"></a>(b)</h5><p>$$E[100X + 200Y] &#x3D; \sum_{(x, y)}(100x + 200y)P_{X, Y}(x, y) &#x3D; \frac{8200}{21} \approx 390$$</p><h4 id="25-1"><a href="#25-1" class="headerlink" title="25."></a>25.</h4><h5 id="a-24"><a href="#a-24" class="headerlink" title="(a)"></a>(a)</h5><p>因为答案被选中的可能性相等，故$P_{I, J}$满足均匀分布，所以<br>$$P_{I, J}(i, j) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{1}{\sum_{k &#x3D; 1}^{n}m_k} &amp; &amp; 0 &lt; j \leq m_i\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>                \right.$$</p><p>不难得到边缘分布列$P_I(i) &#x3D; \frac{m_i}{\sum_{k &#x3D; 1}^{n}m_k}, P_J(j) &#x3D; \frac{\sum_{k &#x3D; 1}^{n}[j \leq m_k]}{\sum_{k &#x3D; 1}^{n}m_k}$。</p><h5 id="b-24"><a href="#b-24" class="headerlink" title="(b)"></a>(b)</h5><p>对于第i个学生，可以将其回答情况视为$m_i$个独立的题目的得分期望的和，所以总分的期望值为$\sum_{k &#x3D; 0}^{m_i}ap_{i, k} + b(1 - p_{i, k})$。</p><h4 id="26-1"><a href="#26-1" class="headerlink" title="26."></a>26.</h4><h5 id="a-25"><a href="#a-25" class="headerlink" title="(a)"></a>(a)</h5><p>因为最低分不小于x的分布列$P(X_1 \geq x, X_2 \geq x, X_3 \geq x) &#x3D; \Pi_{i &#x3D; 1}^3\frac{111 - x}{10}$</p><p>所以$P_X(x) &#x3D; (\frac{111 - x}{10})^3 - (\frac{110 - x}{10})^3$</p><h5 id="b-25"><a href="#b-25" class="headerlink" title="(b)"></a>(b)</h5><p>三天的平均得分为$\frac{x_1 + x_2 + x_3}{3}$，而X的期望$E[X] &#x3D; \sum_{i &#x3D; 101}^{110}x((\frac{111 - x}{10})^3 - (\frac{110 - x}{10})^3)$。</p><p>平均分的期望值为$105.5$，而X的期望为$103.025$。</p><h4 id="27-1"><a href="#27-1" class="headerlink" title="27."></a>27.</h4><p>略。</p><h4 id="28-1"><a href="#28-1" class="headerlink" title="28."></a>28.</h4><p>换一种方式陈述书上的答案，使得答案更加容易理解。</p><p>我们如果按照原来的顺序答题，奖金的期望$E[L] &#x3D; p_1v_1 + p_1p_2v_2 + \dots + p_1p_2\dots p_nv_n$。</p><p>假定我们尝试着交换相邻两道题的顺序（交换任意两道题的顺序可以通过若干次相邻交换实现），<br>则交换后的期望值变为$E[L’] &#x3D; p_1v_1 + \dots + p_1p_2\dots p_{k + 1}v_{k + 1} + p_1p_2\dots p_kp_{k + 1}v_k + \dots + p_1p_2\dots p_nv_n$。<br>将两个式子做差，可以得到$p_1p_2\dots p_{k - 1}(p_kv_k + p_kp_{k + 1}v_{k + 1} - p_{k + 1}v_{k + 1} - p_kp_{k + 1}v_k)$。括号外的式子一定为正，<br>我们只看括号内：$p_kv_k(1 - p_{k + 1}) + p_{k + 1}v_{k + 1}(p_k - 1)$，提出一个$(1 - p_{k + 1})(1 - p_{k})$，得到<br>$\frac{p_kv_k}{1 - p_k} - \frac{p_{k + 1}v_{k + 1}}{1 - p_{k + 1}}$。所以先选$\frac{p_iv_i}{1 - p_i}$是明智的。</p><p>证毕。</p><h4 id="29-1"><a href="#29-1" class="headerlink" title="29."></a>29.</h4><p>略。</p><h4 id="30-1"><a href="#30-1" class="headerlink" title="30."></a>30.</h4><p>略。</p><h4 id="31-1"><a href="#31-1" class="headerlink" title="31."></a>31.</h4><p>因为$P_{X, Y}(x, y) &#x3D; P_Y(y)P_{X|Y}(x|y)$，所以 $P_{X,Y}(x, y) &#x3D;\<br>         C_4^y(\frac{1}{6})^y(\frac{1}{6})^{4 - y}C_{4 - y}^x(\frac{1}{6})^x(\frac{5}{6})^{4 - y - x}$。</p><p>如果不满足$0 \leq x,y \leq 4$且$0 \leq x + y \leq 4$，则$P_{X, Y}(x, y) &#x3D; 0$。</p><h4 id="32-1"><a href="#32-1" class="headerlink" title="32."></a>32.</h4><p>$$E[S|A &#x3D; a] &#x3D; \sum_{s}sP_{S|A &#x3D; a}(s) &#x3D; \sum_{s}\frac{sC_{m}^sC_{m - s}^{a - 2s}}{C_{2m}^a}$$</p><h4 id="33-1"><a href="#33-1" class="headerlink" title="33."></a>33.</h4><p>略。</p><h4 id="34-1"><a href="#34-1" class="headerlink" title="34."></a>34.</h4><p>略</p><h4 id="35-1"><a href="#35-1" class="headerlink" title="35."></a>35.</h4><p>略。</p><h4 id="36-1"><a href="#36-1" class="headerlink" title="36."></a>36.</h4><p>略。</p><h4 id="37-1"><a href="#37-1" class="headerlink" title="37."></a>37.</h4><p>略。</p><h4 id="38-1"><a href="#38-1" class="headerlink" title="38."></a>38.</h4><p>设每个路口遇到红灯的概率为p。</p><h5 id="a-26"><a href="#a-26" class="headerlink" title="(a)"></a>(a)</h5><p>$$P_X(x) &#x3D; C_4^xp^x(1 - p)^{4 - x}$$ $$E[X] &#x3D; 4p$$<br>$$var(X) &#x3D; \sum_{i &#x3D; 0}^4var(X_i) &#x3D; 4p(1 - p)$$</p><h5 id="b-26"><a href="#b-26" class="headerlink" title="(b)"></a>(b)</h5><p>$$var(2X) &#x3D; 16p(1 - p)$$</p><h4 id="39-1"><a href="#39-1" class="headerlink" title="39."></a>39.</h4><p>由独立性，有： $$E[X] &#x3D; \sum_{i &#x3D; 1}^{10}E[X_i] &#x3D; 35$$<br>$$var(X) &#x3D; \sum_{i &#x3D; 1}^{10}var(X_i) &#x3D; \frac{175}{6}$$</p><h4 id="40-1"><a href="#40-1" class="headerlink" title="40."></a>40.</h4><p>由于论文之间的成绩完全独立，我们不妨在论文与论文之间插入隔板，以此区分不同成绩的两批论文。<br>如果上交了k篇论文，每种评分的论文至少有一篇的概率就为$P(k) &#x3D; \frac{5!C_{k - 1}^5}{(k + 1)^5}$<br>（一共需要安插5个隔板，分为6个不同评分的区间，所有的可能安插的位置一共是$k + 1$个，而合法的安插位置只能在中间的$k - 1$个中选择，且不能重复，隔板自身的顺序有$5!$种）。</p><p>其中$P(24) \approx 0.5$，$\lim_{k \to \infty} &#x3D; 1$。</p><h4 id="41-1"><a href="#41-1" class="headerlink" title="41."></a>41.</h4><h5 id="a-27"><a href="#a-27" class="headerlink" title="(a)"></a>(a)</h5><p>不难得到$P(x) &#x3D; C_{250}^x0.02^x0.98^{250 - x}$，所以$E[X] &#x3D; 50$。</p><p>所以罚单数刚好等于50的概率为$P(50)$。</p><h5 id="b-27"><a href="#b-27" class="headerlink" title="(b)"></a>(b)</h5><p>利用泊松分布近似(a)中的结果：$P(50) \approx e^{-50}\frac{(50)^{50}}{50!} \approx 0.056$。</p><h5 id="c-7"><a href="#c-7" class="headerlink" title="(c)"></a>(c)</h5><p>$$E[0.5 \times 10X + 0.3 \times 20X + 0.2 \times 50X] &#x3D; 1050$$<br>$$var(0.5 \times 10X + 0.3 \times 20X + 0.2 \times 50X) &#x3D; 2160.9$$</p><h5 id="d-3"><a href="#d-3" class="headerlink" title="(d)"></a>(d)</h5><p>因为$\sigma(X) \approx 2.21$，所以$p \in [0, 0.064]$。</p><h4 id="42-1"><a href="#42-1" class="headerlink" title="42."></a>42.</h4><h5 id="a-28"><a href="#a-28" class="headerlink" title="(a)"></a>(a)</h5><p>因为$P_{X_i} &#x3D; S$，所以$E[X_i] &#x3D; S$，故$E[S_n] &#x3D; \frac{nS}{n} &#x3D; S$。</p><p>对于单个点的方差$var(X_i) &#x3D; S(1 - S)$是有限值，<br>所以由独立性$\\lim_{n \to \infty}var(S_n) &#x3D; \frac{nS(1 - S)}{n^2} &#x3D; 0$。</p><p>证毕。</p><h5 id="b-28"><a href="#b-28" class="headerlink" title="(b)"></a>(b)</h5><p>$$S_n &#x3D; \frac{(n - 1)S_{n - 1} + X_n}{n}$$</p><h5 id="c-8"><a href="#c-8" class="headerlink" title="(c)"></a>(c)</h5><p>由于单位正方形的内切圆的半径为$\frac{1}{2}$，所以<br>$$\pi &#x3D; 4E[S_n] &#x3D; \frac{4}{n}\sum_{i &#x3D; 1}^{10000}P_{X_i}$$<br>只要计算落在圆形内部的点的数量即可确定$\pi$的值。</p><h5 id="d-4"><a href="#d-4" class="headerlink" title="(d)"></a>(d)</h5><p>类似于(c)。</p><h4 id="43-1"><a href="#43-1" class="headerlink" title="43."></a>43.</h4><p>略。</p><h4 id="44-1"><a href="#44-1" class="headerlink" title="44."></a>44.</h4><p>略。</p><h4 id="45-1"><a href="#45-1" class="headerlink" title="45."></a>45.</h4><p>略。</p><h4 id="46-1"><a href="#46-1" class="headerlink" title="46."></a>46.</h4><p>略。</p><h3 id="第三章-一般随机变量"><a href="#第三章-一般随机变量" class="headerlink" title="第三章 一般随机变量"></a>第三章 一般随机变量</h3><h4 id="1-4"><a href="#1-4" class="headerlink" title="1."></a>1.</h4><p>因为X是$[0, 1]$上的均匀分布，所以$p_X(x) &#x3D; 1, x \in [0, 1]$。<br>所以Y的分布列为 $$P_Y(y) &#x3D; \left{<br>            \begin{array}{lcr}<br>                \int_{0}^{\frac{1}{3}}p_X(x)dx &#x3D; \frac{1}{3} &amp; &amp; y &#x3D; 1\<br>                \int_{\frac{1}{3}}^{1}p_X(x)dx &#x3D; \frac{2}{3} &amp; &amp; y &#x3D; 2\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>所以$E[Y] &#x3D; \frac{5}{3}$，根据期望规则，<br>$E[Y] &#x3D; \int_{0}^{\frac{1}{3}}dx + \int_{\frac{1}{3}}^{1}2dx &#x3D; \frac{5}{3}$，<br>可以验证我们的结果是正确的。</p><h4 id="2-4"><a href="#2-4" class="headerlink" title="2."></a>2.</h4><p>$\int_{-\infty}^{\infty}\frac{\lambda}{2}e^{-\lambda |x|} &#x3D; \int_{0}^{\infty}\lambda e^{-\lambda x}$，<br>可见积分值与指数分布相同，归一化得证。</p><p>所以拉普拉斯随机变量的期望$E[X] &#x3D; \int_{-\infty}^{\infty}\frac{x\lambda}{2}e^{-\lambda |x|} &#x3D; 0$，<br>则$var(X) &#x3D; E[X^2] &#x3D; \int_{-\infty}^{\infty}\frac{x^2\lambda}{2}e^{-\lambda |x|}dx &#x3D; \frac{2}{\lambda^2}$。</p><p>拉普拉斯分布的更普遍形式为$p_X(x) &#x3D; \frac{1}{2\lambda}e^{-\frac{|x - \mu|}{\lambda}}$，<br>这个形式下的$\E[X] &#x3D; \mu, var(X) &#x3D; 2\lambda^2$。</p><h4 id="3-2"><a href="#3-2" class="headerlink" title="3."></a>3.</h4><p>略。</p><h4 id="4-2"><a href="#4-2" class="headerlink" title="4."></a>4.</h4><p>略。</p><h4 id="5-2"><a href="#5-2" class="headerlink" title="5."></a>5.</h4><p>不妨设三角形的高为H，对应的底边长度为L，<br>则分布函数$P_X(x)$可以用梯形的面积除以三角形面积得到：<br>$P_X(x) &#x3D; \frac{\frac{H - x}{H}L + L}{2}\frac{2}{HL} &#x3D; \frac{x(2H - x)}{H^2}, x \in [0, H]$。</p><p>所以概率密度函数$f_X(x) &#x3D; P_X’(x) &#x3D; \frac{2(H - x)}{H^2}$。</p><h4 id="6-2"><a href="#6-2" class="headerlink" title="6."></a>6.</h4><p>有0个顾客的时候，等候的时间为0；有一个顾客的时候，等候时间是一个指数随机变量。<br>所以： $$P_X(x) &#x3D; \left{<br>            \begin{array}{lcr}<br>                0 &amp; &amp; x &lt; 0\<br>                \frac{1}{2} &amp; &amp; x &#x3D; 0\<br>                \frac{1}{2}(1 + \int_{0}^{x}\lambda e^{-\lambda t}dt) &amp; &amp; x &gt; 0<br>            \end{array}<br>        \right.$$</p><p>解之得： $$P_X(x) &#x3D; \left{<br>            \begin{array}{lcr}<br>                0 &amp; &amp; x &lt; 0\<br>                \frac{1}{2} &amp; &amp; x &#x3D; 0\<br>                \frac{1}{2}(2 - e^{-\lambda x}) &amp; &amp; x &gt; 0<br>            \end{array}<br>        \right.$$</p><h4 id="7-2"><a href="#7-2" class="headerlink" title="7."></a>7.</h4><h5 id="a-29"><a href="#a-29" class="headerlink" title="(a)"></a>(a)</h5><p>X的分布函数为两个圆形面积的比值，所以$P_X(x) &#x3D; \frac{x^2}{r^2}, x \in [0, r]$。<br>求导后可以得到概率密度函数$f_X(x) &#x3D; \frac{2x}{r^2}$。所以$E[X] &#x3D; \int_{0}^{r}\frac{2x^2}{r^2}dx &#x3D; \frac{2r}{3}$，<br>$var(X) &#x3D; E[X^2] - E[X]^2 &#x3D; \frac{r^2}{18}$。</p><h5 id="b-29"><a href="#b-29" class="headerlink" title="(b)"></a>(b)</h5><p>由上一问可知： $$P_S(s) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    0 &amp; &amp; s &lt; 0\<br>                    \frac{r^2 - t^2}{r^2} &amp; &amp; s &#x3D; 0\<br>                    \frac{r^2 - t^2}{r^2} &amp; &amp; 0 &lt; s &lt; \frac{1}{t}\<br>                    \frac{r^2 - t^2}{r^2} + \frac{t^2 - (\frac{1}{s})^2}{r^2} &amp; &amp; s \geq \frac{1}{t}<br>                \end{array}<br>            \right.$$</p><p>是连续随机变量。</p><h4 id="8-2"><a href="#8-2" class="headerlink" title="8."></a>8.</h4><h5 id="a-30"><a href="#a-30" class="headerlink" title="(a)"></a>(a)</h5><p>X的分布函数为$P_X(X \leq x)$，由全概率公式，可以得到$P_X(X \leq x) &#x3D; pP_Y(Y \leq x) + (1 - p)P_Z(Z \leq x)$。<br>对式子两侧分别求导可得$f_X(x) &#x3D; pf_Y(x) + (1 - p)f_Z(x)$。</p><p>证毕。</p><h5 id="b-30"><a href="#b-30" class="headerlink" title="(b)"></a>(b)</h5><p>$$P_X(x) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    pe^{\lambda x} &amp; &amp; x &lt; 0\<br>                    1 + (p - 1)e^{-\lambda x} &amp; &amp; x \geq 0<br>                \end{array}<br>            \right.$$</p><h4 id="9-2"><a href="#9-2" class="headerlink" title="9."></a>9.</h4><p>略。</p><h4 id="10-2"><a href="#10-2" class="headerlink" title="10."></a>10.</h4><p>略。</p><h4 id="11-2"><a href="#11-2" class="headerlink" title="11."></a>11.</h4><h5 id="a-31"><a href="#a-31" class="headerlink" title="(a)"></a>(a)</h5><p>因为$\sigma_X &#x3D; 1, \mu_X &#x3D; 0$，所以X服从标准正态分布。<br>所以我们根据标准正态分布表，可以得到： $P(X \leq 1.5) \approx 0.9332,\<br>             P(X leq -1) &#x3D; 1 - \Phi(1) \approx 0.1587$。</p><h5 id="b-31"><a href="#b-31" class="headerlink" title="(b)"></a>(b)</h5><p>因为$\sigma_Y &#x3D; 2, \mu_Y &#x3D; 1$，而$Z &#x3D; \frac{Y - 1}{2}$，所以<br>$\sigma_Z &#x3D; 1, \mu_Z &#x3D; 0$，所以<br>$f_Z(z) &#x3D; \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}$。</p><h5 id="c-9"><a href="#c-9" class="headerlink" title="(c)"></a>(c)</h5><p>由(b)可知：$P(-1 \leq Y \leq 1) &#x3D; P(-1 \leq Z \leq 0) &#x3D;\ 0.5 - P(Z \leq -1) \approx 0.3413$。</p><h4 id="12-2"><a href="#12-2" class="headerlink" title="12."></a>12.</h4><p>$P(X \geq k\sigma) &#x3D; 1 - P(X \leq k\sigma)$，令$Y &#x3D; \frac{X}{\sigma}$，<br>则$P(X \geq k\sigma) &#x3D; 1 - P(Y \leq k)$，于是：$P(X \geq \sigma) \approx 0.1587,<br>        P(X \geq 2\sigma) \approx 0.0228, P(X \geq 3\sigma) \approx 0.0013$。</p><p>由于$\mu &#x3D; 0$，所以$P(|X| \leq k\sigma) &#x3D; 1 - 2P(X \geq k\sigma)$，<br>所以$P(X \geq \sigma) \approx 0.6826,<br>        P(X \geq 2\sigma) \approx 0.9544, P(X \geq 3\sigma) \approx 0.9974$。</p><h4 id="13-2"><a href="#13-2" class="headerlink" title="13."></a>13.</h4><p>因为$C &#x3D; \frac{F - 32}{1.8}$，所以$P(F \leq 59) &#x3D; P(C \leq 15)$，<br>其中C满足正态分布，$\mu_C &#x3D; 10, \sigma_C &#x3D; 10$。</p><p>再令$X &#x3D; \frac{C - \mu_C}{\sigma_C}$，则$P(F \leq 59) &#x3D; P(X \leq 0.5) \approx 0.6915$。</p><h4 id="14-2"><a href="#14-2" class="headerlink" title="14."></a>14.</h4><p>略。</p><h4 id="15-2"><a href="#15-2" class="headerlink" title="15."></a>15.</h4><h5 id="i-1"><a href="#i-1" class="headerlink" title="(i)"></a>(i)</h5><p>$$f_{X,Y}(x, y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{2}{\pi r} &amp; &amp; x^2 + y^2 \leq r, y &gt; 0\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><h5 id="ii-1"><a href="#ii-1" class="headerlink" title="(ii)"></a>(ii)</h5><p>由(i)得：$f_Y(y) &#x3D; \int_{-\sqrt{r - y^2}}^{\sqrt{r - y^2}}\frac{2}{\pi r}dx &#x3D; \frac{4}{\pi r}\sqrt{r - y^2}$，<br>所以$E[Y] &#x3D;\ \int_{0}^{\sqrt{r}}\frac{4y}{\pi r}\sqrt{r - y^2}dy &#x3D; \frac{4\sqrt{2}}{3 \pi}$。</p><h5 id="iii-1"><a href="#iii-1" class="headerlink" title="(iii)"></a>(iii)</h5><p>利用期望规则直接积分：<br>$E[Y] &#x3D; \int_{0}^{\sqrt{r}}ydy\int_{-\sqrt{r - y^2}}^{\sqrt{r - y^2}}\frac{2}{\pi r}dx &#x3D; \frac{4\sqrt{2}}{3 \pi}$。</p><h4 id="16-2"><a href="#16-2" class="headerlink" title="16."></a>16.</h4><p>不妨设针的中点到最近的水平线的距离为X，到最近的垂直线的距离为Y，针与水平线的夹角为$\Theta$，<br>根据布丰抛针问题，我们可以得到： $$f_{X, Y, \Theta} &#x3D; \left{<br>            \begin{array}{lcr}<br>                \frac{8}{ab\pi} &amp; &amp; x \in [0, \frac{b}{2}], y \in [0, \frac{a}{2}], \theta \in [0, \frac{\pi}{2}]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>所以针与水平线相交的概率为<br>$\P(X \leq \frac{l}{2}sin\Theta) &#x3D; \frac{8}{ab\pi}\int_{0}^{\frac{a}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{l}{2}sin\theta}dxd\theta dy &#x3D; \frac{2l}{b\pi}$。</p><p>同理，针与垂直线相交的概率为：<br>$\P(Y \leq \frac{l}{2}cos\Theta) &#x3D; \frac{8}{ab\pi}\int_{0}^{\frac{b}{2}}\int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{l}{2}cos\theta}dyd\theta dx &#x3D; \frac{2l}{a\pi}$。</p><p>所以相交边数的期望值为$\frac{2l}{b\pi}(1 - \frac{2l}{a\pi}) + \frac{2l}{a\pi}(1 - \frac{2l}{b\pi}) + 2\frac{2l}{a\pi}\frac{2l}{b\pi} &#x3D; \frac{2l}{b\pi} + \frac{2l}{a\pi}$，<br>至少交于一条边的概率为$\frac{2l}{b\pi} + \frac{2l}{a\pi} - \frac{2l}{a\pi}\frac{2l}{b\pi}$。</p><h4 id="17-2"><a href="#17-2" class="headerlink" title="17."></a>17.</h4><p>略。</p><h4 id="18-2"><a href="#18-2" class="headerlink" title="18."></a>18.</h4><h5 id="a-32"><a href="#a-32" class="headerlink" title="(a)"></a>(a)</h5><p>$$E[X] &#x3D; \int_{1}^{3}\frac{x^2}{4}dx &#x3D; \frac{13}{6}$$<br>$$P(A) &#x3D; 1 - P(X \leq 2) &#x3D; 1 - \int_{1}^{2}\frac{x}{4}dx &#x3D; \frac{5}{8}$$<br>$$f_{X|A}(x) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{2x}{5} &amp; &amp; 2 \leq x \leq 3\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$<br>$$E[X|A] &#x3D; \int_{2}^{3}\frac{2x^2}{5}dx &#x3D; \frac{38}{15}$$</p><h5 id="b-32"><a href="#b-32" class="headerlink" title="(b)"></a>(b)</h5><p>$$E[Y] &#x3D; E[X^2] &#x3D; \int_{1}^{3}\frac{x^3}{4}dx &#x3D; 5$$<br>$$var(Y) &#x3D; E[Y^2] - E[Y]^2 &#x3D; \int_{1}^{3}\frac{x^5}{4}dx - 25 &#x3D; \frac{16}{3}$$</p><h4 id="19-2"><a href="#19-2" class="headerlink" title="19."></a>19.</h4><h5 id="a-33"><a href="#a-33" class="headerlink" title="(a)"></a>(a)</h5><p>由归一性：$\int_{1}^{2}cx^{-2}dx &#x3D; 1$，解得$c &#x3D; 2$。</p><h5 id="b-33"><a href="#b-33" class="headerlink" title="(b)"></a>(b)</h5><p>$$P(A) &#x3D; 1 - P(X \leq 1.5) &#x3D; 1 - \int_{1}^{1.5}\frac{2dx}{x^2} &#x3D; \frac{1}{3}$$<br>$$f_{X|A}(x) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{6}{x^2} &amp; &amp; 1.5 \leq x \leq 2\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><h5 id="c-10"><a href="#c-10" class="headerlink" title="(c)"></a>(c)</h5><p>$$E[Y|A] &#x3D; E[X^2|A] &#x3D; \int_{1.5}^{2}6dx &#x3D; 3$$<br>$$var(Y|A) &#x3D; E[Y^2|A] - E[Y|A]^2 &#x3D; \frac{1}{4}$$</p><h4 id="20-2"><a href="#20-2" class="headerlink" title="20."></a>20.</h4><p>因为共同的分布为指数分布，且期望值为30，所以$f_X(x) &#x3D; \frac{1}{30}e^{-\frac{x}{30}}$。<br>第二个学生到达时，第一个学生已经到达了5分钟，所以$P(X \geq 5) &#x3D; 1 - \int_{0}^{5}f_X(x) &#x3D; e^{-\frac{1}{6}}$。<br>所以第一个学生答疑时间的概率密度函数变为$f_{X|A}(x) &#x3D; \frac{e^{\frac{1}{6}}}{30}e^{-\frac{x}{30}}$，<br>其期望值为$E[X|A] &#x3D; \int_{5}^{\infty}\frac{e^{\frac{1}{6}}x}{30}e^{-\frac{x}{30}}dx &#x3D; 35$，所以所花时间的期望值为65。</p><h4 id="21-2"><a href="#21-2" class="headerlink" title="21."></a>21.</h4><h5 id="a-34"><a href="#a-34" class="headerlink" title="(a)"></a>(a)</h5><p>$$f_{X, Y}(x, y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{1}{xl} &amp; &amp; y \leq x \leq l\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><h5 id="b-34"><a href="#b-34" class="headerlink" title="(b)"></a>(b)</h5><p>$$f_{Y}(y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \int_{y}^{l}\frac{dx}{xl} &amp; &amp; 0 \leq y \leq l\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><p>解之得： $$f_{Y}(y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{lnl - lny}{l} &amp; &amp; 0 \leq y \leq l\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><h5 id="c-11"><a href="#c-11" class="headerlink" title="(c)"></a>(c)</h5><p>$$E[Y] &#x3D; \int_{0}^{l}\frac{y(lnl - lny)}{l}dy &#x3D; \frac{l}{4}$$</p><h5 id="d-5"><a href="#d-5" class="headerlink" title="(d)"></a>(d)</h5><p>$E[Y] &#x3D; E[X\frac{Y}{X}] &#x3D; E[X]E[\frac{Y}{X}] &#x3D; \frac{l}{2}\frac{1}{2} &#x3D; \frac{l}{4}$。</p><h4 id="22-2"><a href="#22-2" class="headerlink" title="22."></a>22.</h4><h5 id="i-2"><a href="#i-2" class="headerlink" title="(i)"></a>(i)</h5><p>设随机选取的两个点距离左侧的距离为X和Y，不妨认为X为更靠右侧的那个点，则概率密度函数为：<br>$$f_{X, Y}(x, y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{1}{x} &amp; &amp; 0 \leq y \leq x \leq 1\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><p>由于三角形的两边之和大于第三边，我们可以列出满足条件的三个式子：<br>$$\begin{array}{lcr}<br>                Y + X - Y &amp; &gt; &amp; 1 - X\<br>                Y + 1 - X &amp; &gt; &amp; X - Y\<br>                X - Y + 1 - X &amp; &gt; &amp; Y\<br>            \end{array}$$</p><p>整理后得到： $$\begin{array}{lccrr}<br>                X &amp; &gt; &amp; \frac{1}{2} &amp; \rightarrow &amp; A\<br>                Y &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; B\<br>                X - Y &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; C\<br>            \end{array}$$</p><p>所以$P(A) &#x3D; 1 - \int_{0}^{\frac{1}{2}}\int_{0}^{x}\frac{dydx}{x} &#x3D; \frac{1}{2}$，<br>$P(B|A) &#x3D; \int_{\frac{1}{2}}^{1}\int_{0}^{\frac{1}{2}}\frac{2}{x}dydx &#x3D; ln2$，<br>$P(C|(B|A)) &#x3D; \int_{\frac{1}{2}}^{1}\int_{x - \frac{1}{2}}^{\frac{1}{2}}\frac{dydx}{xln2} &#x3D; 1 - \frac{1}{2ln2}$。</p><h5 id="ii-2"><a href="#ii-2" class="headerlink" title="(ii)"></a>(ii)</h5><p>本质上和(i)是一样的，仅仅只是选取方向的不同。</p><h5 id="iii-2"><a href="#iii-2" class="headerlink" title="(iii)"></a>(iii)</h5><p>设Y为第一次选取的点距离最近的一端的距离，X为第二次选区的点距离Y的距离，则概率密度函数为：<br>$$f_{X, Y}(x, y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{2}{1 - y} &amp; &amp; 0 \leq y \leq \frac{1}{2}, y \leq x \leq 1\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><p>同(i)，我们可以列出以下的条件（注意，此时的三段杆的距离已经变成了$X, Y, 1 - X - y$）：<br>$$\begin{array}{lccrr}<br>                X &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; A\<br>                Y &amp; &lt; &amp; \frac{1}{2} &amp; \rightarrow &amp; B\<br>                X + Y &amp; &gt; &amp; \frac{1}{2} &amp; \rightarrow &amp; C\<br>            \end{array}$$</p><p>所以$P(A) &#x3D; \int_{0}^{\frac{1}{2}}\int_{y}^{\frac{1}{2}}\frac{2}{1 - y}dxdy &#x3D; 1 - ln2$，<br>$P(B|A) &#x3D; 1$，$P(C|(B|A)) &#x3D; 2ln\frac{4}{3} + \frac{1}{2}ln\frac{3}{2} - \frac{1}{4}$。</p><h4 id="23-2"><a href="#23-2" class="headerlink" title="23."></a>23.</h4><h5 id="a-35"><a href="#a-35" class="headerlink" title="(a)"></a>(a)</h5><p>$$f_{X, Y}(x, y) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    2 &amp; &amp; x \geq 0, y \geq 0, x + y \leq 1\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><h5 id="b-35"><a href="#b-35" class="headerlink" title="(b)"></a>(b)</h5><p>$$f_Y(y) &#x3D; \int_{0}^{1 - y}2dx &#x3D; 2(1 - y)$$</p><h5 id="c-12"><a href="#c-12" class="headerlink" title="(c)"></a>(c)</h5><p>$$f_{X|Y &#x3D; y}(x) &#x3D; \frac{1}{1 - y}$$</p><h5 id="d-6"><a href="#d-6" class="headerlink" title="(d)"></a>(d)</h5><p>$$E[X|Y &#x3D; y] &#x3D; \frac{1 - y}{2}$$<br>$$E[X] &#x3D; \int_{0}^{1}E[X|Y &#x3D; y]F_{Y}(y)dy &#x3D; \int_{0}^{1}(1 - y)^2dy &#x3D; \frac{1}{3}$$</p><h5 id="e-1"><a href="#e-1" class="headerlink" title="(e)"></a>(e)</h5><p>利用对称性，$E[Y] &#x3D; E[X] &#x3D; \frac{1}{3}$。</p><h4 id="24-2"><a href="#24-2" class="headerlink" title="24."></a>24.</h4><p>概率密度函数： $$f_{X, Y}(x, y) &#x3D; \left{<br>            \begin{array}{lcr}<br>                1 &amp; &amp; x \geq 0, y \geq 0, 2x + y \leq 2\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>边缘概率密度函数： $$f_X(x) &#x3D; \int_{0}^{-2x + 2}dy &#x3D; -2x + 2$$<br>$$f_Y(y) &#x3D; \int_{0}^{\frac{2 - y}{2}}dx &#x3D; \frac{2 - y}{2}$$</p><p>条件期望： $$E[X|Y &#x3D; y] &#x3D; \frac{2 - y}{4}$$ $$E[Y|X &#x3D; x] &#x3D; 1 - x$$</p><p>所以由全期望定理可得：<br>$$E[X] &#x3D; \int_{0}^{1}E[X|Y &#x3D; y]f_Y(y)dy &#x3D; \frac{5}{24}$$<br>$$E[Y] &#x3D; \int_{0}^{2}E[Y|X &#x3D; x]f_X(x)dx &#x3D; \frac{4}{3}$$</p><h4 id="25-2"><a href="#25-2" class="headerlink" title="25."></a>25.</h4><p>$(X, Y)$离原点的距离至少为c，故$\sqrt{X^2 + Y^2} \geq c$，<br>所以$P(A) &#x3D;\ P(\sqrt{X^2 + Y^2} \geq c) &#x3D; 1 - \int_{0}^{2\pi}\int_{0}^{c}\frac{\rho}{2\pi \sigma^2}e^{\frac{-\rho^2}{2\sigma^2}}d\rho d\theta &#x3D; e^{\frac{-c^2}{2\sigma^2}}$。</p><p>于是得到条件联合概率密度函数：<br>$$f_{(X, Y)|A} &#x3D; \frac{1}{2\pi \sigma^2}e^{\frac{-x^2 - y^2 + c^2}{2\sigma^2}}$$</p><h4 id="26-2"><a href="#26-2" class="headerlink" title="26."></a>26.</h4><p>略。</p><h4 id="27-2"><a href="#27-2" class="headerlink" title="27."></a>27.</h4><h5 id="a-36"><a href="#a-36" class="headerlink" title="(a)"></a>(a)</h5><p>$$\int_{(x, y) \in A}f_{X,Y|C}(x, y)dxdy &#x3D; \frac{1}{P(C)}\int_{(x, y) \in A}f_{X, Y}(x, y)dxdy &#x3D; \frac{P(C)}{P(C)} &#x3D; 1$$<br>所以$f_{X,Y|C}(x, y)$是一个合格的联合概率密度函数。</p><h5 id="b-36"><a href="#b-36" class="headerlink" title="(b)"></a>(b)</h5><p>令$C &#x3D; \bigcup_{i &#x3D; 1}^{n}C_i$，因为$A_i$是二维平面的分割，所以$C_i$也是$C$的一个分割。<br>由(a)可知：$f_{X, Y}(x, y) &#x3D; P(C)f_{X, Y|C}(x, y)$，所以由全概率公式即可得到<br>$f_{X, Y}(x, y) &#x3D; \sum_{i &#x3D; 1}^{n}P(C_i)f_{X, Y|C_i}(x, y)$。</p><h4 id="28-2"><a href="#28-2" class="headerlink" title="28."></a>28.</h4><p>略。</p><h4 id="29-2"><a href="#29-2" class="headerlink" title="29."></a>29.</h4><p>略。</p><h4 id="30-2"><a href="#30-2" class="headerlink" title="30."></a>30.</h4><p>略。</p><h4 id="31-2"><a href="#31-2" class="headerlink" title="31."></a>31.</h4><p>略。</p><h4 id="32-2"><a href="#32-2" class="headerlink" title="32."></a>32.</h4><p>略。</p><h4 id="33-2"><a href="#33-2" class="headerlink" title="33."></a>33.</h4><p>略。</p><h4 id="34-2"><a href="#34-2" class="headerlink" title="34."></a>34.</h4><h5 id="a-37"><a href="#a-37" class="headerlink" title="(a)"></a>(a)</h5><p>不妨设硬币正面朝上为事件A，由连续的全期望定理：$P(A) &#x3D;\ \int_{0}^{1}p^2e^pdp &#x3D; e - 2$</p><h5 id="b-37"><a href="#b-37" class="headerlink" title="(b)"></a>(b)</h5><p>$$f_{P|A}(p) &#x3D; \left{<br>                \begin{array}{lcr}<br>                    \frac{pe^p}{e - 2} &amp; &amp; p \in A\<br>                    0 &amp; &amp; otherwise<br>                \end{array}<br>            \right.$$</p><h5 id="c-13"><a href="#c-13" class="headerlink" title="(c)"></a>(c)</h5><p>由连续贝叶斯准则：<br>$f_{P|X}(p|x) &#x3D; \frac{f_{P}(p)P(X &#x3D; x|P &#x3D; p)}{P(x)} &#x3D; \frac{p^2e^p}{e - 2}$，<br>所以第二次抛掷正面朝上的条件概率为$\int_{0}^{1}\frac{p^3e^p}{e - 2}dp &#x3D; \frac{6 - 2e}{e - 2}$</p><h4 id="35-2"><a href="#35-2" class="headerlink" title="35."></a>35.</h4><p>略。</p><h3 id="第四章-随机变量的深入内容"><a href="#第四章-随机变量的深入内容" class="headerlink" title="第四章 随机变量的深入内容"></a>第四章 随机变量的深入内容</h3><h4 id="1-5"><a href="#1-5" class="headerlink" title="1."></a>1.</h4><p>由题意得，$f_X(x) &#x3D; \frac{1}{2}$，所以<br>$F_Y(y) &#x3D; P(Y \leq y) &#x3D; P(\sqrt{|X|} \leq y) &#x3D; P(|X| \leq y^2)$。</p><p>$$F_Y(y) &#x3D; \left{<br>            \begin{array}{lcr}<br>                y^2 &amp; &amp; y \in [0, 1]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>求导后即可得到： $$f_Y(y) &#x3D; \left{<br>            \begin{array}{lcr}<br>                2y &amp; &amp; y \in [0, 1]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>因为$F_Z(z) &#x3D; P(Z \leq z) &#x3D; P(-ln|X| \leq z) &#x3D; P(|X| \geq e^{-z})$</p><p>$$F_Z(z) &#x3D; \left{<br>            \begin{array}{lcr}<br>                1 - e^{-z} &amp; &amp; z \in [0, \infty]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>所以 $$f_Z(z) &#x3D; \left{<br>            \begin{array}{lcr}<br>                e^{-z} &amp; &amp; z \in [0, \infty]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><h4 id="2-5"><a href="#2-5" class="headerlink" title="2."></a>2.</h4><p>令$Y &#x3D; e^X$，则$F_Y(y) &#x3D; P(Y \leq y) &#x3D; P(e^X \leq y) &#x3D; P(X \leq lny)$，<br>所以$F_Y(y) &#x3D; \int_{-\infty}^{lny}f_X(x)dx$，求导后$f_Y(y) &#x3D; \frac{f_X(lny)}{y}$。</p><p>当X服从$[0, 1]$上的均匀分布时： $$f_Y(y) &#x3D; \left{<br>            \begin{array}{lcr}<br>                \frac{1}{y} &amp; &amp; y \in [1, e]\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><h4 id="3-3"><a href="#3-3" class="headerlink" title="3."></a>3.</h4><p>令$Y &#x3D; |X|^{\frac{1}{3}}, Z &#x3D; |X|^{\frac{1}{4}}$，则：<br>$F_Y(y) &#x3D; P(Y \leq y) &#x3D; P(|X|^{\frac{1}{3}} \leq y) &#x3D; P(|X| \leq y^3) &#x3D; \int_{0}^{y^3}f_{|X|}(|x|)dx$，<br>所以$f_Y(y) &#x3D; 3y^2f_{|X|}(y^3)$，同理$f_Z(z) &#x3D; 4z^3f_{|X|}(y^4)$。</p><h4 id="4-3"><a href="#4-3" class="headerlink" title="4."></a>4.</h4><p>因为$F_Y(y) &#x3D; P(Y \leq y)$，当$0 \leq y \leq 5$时，由全概率公式，<br>$P(Y \leq y) &#x3D; \frac{1}{4}P(X \leq y) + \frac{3}{4}P(X - 5 \leq y) &#x3D; \frac{1}{4}P(X \leq y) + \frac{3}{4}P(X \leq y + 5)$，<br>所以$P(Y \leq y) &#x3D; \frac{1}{4}(\int_{0}^{y}f_{X_A}(x)dx + 3\int_{0}^{y + 5}f_{X_B}(x)dx) &#x3D; \frac{y}{10} + \frac{1}{4}$，故$f_Y(y) &#x3D; \frac{1}{10}$；<br>同理当$5 &lt; y \leq 15$时，$P(Y \leq y) &#x3D; \frac{3}{4}\int_{0}^{y + 5}f_{X_B}(x)dx &#x3D; \frac{y + 5}{20}$，所以$f_Y(y) &#x3D; \frac{1}{20}$。</p><h4 id="5-3"><a href="#5-3" class="headerlink" title="5."></a>5.</h4><p>$|z| &#x3D; 0.5$的情况</p><p><img src="https://s3.ax1x.com/2021/02/15/y6GADf.jpg"></p><p>由于$X, Y$均服从均匀分布，两个随机变量的联合概率可以用以上的正方形区域面积表示。<br>考虑$|X - Y| \leq z$，这个式子可以被拆开为两个式子$X - Y \leq z$和$Y - X \leq z$，在图中做出这两个函数的图像。<br>不难发现$P(Z \leq z)$就是两条平行线与正方形所包围的图形的面积，即$P(Z \leq z) &#x3D; 2z - z^2$，所以$f_Z(z) &#x3D; 2 - 2z$。</p><h4 id="6-3"><a href="#6-3" class="headerlink" title="6."></a>6.</h4><p>使用和第五题类似的做法：$P(Z \leq z) &#x3D; P(|X - Y| \leq z) &#x3D; \frac{\sqrt{2}z}{2}(\sqrt{2}z + \frac{\sqrt{2}}{2}(1 - z) * 2) &#x3D; \frac{z}{2}$，<br>所以$f_Z(z) &#x3D; \frac{1}{2}$。</p><h4 id="7-3"><a href="#7-3" class="headerlink" title="7."></a>7.</h4><p>不难看出这题的随机变量就是第五题中的Z。所以$E[Z] &#x3D; \int_{0}^{1}2z^2 - z^3dz &#x3D; \frac{1}{3}$。</p><p>证毕。</p><h4 id="8-3"><a href="#8-3" class="headerlink" title="8."></a>8.</h4><p>$$f_Z(z) &#x3D; \int_{-\infty}^{\infty}f_X(x)f_Y(z - x)dx &#x3D; \int_{0}^{z}\lambda^2e^{-\lambda x}e^{-\lambda (z - x)}dx &#x3D; \lambda^2ze^{-\lambda z}$$</p><h4 id="9-3"><a href="#9-3" class="headerlink" title="9."></a>9.</h4><p>当$z \leq 0$， $$\begin{array}{l}<br>            F_Z(z) &#x3D; P(X - Y \leq z) &#x3D; 1 - P(X - Y &gt; z)\<br>            &#x3D; 1 - \int_{0}^{\infty}\mu e^{-\mu y}dy\int_{z + y}^{\infty}\lambda e^{-\lambda x}dx\<br>            &#x3D; 1 - \frac{\mu}{\lambda + \mu}e^{-\lambda z}<br>        \end{array}$$</p><p>当$z &lt; 0$时，$-z \leq 0$，所以$F_Z(z) &#x3D; 1 - F_Z(-z) &#x3D; \frac{\mu}{\lambda + \mu}e^{\lambda z}$。</p><h4 id="10-3"><a href="#10-3" class="headerlink" title="10."></a>10.</h4><p>由离散卷积公式$f_Z(z) &#x3D; \sum f_X(x)f_Y(z - x)$可以求得：<br>$$p_Z(z) &#x3D; \left{<br>            \begin{array}{lcr}<br>                \frac{1}{6} &amp; &amp; z &#x3D; 1\<br>                \frac{5}{18} &amp; &amp; z &#x3D; 2\<br>                \frac{1}{3} &amp; &amp; z &#x3D; 3\<br>                \frac{1}{6} &amp; &amp; z &#x3D; 4\<br>                \frac{1}{18} &amp; &amp; z &#x3D; 5<br>            \end{array}<br>        \right.$$</p><h4 id="11-3"><a href="#11-3" class="headerlink" title="11."></a>11.</h4><p>设$P_X(k) &#x3D; e^{-\lambda}\frac{\lambda^k}{k!}, P_Y(k) &#x3D; e^{-\mu}\frac{\mu^k}{k!}$，<br>由卷积公式，有$P_Z(z) &#x3D;\ \sum P_X(k)P_Y(z - k)$，因为$C_z^k &#x3D; \frac{z!}{k!(z - k)!}$，<br>所以$P_Z(z) &#x3D; \frac{e^{(-\lambda + \mu)}}{z!}\sum C_z^k \lambda^k \mu^{z - k}\ &#x3D; e^{-(\lambda + \mu)}\frac{(\lambda + \mu)^z}{z!}$。</p><p>证毕。</p><h4 id="12-3"><a href="#12-3" class="headerlink" title="12."></a>12.</h4><p>先求$W &#x3D; X + Y$的概率密度函数： $$f_W(w) &#x3D; \left{<br>            \begin{array}{llccr}<br>                \int_{0}^{w}dx &amp; &#x3D; &amp; w &amp; &amp; 0 \leq w \leq 1\<br>                \int_{w - 1}^{1}dx &amp; &#x3D; &amp; 2 - w &amp; &amp; 1 &lt; w \leq 2\<br>                0 &amp; &amp; &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><p>再求$R &#x3D; W + Z$： $$f_R(r) &#x3D; \left{<br>            \begin{array}{llccr}<br>                \int_{0}^{r}wdw &amp; &#x3D; &amp; \frac{r^2}{2} &amp; 0 \leq r \leq 1\<br>                \int_{r - 1}^{1}wdw + \int_{1}^{r}2-wdw &amp; &#x3D; &amp; 3r - r^2 - \frac{3}{2} &amp; 1 &lt; r \leq 2\<br>                \int_{r - 1}^{2}2 - wdw &amp; &#x3D; &amp; \frac{r^2}{2} - 3r + \frac{9}{2} &amp; 2 &lt; r \leq 3\<br>                0 &amp; &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><h4 id="13-3"><a href="#13-3" class="headerlink" title="13."></a>13.</h4><p>因为概率密度函数的对称轴为$\frac{a + b}{2}$，所以有$f(x) &#x3D; f(a + b - x)$，<br>而$X + Y$和$X - Y$的积分区间正好差着$a + b$的距离，所以只需要将$f_{X + Y}$平移即可。</p><h4 id="14-3"><a href="#14-3" class="headerlink" title="14."></a>14.</h4><p>$$\begin{array}{l}<br>            P_Z(z) &#x3D; P(min{X, Y} \leq z)\<br>            &#x3D; 1 - P((min{X, Y} &gt; z)\<br>            &#x3D; 1 - P(X &gt; z)P(Y &gt; z)\<br>            &#x3D; 1 - (1 - P(X \leq z))(1 - P(Y \leq z))\<br>            &#x3D; 1 - (1 - \int_{0}^{z}\lambda e^{-\lambda x}dx)(1 - \int_{0}^{z}\mu e^{-\mu y}dy)\<br>            &#x3D; 1 - e^{-(\lambda + \mu)z}<br>        \end{array}$$</p><p>所以$Z &#x3D; min{X, Y}$服从参数为$\lambda + \mu$的指数分布。</p><p>证毕。</p><h4 id="15-3"><a href="#15-3" class="headerlink" title="15."></a>15.</h4><p>略。</p><h4 id="16-3"><a href="#16-3" class="headerlink" title="16."></a>16.</h4><p>略。</p><h4 id="17-3"><a href="#17-3" class="headerlink" title="17."></a>17.</h4><p>因为$var(X) &#x3D; var(Y)$，所以 $$\begin{array}{l}<br>            cov(X + Y, X - Y) &#x3D; cov(X + Y, X) - cov(X + Y, Y)\<br>            &#x3D; cov(X, X) + cov(Y, X) - cov(X, Y) - cov(Y, Y)\<br>            &#x3D; cov(X, X) - cov(Y, Y)\<br>            &#x3D; var(X) - var(Y)\<br>            &#x3D; 0<br>        \end{array}$$</p><p>所以$X + Y, X - Y$不相关。</p><p>证毕。</p><h4 id="18-3"><a href="#18-3" class="headerlink" title="18."></a>18.</h4><p>由题意得：<br>$cov(W, X) &#x3D; cov(W, Y) &#x3D; cov(W, Z) &#x3D; cov(X, Y) &#x3D;\ cov(X, Z) &#x3D; cov(Y, Z) &#x3D; 0$，<br>所以： $$\begin{array}{l}<br>            \rho(R, S) &#x3D; \frac{cov(R, S)}{\sqrt{var(R)var(S)}}\<br>            &#x3D; \frac{cov(W + X, X + Y)}{\sqrt{var(W + X)var(X + Y)}}\<br>            &#x3D; \frac{cov(W, X) + cov(W, Y) + cov(X, X) + cov(X, Y)}{\sqrt{(var(W) + var(X) + 2cov(W, X))(var(X) + var(Y) + 2cov(X, Y))}}\<br>            &#x3D; \frac{var(X)}{\sqrt{(var(W) + var(X))(var(X) + var(Y)}}\<br>            &#x3D; \frac{1}{2}<br>        \end{array}$$</p><p>同理$\rho(S, T) &#x3D; \frac{1}{2}$</p><h4 id="19-3"><a href="#19-3" class="headerlink" title="19."></a>19.</h4><p>$$\begin{array}{l}<br>            \rho(X, Y) &#x3D; \frac{cov(X, Y)}{\sqrt{var(X)var(Y)}}\<br>            &#x3D; \frac{cov(X, a + bX + cX^2)}{\sqrt{var(X)var(a + bX + cX^2)}}\<br>            &#x3D; \frac{cov(X, bX) + cov(X, cX^2)}{\sqrt{var(X)(var(a + bX) + var(cX^2) +2cov(a + bX, cX^2))}}\<br>            &#x3D; \frac{b\times var(X) + c \times cov(X, X^2)}{\sqrt{var(X)(a^2var(X) + c^2var(X^2) + 2bc \times cov(X, X^2))}}\<br>            &#x3D; \frac{b}{\sqrt{a + 2c^2}}<br>        \end{array}$$</p><h4 id="20-3"><a href="#20-3" class="headerlink" title="20."></a>20.</h4><p>略。</p><h4 id="21-3"><a href="#21-3" class="headerlink" title="21."></a>21.</h4><p>略。</p><h4 id="22-3"><a href="#22-3" class="headerlink" title="22."></a>22.</h4><p>设第i次赌博的收益比率为常量$R_i$，资产值为随机变量$X_i$，我们可以得到：</p><ul><li><p>$E[X_1] &#x3D; (pR_1 + 2(1 - p)^2)x$</p></li><li><p>$E[X_2] &#x3D; E[E[X_2|X_1]] &#x3D; (pR_2 + 2(1 - p)^2)X_1$</p></li><li><p>$E[X_3] &#x3D; E[E[X_3|X_2]] &#x3D; (pR_3 + 2(1 - p)^2)X_2$</p></li><li><p>…</p></li></ul><p>综上：$E[X_n] &#x3D; x\Pi_{i &#x3D; 1}^n(pR_i + 2(1 - p)^2)$</p><h4 id="23-3"><a href="#23-3" class="headerlink" title="23."></a>23.</h4><h5 id="a-38"><a href="#a-38" class="headerlink" title="(a)"></a>(a)</h5><p>当$X \leq 1$时，纳特的等待时间为0；当$X &gt; 1$时，等待时间的期望值为$\int_{1}^{2}\frac{x - 1}{2}dx &#x3D; \frac{1}{4}$，<br>所以总的等待时间的期望值为15分钟。</p><h5 id="b-38"><a href="#b-38" class="headerlink" title="(b)"></a>(b)</h5><p>当$X \leq 1$时，约会的时长是3小时；当$X &gt; 1$时，约会时间的期望值为$E[Y] &#x3D; E[E[Y|X]]$，<br>因为$E[Y|X] &#x3D; \frac{3 - X}{2}$，所以$E[Y] &#x3D; \frac{3}{2} - \frac{E[X]}{2} &#x3D; 1$。<br>所以总的约会时间的期望值为2小时。</p><h5 id="c-14"><a href="#c-14" class="headerlink" title="(c)"></a>(c)</h5><p>TODO:</p><h4 id="24-3"><a href="#24-3" class="headerlink" title="24."></a>24.</h4><h5 id="a-39"><a href="#a-39" class="headerlink" title="(a)"></a>(a)</h5><p>由重期望法则： $$\begin{array}{l}<br>                E[X] &#x3D; E[E[X|Y]] \<br>                &#x3D; E[5 - Y] \<br>                &#x3D; 5 - E[Y] \<br>                &#x3D; 3<br>            \end{array}$$</p><h5 id="b-39"><a href="#b-39" class="headerlink" title="(b)"></a>(b)</h5><p>$E[X + Y] &#x3D; E[X] + E[Y] &#x3D; 3 + 2 &#x3D; 5$小时，所以是下午两点。</p><h5 id="c-15"><a href="#c-15" class="headerlink" title="(c)"></a>(c)</h5><p>TODO:</p><h4 id="25-3"><a href="#25-3" class="headerlink" title="25."></a>25.</h4><p>略。</p><h4 id="26-3"><a href="#26-3" class="headerlink" title="26."></a>26.</h4><p>略。</p><h4 id="27-3"><a href="#27-3" class="headerlink" title="27."></a>27.</h4><p>略。</p><h4 id="28-3"><a href="#28-3" class="headerlink" title="28."></a>28.</h4><p>略。</p><h4 id="29-3"><a href="#29-3" class="headerlink" title="29."></a>29.</h4><p>$$\begin{array}{l}<br>            M(s) &#x3D; \sum_{x}e^{sx}p_X(x)\<br>            &#x3D; e^s \times \frac{1}{2} + e^{2s} \times \frac{1}{4} + e^{3s} \times \frac{1}{4}\<br>        \end{array}$$</p><p>所以：</p><ul><li><p>$E[X] &#x3D; \frac{d}{ds}M(s)|_{s &#x3D; 0} &#x3D; \frac{7}{4}$</p></li><li><p>$E[X^2] &#x3D; \frac{d^2}{ds^2}M(s)|_{s &#x3D; 0} &#x3D; \frac{15}{4}$</p></li><li><p>$E[X^3] &#x3D; \frac{d^3}{ds^3}M(s)|_{s &#x3D; 0} &#x3D; \frac{37}{4}$</p></li></ul><h4 id="30-3"><a href="#30-3" class="headerlink" title="30."></a>30.</h4><p>标准正态分布$f_X(x) &#x3D; \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}}$，<br>其矩母函数为 $$\begin{array}{l}<br>            M(s) &#x3D; \int_{-\infty}^{\infty}e^{sx}f_X(x)dx\<br>            &#x3D; \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{\frac{-x^2}{2} + sx}dx\<br>            &#x3D; \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{x^2 - 2sx + s^2 - s^2}{2}}dx\<br>            &#x3D; \frac{e^{\frac{s^2}{2}}}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{(x - s)^2}{2}}dx\<br>            &#x3D; e^{\frac{s^2}{2}}<br>        \end{array}$$</p><p>所以$E[X^3] &#x3D; 0, E[X^4] &#x3D; 3$。</p><h4 id="31-3"><a href="#31-3" class="headerlink" title="31."></a>31.</h4><p>$M(s) &#x3D; \int_{0}^{\infty}\lambda e^{-\lambda x + sx}dx &#x3D; \frac{-\lambda}{s - \lambda}$，<br>所以$E[X^3] &#x3D; \frac{6}{\lambda^4}, E[X^4] &#x3D; \frac{24}{\lambda^5},\ E[X^5] &#x3D; \frac{120}{\lambda^6}$</p><h4 id="32-3"><a href="#32-3" class="headerlink" title="32."></a>32.</h4><h5 id="a-40"><a href="#a-40" class="headerlink" title="(a)"></a>(a)</h5><p>第二个不是矩母函数。根据矩母函数的定义$M_X(s) &#x3D; E[e^{sX}]$,$M_X(0) &#x3D; E[1] &#x3D; 1$，<br>很明显第二个函数不满足这个条件。</p><h5 id="b-40"><a href="#b-40" class="headerlink" title="(b)"></a>(b)</h5><p>设N为满足$\lambda &#x3D; 2$的泊松分布的随机变量，Y为满足$\lambda &#x3D; 1$的泊松分布的随机变量，<br>所以X为N个Y之和（见4.5节），$P(X &#x3D; 0) &#x3D; \sum_{n &#x3D; 0}^{\infty}p_N(n)p_Y(0)\ &#x3D; e^{-1}$</p><h4 id="33-3"><a href="#33-3" class="headerlink" title="33."></a>33.</h4><p>不难看出加号两侧是指数分布的矩母函数的形式，所以概率密度函数：<br>$$f_X(x) &#x3D; \left{<br>            \begin{array}{lcr}<br>                \frac{1}{3} \times 2e^{-2x} + \frac{2}{3} \times 3e^{-3x} &amp; &amp; x \geq 0\<br>                0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$$</p><h4 id="34-3"><a href="#34-3" class="headerlink" title="34."></a>34.</h4><p>利用卷积计算的分布列： $$p(x) &#x3D; \left{<br>            \begin{array}{lcr}<br>                1 - p_1 - p_2 - p_3 + p_1p_2 + p_1p_3 + p_2p_3 - p_1p_2p_3 &amp; &amp; x &#x3D; 0\<br>                3p_1p_2p_3 + p_1 + p_2 + p_3 - 2(p_1p_2 + p_1p_3 + p_2p_3) &amp; &amp; x &#x3D; 1\<br>                p_1p_2 + p_1p_3 + p_2p_3 - 3p_1p_2p_3 &amp; &amp; x &#x3D; 2\<br>                p_1p_2p_3 &amp; &amp; x &#x3D; 3<br>            \end{array}<br>        \right.$$</p><p>对于某一个球员，其矩母函数为$M_{X_i}(s) &#x3D; 1 - p_i + p_ie^s$，<br>又因为独立随机变量的和的矩母函数是和项的矩母函数的乘积，<br>所以$M_X(s) &#x3D; \Pi_{i &#x3D; 1}^{3}(1 - p_i + p_ie^s)$，展开后去掉$e^{sk}$，<br>形式与上面的分布列是一样的。</p><h4 id="35-3"><a href="#35-3" class="headerlink" title="35."></a>35.</h4><p>由$M(0) &#x3D; 1$，可以计算得$c &#x3D; \frac{2}{9}$，所以$E[X] &#x3D; \frac{d}{ds}M(s)|_{s &#x3D; 0} &#x3D; \frac{37}{18}$；<br>令$\alpha &#x3D; \frac{e^s}{3}$，那么$M(s) &#x3D; \frac{c(3 + 4e^2 + 2e^3)}{3(1 - \alpha)} &#x3D; \frac{2}{27}(3 + 4e^2 + 2e^3)(1 + \alpha + \alpha^2 + \dots)$，<br>所以$p_X(1) &#x3D; \frac{2}{27}, p_X(0) &#x3D; \frac{2}{9}$，由条件期望：$E[X|X \ne 0] &#x3D; \frac{E[X]}{1 - p_X(0)} &#x3D; \frac{37}{14}$。</p><h4 id="36-2"><a href="#36-2" class="headerlink" title="36."></a>36.</h4><h5 id="a-41"><a href="#a-41" class="headerlink" title="(a)"></a>(a)</h5><p>由题意得：$M_{XY}(s) &#x3D; \frac{1}{3}M_Y(s) &#x3D; \frac{1}{3}\frac{s}{2 - s}$，<br>$M_{(1 - X)Z} &#x3D; \frac{2}{3}M_Z(s) &#x3D; \frac{2}{3}e^{3(e^s - 1)}$，所以$M_{U}(s) &#x3D; \frac{2s}{9(2 - s)}e^{3(e^s - 1)}$。</p><h5 id="b-41"><a href="#b-41" class="headerlink" title="(b)"></a>(b)</h5><p>$M_{2Z + 3}(s) &#x3D; M_{2Z}(s)M_{3}(s) &#x3D; e^{3(e^{2s} - 1) + 3s}$</p><h5 id="c-16"><a href="#c-16" class="headerlink" title="(c)"></a>(c)</h5><p>$M_{Y + Z}(s) &#x3D; \frac{s}{2 - s}e^{3(e^s - 1)}$</p><h4 id="37-2"><a href="#37-2" class="headerlink" title="37."></a>37.</h4><p>TODO:</p><h4 id="38-2"><a href="#38-2" class="headerlink" title="38."></a>38.</h4><p>略。</p><h4 id="39-2"><a href="#39-2" class="headerlink" title="39."></a>39.</h4><p>略。</p><h4 id="40-2"><a href="#40-2" class="headerlink" title="40."></a>40.</h4><p>略。</p><h4 id="41-2"><a href="#41-2" class="headerlink" title="41."></a>41.</h4><h5 id="a-42"><a href="#a-42" class="headerlink" title="(a)"></a>(a)</h5><p>因为$X_i$服从$[0, 1]$上的均匀分布，所以$M_X(s) &#x3D; \frac{e^s - 1}{s}$，<br>由于人数服从泊松分布，所以$M_N(s) &#x3D; e^{\lambda(e^s - 1)}$，将$e^s$替换后得到<br>$M_Y(s) &#x3D; e^{\lambda(\frac{e^s - 1}{s} - 1)}$</p><h5 id="b-42"><a href="#b-42" class="headerlink" title="(b)"></a>(b)</h5><p>由(a)中的矩母函数可得：$E[Y] &#x3D; \frac{d}{ds}M_Y(s)|_{s &#x3D; 0} &#x3D; \frac{\lambda}{2}$</p><h5 id="c-17"><a href="#c-17" class="headerlink" title="(c)"></a>(c)</h5><p>由重期望法则：$E[Y] &#x3D; E[E[Y|N]] &#x3D; E[X]E[N] &#x3D; \frac{\lambda}{2}$，与(b)中的结果一致。</p><h4 id="42-2"><a href="#42-2" class="headerlink" title="42."></a>42.</h4><p>个数服从参数为$\lambda$的泊松分布的情况下，<br>$M_X(s) &#x3D; e^{\lambda(e^{\frac{\sigma^2s^2}{2}} + \mu s) - 1}$，<br>显然不满足正态分布的矩母函数的形式。</p><h4 id="43-2"><a href="#43-2" class="headerlink" title="43."></a>43.</h4><h5 id="a-43"><a href="#a-43" class="headerlink" title="(a)"></a>(a)</h5><p>由全概率定理： $P(X \leq x) &#x3D;<br>            \frac{1}{16}(1 + 4\int_{0}^{x}f_Y(y)dy + 6\int_{0}^{x}f_{2Y}(y)dy + 4\int_{0}^{x}f_{3Y}(y)dy + \int_{0}^{x}f_{4Y}(y)dy)$<br>混合分布的对应的矩母函数为：<br>$M_X(s) &#x3D; \frac{1}{16}(1 + 4e^{\frac{s^2}{8} + s} + 6e^{\frac{s^2}{4} + 2s} + 4e^{\frac{3s^2}{8} + 3s} + e^{\frac{s^2}{2} + 4s})$，<br>所以超过4分钟的概率为$1 - P(X \leq 4) \approx 0.06$。X不是正态的。</p><h5 id="b-43"><a href="#b-43" class="headerlink" title="(b)"></a>(b)</h5><p>二项分布的矩母函数：$M_N(s) &#x3D; (\frac{1}{2} + \frac{1}{2}e^s)^4$，用正态分布的矩母函数替换：<br>$M_X(s) &#x3D; \frac{1}{16}(1 + e^{\frac{s^2}{8}} + s)^4$，展开后与(a)中结果一致。</p><h4 id="44-2"><a href="#44-2" class="headerlink" title="44."></a>44.</h4><h5 id="a-44"><a href="#a-44" class="headerlink" title="(a)"></a>(a)</h5><p>$E[N] &#x3D; E[M]E[K], var(N) &#x3D; E[M]var(K) + E[K]^2var(M)$</p><h5 id="b-44"><a href="#b-44" class="headerlink" title="(b)"></a>(b)</h5><p>$E[Y] &#x3D; E[X]E[N] &#x3D; E[X]E[M]E[K], var(Y) &#x3D; E[N]var(X) +\ E[X]^2var(N) &#x3D; E[M]E[K]var(X) + E[X]^2(E[M]var(K) + E[K]^2var(M))$</p><h5 id="c-18"><a href="#c-18" class="headerlink" title="(c)"></a>(c)</h5><p>由题意得：$E[K] &#x3D; \mu, E[X] &#x3D; \frac{1}{\lambda}, E[M] &#x3D; \frac{1}{p}, var(K) &#x3D; \mu, var(X) &#x3D; \frac{1}{\lambda^2}, var(M) &#x3D; \frac{1 - p}{p^2}$，<br>由(b)中的公式，我们可以得到：$E[Y] &#x3D; \frac{\mu}{p\lambda}, var(Y) &#x3D; \frac{\mu}{p\lambda^2} + \frac{\mu p + \mu^2 - \mu^2p}{\lambda^2p^2}$</p><h4 id="45-2"><a href="#45-2" class="headerlink" title="45."></a>45.</h4><p>略。</p><h3 id="第五章-极限理论"><a href="#第五章-极限理论" class="headerlink" title="第五章 极限理论"></a>第五章 极限理论</h3><h4 id="1-6"><a href="#1-6" class="headerlink" title="1."></a>1.</h4><h5 id="a-45"><a href="#a-45" class="headerlink" title="(a)"></a>(a)</h5><p>因为$\sigma &#x3D; 1, var(M_n) &#x3D; \frac{\sigma^2}{n}$，<br>要求$var(M_n) &#x3D; \frac{\sigma^2}{n} \leq 0.0001$， 所以$n \geq 10000$。</p><h5 id="b-45"><a href="#b-45" class="headerlink" title="(b)"></a>(b)</h5><p>由切比雪夫不等式：<br>$P(|X - h| \geq 0.05) \leq \frac{1^2}{0.05^2n} \leq 0.01$，<br>解之得$n \geq 40000$。</p><h5 id="c-19"><a href="#c-19" class="headerlink" title="(c)"></a>(c)</h5><p>令$\sigma &#x3D; \frac{(b - a)}{2} &#x3D; 0.3$，<br>则(a)中的不等式变为$\frac{0.09}{n} \leq 0.0001$，解之得$n \geq 900$；<br>(b)中的不等式变为$\frac{0.09}{0.0025n} \leq 0.01$，解之得$n \geq 3600$。</p><h4 id="2-6"><a href="#2-6" class="headerlink" title="2."></a>2.</h4><p>略。</p><h4 id="3-4"><a href="#3-4" class="headerlink" title="3."></a>3.</h4><p>略。</p><h4 id="4-4"><a href="#4-4" class="headerlink" title="4."></a>4.</h4><p>由弱大数定律和切比雪夫不等式，我们可以得到：<br>$P(|M_n - \mu| \geq \epsilon) \leq \frac{\sigma^2}{n\epsilon^2} &#x3D; \delta$：</p><h5 id="a-46"><a href="#a-46" class="headerlink" title="(a)"></a>(a)</h5><p>当$\epsilon$缩小为原来的一半时，为了维持不等号不变，n至少要扩大到原来的4倍。</p><h5 id="b-46"><a href="#b-46" class="headerlink" title="(b)"></a>(b)</h5><p>当$\delta$缩小为原来的一半时，为了维持不等号不变，n至少要扩大到原来的2倍。</p><h4 id="5-4"><a href="#5-4" class="headerlink" title="5."></a>5.</h4><p>由题意得：$f_{X_i}(x) &#x3D; \left{<br>            \begin{array}{lcr}<br>                \frac{1}{2} &amp; &amp; x \in [-1, 1]\<br>                 0 &amp; &amp; otherwise<br>            \end{array}<br>        \right.$</p><h5 id="a-47"><a href="#a-47" class="headerlink" title="(a)"></a>(a)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}2P(Y_n \geq \epsilon)\<br>                &#x3D; P(0 \geq \epsilon)\<br>                &#x3D; 0<br>            \end{array}$$</p><p>利用夹逼定理，容易得出极限值为0。</p><h5 id="b-47"><a href="#b-47" class="headerlink" title="(b)"></a>(b)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}P(|X_n^n| \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}2P(X_n \geq \epsilon^n)\<br>                &#x3D; \lim_{n \to \infty}2P(X_n \geq 1)\<br>                &#x3D; 0<br>            \end{array}$$</p><p>不难看出$Y_n$的极限不存在。</p><h5 id="c-20"><a href="#c-20" class="headerlink" title="(c)"></a>(c)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}2P(Y_n \geq \epsilon)\<br>                &#x3D; P(0 \geq \epsilon)\<br>                &#x3D; 0<br>            \end{array}$$</p><p>$Y_n$的极限为0。</p><h5 id="d-7"><a href="#d-7" class="headerlink" title="(d)"></a>(d)</h5><p>$Y_n$依概率收敛于0。 $$\begin{array}{l}<br>                \lim_{n \to \infty}P(|Y_n - a| \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}2P(Y_n \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}P(max{X_1, X_2, \dots X_n}) \geq \epsilon)\<br>                &#x3D; \lim_{n \to \infty}(\frac{1 - \epsilon}{2})^n\<br>                &#x3D; 0<br>            \end{array}$$</p><p>$Y_n$的极限为1。</p><h4 id="6-4"><a href="#6-4" class="headerlink" title="6."></a>6.</h4><p>略。</p><h4 id="7-4"><a href="#7-4" class="headerlink" title="7."></a>7.</h4><p>略。</p><h4 id="8-4"><a href="#8-4" class="headerlink" title="8."></a>8.</h4><p>我们设第i次转动的结果为$X_i$，当转到奇数的时候值为1，偶数时为0。<br>如果轮盘公正，那么$\mu &#x3D; 0.5, \sigma^2 &#x3D; 0.25$，<br>则由中心极限定理，转到奇数的次数大于55次的概率为<br>$P(Z_n &gt; x) &#x3D; P(Z_n &gt; \frac{55 - 100\mu}{10\sigma}) &#x3D; 1 - P(Z_n \leq 1) &#x3D; 2 - \Phi(1) \approx 0.1587$</p><h4 id="9-4"><a href="#9-4" class="headerlink" title="9."></a>9.</h4><h5 id="a-48"><a href="#a-48" class="headerlink" title="(a)"></a>(a)</h5><p>设第i天内是否死机为$X_i$，当死机发生时记为1，没有发生记为0。<br>由题意得$\mu &#x3D; 0.95, \sigma^2 &#x3D; 0.0475$，<br>所以由中心极限定理，至少有45天没有死机的概率为<br>$P(Z_n &gt; \frac{45 - 50\mu}{\sqrt{50}\sigma}) \approx \Phi(1.62) &#x3D; 0.9474$</p><h5 id="b-48"><a href="#b-48" class="headerlink" title="(b)"></a>(b)</h5><p>由题意得：$\lambda &#x3D; np &#x3D; 47.5$，<br>那么$P(Z_n \leq 5) &#x3D; \sum_{k &#x3D; 0}^{5}e^{-\lambda}\frac{\lambda^k}{k!} \approx 1$</p><h4 id="10-4"><a href="#10-4" class="headerlink" title="10."></a>10.</h4><h5 id="a-49"><a href="#a-49" class="headerlink" title="(a)"></a>(a)</h5><p>$P(Z_n &gt; x) &#x3D; P(Z_n &gt; \frac{440 - 100\mu}{90}) &#x3D; 1 - P(Z_n \leq -0.667) &#x3D; \Phi(0.667) \approx 0.7454$</p><h5 id="b-49"><a href="#b-49" class="headerlink" title="(b)"></a>(b)</h5><p>由题意得：$P(X_1 + \dots + X_n \leq 200 + 5n) &#x3D; P(X_1 + \dots + X_n \leq \frac{200}{3\sqrt{n}}) \geq 0.95$，<br>所以$\frac{200}{3\sqrt{n}} \geq 1.65$，解得n的最大值为1632。</p><h5 id="c-21"><a href="#c-21" class="headerlink" title="(c)"></a>(c)</h5><p>若恰好220天生产到第1000个，则每天平均需要生产约4.55个；<br>若219天就可以生产到第1000个，则每天平均需要生产约4.57个。<br>所以，由中心极限定理，我们有 $P(N \geq 220) \approx<br>            P(4.55 \times 220 \leq S_n \leq 4.57 \times 220) \approx<br>            \Phi(-2.12) - (1 - \Phi(2.22)) &#x3D; \Phi(2.22) - \Phi(2.12) \approx 0.0038$</p><h4 id="11-4"><a href="#11-4" class="headerlink" title="11."></a>11.</h4><p>不妨令$W_i &#x3D; X_i - Y_i$，由于$X_i, Y_i$都是$[0, 1]$上的均匀分布，<br>所以$E[W_i] &#x3D; E[X_i] - E[Y_i] &#x3D; 0,<br>        var(W_i) &#x3D; var(X_i) + var(Y_i) + 2cov(X_i + Y_i) &#x3D; 1$。<br>由中心极限定理，原式$P(|W - E[W]| &lt; 0.001) &#x3D; P(|W| \leq 0.001)<br>        &#x3D; 2P(W \leq 0.001) \approx 2\Phi(0) &#x3D; 1$。</p><h4 id="12-4"><a href="#12-4" class="headerlink" title="12."></a>12.</h4><p>略。</p><h4 id="13-4"><a href="#13-4" class="headerlink" title="13."></a>13.</h4><p>略。</p><h4 id="14-4"><a href="#14-4" class="headerlink" title="14."></a>14.</h4><p>略。</p><h4 id="15-4"><a href="#15-4" class="headerlink" title="15."></a>15.</h4><p>略。</p><h4 id="16-4"><a href="#16-4" class="headerlink" title="16."></a>16.</h4><p>略。</p><h4 id="17-4"><a href="#17-4" class="headerlink" title="17."></a>17.</h4><p>略。</p><h4 id="18-4"><a href="#18-4" class="headerlink" title="18."></a>18.</h4><p>略。</p>]]></content>
    
    
    
    <tags>
      
      <tag>答案</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《概率导论》学习笔记(持续更新中)</title>
    <link href="/2021/20210122/"/>
    <url>/2021/20210122/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>开始时间：2021&#x2F;01&#x2F;22</p><span id="more"></span><h3 id="第一章-样本空间与概率"><a href="#第一章-样本空间与概率" class="headerlink" title="第一章 样本空间与概率"></a>第一章 样本空间与概率</h3><p><strong>样本空间$\Omega$</strong>：这是一个试验的所有可能结果的集合.</p><p><strong>概率律</strong>：概率律为试验结果的集合$A$(称为<strong>事件</strong>)确定一个非负数$P(A)$(称为事件$A$的概率).</p><p>概率律满足以下几条公理(<strong>概率公理</strong>)：</p><blockquote><p>(1)**(非负性)<strong>对一切事件$A$,满足$P(A)\geq 0$.<br>(2)</strong>(可加性)<strong>设$A$和$B$为两个互不相交的集合(概率论中称为互不相容的事件),则它们的并满足$P(A\cup B)&#x3D;P(A)+P(B)$.<br>(3)</strong>(归一化)**整个样本空间$\Omega$(称为必然事件)的概率为1,即$P(\Omega)&#x3D;1$.</p></blockquote><p>由概率公理可导出处许多性质,下面为部分性质</p><blockquote><p><strong>概率论的若干性质：</strong><br>考虑一个概率律,令$A、B、C$为事件.<br>(a)若$A\subset B$,则$P(A)\leq P(B)$.<br>(b)$P(A\cup B)&#x3D;P(A)+P(B)-P(A\cap B)$.<br>(c)$P(A\cup B)\leq P(A)+P(B)$.<br>(d)$P(A\cup B\cup C)&#x3D;P(A)+P(A^c\cap B)+P(A^c\cap B\cap C)$.</p></blockquote><p>条件概率：给定的事件$B$发生了,另一个给定的事件$A$发生的概率,记为$P(A \mid B)&#x3D;\frac{P(A\cap B)}{P(B)}$.<br>显然地,条件概率是一个概率律,其满足概率律的三条公理.</p><p><strong>相关定理:</strong></p><blockquote><p><strong>乘法法则：</strong><br>假定所有涉及的条件概率都是正的,我们有:</p><p>$$<br>P(\cap _{i&#x3D;1}^n A_i)&#x3D;<br>P(A_1)P(A_2 \mid A_1)P(A_3 \mid A_1\cap A_2)…P(A_n \mid \cap _{i&#x3D;1}^{n-1} A_i)<br>$$</p></blockquote><blockquote><p><strong>全概率定理：</strong><br>设$A_1,A_2,…A_n$是一组互不相容的事件,形成样本空间的一个分割(每一个试验结果必定使得其中一个发生).又假定对每一个$i$,$P(A_i)&gt;0$.则对于任何事件$B$,下列公式成立<br>$$<br>P(B)&#x3D;P(A_1\cap B)+…+P(A_n\cap B)</p><p>&#x3D;P(A_1)P(B \mid A_1)+…+P(A_n)P(B \mid A_n).<br>$$</p></blockquote><blockquote><p><strong>关于条件概率的全概率公式：</strong><br>设$C_1,C_2,…C_n$是一组互不相容的事件,形成样本空间的一个分割(每一个试验结果必定使得其中一个发生).令A和B是两个事件，满足$P(B\cap C_i)&gt;0$对一切$i$成立，则：<br>$$<br>P(A\mid B)&#x3D;\sum_{i&#x3D;1}^n P(C_i\cap B)+…+P(A\mid B\cap C_i)<br>$$</p></blockquote><blockquote><p><strong>贝叶斯准则</strong><br>设$A_1,A_2,…A_n$是一组互不相容的事件,形成样本空间的一个分割(每一个试验结果必定使得其中一个事件发生).又假定对每一个$i$,$P(A_i)&gt;0.$则对于任何事件$B$,只要它满足$P(B)&gt;0$,下列公式成立:<br>$$<br>P(A_i \mid B)&#x3D;\frac{P(A_i)P(B \mid A_i)}{P(B)}<br>$$</p><p>$$<br>&#x3D;\frac{P(A_i)P(B \mid A_i)}{P(A_1)P(B \mid A_1)+…+P(A_n)P(B \mid A_n)}.<br>$$</p><p>$&#x3D;\frac{P(A_i)P(B \mid A_i)}{P(A_1)P(B \mid A_1)+…+P(A_n)P(B \mid A_n)}$.</p></blockquote><p><strong>独立性:</strong></p><ul><li><p>当等式$P(A\cap B)&#x3D;P(A)P(B)$成立时,我们称$A$和$B$是相互独立的事件.<br>若$B$还满足$P(B)&gt;0$,则独立性等价于$P(A \mid B)&#x3D;P(A)$.</p></li><li><p>$A$与$B$相互独立,则$A$与$B^c$也相互独立</p></li><li><p>设事件$C$满足$P(C)&gt;0$,两个事件$A$和$B$称为给定$C$的条件下条件独立,如果它们满足$P(A\cap B \mid C)&#x3D;P(A \mid C)P(B \mid C)$.<br>若进一步假定$P(B\cap C)&gt;0$,则$A$和$B$在给定$C$的条件下的条件独立性和以下条件是等价的：$P(A \mid B\cap C)&#x3D;P(A \mid C)$.</p></li><li><p>独立性并不蕴含条件独立性,反之亦然.</p></li><li><p><strong>几个事件的相互独立性的定义</strong>：<br>设$A_1,…A_n$为n个事件,<br>若它们满足<br>:<br>$$<br>P(\cup_{i\in S}A_i)&#x3D;\prod_{i\in S}P(A_i)对{1,2,3…n}的任意子集S成立<br>$$<br>则称$A_1,…A_n$为相互独立的事件.</p></li></ul><p><strong>德摩根公式：</strong>  $(A\cup B)^c&#x3D;A^c\cap B^c,(A\cap B)^c&#x3D;A^c\cup B^c$<br><strong>邦费罗尼不等式(Bonferroni’s inequality):</strong><br>$$<br>P(A\cap B)\geq P(A)+P(B)-1<br>$$<br>可推广到n个事件$A_1,A_2…A_n$的情况：<br>$$<br>P(A\cap B\cap…\cap A_n)\geq P(A)+P(B)+…+P(A_n)-(n-1)<br>$$</p><p><strong>容斥原理：</strong><br>设$A_1,A_2…A_n$为n个事件.记$S_1&#x3D;\left{i\mid 1\geq i\geq n\right},S_2&#x3D;\left{(i_1,i_2)\mid 1\leq i_1\leq  i_2 &lt; n\right}$…<br>$$<br>P(\cup_{k&#x3D;1}^n)<br>&#x3D;\sum_{i\in S_1}P(A_i)-\sum_{(i_1,i_2)\in S_2}P(A_{i_1}\cap A_{i_2})<br>+\sum_{(i_1,i_2,i_3)\in S_3}P(A_{i_1}\cap A_{i_2}\cap A_{i_3})<br>-…+(-1)^{n-1}P(\cap_{k&#x3D;1}^n A_k)<br>$$</p><h3 id="第二章-离散随机变量"><a href="#第二章-离散随机变量" class="headerlink" title="第二章 离散随机变量"></a>第二章 离散随机变量</h3><p><strong>与随机变量相关的主要概念</strong></p><p>在一个试验的概率模型之下：</p><ul><li><p><strong>随机变量</strong>是试验结果的实值函数；</p></li><li><p><strong>随机变量的函数</strong>定义了另一个随机函数；</p></li><li><p>对于一个随机变量,我们可以定义一些平均量,例如<strong>均值</strong>和<strong>方差</strong>；</p></li><li><p>可以在某事件或某随机变量的<strong>条件</strong>之下定义一个随机变量；</p></li><li><p>存在一个随机变量与某事件或某随机变量相互<strong>独立</strong>的概念；</p></li></ul><p><strong>与离散随机变量相关的主要概念</strong></p><p>在一个试验的概率模型之下：</p><ul><li><p><strong>离散随机变量</strong>是试验结果的一个实值函数,但是它的取值范围只能是有限多个值或可数无限多个值；</p></li><li><p>一个离散随机变量有一个<strong>分布列</strong>,它对于随机变量的每一个取值,给出一个概率；</p></li><li><p><strong>离散随机变量的函数</strong>也是一个离散随机变量,它的分布列可以从原随机变量的分布列得到.</p></li></ul><p><strong>一些分布列：</strong></p><blockquote><p><strong>二项随机变量</strong>:将一枚硬币抛掷n次,每次抛掷,正面出现概率为p,反面出现的概率为1-p,而且每次抛掷是相互独立的.令X为n次抛掷得到正面的次数.我们称X为<strong>二项随机变量</strong>.<br>$$<br>P(X&#x3D;k)&#x3D;\binom{n}{k}p^k(1-p)^{n-k},k&#x3D;0,1,…n.<br>$$<br>**几何随机变量：**在上述抛硬币试验中,令X为连续地抛掷一枚硬币,直到第一次正面所需要抛掷的次数.<br>$$<br>P(X&#x3D;k)&#x3D;(1-p)^{k-1}p,k&#x3D;1,2…<br>$$<br>**泊松随机变量：**设随机变量X的分布列由下式给出：</p><p>$$<br>P(X&#x3D;k)&#x3D;e^{-\lambda }\frac{\lambda ^k}{k!},k&#x3D;0,1,2…<br>$$</p></blockquote><p><strong>期望:</strong></p><p>设随机变量X的分布列为p{X},X的<strong>期望值</strong>(也称<strong>期望</strong>或<strong>均值</strong>)由下式给出：<br>$$<br>E[X]&#x3D;\sum_{x} xp_{X}(x).<br>$$<br><strong>方差：</strong>(记作$var(X)$)</p><p>随机变量$X$的方差由下列公式所定义：</p><p>$$<br>var(X)&#x3D;E[(X-E[x])^2].<br>$$<br>并且可以用下式进行计算：</p><p>$$<br>var(X)&#x3D;\sum_{x}(x-E[X])^2p_{X}(x)<br>$$</p><p>$$<br>var(X)&#x3D;E[X^2]-(E[X])^2<br>$$</p><p>它是非负的,其平方根称为<strong>标准差</strong></p><p><strong>随机变量的线性函数的均值与方差：</strong></p><p>设$X$为随机变量,令$Y&#x3D;aX+b$,其中a和b为给定的常数,则</p><p>$$<br>E[Y]&#x3D;aE[X]+b,var(Y)&#x3D;a^2var(X)<br>$$</p><p><strong>某些常用的随机变量的均值及方差：</strong></p><blockquote><p><strong>伯努利随机变量：</strong>$E[X]&#x3D;p$.$var(X)&#x3D;p(1-p)$</p><p><strong>泊松随机变量：</strong>$E[X]&#x3D;\lambda$.$var(X)&#x3D;\lambda$.</p></blockquote><p><strong>关于联合分布列：</strong></p><p>设$X$和$Y$为在某个试验中的随机变量.</p><ul><li>$X$和$Y$的联合分布列$p_{X,Y}$由下式定义：</li></ul><p>$$<br>p_{X,Y}(x,y)&#x3D;P(X&#x3D;x,Y&#x3D;y)<br>$$</p><ul><li>$X$和$Y$的边缘分布列可由下式得到：</li></ul><p>$$<br>p_{X}(x)&#x3D;\sum_{y} p_{X,Y}(x,y) , p_{Y}(y)&#x3D;\sum_{x} p_{X,Y}(x,y).<br>$$</p><ul><li>$X$和$Y$的函数$g(X,Y)$是一个随机变量,并且</li></ul><p>$$<br>E[g(X,Y)]&#x3D;\sum_x \sum_y g(x,y)p_{X,Y}(x,y)<br>$$</p><p>若g是线性的,且$g&#x3D;aX+bY+x$,则<br>$$<br>E[aX+bY+c]&#x3D;aE[X]+bE[Y]+c.<br>$$<br>.</p><ul><li>上面的结论可以类似地自然地推广到两个以上的随机变量的情况.</li></ul><p><strong>关于条件分布列</strong></p><p>设$X$和$Y$为某一试验中的两个随机变量.</p><ul><li><p>条件分布列与之前所学的无条件分布列完全相似,其差别只是前者是在已知某事件发生的条件下的随机变量的分布列.、</p></li><li><p>设A为某事件,$P(A)&gt;0$.随机变量X在给定A发生的条件下的条件分布列为：</p></li></ul><p>$$<br>p_{X \mid A}(x)&#x3D;P(X&#x3D;x \mid A)<br>$$</p><p>并且满足<br>$$<br>\sum_{x}p_{X \mid A}(x)&#x3D;1 .<br>$$</p><ul><li>设$A_1,…,A_n$是一组互不相容的事件,并且形成样本空间的一个分割,进一步假定$p(A_i)&gt;0$,则<br>$$<br>p_{X}(x)&#x3D;\sum_{i&#x3D;1}^{n}P(A_i)p_{X \mid A_i}(x)<br>$$</li></ul><p>(这是全概率定理的一种特殊情况.)进一步假定事件B满足对一切$i$,$P(A_i\cap B)&gt;0$.则</p><p>$$<br>p_{X \mid B}(x)&#x3D;\sum_{i&#x3D;1}^n P(A_i \mid b)p_{X \mid A_i \cap B}(x)<br>$$</p><ul><li>给定$Y &#x3D;y$ 的条件下 $X$ 的条件分布列与联合分布列之间有下列关系:</li></ul><p>$$<br>p_{X, Y}(x, y)&#x3D;p_{Y}(y) p_{X \mid Y}(x \mid y)<br>$$</p><ul><li>给定 $Y$ 之下的 $X$ 的条件分布列可以通过以下公式计算 $X$ 的边缘分布列:<br>$$<br>p_{X}(x)&#x3D;\sum_{y} p_{Y}(y) p_{X \mid Y}(x \mid y)<br>$$</li><li>同样地,上面的结论可以自然推广到两个以上的随机变量的情况.</li></ul><p><strong>关于条件期望：</strong></p><p>设$X$ 和 $Y$ 为某一试验中的两个随机变量.</p><ul><li><p>设A为某事件,$P(A)&gt;0$.随机变量X在给定A发生的条件下的条件期望为<br>$$<br>E [X \mid A]&#x3D;\sum_{\infty} x p_{X \mid A}(x)<br>$$<br>对于函数数 $g(X)$. 我们有<br>$$<br>E [g(X) \mid A]&#x3D;\sum_{x} g(x) p_{X \mid A}(x)<br>$$</p></li><li><p>给定 $Y&#x3D;y$ 的条件下 $X$ 的条件期望由下式定义<br>$$<br>E [X \mid Y&#x3D;y]&#x3D;\sum_{x} x p_{X \mid Y}(x \mid y)<br>$$</p></li><li><p>设$A_1,…,A_n$ 是互不相容的事件并且形成样本空间的一个分割.</p><p>假定$P(A_{i})&gt;0 \text{ 对一切 } i$ 成立. 则</p></li></ul><p>$$<br>E [X]&#x3D;\sum_{i&#x3D;1}^{n} P \left(A_{i}\right) E \left[X \mid A_{i}\right]<br>$$<br>进一步假定事件 $B$ 满足对一切 $i$, $P (A_{i} \cap B)&gt;0,$ 则<br>$$<br>E [X \mid B]&#x3D;\sum_{i&#x3D;1}^{n} P \left(A_{ i } \mid B\right) E \left[X \mid A_{i} \cap B\right]<br>$$</p><ul><li>我们有</li></ul><p>$$<br>E [X]&#x3D;\sum_{y} p_{Y}(y) E [X \mid Y&#x3D;y]<br>$$</p><p><strong>关于独立随机变量：</strong><br>设在某一试验中,A是一个时间段,满足条件$P(A)&gt;0$,又设X和Y是在同一试验中的两个随机变量.</p><ul><li>称X为相对于事件A独立,如果满足</li></ul><p>$$<br>p_{ X \mid A}(x)&#x3D;p_{X}(x) \text { 对一切 } x \text { 成立 }<br>$$<br>即对一切x,事件${X&#x3D;x}$与A相互独立.</p><ul><li>称$X$和$Y$为相互独立的随机变量,如果对一切可能的数对(x,y),事件${X&#x3D;x}$ 和 ${Y&#x3D;y}$ 相互独立, 或等价地</li></ul><p>$$<br>p_{X, Y}(x, y)&#x3D;p_{X}(x) p_{Y}(y) \text { 对一切 } x \text { 和 } y \text { 成立 }<br>$$</p><ul><li>若 $X$ 和 $Y$ 相互独立, 则<br>$$<br>E [X Y]&#x3D; E [X] E [Y]<br>$$<br>进一步地,对于任意函数$g$和$h$,随机变量$g(X)$和$h(Y)$也是相互独立的,并且<br>$$<br>E [g(X) h(Y)]&#x3D; E [g(X)] E [h(Y)]<br>$$</li><li>若 $X$ 和 $Y$ 相互独立, 则<br>$$<br>var(X+Y)&#x3D;var(X)+var(Y)<br>$$</li></ul><h3 id="第三章-一般随机变量"><a href="#第三章-一般随机变量" class="headerlink" title="第三章 一般随机变量"></a>第三章 一般随机变量</h3><p><strong>关于概率密度函数(PDF)：</strong></p><p>对于随机变量X,若存在一个非负函数$f_X$,使得<br>$$<br>P(X\in B)&#x3D;\int <em>B f</em>{X}(x)dx<br>$$<br>对每一个实数轴上的集合B都成立,则称X为连续的随机变量,函数$f_X$就称为X的概率密度函数,或简称PDF.</p><ul><li><p>$f_{X}(x) \geq 0$ 对一切 $x$ 成立</p></li><li><p>归一化$ \int_{-\infty}^{\infty} f_{X}(x) d x&#x3D;1 $</p></li><li><p>设 $ \delta $ 是一个充分小的正数, 则 $P ([x, x+ \delta ]) \approx f_{X}(x) \cdot \delta .$</p><blockquote><p>注：由于$f_X(x)$是概率律而非某一事件的概率,故其可以取任意大的值,例如：<br>$$<br>f_{X}(x)&#x3D;\left{\begin{array}{ll}\frac{1}{2\sqrt{x}}, \text{若}0&lt;x\leq 1 \<br>0, \text { 其他 }<br>\end{array}\right.<br>$$<br>在$x$趋于0时,$f_X(x)$可以任意的大</p></blockquote></li></ul><p><strong>连续随机变量的期望：</strong></p><p>记$X$为连续随机变量,其相应的PDF为$f_{X}x$.</p><ul><li><p>X的期望由下式定义：<br>$$<br>E[X]&#x3D;\int_{-\infty}^{\infty} xf_{X}(x)dx.<br>$$</p></li><li><p>关于随机变量$g(X)$的期望规则为：<br>$$<br>E[g(X)]&#x3D;\int_{-\infty}^{\infty} g(x)f_{X}(x) d x<br>$$</p></li><li><p>X的方差由下式给出：<br>$$<br>var(X)&#x3D;E[(X-E[X])^2]&#x3D;\int_{-\infty}^{\infty} (X-E[X])^2f_{X}(x) d x<br>$$</p></li><li><p>关于方差,下列公式成立：<br>$$<br>0\leq var(X)&#x3D;E[X^2]-(E[X])^2.<br>$$</p></li><li><p>设$Y&#x3D;aX+b$,其中a和b为常数,则<br>$$<br>E[Y]&#x3D;aE[X]+b,var(Y)&#x3D;a^2var(X)<br>$$</p></li></ul><p><strong>$分布函数(CDF)$ 的性质:</strong></p><p>随机变量$X$的$CDF$ $F_X$由下式定义<br>$$<br>\text { 对每一个 } x, F_{X}(x)&#x3D; P (X \leqslant x)<br>$$<br>并且 $F_{X}$ 具有下列性质._</p><ul><li>$F_{X}$ 是单调非减函数:</li></ul><p>$$<br>\text { 若 } x \leqslant y, \quad \text { 则 } F_{X}(x) \leqslant F_{X}(y) .<br>$$</p><ul><li><p>当 $x \rightarrow-\infty$ 的时侯, $F_{X}(x)$ 趋于 0 . 当 $x \rightarrow \infty$ 的时侯, $F_{X}(x)$ 趋于 1 .</p></li><li><p>当 $X$ 是离散随机变量的时候,$F_{X}(x)$为$x$的阶梯函数.</p></li><li><p>当 $X$ 是连续随机变量的时候,$F_{X}(x)$为$x$的连续函数.</p></li><li><p>当 $X$ 是离散随机变量并且取整数值的时,分布函数和分布列可以利用求和或差分互求:</p></li></ul><p>$$<br>\begin{array}{c}<br>F_{X}(k)&#x3D;\sum_{i&#x3D;-\infty}^{k} p_{X}(i) \<br>p_X(k)&#x3D;P(X \leqslant k)- P (X \leqslant k-1)&#x3D;F_{X}(k)-F_{X}(k-1),<br>\end{array}<br>$$<br>其中 $k$ 可以是任意整数.</p><ul><li>当$X$是连续随机变量的时候,分布函数和概率密度函数可以利用积分或微分互求：<br>$$<br>F_X(x)&#x3D;\int ^x_{-\infty}f_{X}(t)dt,f_{X}(t)&#x3D;\frac{dF_X}{dx}(x).<br>$$<br>(第二个等式只在分布函数可微的那些点上成立.)</li></ul><p><strong>正态随机变量：</strong></p><p>一个连续随机变量X称为正态的或高斯的,若它的概率密度函数具有下列形式：<br>$$<br>f_{X}(x)&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} e ^{-(x-\mu)^{2} &#x2F;\left(2 \sigma^{2}\right)}<br>$$<br>其中$\mu、\sigma$是概率密度的两个参数,其中$\sigma $还必须是正数.<br>其均值及方差：<br>$$<br>E [ X ]&#x3D;\mu, \quad \operatorname{var}(X)&#x3D;\sigma^{2}<br>$$</p><blockquote><p>线性变换下($Y&#x3D;aX+b$)随机变量的正态性保持不变.</p><p>其均值和方差由下式给出：<br>$$<br>E[Y]&#x3D;a\mu+b,var(Y)&#x3D;a^2{\sigma}^2<br>$$</p></blockquote><p><strong>多元连续随机变量性质：</strong></p><p>令X和Y为联合连续随机变量,其联合概率密度函数为$f_{X,Y}$</p><ul><li>利用<strong>联合概率密度函数</strong>可以进行概率计算：</li></ul><p>$$<br>P ((X, Y) \in B)&#x3D;\int_{(x, y) \in B} \int f_{X, Y}(x, y) d x d y<br>$$</p><ul><li>X和Y的边缘概率密度函数可利用联合概率密度函数进行计算得到：</li></ul><p>$$<br>f_{X}(x)&#x3D;\int_{-\infty}^{\infty} f_{(X, Y)}(x, y) d y, \quad f_{Y}(y)&#x3D;\int_{-\infty}^{\infty} f_{(X, Y)}(x, y) d x<br>$$</p><ul><li><strong>联合分布函数</strong>由公式$F_{X,Y}(x,y)&#x3D;P(X\leq x,Y\leq y)$定义,并且,在联合概率密度函数的连续点上,下面的公式成立：</li></ul><p>$$<br>f_{X, Y}(x, y)&#x3D;\frac{\partial^{2} F_{X, Y}}{\partial x \partial y}(x, y)<br>$$</p><ul><li>X和Y的函数$g(X,Y)$定义了一个新的随机变量,并且</li></ul><p>$$<br>E [g(X, Y)]&#x3D;\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f_{X, Y}(x, y) d x d y<br>$$</p><ul><li>若g是一个线性函数$aX+bY+c$,则</li></ul><p>$$<br>E[a X+b Y+c]&#x3D;a B[X]+b E[Y]+c<br>$$</p><ul><li>上面的结论能够很自然地推广到多于两个随机变量的情况.</li></ul><p><strong>以事件为条件的条件概率密度函数：</strong></p><ul><li>对于给定的事件$A(P(A)&gt;0)$,连续随机变量X的条件概率密度函数$f_{X|A}$是满足下列条件的函数：</li></ul><p>$$<br>P (X \in B \mid A)&#x3D;\left.\int_{B} f_{X}\right|<em>{A}(x) d x<br>$$<br>其中 $B$ 是实数轴上的任意集合.<br>$$<br>f</em>{X \mid{X \in A}}(x)&#x3D;\left{\begin{array}{ll}<br>\frac{f_{X}(x)}{P(X \in A)}, &amp; \text { 若 } x \in A, \<br>0, &amp; \text { 其他. }<br>\end{array}\right.<br>$$</p><ul><li>设$A_1,A_2,…,A_n$为互不相容的n个事件,对每个$i$,$P(A_i)&gt;0$,并且这些事件形成样本空间的一个分割.则</li></ul><p>$$<br>f_{X}(x)&#x3D;\sum_{i&#x3D;1}^{n} P\left(A_{i}\right) f_{X \mid A_{i}}(x)<br>$$<br>(全概率定理的一种变形).</p><p><strong>以另一个随机变量为条件的条件概率密度函数：</strong><br>设X和Y为联合连续随机变量,其联合概率密度函数为$f_{X,Y}$.</p><ul><li>X和Y的联合、边缘和条件概率密度函数是相互关联的.它们的关系用下面的公式表示：</li></ul><p>$$<br>\begin{aligned}<br>f_{X, Y}(x, y) &amp;&#x3D;f_{Y}(y) f_{X \mid Y}(x \mid y) \<br>f_{X}(x) &amp;&#x3D;\int_{-\infty}^{\infty} f_{Y}(y) f_{X \mid Y}(x \mid y) d y .<br>\end{aligned}<br>$$<br>条件概率密度函数$f_{X|Y}(x|y)$只在集合${y|f_Y(y)&gt;0}$上有定义.</p><ul><li>关于条件概率我们有<br>$$<br>P(X\in A \mid Y&#x3D;y)&#x3D;\int_{A} f_{X\mid Y}(x\mid y)dx<br>$$</li></ul><p><strong>条件期望性质：</strong></p><p>记 $X$ 和 $Y$ 为联合连续随机变量, $A$ 是满足 $P (A)&gt;0$ 的事件.</p><ul><li>$X$ 在给定事件$A$ 之下的条件期望由下式定义</li></ul><p>$$<br>E [X \mid A]&#x3D;\int_{-\infty}^{\infty} x f_{X \mid A}(x) d x<br>$$<br>给定 $Y&#x3D;y$ 之下的条件期望由下式定义<br>$$<br>E[X \mid Y&#x3D;y]&#x3D;\int_{-\infty}^{\infty} x f_{X \mid Y}(x \mid y) d x<br>$$</p><ul><li><strong>期望规则</strong>仍然有效:</li></ul><p>$$<br>\begin{array}{c}<br>E [g(X) \mid A]&#x3D;\int_{-\infty}^{\infty} g(x) f_{X \mid A}(x) d x \<br>E [g(X) \mid Y&#x3D;y]&#x3D;\int_{-\infty}^{\infty} g(x) f_{X \mid Y}(x \mid y) d x<br>\end{array}<br>$$</p><ul><li>**全期望定理：**设$A_1,A_2,…,A_n$为互不相容的n个事件,对每个$i$,$P(A_i)&gt;0$,并且这些事件形成样本空间的一个分割,则</li></ul><p>$$<br>E (X]&#x3D;\sum_{i&#x3D;1}^{n} P \left( A <em>{i}\right) E \left[X \mid A</em>{i}\right]<br>$$<br>相似地,<br>$$<br>E [X]&#x3D;\int_{-\infty}^{\infty} E \left[X|Y&#x3D;y| f_{Y}(y) d y\right.<br>$$</p><ul><li><p>涉及几个随机变量的函数的情况,具有完全相似的结果,例如：<br>$$<br>E \left[g\left(X,Y\right)|Y&#x3D;y|\right.&#x3D;\int g(x, y) f_{X \mid Y}(x \mid y) d x \</p><p>E[g(X, Y) \mid&#x3D;\int E [g(X, Y) \mid Y&#x3D;y] f_{Y}(y) d y<br>$$<br><strong>连续随机变量的相互独立性</strong></p><p>令X和Y为联合连续随机变量.</p></li><li><p>若</p><p>$$<br>f_{X, Y}(x, y)&#x3D;f_{X}(x) f_{Y}(y) \quad \text { 对一切 }x\text{和}y\text{成立}<br>$$</p><p>则 $X$ 和 $Y$ 相互独立.</p></li><li><p>若X 和 $Y$ 相互独立, 则</p><p>$$<br>E [X Y]&#x3D; E [X] E [Y]<br>$$<br>进一步,对于任意函数$g$和$h$,随机变量$g(X)$和$h(Y)$也是相互独立的, 于是<br>$$<br>E [g(X) h(Y)]&#x3D; E [g(X)] E [h(Y)]<br>$$</p></li><li><p>若 $X$ 和 $Y$ 相互独立,则</p><p>$$<br>\operatorname{var}(X+Y)&#x3D;\operatorname{var}(X)+\operatorname{var}(Y)<br>$$</p></li></ul><p><strong>连续随机变量的贝叶斯准则:</strong></p><p>令Y为连续随机变量.</p><ul><li>若X为连续随机变量,我们有</li></ul><p>$$<br>\begin{array}{c}<br>f_{X} y(x \mid y) f_{Y}(y)&#x3D;f_{X}(x) f_{Y \mid X}(y \mid x) \<br>\end{array}<br>$$</p><p>和<br>$$<br>\begin{array}{c}<br>f_{X \mid Y}(x \mid y)&#x3D;\frac{f_{X}(x) f_{Y \mid X}(y \mid x)}{f_{Y}(y)}&#x3D;\frac{f_{X}(x) f_{Y \mid X}(y \mid x)}{\int_{-\infty}^{\infty} f_{X}(t) f_{Y} x(y \mid t) d t}<br>\end{array}<br>$$</p><ul><li>若N为离散随机变量,我们有</li></ul><p>$$<br>f_{Y}(y) P (N&#x3D;n \mid Y&#x3D;y)&#x3D;p_{N}(n) f_{Y \mid N}(y \mid n)<br>$$<br>得到的贝叶斯公式为<br>$$<br>P ( N &#x3D;n \mid Y&#x3D;y)&#x3D;\frac{p_{ N }(n) f_{Y \mid N}(y \mid n)}{f_{Y}(y)}&#x3D;\frac{p_{N}(n) f_{Y} \mid N(y \mid n)}{\left.\sum_{i} p_{N}(\hat{i}) f_{Y}\right|<em>{N}(u \mid i)}<br>$$<br>和<br>$$<br>f</em>{Y \mid N}(y \mid n)&#x3D;\frac{f_{Y}(y) P (N&#x3D;n \mid Y&#x3D;y)}{p_{N}(n)}&#x3D;\frac{f_{Y}(y) P(N&#x3D;n Y&#x3D;y)}{\int_{-\infty}^{\infty} f_{Y}(t) P (N&#x3D;n \mid Y&#x3D;t) d t}<br>$$</p><ul><li>对于事件A,关于$P(A\mid Y&#x3D;y)$和$f_{Y\mid A}(y)$具有类似的贝叶斯公式.</li></ul><h3 id="第四章-随机变量的深入内容"><a href="#第四章-随机变量的深入内容" class="headerlink" title="第四章 随机变量的深入内容"></a>第四章 随机变量的深入内容</h3><p><strong>计算连续随机变量X的函数$Y&#x3D;g(X)$的概率密度函数(PDF)：</strong></p><p>(1)使用如下公式计算Y的概率函数(CDF)$F_Y$<br>$$<br>F_{Y}(y)&#x3D;P(g(X)\leq y)&#x3D;\int_{x\mid g(x)\leq y}f_X(x)dx.<br>$$<br>(2)对$F_Y$求导,得到Y的PDF：<br>$$<br>f_{Y}(y)&#x3D;\frac{dF_{Y}}{dy}(y)<br>$$<br><strong>随机变量X的线性函数的概率密度函数：</strong></p><p>假设X是连续随机变量,概率密度函数为$f_X$,a和b是实数且$a\neq 0$,如果<br>$$<br>Y&#x3D;aX+b<br>$$<br>则<br>$$<br>f_Y(y)&#x3D;\frac{1}{\mid X\mid}f_X(\frac{y-b}{a}).<br>$$</p><p><strong>连续随机变量X的严格单调函数$Y&#x3D;g(x)$的概率密度函数计算公式：</strong></p><p>假设g是严格单调函数,其逆函数h满足：对X的取值空间内任意一点$x$,$Y&#x3D;g(x) \quad$ 当且仅当 $\quad x&#x3D;h(y)$,<br>而且函数 $h$ 是可微的,则 $Y$ 在支撑集$y \mid f_{Y}(y)&gt;0$内的概率密度函数是<br>$$<br>f_{Y}(y)&#x3D;f_{X}(h(y))\left|\frac{ d h}{ d y}(y)\right|<br>$$<br><strong>卷积：</strong></p><p>定义：设X和Y是两个独立的随机变量考虑他们的和Z&#x3D;X+Y的分布.<br>$$<br>p_Z(z)&#x3D;P(X+Y&#x3D;z)<br>$$<br>分布列$p_Z$称为X和Y的分布列的卷积.</p><p><strong>卷积公式：</strong></p><p>变量$Z&#x3D;X+Y$的概率密度函数为<br>$$<br>f_Z(z)&#x3D;\int_{-\infty}^{\infty} f_X(x)f_Y(z-x)dx<br>$$<br><strong>协方差和相关：</strong></p><p>**协方差：**X和Y的协方差记为cov(X,Y),其定义如下：$cov(X,Y)&#x3D;E[(X-E(X))(Y-E(Y))]$.当cov(X,Y)&#x3D;0时,我们说X和Y不相关的.</p><p>另一种表达为$cov(X,Y)&#x3D;E[XY]-E[X]E[Y]$.</p><blockquote><p><strong>一些性质：</strong></p><p>$cov(X,X)&#x3D;var(X)$</p><p>$cov(X,aY+b)&#x3D;a*cov(X,Y)$</p><p>$cov(X,Y+Z)&#x3D;cov(X,Y)+cov(X,Z)$</p></blockquote><p>注：X和Y是相互独立的,则E[XY]&#x3D;E[X]E[Y],故cov(X,Y)&#x3D;0,它们是不相关的,但是逆命题并不成立.</p><p>有下列结论：</p><blockquote><p>假设$E[X\mid Y&#x3D;y]&#x3D;E[X]$ 对任意的y成立,则如果X和Y是离散变量时,E[XY]&#x3D;E[X]E[Y].(在连续的情形下依然成立.)</p></blockquote><p><strong>相关系数：</strong></p><p>两个方差非零的随机变量X和Y的相关系数$\rho (X,Y)$定义如下：<br>$$<br>\rho (X,Y)&#x3D;\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}<br>$$<br>易证$\rho\in [-1,1]$</p><p><strong>随机变量和的方差：</strong></p><p>协方差可以用于计算多个随机变量(不必独立)之和的方差.特别地,设随机变量$X_1…X_n$具有有限的方差,则<br>$$<br>var(X_1+X_2)&#x3D;var(X_1)+var(X_2)+2cov(X1,X2)<br>$$<br>更一般的结论是<br>$$<br>var(\sum_{i&#x3D;1}^n X_i)&#x3D;\sum_{i&#x3D;1}^n var(X_i)+\sum_{\left{ (i,j)\mid i\neq j\right} } cov(X_i,X_j).<br>$$<br><strong>条件期望和条件方差的性质</strong>：</p><ul><li>$E[X\mid Y&#x3D;y]$的值依赖于y.</li><li>$E[X\mid Y]$是随机变量Y的函数,因此它也是一个随机变量.当Y的值为y时,它的值就等于$E[X\mid Y&#x3D;y]$</li><li>$E[E[X\mid Y]]&#x3D;E[X]$ (<strong>重期望法则</strong>)</li><li>$E[X\mid Y&#x3D;y]$可视为已知Y&#x3D;y时对X的估计.相应的估计误差$E[X\mid Y]-X$是一个零均值的随机变量,且与$E[X\mid Y]$是不相关的.</li><li>$var(X)&#x3D;E[var(X\mid Y)]+var(E[X\mid Y])$ (<strong>全方差法则</strong>)</li></ul><p><strong>矩母函数：</strong></p><p>一个与随机变量X相关的矩母函数是参数s的函数$M_X(s)$,定义如下:<br>$$<br>M_X(s)&#x3D;E[e^{sX}]<br>$$<br>当X是离散随机变量,相关矩母函数为$M(s)&#x3D;\sum _x e^{sx}p_X(x)$.</p><p>当X是连续随机变量,相关矩母函数为$M(s)&#x3D;\int_{-\infty}^{\infty} e^{sx}f_X(x)dx$.</p><blockquote><p><strong>利用矩母函数计算随机变量的各阶矩：</strong><br>$$<br>M_X(0)&#x3D;1,\ \ \ \ \ \frac{d}{ds}M_X(s) \bigg|<em>{s&#x3D;0}&#x3D;E[X], \ \ \  \frac{d^n}{ds^n}M_X(s)\bigg|</em>{s&#x3D;0}&#x3D;E[X^n].<br>$$</p></blockquote><p>若$Y&#x3D;aX+b$,则$M_Y(s)&#x3D;e^{sb}M_X(as)$.</p><p>若X和Y相互独立,则$M_{X+Y}(s)&#x3D;M_X(s)M_Y{s}$.</p><p><strong>常见的离散随机变量的矩母函数：</strong></p><ul><li><p>参数为p的伯努利分布(k&#x3D;0,1)<br>$$<br>p_X(k)&#x3D;\left{\begin{array}{}<br>p, &amp; \text{若}k&#x3D;1,<br>\1-p,&amp;  \text { 若k&#x3D;0, }<br>\end{array}\right.<br>M_X(s)&#x3D;1-p+pe^s.<br>$$</p></li><li><p>参数为(n,p)的二项分布(k&#x3D;0,1,…,n)<br>$$<br>p_X(k)&#x3D;\binom{n}{k}p^k(1-p)^{n-k},M_X(s)&#x3D;(1-p+pe^s)^n.<br>$$</p></li><li><p>参数为 $p$ 的几何分布 $(k&#x3D;1,2 \ldots)$<br>$$<br>p_{X}(k)&#x3D;p(1-p)^{k-1}, \quad M_{X}(s)&#x3D;\frac{p e^{s}}{1-(1-p) e^{s}}<br>$$</p></li><li><p>参数为$\lambda$的泊松分布 $(k&#x3D;1,2 \ldots)$<br>$$<br>p_{X}(k)&#x3D;\frac{e^{-\lambda} \lambda^{k}}{k !}, \quad M_{X}(s)&#x3D;e^{\lambda\left(e^{s}-1\right)}<br>$$<br>$\bullet(a, b)$ 上的均匀分布 $(k&#x3D;a, a+1, \cdots, b)$<br>$$<br>p x(k)&#x3D;\frac{1}{b-a+1}, \quad M_{X}(s)&#x3D;\frac{e^{a s}}{b-a+1} \cdot \frac{e^{(b-a+1) s}-1}{e^{s}-1}<br>$$</p></li></ul><p><strong>常见连续随机变量的矩母函数：</strong></p><ul><li><p>$(a, b)$ 上的均匀分布 $(a \leqslant x \leqslant b)$<br>$$<br>f_{X}(x)&#x3D;\frac{1}{b-a},<br>M_{X}(s)&#x3D;\frac{1}{b-a} \cdot \frac{e^{s b}-e^{s a}}{s}<br>$$</p></li><li><p>参数为$\lambda$的指数分布$(x\geq 0)$<br>$$<br>f_{X}(x)&#x3D;\lambda e^{\lambda x}<br>,<br>M_{X}(s)&#x3D;\frac{\lambda}{\lambda-s}, (s&lt;\lambda)<br>$$</p></li><li><p>参数为$(\lambda,\mu)$的正态分布$(-\infty&lt;x\infty)$<br>$$<br>f_{X}(x)&#x3D;\frac{1}{\sqrt{2 \pi} \sigma} e ^{-(x-\mu)^{2} &#x2F; 2 \sigma^{2}}, \quad M_{X}(s)&#x3D; e ^{\left(\sigma^{2} s^{2} &#x2F; 2\right)+\mu s}<br>$$</p></li></ul><h3 id="第五章-极限理论"><a href="#第五章-极限理论" class="headerlink" title="第五章 极限理论"></a>第五章 极限理论</h3><p><strong>马尔科夫不等式:</strong></p><p>设随机变量$X$只取非负值，则对任意$a&gt;0$，<br>$$<br>P(X\geq a)\leq \frac{E[X]}{a}<br>$$<br><strong>切尔雪夫不等式：</strong></p><p>设随机变量$X$的均值为$\mu$,方差为$\sigma ^2$，则对任意$c&gt;0$，<br>$$<br>P(|X-\mu|\geq c)\leq \frac{\sigma ^2 }{c^2}<br>$$<br><strong>弱大数定律：</strong></p><p>设$X _1,…,X _n$独立同分布, 其分布的均值为$\mu$, </p><p>则对任意的$\epsilon&gt;0$, 当$n \rightarrow \infty$ 时,<br>$$<br>P \left(\left|M_{n}-\mu\right| \geqslant \epsilon\right)&#x3D; P \left(\left|\frac{X_{1}+\cdots+X_{n}}{n}-\mu\right| \geqslant \epsilon\right) \rightarrow 0<br>$$<br><strong>中心极限定理：</strong><br>设$X _1, X _2 …$是独立同分布的随机变量序列，序列的每一项的均值为$\mu$, 方差为$\sigma^2$.<br>记<br>$$<br>Z _n&#x3D;\frac{X _1+…+X _n-n \mu}{\sqrt{n} \sigma}<br>$$</p><p>则$Z _n$的分布函数的极限分布为标准正态分布函数：</p><p>$$<br>\Phi(x)&#x3D;\frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^{-z^2&#x2F;2}dz<br>$$</p><p>即<br>$$<br>\lim _{n \rightarrow \infty} P \left(Z _n \leqslant x\right)&#x3D;\Phi(x), \text { 对任意的 } x \text { 成立. }<br>$$<br><strong>强大数定律：</strong><br>设$X_1,$ $X _{2}, \cdots, X _{n}$是均值为$\mu$ 的独立同分布随机变量序列, 则样本均值$M _{n}&#x3D;(X _{1}+X _{2}+…+X _{n}) &#x2F; n$以概率 1 收敛于$\mu$, 即</p><p>$$<br>P \left(\lim _{n \rightarrow \infty} \frac{X _1+X _2+\cdots+X _n}{n}&#x3D;\mu\right)&#x3D;1<br>$$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>概率论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>随记</title>
    <link href="/2021/20210115/"/>
    <url>/2021/20210115/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="密码错误." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容">  <script id="hbeData" type="hbeData" data-hmacdigest="0f53c82593eba223e41ed0b9bca6ec9ef27f768416c2f8a3402e916c610481e3">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一道SYSU期末高数题</title>
    <link href="/2021/20210107/"/>
    <url>/2021/20210107/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>证明：当$x\in(0,\frac{\pi}{2})$, $\frac{tanx}{x}&gt; \frac{x}{sinx}$.</p><span id="more"></span><p>证法一：<br>$\sqrt{ \sin x \tan x } \geq \frac{2} { \frac{1} {\sin x} + \frac{ 1}{ \tan x} } &#x3D; \frac{2 \sin x} { 1 + \cos x } &#x3D; 2 \tan \frac{x}{2} \geq x$</p><p>证法二：<br>$\frac{\sin x}{x}&#x3D;\int_0^1 \cos(t x)dt$,</p><p>$\frac{\tan x}{x}&#x3D;\int_0^1\frac{1}{\cos^2(tx)}dt(0&lt;x&lt;\frac{\pi}{2}).$</p><p>根据Cauchy–Schwarz不等式有：<br>$\frac{\sin x}{x}\frac{\tan x}{x} $</p><p>$\geq (\int_0^1\frac{1}{\sqrt{\cos(tx)}}dt)^2&gt;1(0&lt;x&lt;\frac{\pi}{2}).$</p><p>证法三：<br>即证$f(x)&#x3D;sinxtanx-x^2&gt;0$<br>而$f(0)&#x3D;f’(0)&#x3D;f’’(0)&#x3D;0$<br>$f’’’(x)&#x3D;sinx(5sec^2x-1)+bsin^3xsec^4x&gt;0$<br>故$f(x)&gt;0$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020年度总结</title>
    <link href="/2020/20201231/"/>
    <url>/2020/20201231/</url>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="密码错误." data-whm="抱歉, 这个文章不能被校验, 不过您还是能看看解密后的内容">  <script id="hbeData" type="hbeData" data-hmacdigest="ba872a03b9732c2ad4cb3691f08efa6d1b787c8db5caefe780560481501f76de">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">需要密码.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>她</title>
    <link href="/2020/20201221/"/>
    <url>/2020/20201221/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>马孔多在下雨。<span id="more"></span></p><hr><p>来看场烟花吧。</p><div id="fireworks-container" style="width: 100%; height: 450px; background-color: #000; position: relative; margin-bottom: 20px;">   <canvas id="fireworksCanvas"></canvas> </div><script>  document.addEventListener('DOMContentLoaded', function() {    const canvasContainer = document.getElementById('fireworks-container');    const canvas = document.getElementById('fireworksCanvas');    // 确保容器和画布元素都存在    if (!canvasContainer || !canvas) {      console.error("烟花容器或画布元素未找到!");      return;    }        const ctx = canvas.getContext('2d');        // 设置画布大小为容器大小    function resizeCanvas() {      canvas.width = canvasContainer.offsetWidth;      canvas.height = canvasContainer.offsetHeight;    }    resizeCanvas(); // 初始调整大小        const fireworks = [];    const particles = [];    let hue = 120; // 基础色调        // 烟花类    class Firework {      constructor() {        this.x = Math.random() * canvas.width;        this.y = canvas.height;        this.targetX = Math.random() * canvas.width;        // 向上发射到容器高度的 10% 到 60% 之间        this.targetY = Math.random() * (canvas.height * 0.5) + (canvas.height * 0.1);        this.speed = Math.random() * 2 + 2; // 速度快一点        this.angle = Math.atan2(this.targetY - this.y, this.targetX - this.x);        this.brightness = Math.random() * 50 + 50;        this.targetRadius = 1;            this.coordinates = [];        this.coordinateCount = 3;        while (this.coordinateCount--) {          this.coordinates.push([this.x, this.y]);        }      }          update(index) {        this.coordinates.pop();        this.coordinates.unshift([this.x, this.y]);            if (this.targetRadius < 8) {          this.targetRadius += 0.3;        } else {          this.targetRadius = 1;        }            const dx = this.targetX - this.x;        const dy = this.targetY - this.y;        const distanceToTarget = Math.sqrt(dx * dx + dy * dy);            // 检查是否接近目标点        if (distanceToTarget < this.speed * 1.5) { // 调整此阈值以确保到达          fireworks.splice(index, 1);          createParticles(this.targetX, this.targetY);        } else {          this.x += Math.cos(this.angle) * this.speed;          this.y += Math.sin(this.angle) * this.speed;        }      }          draw() {        ctx.beginPath();        ctx.moveTo(this.coordinates[this.coordinates.length - 1][0], this.coordinates[this.coordinates.length - 1][1]);        ctx.lineTo(this.x, this.y);        ctx.strokeStyle = `hsl(${hue}, 100%, ${this.brightness}%)`;        ctx.stroke();            // 可选：绘制目标指示器        // ctx.beginPath();        // ctx.arc(this.targetX, this.targetY, this.targetRadius, 0, Math.PI * 2);        // ctx.stroke();      }    }        // 粒子类    class Particle {      constructor(x, y) {        this.x = x;        this.y = y;        this.angle = Math.random() * Math.PI * 2;        this.speed = Math.random() * 10 + 1;        this.friction = 0.95;        this.gravity = 1.5;        this.currentHue = Math.random() * 50 + (hue - 25);        this.brightness = Math.random() * 50 + 50;        this.alpha = 1;        this.decay = Math.random() * 0.03 + 0.015;        this.coordinates = [];        this.coordinateCount = 5;        while (this.coordinateCount--) {          this.coordinates.push([this.x, this.y]);        }      }          update(index) {        this.coordinates.pop();        this.coordinates.unshift([this.x, this.y]);        this.speed *= this.friction;        this.x += Math.cos(this.angle) * this.speed;        this.y += Math.sin(this.angle) * this.speed + this.gravity;        this.alpha -= this.decay;            if (this.alpha <= this.decay) {          particles.splice(index, 1);        }      }          draw() {        ctx.beginPath();        ctx.moveTo(this.coordinates[this.coordinates.length - 1][0], this.coordinates[this.coordinates.length - 1][1]);        ctx.lineTo(this.x, this.y);        ctx.strokeStyle = `hsla(${this.currentHue}, 100%, ${this.brightness}%, ${this.alpha})`;        ctx.stroke();      }    }        // 创建粒子效果    function createParticles(x, y) {      let particleCount = Math.floor(Math.random() * 50) + 80; // 粒子数量随机化      while (particleCount--) {        particles.push(new Particle(x, y));      }    }        let animationFrameId;    // 动画循环    function loop() {      animationFrameId = requestAnimationFrame(loop);      hue += 0.5; // 色调随时间变化          // 使用 destination-out 创建拖尾效果，而不是完全清除      ctx.globalCompositeOperation = 'destination-out';      ctx.fillStyle = 'rgba(0, 0, 0, 0.3)'; // 拖尾透明度，值越小拖尾越长      ctx.fillRect(0, 0, canvas.width, canvas.height);      ctx.globalCompositeOperation = 'lighter'; // 粒子叠加时颜色更亮          let i = fireworks.length;      while (i--) {        fireworks[i].draw();        fireworks[i].update(i);      }          let j = particles.length;      while (j--) {        particles[j].draw();        particles[j].update(j);      }          // 随机发射烟花      if (Math.random() < 0.04) { // 调整此值控制烟花密度        fireworks.push(new Firework());      }    }        // 监听窗口大小变化，重新设置画布大小    // 对于嵌入式canvas，其容器大小变化时才需要调整    // 如果容器大小是固定的，则不需要监听 window resize    // 如果容器大小是百分比的，则 window resize 时容器会变，canvas 也需要变    window.addEventListener('resize', resizeCanvas);        // 启动动画    loop();        // 可选：如果此 canvas 只在当前页面，当用户离开页面时停止动画以节省资源    // 这需要更复杂的逻辑来检测页面是否还可见或元素是否还在DOM中  });</script><p>2024年10月更新。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>来自斯坦福轮回赛的有趣积分题目</title>
    <link href="/2020/20201210/"/>
    <url>/2020/20201210/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><span id="more"></span><p><img src="https://s3.ax1x.com/2020/12/22/rr5ZFI.jpg" alt="1"></p><p>另法： $I(a)&#x3D;\int_{0}^{\pi}\ln\left(1-2a\cos(x)+a^2\right)\ \text{d}x,;\ a&gt;1$ </p><p>设三角形ABC，边$a,b,c$所对的角为$\alpha,\beta,\gamma$ </p><p>根据余弦定理有： $c^2&#x3D;a^2+b^2-2ab\cos(\gamma)$ </p><p>令$b&#x3D;1,\gamma&#x3D;x$ </p><p>则：</p><p> $I(a) &#x3D; \int_{0}^{\pi}\ln\left(c^2\right)\ \text{d}\gamma \ &#x3D; 2\int_{0}^{\pi}\ln\left(c\right)\ \text{d}\gamma $ </p><p>根据正弦定理有： $\frac{\sin(\alpha)}{a}&#x3D;\frac{\sin(\beta)}{b}&#x3D;\frac{\sin(\gamma)}{c}$ </p><p>故：</p><p> $I(a) &#x3D; 2\int_{0}^{\pi}\ln\left(a\ \frac{\sin(\gamma)}{\sin(\alpha)}\right)\ \text{d}\gamma \ &#x3D; 2\pi \ln(a)+2\int_{0}^{\pi}\ln\left(\sin(\gamma)\right)\ \text{d}\gamma-2\int_{0}^{\pi}\ln\left(\sin(\alpha)\right)\ \text{d}\gamma $</p><p> 又因为：$\gamma&#x3D;\pi-\alpha-\beta$,$\text{d}\gamma&#x3D;-\text{d}\alpha-\text{d}\beta$ </p><p>当$\gamma\rightarrow 0:\alpha\rightarrow \pi,\beta\rightarrow 0$ </p><p>当$\gamma\rightarrow \pi:\alpha\rightarrow 0,\beta\rightarrow 0$ </p><p>故 $I(a)  &#x3D; 2\pi \ln(a)+2\int_{0}^{\pi}\ln\left(\sin(\gamma)\right)\ \text{d}\gamma\  -2\left[\int_{\pi}^{0}\ln\left(\sin(\alpha)\right)(-\text{d}\alpha)+\int_{0}^{0}\ln\left(\sin(\alpha)\right)(-\text{d}\beta)\right] \  &#x3D; 2\pi \ln(a)+2\int_{0}^{\pi}\ln\left(\sin(\gamma)\right)\ \text{d}\gamma-2\int_{0}^{\pi}\ln\left(\sin(\alpha)\right)\ \text{d}\alpha  &#x3D; 2\pi \ln(a)$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>旅行商问题及其拓展</title>
    <link href="/2020/20201207/"/>
    <url>/2020/20201207/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>旅行商问题（英文：Travelling salesman problem, TSP）是图论、组合数学中一个非常经典的问题。</p><span id="more"></span><p>旅行商问题具体内容：给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。其与哈密顿有着密切的联系，TSP可以约化为哈密顿回路。</p><p>TSP属于NPC问题，故无多项式时间算法，常用求解算法有回溯法、分支限界法、等，以上均为O（n!）的时间复杂度。</p><p>TSP对路线规划有着重要应用，而路线往往为非哈密顿图。可以适当地对TSP进行拓展：（正是SYSU2020年新手赛A题。）</p><p>给定一系列城市和每对城市之间的距离，求解访问每一座城市并回到起始城市的最短回路。（该形式貌似又被称为Euclidean TSP）</p><p>该问题可以约化为TSP。</p><p>解法：</p><p>获得每个城市到其他城市的最短路径，若该城市与某城市不可直接到达，那么这两站之间增加一条边，边的权为以上所求最短路径的长度。而这个图一定是哈密顿圈。该问题现在约化为了TSP。</p>]]></content>
    
    
    
    <tags>
      
      <tag>算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《八阵图》（节选）-温瑞安</title>
    <link href="/2020/20201117/"/>
    <url>/2020/20201117/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>《八阵图》是武侠大家温瑞安十八岁所作的万字散文。在十八岁的年龄，少年温瑞安着实狂傲不凡，意气风发，“狂笑当歌江湖路”，肆意挥洒，纵情高歌。<br>在这个年龄，有的人却不过是些老弱不济之徒，被过去的梦想压着，从此再无力于乌托邦，只是一群倦怠未来的掘墓人。<span id="more"></span></p><p>《八阵图》节选如下：</p><hr><p>“青春是不中用的东西”？我们唱了，然后互相对望一眼，再唱下去。我们歪歪斜斜地乱步着，沙原上一片空漠，夜深沉;夜夜深深沉沉。我们如月下的精灵，酩酊于太白的月下，或起舞、或弄清影,但绝不止三人！想我们第一次在诗会中喝过的酒— 酒瓶狼藉高高低低，东倒西歪。一夜的哭诉,一夜的呕吐。想那次胡笳十八拍的月夜,我拔出两把莹亮的小刀，飞舞于思君令人老的月下，刀起刀落，刀去刀来，灿闪如我年轻的生命！啊白衣，我学的是国粹，练的是国术，但寥落江湖，竟无一可谈之人。我活着，是因为我的诚和真，我的劲和热，还有不能忽略的是： </p><p>​我的<br>​狂傲<br>​啊<br>​狂傲</p><p>我必须面对它了。我只怕一件事，我不怕打击，不怕失败，不怕失望，只怕死亡，因为它是我惟一克服不了的命运，改变不了的终局。或许，为了一件具有伟大的意义或真理,我不惜以生命去换取;但我憎恶死亡。可是没有人能自死亡的巨网中逃脱；既然如此，我惟有以生命的光芒去照亮死亡。我本身的光芒也许是很微弱的，但我会以我的撞击去发出我所有的星花 ，生命是悲哀的，死亡是可叹的。但我蔑视它们，既然我无法逃避，等它来吧！在它未来之前，我会尽量放发我的星花，让更多一些人能分享它片刻的温热与光芒。</p><p>不能征服死亡，那么，不要被它征服吧！死亡是硕大无比的，也许它的本身并不可怕，可怕的是它的灭绝。死亡造成的是对生存的遗憾与生命的留恋，但连遗憾与留恋都不能再容纳了，因为死了便等于什么都没有了。死是痛苦的，他，或她，只能静静静静地蜷伏在冷冷湿湿的黄土中，每一昼每一夜昼昼夜夜地躺着，不能移动也不能说话，没有思想也没有感觉,直至有一天他们变成了一堆白骨，由白骨再化成泥尘，永远，永远永远永远氷远地消失在世上;尽管他们在生前或许是圣贤豪杰，或是绝世红颜，但那都是些过去的事了；如果他们曾在世上擦亮起一片灿烂的星火，那么，或许有人会追念他们在星月下。但追念又能弥补些什么呢？仍生存着的，只平添一种淡淡的轻愁；已死去了的，已完全没有感觉了。我们活着的一天，有多少时候是去为那些逝者而缅怀而追念呢？多少名将会忆起岳武穆的雄风？多少名士会追念苏东坡的豪情？就像我们这群专研文学的，有多少时候，会默默地为那给世界思潮巨大影响的学人柏拉图、亚里士多德等低首追回过？更休说那些成就不及这些伟人的冤魂了。—将功成.枯朽的岂仅是万骨而已？ 一次改革，熬白了多少人的青发！但那些人呢？枯了，朽了，随风而逝了，他们曾经活过，曾为自己想过，也为别人想过，但而今呢？偶尔想起他们的，又有谁为他们呢？甚至他们已被淡忘了，他们的名宇已随历史的蹄尘而湮远了— 最残忍的是：他们已化为泥尘，不管被忆或被忘，都与他们无关了，死去便是什么都没有，包括追念和何忆。或许如今我们正踩在他们的头顶上，在他们在冷湿的黄土中，亘古以来所发生的事，已与他们无涉:他们已什么都不知道！他们连什么都不是了！覆盖在他们顶上的，是如此美丽而古典的星<br>空，但他们知道吗？他们知道吗？</p><p>我们仍疾步走着，被蛊惑似的走着，被赶尸似的走着。如果前面忽然出现的是一具无瞳无目长舌滴血的慑青鬼，对我们来说，或许还是一种存在的证实— 至少我们知道，死后还有再生，虽然这种“再生”是等于“死活”，或许是活在一个更惨诡的世界里，但这毕竟是存在啊存在！只要有存在、有意识，便算是有价值的活了！只要死去不是什么都没有:没有回忆，没有过去的痛苦和欢乐，也没有现在的痛苦和欢乐，更没有将来的痛苦和欢乐！就连我们现在正想着生存和死亡，但有一天，忽然连这一点痛苦的思索都无存了，那该是多残忍啊残忍<br>残<br>忍。<br>人渺小，人太渺小了，很容易被死亡所击败。夭折，寿终，都逃离不了宇宙的冷视。宇宙究竟是什么呢？如果我们抬目，只见一片黑压压的枝叶，就这样扼杀了人的视力。就算我们能仰望星空吧！我们所能见到的，是浩淼的星海。哪一颗星离你最近?设若你神游到那星上去，在那儿望见的地球，是不是也是星海中一颗随时可以幻灭的小星？而你只不过是这小星星上的一丁点儿些微的小东西罢了！你可以干出一些什么伟大的事业来吗？也许吧！在这大宇宙里，我们能了解的是多少颗星星？唔，从这里望去，是一片无尽的云海，那么无尽的云海外又是什么？无尽的云海之外的无尽云海之外的无尽的云海之外之外之外又是什么？是边际吗？边际之外又是些什么？要永垂千古，要永恒，要不朽，在我们的星球上已难做到，每一个星球都有它们自己的经典，我们能做到的又是什么？星星之外的星星之外—我是说在最强的暸望镜中，能看到的是多少颗星星？星星之外呢？这浩淼的宇宙啊— 宇宙真的浩淼吗？这整个“无限”的大宇宙，是不是一个“无限”的神它指下的棋盘，棋盘上放满小星星，所谓时间，便是它们的对弈的黑子白子呢？我不知道，没有人能知道，如果人类以后还有千千万万的历史，等有一天他们“征服”整个宇宙后，才惊觉他们只是从一个太极八卦图里跳出来而已，那是何等荒唐啊！八卦两仪以外的呢？所以当我们望星，当我们整个融人大自然时，我们早已被那蓝得深永的云海所溺毙了！</p><p>一霎那间，时间和命运的洪流淹盖了一切，我，人类，以及一切一切。未知的无限,对悠悠之天地的无奈与哀恨，都浮现在我所有的感官里。前人的遗恨，今人的虚寂，如戏剧而且是悲剧地在空漠的时间之流里鱼贯走过，多么ironical!但是我能做些什么?我们能做些什么呢？我们仍然年少，仍然狂热，仍然渴切着把自<br>己的辉煌映照在别人的身上！怎么能因为时间、空间与命运的汪洋便丧失了渡航的勇气呢？如果有命运，如果真的有命运的话，命定了我现在要因恐惧而停顿我的步伐，我偏走偏要走要走要走要走—— 如果有命运，命运那厮要我现在不能开口，我偏要开口，开口笑:哈哈哈哈哈哈。这算是给命运的一种反击？究竟是我败了它？还是它败了我？是它本来要我没来由地笑起来？还是我没来由的笑已惊破它的掌握？我不知道，我知道我的伙伴因为我笑声而放缓了步伐。我不能知道得那么多了！我还年轻，我仍豪放，我的刀尖而利，我的箫并不凄凉！我是龙啊龙是我我是龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙龙……周遭还是无天无地无边无际无岸无涯无远无近无生命的黑暗。、</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一道三角函数极限计算的一题多解</title>
    <link href="/2020/20201115/"/>
    <url>/2020/20201115/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>求极限$\lim_{x\rightarrow 0}\frac{\tan(\tan x)-\sin(\sin x)}{\tan x-\sin x}$ </p><span id="more"></span><p>法一：</p><p> $\lim_{x\to0} \frac{\tan(\tan x) - \sin(\sin x)}{\tan x -\sin x}&#x3D;\ \lim_{x\to0} \frac{\tan(\tan x) - \tan(\sin x)+\tan(\sin x)-\sin(\sin x)}{\tan x -\sin x} &#x3D;\ \lim_{x\to0} \frac{[\tan(\tan x -\sin x)][1-\tan(\tan x)\tan(\sin x)]}{\tan x -\sin x}+\lim_{x\to0} \frac{\sin(\sin x)[1 - \cos(\sin x)]}{\cos(\sin x)(\tan x -\sin x)}&#x3D;$ $&#x3D;1+\lim_{x\to0} \frac{\sin(\sin x)}{\sin x}\lim_{x\to0} \frac{1 - \cos(\sin x)}{\sin^2 x}\lim_{x\to0}\frac{x^2}{1-\cos x}\lim_{x\to0}\frac{\sin^2x}{x^2} &#x3D; 1+1*\frac{1}{2}<em>2</em>1 &#x3D;2$ </p><p>法二：</p><p> $\tan(x)&#x3D;x+\frac{x^3}{3}+\frac{2 x^5}{15}+O\left(x^7\right) $ $\sin(x)&#x3D;x-\frac{x^3}{6}+\frac{x^5}{120}+O\left(x^7\right) $ $\tan(\tan(x))&#x3D;x+\frac{2 x^3}{3}+\frac{3 x^5}{5}+O\left(x^7\right) $ $\sin(\sin(x))&#x3D;x-\frac{x^3}{3}+\frac{x^5}{10}+O\left(x^7\right) $ </p><p>故$\frac{tan(tan(x)) - sin (sin( x)}{ tan (x) - sin (x)}$</p><p>$ &#x3D;\frac {x^3+\frac{x^5}{2}+O\left(x^7\right) } {\frac{x^3}{2}+\frac{x^5}{8}+O\left(x^7\right) } \&#x3D; 2+\frac{x^2}{2}+O\left(x^4\right) &#x3D;2$ </p><p>法三： </p><p>$\lim_{x\to 0}\frac{\tan(\tan x)-\sin(\sin x)}{\tan x-\sin x}$ $&#x3D; \lim_{x\to 0}\frac{\tan(\tan x)-\tan(\sin x)\cos(\sin x)}{\tan x - \sin x}$ $&#x3D;\lim_{x\to 0}\frac{\tan(\tan x)-\tan(\sin x)+\tan(\sin x)-\tan(\sin x)\cos(\sin x)}{\tan x -\sin x}$ $&#x3D;\lim_{x\to 0}\left(\frac{\tan(\tan x)-\tan(\sin x)}{\tan x-\sin x}+\frac{\tan(\sin x)(1-\cos(\sin x))}{\tan x - \sin x}\right)$ </p><p>根据拉格朗日中值定理得：</p><p> $\frac{\tan(\tan x)-\tan(\sin x)}{\tan x-\sin x}&#x3D;\sec^2 c $ , $c\in[sinx,tanx]$  </p><p>故$\lim_{x\to 0} \frac{\tan(\tan x)-\tan(\sin x)}{\tan x-\sin x} &#x3D;1$ </p><p>而$\lim_{x\to 0}\frac{\tan(\sin x)(1-\cos(\sin x))}{\tan x - \sin x}$ $&#x3D;\lim_{x\to 0}\frac{\tan(\sin x)(1-\cos(\sin x))\cos x}{\sin x(1 - \cos x)}$ $&#x3D;\lim_{x\to 0}\frac{\tan(\sin x)}{\sin x}\cdot \frac{1-\cos(\sin x)}{\sin^2 x}\cdot \frac{\sin^2 x}{1-\cos x}\cdot \cos x$ $&#x3D;\lim_{x\to 0}\frac{\tan(\sin x)}{\sin x}\lim_{x\to 0}\frac{1-\cos(\sin x)}{\sin^2 x}\lim_{x\to 0}(1+\cos x)\lim_{x\to 0}\cos x$ $&#x3D;1\cdot \frac{1}{2}\cdot 2\cdot 1$ $&#x3D;1.$ </p><p>故原极限为2. </p><p>法四：</p><p> 令$A&#x3D;\frac{\tan (\tan x)-x}{\tan x-\sin x}$，$B&#x3D;\frac{x-\sin (\sin x)}{\tan x-\sin x}$ </p><p>通过洛必达可得： </p><p>$A&#x3D;\frac{tan (tan x)-x}{\tan x-\sin x}$</p><p>$\sim \frac{\frac{1}{cos ^2x·cos ^2(tan x)}-1}{\frac{1-cos ^3x}{cos ^2x}}$ </p><p>$\sim \frac{1-cos ^2x·cos ^2(\tan x)}{3(1-\cos x)}$</p><p>$\sim \frac{(1-cos ^2x)-cos ^2x(1-cos ^2(\tan x))}{3(1-\cos x)}$ </p><p>故:</p><p>$A\sim \frac{1+\cos x}{3}-\frac{1-\cos (\tan x)}{3(1-\cos x)}\sim \frac{2}{3}+\frac{\frac{1}{cos ^2x}\sin (\tan x)}{3\sin x}\sim \frac{2}{3}+\frac{1}{3}&#x3D;1$ </p><p>类似地$B&#x3D;1$ </p><p>故原式$&#x3D;A+B&#x3D;2$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>读《西西弗神话》有感</title>
    <link href="/2020/20201026/"/>
    <url>/2020/20201026/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我们都是生活中的西西弗，重复着日复一日地习惯和循环的链条。神话故事中的西西弗亦是如此，被迫接受了无法抗争的命运，重复着滚石头的动作，荒谬且痛苦。<span id="more"></span><br>初读《西西弗神话》，本以为加缪同齐奥朗一般的虚无主义者，将以一种彻底虚无的文字敲击我们的心灵。然而并非如此，在加缪的文字中，我看到更多的是积极反抗，不消极避世或是像犬儒主义者般消极入世。<br>在《西西弗神话》中，加缪首先探讨了荒谬与自杀的关联。我们被时间裹挟着向前，未来的确定与不确定使我们染上了陌生和诡异感，进而感受到非常痛苦的抵抗，荒谬感。<br>加缪将荒谬人分为三种。一是生理自杀者，无法忍受生活的荒缪与无意义，选择了结自己的生命。二是哲学自杀者，通过神（上帝）和宗教屏蔽自己的感知，不去感受荒谬，像把头埋在沙里的鸵鸟，掩耳盗铃。三是被加缪称赞的反抗者，荒谬英雄。重复着每天痛苦而不终止的悲剧。无法拒绝命运，那我就为现在而活，欣然坦然接受自己的命运，将巨石徒劳无功地一次次推上山顶，不作荒谬的逃避者，清醒地体验生活的荒谬感，以激情的态度向前、向前、向前，在无意义、荒谬的工作中，找到属于自己的意义，找到自己的存在。<br>罗曼罗兰曾言：世界上只有一种英雄主义,就是看清生活的真相之后依然热爱生活。西西弗正是这样的英雄，这样的“荒谬英雄”。生命本身是荒谬的，是毫无意义的，但我们要觅寻到属于我们的命运，属于我们的某种意义，在心中坚守一生。也正如《德米安》中所言：其他的路都是不完整的，是人的逃避方式，是随波逐流，是对内心的恐惧。</p><p>我也曾切身体会生活中的荒谬感，我也不争气地成为过哲学自杀者，也曾主动“拥抱“上帝，也略有涉猎等宗教书籍，但发现这并不是我的道路。偏激的我甚至转向了神的对立面，阅读了一些渎神之作。但这无济于事，于事无补，我变得更加虚无，犬儒，消极，一词以蔽之就是——丧。我很感谢我能遇上这本书还有许许多多给予我希望，”拯救“了我的书。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《黄河》——《山河录》</title>
    <link href="/2020/20201025/"/>
    <url>/2020/20201025/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>是曰：</p><p>我的歌</p><p>是一道静静的水流穿出幽谷</p><p>本是悠闲，而后激越。越是荒漠，越是悲壮。</p><span id="more"></span><p>转转折折，许许多多汇合后，</p><p>化成一条万古云霄万古愁的身姿，浩浩荡荡地唱：</p><p>我是黄河我是黄河</p><p>我的悲伤是千万人的悲伤</p><p>我的歌是千万人的歌</p><p>我是黄河我是黄河我</p><p>是黄河我是黄河我是黄河</p><p>我是黄河……</p><p>流动是可喜的</p><p>成为一池碧潭却是……</p><p>在所有的东树里</p><p>我是风，自湖水的衣襟褶过</p><p>在一棵枯之间停留</p><p>惊见两掌红红而纤小的叶。</p><p>我是幽静的水流</p><p>上可以几千万里</p><p>成千军万马的降临</p><p>下可以成瀑布</p><p>把岩石冲激成冲激的岩石</p><p>那我就化身成人吧</p><p>杀身成仁，风涌云动</p><p>在断崖上，断日下</p><p>一件白衣荡荡而飘</p><p>轻愁是美好的</p><p>可是执着呢？……</p><p>在大梦中，我是那寻寻觅觅叩访惊喜的人。</p><p>究竟谁是侠骨的真？</p><p>今天我写诗</p><p>明天我的路更远</p><p>从等待惊喜到迷惘得在暮色里摘花</p><p>在苍茫中回首</p><p>看月窗前的自己和她不甚清楚</p><p>我今天要走</p><p>明天雪鱼冻林</p><p>在迟了千百年后的今宵</p><p>我们于风尘中相见</p><p>仅仅让君子知道</p><p>许多感动因年龄而不再</p><p>我难以再作悲伤的流露……</p><p>而今大江一重，搁在身前</p><p>兄弟，读你的诗才几行</p><p>大江已寒……</p><p>今天我送你</p><p>明天路可以远至逍遥千里</p><p>冷漠是可喜的</p><p>真挚的一惊呢？</p><p>在全然的黑暗中</p><p>风和风在呼啸叶子和叶子在回应</p><p>我感觉到你就是和我走哪不了解长路的人。</p><p>没有关怀，不说一句话</p><p>怕更受伤。怕没有风。</p><p>怕没有温暖的黑暗。</p><p>怕一朵花谢和她的开……</p><p>灯乍亮，</p><p>你还是端坐在千万人中</p><p>那么脆弱而易受伤</p><p>或作嗔喜，或作自卫而笑……</p><p>而千万人中，</p><p>我就渴望那么一眼</p><p>千万年中，我生来就为等着</p><p>千次万次中，就白衣那么一次</p><p>当杏花 烟雨 绿水江南岸。</p><p>当我诗篇背后</p><p>透出银色的字</p><p>你喜悦不喜悦？</p><p>感动是可忧的</p><p>而我年岁悠悠……</p><p>就化身为枯藤松柏吧</p><p>我有更长而倦的守望</p><p>在许多敬佩与不敬佩的目光中</p><p>你的了解更是抹不去的一笔。</p><p>容颜可以秀动峨眉</p><p>我是多么向往那绿水的情怀</p><p>你纵化为悄悄的女魂</p><p>小心我便是那珍藏古镜的书生</p><p>把你摄入镜中，是时候</p><p>便轻声一声二声三声呵暖你</p><p>要年出来伴我长夜枯灯</p><p>我一剑西来</p><p>你衣裙袅动</p><p>那么小小的可爱</p><p>流过庭院</p><p>我在寺中抄经</p><p>而明天要练拳易筋……</p><p>春山爱笑</p><p>明天我的路更远</p><p>马蹄成了蝴蝶</p><p>弯弓射箭，走过绿林</p><p>我是那上京应考而不读书的书生</p><p>来洛阳是为求看你的倒影</p><p>水里的绝笔，天光里的遗言</p><p>挽绝你小小的清瘦</p><p>一瓢饮你小小的丰满</p><p>就是爱情和失恋</p><p>是我一首诗又一首诗</p><p>活得像泰山刻石惊涛裂岸的第一笔……</p><p>我的笔又苦又尖</p><p>梦是可喜</p><p>爱是可忧</p><p>我还有静静的玄关要迎送</p><p>你听我步履远去</p><p>我送你迎风</p><p>浩浩荡荡，长洲巨滩</p><p>就洞庭，就太华</p><p>括苍到点苍，</p><p>我的金刚经</p><p>比出匣时更势若沧浪</p><p>我是那自出阳关的第一水</p><p>从柔情传达给我的激情</p><p>剪刀峰，大小龙秋飞瀑</p><p>一气呵成而泻千里</p><p>我的歌不尽</p><p>上可以九万里而不止</p><p>下可以……</p><p>我还是那不应考而为骑骏马上京的一介寒生</p><p>秋水成剑，生平最乐</p><p>无数知音可刎颈</p><p>红颜能为长剑而琴断，宝刀能为知己而轻用</p><p>有女拂袖。有女明灯。有女答客。</p><p>沏茶还是茗酒</p><p>为剑可以白衣</p><p>可以飘行千里</p><p>而我正有远远的路要走……</p><p>越来越接近那吼声了</p><p>那是没有终止的冲决</p><p>崩却原是苍茫滩上的</p><p>一夫当关，狠命一击</p><p>气势自出，岁月俞久</p><p>我的京试愈垂青史……</p><p>这首诗我不停而写</p><p>才气你究竟什么时候才断绝？</p><p>水声更近，天涯无尽</p><p>在此诀别，红颜知音</p><p>那在雁荡飞跃之君子</p><p>那烛光中仍独悒清芬之秀颜</p><p>几时才在明月天山间</p><p>我化成大海</p><p>你化成清风</p><p>我们再守一守</p><p>那锦绣的神州……</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《德米安》</title>
    <link href="/2020/20201022/"/>
    <url>/2020/20201022/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>我常幻想未来，想象着自己可能会成为一名诗人、先知或画家。</p><span id="more"></span>但这些都徒劳无益。我人生的意义并不是写诗、讲道或绘画，其他人的也不应如此，所有这些应仅被视为爱好。每个人都只有一个使命，那就是寻找自我，无论最终成为诗人还是疯子、先知还是罪犯，都不关紧要。一个人的主要任务就是找到属于自己的命运，并全心全意地沿着命运之路前行。其他一切都是逃避的借口，是泯然众人的退缩、随波逐流，也是内心的恐惧。我心中的不断出现新意象，它们可能曾在我面前出现过，但直到现在我才第一次经历。这是一次本性的实验，也是一场结果未知的博弈，它可能会找到新的出路，也可能什么也找不到。我唯一的使命就是任其自由发展，感受它的意志，并完全掌握住。除此以外，别无其他。我早已感受过孤独，现在我将要忍受更刻骨铭心的孤独，一切都无法避免。]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一个关于e的证明的一题多解</title>
    <link href="/2020/20201009/"/>
    <url>/2020/20201009/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>求证：$e&gt;1+1+\frac{1}{2!}+$···$+\frac{1}{k!}$</p><span id="more"></span><p><img src="https://s3.ax1x.com/2020/11/18/DnW1SJ.jpg" alt="1.jpg"></p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一个关于不定方程的猜想</title>
    <link href="/2020/20200905/"/>
    <url>/2020/20200905/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>不定方程：$a^x+(a+1)^y&#x3D;(2a+1)^z$.<br>$a,x,y,z$均为自然数，且$a&gt;1$<br>证明：若$x,y,z$不全为偶数，则$x&#x3D;y&#x3D;z&#x3D;1$</p><p>该猜想由本人约一年半前提出，曾发在StackExchange、数学研发论坛、AOPS论坛，可惜未有人给出完整证明。</p><hr><p><strong>2025&#x2F;06&#x2F;15更新证明：</strong></p><p>对方程在模 $(a+1)$ 有：<br>$a \equiv -1 \pmod{a+1}$<br>$a+1 \equiv 0 \pmod{a+1}$<br>$2a+1 &#x3D; 2(a+1) - 1 \equiv -1 \pmod{a+1}$</p><p>将这些关系代入原方程：<br>$$a^x + (a+1)^y \equiv (2a+1)^z \pmod{a+1}$$<br>$$(-1)^x + 0^y \equiv (-1)^z \pmod{a+1}$$<br>$$(-1)^x \equiv (-1)^z \pmod{a+1}$$</p><p>故**$x$ 和 $z$ 的奇偶性必须相同** (即，同为奇数或同为偶数)。</p><p>题目给出的条件是 “$x, y, z$ 不全为偶数”。这意味着 $x, y, z$ 中至少有一个是奇数。<br>结合第一步的结论：</p><ul><li>如果 $x$ 是奇数，则 $z$ 也必须是奇数。</li><li>如果 $z$ 是奇数，则 $x$ 也必须是奇数。</li><li>如果 $y$ 是奇数，则条件 “不全为偶数” 已满足。</li></ul><hr><h4 id="z-1-的情况"><a href="#z-1-的情况" class="headerlink" title="$z&#x3D;1$ 的情况"></a>$z&#x3D;1$ 的情况</h4><p>当 $z&#x3D;1$ 时，原方程变为：<br>$$a^x + (a+1)^y &#x3D; 2a+1$$</p><ul><li><p><strong>情况 3.1: $x&#x3D;1$</strong><br>  如果 $x&#x3D;1$, 方程为 $a + (a+1)^y &#x3D; 2a+1$，可化为 $(a+1)^y &#x3D; a+1$。因为 $a+1 &gt; 1$，所以唯一的解是 $y&#x3D;1$。<br>  此时，我们得到一组解 $(x,y,z) &#x3D; (1,1,1)$。这组解中所有指数都为奇数，满足 “不全为偶数” 的条件。</p></li><li><p><strong>情况 3.2: $x \ge 2$</strong><br>  如果 $x \ge 2$，则 $a^x \ge a^2$。<br>  此时 $a^x + (a+1)^y &#x3D; 2a+1$ 意味着 $a^x &lt; 2a+1$。<br>  所以我们必须有 $a^2 \le a^x &lt; 2a+1$。<br>  这导出一个不等式 $a^2 &lt; 2a+1$，即 $a^2 - 2a - 1 &lt; 0$。<br>  解二次方程 $t^2 - 2t - 1 &#x3D; 0$ 的根为 $t &#x3D; 1 \pm \sqrt{2}$。所以，不等式 $a^2 - 2a - 1 &lt; 0$ 的解为 $1-\sqrt{2} &lt; a &lt; 1+\sqrt{2}$。<br>  因为 $a$ 是大于 1 的自然数，所以唯一可能的整数解是 $a&#x3D;2$。</p><p>  现在我们检验当 $a&#x3D;2$ 且 $x \ge 2$ 时的情况。方程变为 $2^x + 3^y &#x3D; 5$。</p><ul><li>若 $x&#x3D;2$，则 $4 + 3^y &#x3D; 5$，得到 $3^y &#x3D; 1$，所以 $y&#x3D;0$。但这不符合 $y$ 是自然数（正整数）的条件。</li><li>若 $x \ge 3$，则 $2^x \ge 8 &gt; 5$，此时 $2^x + 3^y &gt; 5$，方程无解。</li></ul></li><li><p><strong>情况 3.3: $y \ge 2$</strong><br>  如果 $y \ge 2$, 那么 $(a+1)^y \ge (a+1)^2 &#x3D; a^2+2a+1$。<br>  而原方程右边是 $2a+1$。显然 $a^x+(a+1)^y \ge (a+1)^y \ge a^2+2a+1 &gt; 2a+1$ (因为 $a&gt;1$ 且 $x \ge 1$)。这与方程矛盾。因此 $y$ 不可能大于等于 2。</p></li></ul><p>综合以上分析，当 $z&#x3D;1$ 时，唯一可能的自然数解就是 $x&#x3D;1, y&#x3D;1$。<br>所以，<strong>如果 $z&#x3D;1$，则必有 $x&#x3D;y&#x3D;z&#x3D;1$</strong>。</p><hr><h4 id="z-ge-2-时"><a href="#z-ge-2-时" class="headerlink" title="$z \ge 2$ 时"></a>$z \ge 2$ 时</h4><p>现在我们来证明在“$x, y, z$ 不全为偶数”的条件下，$z$ 不可能大于等于 2。<br>我们使用反证法，<strong>假设存在一组解满足 $z \ge 2$</strong>。</p><p>根据二项式定理，对于正数 $A, B$ 和整数 $k \ge 2$，有 $(A+B)^k &gt; A^k + B^k$。<br>将此应用于我们的方程：<br>$$(2a+1)^z &#x3D; (a + (a+1))^z &gt; a^z + (a+1)^z \quad (\text{因为 } z \ge 2)$$</p><p>将此不等式与原方程结合：<br>$$a^x + (a+1)^y &#x3D; (2a+1)^z &gt; a^z + (a+1)^z$$<br>$$a^x + (a+1)^y &gt; a^z + (a+1)^z$$</p><p>这个不等式意味着，<strong>必然有 $x &gt; z$ 或 $y &gt; z$</strong> (因为如果 $x \le z$ 且 $y \le z$，这个不等式不可能成立)。</p><ul><li><p><strong>情况 4.1: 假设 $y &gt; z$</strong><br>  如果 $y&gt;z$, 那么 $y \ge z+1$。<br>  所以 $(a+1)^y \ge (a+1)^{z+1}$。<br>  然而，从原方程我们知道 $(a+1)^y &lt; a^x+(a+1)^y &#x3D; (2a+1)^z$。<br>  所以，如果这种情况要成立，必须有 $(a+1)^{z+1} &lt; (2a+1)^z$。<br>  我们来检验这个不等式。可以证明，对于所有 $a \ge 2$ 和 $z \ge 2$，不等式 $(a+1)^{z+1} &lt; (2a+1)^z$ 都不成立，实际上 $(a+1)^{z+1} &gt; (2a+1)^z$。<br>  因此，$(a+1)^y &lt; (2a+1)^z$ 和 $(a+1)^y \ge (a+1)^{z+1} &gt; (2a+1)^z$ 形成矛盾。<br>  所以 <strong>$y &gt; z$ 的情况是不可能的</strong>。</p></li><li><p><strong>情况 4.2: 假设 $x &gt; z$</strong><br>  由情况 4.1 的结论，我们必须有 $y \le z$ 和 $x &gt; z$。<br>  又由第一步，我们知道 $x, z$ 奇偶性相同。所以如果 $x&gt;z$，则 $x \ge z+2$。<br>  因此，我们有 $a^x \ge a^{z+2}$。<br>  从原方程可知 $a^x &lt; (2a+1)^z$。<br>  结合起来，必须满足 $a^{z+2} &lt; (2a+1)^z$。<br>  我们来检验这个不等式：</p><ul><li><strong>对于 $a \ge 3$</strong>：可以证明，对于所有 $z \ge 2, a \ge 3$, 不等式 $a^{z+2} &lt; (2a+1)^z$ 均不成立。因此，对于 $a \ge 3$，不存在 $z \ge 2$ 的解。</li><li><strong>对于 $a&#x3D;2$</strong>：此时原方程为 $2^x+3^y&#x3D;5^z$。<br>  $z \ge 2$, $y \le z$, $x &gt; z$ 且 $x,z$ 同奇偶。<br>  在 mod 4 下分析，因为 $x&gt;z \ge 2$, 所以 $x \ge 3$，$2^x \equiv 0 \pmod 4$。<br>  方程变为 $0+3^y \equiv 5^z \pmod 4$，即 $(-1)^y \equiv 1^z \pmod 4$。这要求 <strong>$y$ 必须是偶数</strong>。<ul><li>若 $x,z$ 为偶数，则 $y$ 也为偶数。这样 $x,y,z$ 全为偶数，与题设 “不全为偶数” 矛盾。</li><li>若 $x,z$ 为奇数 ($z \ge 3, x \ge 5$)，$y$ 为偶数。在 mod 5 下检验 $2^x+3^y \equiv 0 \pmod 5$。可以证明此同余式无解。<br>  因此，对于 $a&#x3D;2$, 也不存在 $z \ge 2$ 的解。</li></ul></li></ul></li></ul><p>综合以上所有情况，$z \ge 2$ 不可能成立。</p><p>Q.E.D.</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>不切实际的梦</title>
    <link href="/2020/20200426/"/>
    <url>/2020/20200426/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><hr><span id="more"></span><p>回想起来不过是看了几篇鸡汤的无谓的冲动 -2021&#x2F;7月</p><hr><p>终究是一场梦  -2020&#x2F;8月</p><hr><p>记于庚子年庚辰月己亥日。</p><p>xxxx</p><hr>]]></content>
    
    
    
    <tags>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>《相忘于江湖》-简祯</title>
    <link href="/2020/20200111/"/>
    <url>/2020/20200111/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>​        回忆若能下酒，往事便可来一场宿醉，醒来时天依旧分明，风依旧清亮，敛裳立于光阴的两岸目送漂泊者的远去，便可知过去、现在、将来都是匆匆。<span id="more"></span>我们必定是在被选择的命运中挣扎，因此那壶酒并不总是香醇的，还有数不尽的苦涩。生活中穿插着大大小小的不情不愿，有时悲从中来，也会鞠一把泪，更多的时候是迷茫，是惘然，是不知所措。这是必然明白生命本身的残忍，正如果实的故事——它不容我们不献出积累的馨芳，交出受过光热的每一层颜色，点点沥尽最难看的酸怆。不过也是因为这样，活着才有更多的乐趣，终究我们是可以不断抗争的八九点种的太阳，可以从超越了晨昏的日界线后重新出发，让所有流动的血和热情来坚守灵魂的高贵，甚至去扼住命运的咽喉。</p>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>对数平均的一个上界</title>
    <link href="/2019/20190831/"/>
    <url>/2019/20190831/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>$\frac{x+y}{2}+\sqrt{xy}-\frac{x+\sqrt{xy}+y}{3}&gt;\frac{x-y}{lnx-lny}$</p><p>巧妙的将算术平均、几何平均、海伦平均、对数平均结合在了一起。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>条条大路通罗马，巧比大小多方法</title>
    <link href="/2019/20190815/"/>
    <url>/2019/20190815/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（2014·湖北·22）<br>$\pi$为圆周率，e&#x3D;2.71828…为自然对数的底数。<br>（1）求函数$f(x)&#x3D;\frac{lnx}{x}$的单调区间<br>（2）将$e^3,3^e,3^{\pi},{\pi}^3,e^{\pi},{\pi}^e$这六个数按从小到大的顺序排列，并证明你的结论。</p><span id="more"></span><p>解：（3）本题的关键是比较$e^3$与${\pi}^e$、$e^{\pi}$与${\pi}^3$</p><p>$\because  $9&gt;${\pi}e$</p><p>$\therefore$ 只需证 ${\pi}^e$&gt;$e^3$，剩下的不言自明。</p><p>这道题看似是“空中楼阁”，但好心的命题者设计了梯子（第一问） 。</p><p>解法一：由（1）知，</p><p>$f(x)&lt;f(e)&#x3D;\frac{1}{e}$</p><p>$f(\frac{e^2}{\pi})&#x3D;\frac{ln{\frac{e^2}{\pi}}}{\frac{e^2}{\pi}}&lt;\frac{1}{e}$</p><p>即$ln{\pi}&gt;2-\frac{e}{\pi}$</p><p>$\therefore eln\pi&gt;e(2-\frac{e}{\pi})&gt;2.7(2-\frac{2.7}{3.14}) &gt;3$</p><p>$ln{\pi^e}&gt;3&#x3D;ln{e^3} $</p><p>$\Rightarrow e^3 &lt;\pi ^e$</p><p>事实上，这梯子不太容易攀爬，我们不妨舍弃该梯子，换一条道路直捣黄龙。<br>解法二：$e^3$&lt;$\pi^e$<br>$\Leftrightarrow {\pi}^{\frac{e}{3}}&gt;e$<br>而$\frac{e}{3}&gt;\frac{9}{10}$<br>$\therefore $只需证$ \pi^{\frac{9}{10}}&gt;e$<br>而 ${1+\frac{1}{x}}^{x+1}\geq e$<br>$\therefore $只需证 $\frac{\pi}{e}&gt;1+\frac{1}{8}$<br>即证$8\pi&gt;9e$</p><p>而8x3.14&gt;9x2.72</p><p>证毕。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>巧妙曲线系，减少计算量</title>
    <link href="/2019/20190810/"/>
    <url>/2019/20190810/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>（2014·全国·21·2） </p><p>已知抛物线$C:y^2$&#x3D;$4x$的焦点为F，过点F的直线$l$与C相交于A、B两点，若AB垂直平分线$l’$与C相交于M，N两点，且A,M,B,N四点在同一圆上，求$l$的方程。 </p><span id="more"></span><p>解：</p><p>设AB：$x$&#x3D;$my+1$ </p><p>则MN：$x-(2m^2+1$)&#x3D;-$\frac{1}{m}*(y-2m)$ </p><p>过A,B,M,N四点的曲线系方程为：</p><p> （$x-my-1$）$[x-(2m^2+1)+\frac{1}{m}*(y-2m)]$+$\lambda $（$y^2-4x$）&#x3D;0 </p><p>$\because $A,B,M,N 四点共圆 $\therefore -m+\frac{1}{m}$&#x3D;0 m&#x3D;$\pm 1$ </p><p>$\therefore $直线$l$:y&#x3D;$\pm(x-1)$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>洞察本质，避免分类</title>
    <link href="/2019/20190809/"/>
    <url>/2019/20190809/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>( 2015·浙江14） </p><p>若实数 $x, y$ 满足 $x^{2}+y^{2} \leq 1$, 则 $|2 x+y-2|+|6-x-3 y|$ 的最小值为___。</p><span id="more"></span><p>解析:<br>$|2x+y-2|+|6-x-3 y|_{min}$<br>&#x3D;min{max{$|x-2y+4|,|3x+4y-8|$}}<br>易知 $x-2 y+4&gt;0,\  8-3 x+4 y&gt;0$<br>$\therefore$原式&#x3D;min{max{$x-2 y+4,8-3 x-4 y$}}<br>而$8-3 x-4 y \geq 3$<br>$\therefore$ 原式 $\geq 3$<br>当 $x&#x3D;\frac{3}{5}, y&#x3D;\frac{4}{5}$ 时取得等号。</p><p>故最小值为3。</p><p>事实上，这道题不能使用绝对值不等式来做。</p><p>所以若不用上述方法，只能拆分绝对值来讨论，过程繁琐复杂。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>一个几何不等式</title>
    <link href="/2019/20190729/"/>
    <url>/2019/20190729/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>$R-2r{\ge}\frac{1}{8R}\sum(a-b)^2$</p><span id="more"></span>证明：<p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}\frac{1}{8R^2}\sum(a-b)^2$</p><p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}\sum sin^2{A} -\sum sin{A}sin{B}$</p><p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}2+2cos{A}cos{B}cos{C} -\sum sin{A}sin{B}$</p><p>$\Leftrightarrow 1-2\frac{r}{R}{\ge}2+2*\frac{s^2-(2R+r)^2}{4R^2} -\frac{s^2+4Rr+r^2}{4R^2}$</p><p>$\Leftrightarrow 4R^2+4Rr+3r^2{\ge}s^2 $  (此即Gerretsen’s Inequality)</p><p>这条不等式强化了欧拉不等式$(R\geq 2r)$，还指明了等号成立条件。唯一不足是不太具有对称性，还可以变形为:$\frac{R}{2r}\geq \frac{abc+a^3+b^3+c^3}{4abc}$</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LTE引理[中文版]</title>
    <link href="/2019/20190727/"/>
    <url>/2019/20190727/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Lifting The Exponent Lemma是求解指数丢番图方程(不定方程)的有效方法。它在奥林匹克民间传说中非常有名（例如，参见[3]），尽管其起源很难追溯。在数学上，它是数论中经典Hensel引理（见[2]）的近亲（在证明的陈述和观点中）。在本文中，我们分析了这种方法并介绍了它的一些应用。<span id="more"></span><br>在涉及指数方程的许多问题中，我们可以使用Lifting The ExponentLemma（这是一个长名称，我们称之为LTE！）,特别是我们可以找到某些质因子的时候。有时LTE引理甚至能秒杀一道题。这个引理显示了如何找到素数p的最大幂 —— 通常≥3 —— a^n ±b^n型——本文中定理和引理的证明没有任何复杂难理解之处，所有这些都使用了初等数学。理解定理的用法及其含义对于你来说比记住它的详细证明更重要。 </p><p>….</p><p>英文版来自<a href="https://artofproblemsolving.com/community/c6h393335p2186092">AOPS社区</a> </p><p>本人精心翻译</p><p><a href="https://www.bilibili.com/read/cv6779585">B站专栏链接</a></p><p>完整版见下：<div class="row">    <embed src="/pdf/LTE.pdf" width="100%" height="550" type="application/pdf"></div></p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
      <tag>翻译</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2019IMO P4</title>
    <link href="/2019/20190718/"/>
    <url>/2019/20190718/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p><img src="https://s2.ax1x.com/2019/07/19/Zj2TW4.jpg" alt="1"><span id="more"></span></p><p><img src="https://s2.ax1x.com/2019/07/19/Zj2LO1.jpg" alt="2"></p><p><img src="https://s2.ax1x.com/2019/07/19/Zj2bl9.jpg" alt="3"></p><p><img src="https://s2.ax1x.com/2019/07/19/Zj2jw6.jpg" alt="4"></p><p><img src="https://s2.ax1x.com/2019/07/19/Zj2Xex.jpg" alt="5"></p><p><img src="https://s2.ax1x.com/2019/07/19/Zj2vTK.jpg" alt="6"></p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>高中三年校内图书馆借阅记录</title>
    <link href="/2019/20190624/"/>
    <url>/2019/20190624/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>共借阅121本，图书馆记录了87本（图书馆因饭卡原因部分借阅记录缺失），含重复借阅书籍。</p><span id="more"></span><p><img src="/images/ySqi1f.md.png" alt="ySqi1f.md.png"></p><p>具体书目：</p><table><thead><tr><th>序号</th><th>索书号</th><th>题名</th><th>借出日期</th><th>还书日期</th></tr></thead><tbody><tr><td>1</td><td>C53&#x2F;40</td><td>叩响命运的门：人生必读的一百零二篇人文素养经典</td><td>2020-1-16</td><td>2020-5-14</td></tr><tr><td>2</td><td>F552&#x2F;2</td><td>明代的漕运</td><td>2020-1-15</td><td>2020-5-14</td></tr><tr><td>3</td><td>F06&#x2F;13</td><td>地球人不靠谱</td><td>2020-1-15</td><td>2020-5-14</td></tr><tr><td>4</td><td>I267&#x2F;1649</td><td>下午茶</td><td>2020-1-14</td><td>2020-5-29</td></tr><tr><td>5</td><td>I267&#x2F;1642</td><td>好一座浮岛</td><td>2020-1-9</td><td>2020-1-13</td></tr><tr><td>6</td><td>I267&#x2F;1924</td><td>水问</td><td>2020-1-5</td><td>2020-1-14</td></tr><tr><td>7</td><td>01&#x2F;82</td><td>怎样解题：数学思维的新方法</td><td>2019-12-30</td><td>2020-1-5</td></tr><tr><td>8</td><td>I247.5&#x2F;1989</td><td>丑行或浪漫</td><td>2019-12-24</td><td>2019-12-30</td></tr><tr><td>9</td><td>K207&#x2F;32</td><td>中国大历史</td><td>2019-12-24</td><td>2020-1-14</td></tr><tr><td>10</td><td>K825.6&#x2F;219</td><td>三毛：我需要最狂的风，和最静的海</td><td>2019-12-16</td><td>2019-12-24</td></tr><tr><td>11</td><td>B221&#x2F;44</td><td>卜辞看人生：易经</td><td>2019-12-10</td><td>2019-12-24</td></tr><tr><td>12</td><td>I313.4&#x2F;227</td><td>黎明之街</td><td>2019-12-3</td><td>2019-12-4</td></tr><tr><td>13</td><td>K826.1&#x2F;43</td><td>吴文俊与中国数学</td><td>2019-12-3</td><td>2019-12-10</td></tr><tr><td>14</td><td>F815&#x2F;1</td><td>另一片海：阿拉伯之春、欧债风暴与新自由主义之殇</td><td>2019-11-25</td><td>2019-12-16</td></tr><tr><td>15</td><td>F815&#x2F;1</td><td>另一片海：阿拉伯之春、欧债风暴与新自由主义之殇</td><td>2019-11-25</td><td>2019-11-25</td></tr><tr><td>16</td><td>C912&#x2F;88</td><td>明亮的对话：公共说理十八讲</td><td>2019-11-21</td><td>2019-12-16</td></tr><tr><td>17</td><td>0178&#x2F;2</td><td>初等不等式的证明方法</td><td>2019-11-18</td><td>2019-11-25</td></tr><tr><td>18</td><td>B821&#x2F;193</td><td>段子：人生何求：一个江湖老总的人生私藏</td><td>2019-11-18</td><td>2019-11-20</td></tr><tr><td>19</td><td>B97&#x2F;19</td><td>眼泪与圣徒</td><td>2019-11-5</td><td>2019-11-11</td></tr><tr><td>20</td><td>D931.3&#x2F;1</td><td>与手枪的不幸相遇：日本司法物语</td><td>2019-11-4</td><td>2019-11-11</td></tr><tr><td>21</td><td>D669&#x2F;55</td><td>从有意义到有意思：《新周刊》生活观</td><td>2019-11-4</td><td>2019-11-20</td></tr><tr><td>22</td><td>F014.3&#x2F;5</td><td>合适：从升学择校、相亲配对、牌照拍卖了解新兴实用经济学</td><td>2019-10-23</td><td>2019-11-4</td></tr><tr><td>23</td><td>I313.45&#x2F;73</td><td>一个人的好天气</td><td>2019-10-23</td><td>2019-11-4</td></tr><tr><td>24</td><td>B97&#x2F;15</td><td>在世界的爱心之中：德兰修女的感想、故事与祷辞</td><td>2019-10-10</td><td>2019-10-15</td></tr><tr><td>25</td><td>B94&#x2F;11</td><td>佛教三经·圆觉经：最新图文版</td><td>2019-10-9</td><td>2019-10-21</td></tr><tr><td>26</td><td>J228&#x2F;55</td><td>有兽焉</td><td>2019-9-28</td><td>2019-10-8</td></tr><tr><td>27</td><td>I267&#x2F;1682</td><td>历史，从未这样</td><td>2019-9-28</td><td>2019-10-7</td></tr><tr><td>28</td><td>I253&#x2F;98&#x2F;3</td><td>抗日战争·第三卷：1942年6月-1945年9月</td><td>2019-9-26</td><td>2019-9-28</td></tr><tr><td>29</td><td>I267&#x2F;1283</td><td>在自己心中盖一座花园</td><td>2019-9-25</td><td>2019-10-7</td></tr><tr><td>30</td><td>B21&#x2F;3</td><td>风流去</td><td>2019-9-25</td><td>2019-10-7</td></tr><tr><td>31</td><td>I313.4&#x2F;71&#x2F;[2]</td><td>嫌疑人X的献身</td><td>2019-9-24</td><td>2019-9-25</td></tr><tr><td>32</td><td>J228.2&#x2F;139</td><td>流学的一年</td><td>2019-9-24</td><td>2019-9-25</td></tr><tr><td>33</td><td>I253&#x2F;65</td><td>朝鲜战争</td><td>2019-9-19</td><td>2019-9-24</td></tr><tr><td>34</td><td>D50&#x2F;3</td><td>世界趋势2050</td><td>2019-9-19</td><td>2019-9-28</td></tr><tr><td>35</td><td>D097.12&#x2F;2</td><td>自由的基因：美国自由主义的历史变迁</td><td>2019-9-19</td><td>2019-9-24</td></tr><tr><td>36</td><td>01&#x2F;41</td><td>500个世界著名数学征解问题</td><td>2019-9-19</td><td>2019-10-7</td></tr><tr><td>37</td><td>I253&#x2F;65</td><td>朝鲜战争</td><td>2019-9-19</td><td>2019-9-19</td></tr><tr><td>38</td><td>01&#x2F;41</td><td>500个世界著名数学征解问题</td><td>2019-9-17</td><td>2019-9-19</td></tr><tr><td>39</td><td>K825.2&#x2F;39</td><td>历史可以很精彩之战将传</td><td>2019-9-17</td><td>2019-9-19</td></tr><tr><td>40</td><td>TP18&#x2F;9</td><td>AI：人工智能的本质与未来</td><td>2019-9-16</td><td>2019-9-17</td></tr><tr><td>41</td><td>I565.4&#x2F;60</td><td>卡门</td><td>2019-9-16</td><td>2019-9-19</td></tr><tr><td>42</td><td>D60&#x2F;9</td><td>中国人，你要自信</td><td>2019-9-11</td><td>2019-9-16</td></tr><tr><td>43</td><td>D50&#x2F;3</td><td>世界趋势2050</td><td>2019-9-11</td><td>2019-9-19</td></tr><tr><td>44</td><td>D097.12&#x2F;2</td><td>自由的基因：美国自由主义的历史变迁</td><td>2019-9-11</td><td>2019-9-19</td></tr><tr><td>45</td><td>TS971&#x2F;70</td><td>茶与美</td><td>2019-9-11</td><td>2019-9-11</td></tr><tr><td>46</td><td>TS976&#x2F;36</td><td>万物皆有理：你很熟悉但未必明白的那些事儿</td><td>2019-9-11</td><td>2019-9-16</td></tr><tr><td>47</td><td>I313.4&#x2F;160</td><td>大雪中的山庄</td><td>2019-9-10</td><td>2019-9-11</td></tr><tr><td>48</td><td>0189&#x2F;2</td><td>庞加莱猜想：追寻宇宙的形状</td><td>2019-9-10</td><td>2019-9-11</td></tr><tr><td>49</td><td>G236&#x2F;23</td><td>国家与市场</td><td>2019-9-9</td><td>2019-9-11</td></tr><tr><td>50</td><td>B97&#x2F;19</td><td>眼泪与圣徒</td><td>2019-9-9</td><td>2019-9-10</td></tr><tr><td>51</td><td>J228&#x2F;56</td><td>破耳兔：你不完美的样子也很美</td><td>2019-9-5</td><td>2019-9-9</td></tr><tr><td>52</td><td>C913&#x2F;116</td><td>爱情数学：如何用数学找到真爱?</td><td>2019-9-5</td><td>2019-9-9</td></tr><tr><td>53</td><td>B0&#x2F;6</td><td>大问题：简明哲学导论</td><td>2019-9-5</td><td>2019-9-10</td></tr><tr><td>54</td><td>TP311.5&#x2F;2</td><td>Python密码学编程：Hacking Secret Ciphers with Python</td><td>2018-11-28</td><td>2018-12-26</td></tr><tr><td>55</td><td>I561.4&#x2F;122</td><td>脉搏</td><td>2018-11-28</td><td>2018-11-29</td></tr><tr><td>56</td><td>TP311.5&#x2F;2</td><td>Python密码学编程：Hacking Secret Ciphers with Python</td><td>2018-11-28</td><td>2018-11-28</td></tr><tr><td>57</td><td>I561.4&#x2F;122</td><td>脉搏</td><td>2018-11-28</td><td>2018-11-28</td></tr><tr><td>58</td><td>TP311.5&#x2F;2</td><td>Python密码学编程：Hacking Secret Ciphers with Python</td><td>2018-11-28</td><td>2018-11-28</td></tr><tr><td>59</td><td>I561.4&#x2F;119</td><td>蝴蝶梦</td><td>2018-11-22</td><td>2018-11-28</td></tr><tr><td>60</td><td>I242.43&#x2F;5</td><td>杨家将演义</td><td>2018-11-22</td><td>2018-11-22</td></tr><tr><td>61</td><td>01&#x2F;28</td><td>二战时期密码决战中的数学故事</td><td>2018-11-22</td><td>2018-11-28</td></tr><tr><td>62</td><td>I313.4&#x2F;30</td><td>脑人</td><td>2018-11-15</td><td>2018-11-22</td></tr><tr><td>63</td><td>TP311.5&#x2F;2</td><td>Python密码学编程：Hacking Secret Ciphers with Python</td><td>2018-11-15</td><td>2018-11-28</td></tr><tr><td>64</td><td>I313.4&#x2F;30</td><td>脑人</td><td>2018-11-15</td><td>2018-11-15</td></tr><tr><td>65</td><td>TP311.5&#x2F;2</td><td>Python密码学编程：Hacking Secret Ciphers with Python</td><td>2018-11-15</td><td>2018-11-15</td></tr><tr><td>66</td><td>I247.5&#x2F;1894</td><td>悬案组</td><td>2018-11-1</td><td>2018-11-1</td></tr><tr><td>67</td><td>01&#x2F;2</td><td>新编中学数学手册</td><td>2018-11-1</td><td>2018-11-1</td></tr><tr><td>68</td><td>I247.5&#x2F;1894</td><td>悬案组</td><td>2018-10-31</td><td>2018-11-1</td></tr><tr><td>69</td><td>01&#x2F;2</td><td>新编中学数学手册</td><td>2018-10-31</td><td>2018-11-1</td></tr><tr><td>70</td><td>01&#x2F;2</td><td>新编中学数学手册</td><td>2018-10-25</td><td>2018-10-31</td></tr><tr><td>71</td><td>0121&#x2F;1</td><td>速算速成</td><td>2018-10-25</td><td>2018-10-31</td></tr><tr><td>72</td><td>I247.7&#x2F;357</td><td>失重</td><td>2018-10-24</td><td>2018-10-25</td></tr><tr><td>73</td><td>I247.5&#x2F;1846</td><td>孽子</td><td>2018-10-18</td><td>2018-10-24</td></tr><tr><td>74</td><td>I534.4&#x2F;1</td><td>战友同志</td><td>2018-10-11</td><td>2018-10-18</td></tr><tr><td>75</td><td>I562.4&#x2F;7</td><td>尤利西斯</td><td>2018-10-11</td><td>2018-10-18</td></tr><tr><td>76</td><td>I516.4&#x2F;14</td><td>第十三天</td><td>2018-10-8</td><td>2018-10-11</td></tr><tr><td>77</td><td>I247.5&#x2F;1788</td><td>曲终人在</td><td>2018-9-27</td><td>2018-10-8</td></tr><tr><td>78</td><td>I247.5&#x2F;1694</td><td>英雄时代</td><td>2018-9-27</td><td>2018-10-8</td></tr><tr><td>79</td><td>I247.5&#x2F;1788</td><td>曲终人在</td><td>2018-9-27</td><td>2018-9-27</td></tr><tr><td>80</td><td>I247.5&#x2F;1694</td><td>英雄时代</td><td>2018-9-27</td><td>2018-9-27</td></tr><tr><td>81</td><td>011-53&#x2F;1</td><td>数学珍宝：历史文献精选</td><td>2018-9-26</td><td>2018-9-27</td></tr><tr><td>82</td><td>I247.4&#x2F;16</td><td>金瓶梅·西门公子：故事</td><td>2018-9-26</td><td>2018-9-27</td></tr><tr><td>83</td><td>I712.4&#x2F;243</td><td>沙丘</td><td>2018-9-20</td><td>2018-9-26</td></tr><tr><td>84</td><td>01&#x2F;2</td><td>新编中学数学手册</td><td>2018-6-21</td><td>2018-9-3</td></tr><tr><td>85</td><td>015&#x2F;5</td><td>高等代数</td><td>2018-4-25</td><td>2018-5-10</td></tr><tr><td>86</td><td>I247.5&#x2F;1850</td><td>出家</td><td>2018-4-24</td><td>2018-4-25</td></tr><tr><td>87</td><td>I247.5&#x2F;1442</td><td>纪委书记</td><td>2018-4-19</td><td>2018-4-25</td></tr></tbody></table>]]></content>
    
    
    
    <tags>
      
      <tag>阅读</tag>
      
      <tag>生活</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>丑奴儿·书博山道中壁</title>
    <link href="/2019/20190430/"/>
    <url>/2019/20190430/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>In my younger days, I had tasted only  gladness,</p><p>But loved to mount the top floor,</p><p>But loved to mount the top floor,</p><p>To write a song pretending sadness,</p><span id="more"></span><p>And now I’ve tasted Sorrow’s flavors, bitter  and sour,</p><p>And can’t find a word,</p><p>And can’t find a word,</p><p>But merely say，“what a golden autumn hour！”</p><p>——翻译来自林语堂</p><p>少年不识愁滋味，爱上层楼。爱上层楼。为赋新词强说愁。<br>而今识尽愁滋味，欲说还休。欲说还休。却道天凉好个秋。</p><p><img src="/photos/photo-1478993161925-a849b2c448d1.jpg" alt="prove1"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>一个美妙的不等式</title>
    <link href="/2019/20190323/"/>
    <url>/2019/20190323/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>最近在Aops论坛上发现了这个美妙的不等式，由Pirkuliyev Rovsen提出并被我加强。</p><p>$\ln \sqrt{\frac{n+1}{2}}&lt;\frac{1}{5}+\frac{1}{7}+\ldots+\frac{1}{2 n+3}&lt;\ln \sqrt{\frac{n+2}{2}}$</p><span id="more"></span><p>证明：</p><p>不等式右边：</p><p>$\frac{1}{5}+\frac{1}{7}+\ldots+\frac{1}{2 n+3}&lt;\ln \sqrt{\frac{n+2}{2}}$<br>$\Leftarrow \frac{1}{2 n+3}&lt;\ln \sqrt{\frac{n+2}{2}}-\ln \sqrt{\frac{n+1}{2}}$    </p><p>这个等价于对数平均不等式。</p><p>不等式左边(来自Aniv的证明)：</p><p>即证$\ln \sqrt{\frac{n+2}{2}}&lt;\frac{1}{2 n+5}+\ln \sqrt{\frac{n+1}{2}}&lt;\frac{1}{5}+\frac{1}{7}+\ldots+\frac{1}{2 n+3}+\frac{1}{2 n+5}$<br>$\ln \sqrt{\frac{n+2}{2}}&lt;\frac{1}{2 n+5}+\ln \sqrt{\frac{n+1}{2}} \Longleftrightarrow \ln \left(\frac{n+1}{n+2}\right)+\frac{1}{\left(n+\frac{5}{2}\right)}&gt;0$<br>设 $f(x)&#x3D;\ln (x+1)-\ln (x+2)+\frac{1}{x+\frac{5}{2}}$<br>则 $f^{\prime}(x)&#x3D;\frac{1}{(x+1)(x+2)}-\frac{1}{\left(x+\frac{5}{2}\right)^{2}}&gt;0, \forall x \geq 1$<br>所以 $f(x) \geq f(1)&gt;0, \forall x \geq 1$<br>所以 $\ln \sqrt{\frac{n+2}{2}}&lt;\frac{1}{2 n+5}+\ln \sqrt{\frac{n+1}{2}}, \forall n \geq 1$<br>证毕.</p><p>这个不等式精度很高，ln2的值只相差了大约千分之一。</p>]]></content>
    
    
    
    <tags>
      
      <tag>数学</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2019/hello-world/"/>
    <url>/2019/hello-world/</url>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-tag-hint@0.3.1/dist/hexo-tag-hint.min.css"><link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>测试mathjax:$x^2$ </p><p>$￥<br>$$<br>\frac{sinx}{x}<br>$$<br>$</p><p>测试hexo-pdf</p><div class="row">    <embed src="/pdf/1371-EMVT.pdf" width="100%" height="550" type="application/pdf"></div><p>测试图片</p><figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs scss">！<span class="hljs-selector-attr">[]</span>(/images/<span class="hljs-number">1</span>.jpg)<br></code></pre></td></tr></table></figure><p><img src="/images/1.jpg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
